#!/bin/bash
# SPDX-FileCopyrightText: Copyright (c) 2022-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Error out if a command fails
set -e

#===============================================================================
# Default values for environment variables.
#===============================================================================

init_globals() {
    if [ "$0" != "/bin/bash" ] && [ "$0" != "bash" ]; then
        SCRIPT_DIR=$(dirname "$(readlink -f "$0")")
        export RUN_SCRIPT_FILE="$(readlink -f "$0")"
    else
        export RUN_SCRIPT_FILE="$(readlink -f "${BASH_SOURCE[0]}")"
    fi

    export HOLOHUB_ROOT=$(dirname "${RUN_SCRIPT_FILE}")

    HOLOSCAN_PY_EXE=${HOLOSCAN_PY_EXE:-"python3"}
    export HOLOSCAN_PY_EXE
    HOLOSCAN_DOCKER_EXE=${HOLOSCAN_DOCKER_EXE:-"docker"}
    export HOLOSCAN_DOCKER_EXE

    HOLOSCAN_SDK_VERSION="sdk-latest"
    export HOLOSCAN_SDK_VERSION
    HOLOHUB_CONTAINER_BASE_NAME=holohub
    export HOLOHUB_CONTAINER_BASE_NAME

    DO_DRY_RUN="false" # print commands but do not execute them. Used by run_command
}

################################################################################
# Utility functions
################################################################################

#######################################
# Get list of available commands from a given input file.
#
# Available commands and command summary are extracted by checking a pattern
# "_desc() { c_echo '".
# Section title is extracted by checking a pattern "# Section: ".
# This command is used for listing available commands in CLI.
#
# e.g.)
#   "# Section: String/IO functions"
#     => "# String/IO functions"
#   "to_lower_desc() { c_echo 'Convert to lower case"
#     => "to_lower ----------------- Convert to lower case"
#
# Arguments:
#   $1 - input file that defines commands
# Returns:
#   Print list of available commands from $1
#######################################
get_list_of_available_commands() {
    local mode="color"
    if [ "${1:-}" = "color" ]; then
        mode="color"
        shift
    elif [ "${1:-}" = "nocolor" ]; then
        mode="nocolor"
        shift
    fi

    local file_name="$1"
    if [ ! -e "$1" ]; then
        echo "$1 doesn't exist!"
    fi

    local line_str='--------------------------------'
    local IFS= cmd_lines="$(IFS= cat "$1" | grep -E -e "^(([[:alpha:]_[:digit:]]+)_desc\(\)|# Section: )" | sed "s/_desc() *{ *c_echo '/ : /")"
    local line
    while IFS= read -r line; do
        local cmd=$(echo "$line" | cut -d":" -f1)
        local desc=$(echo "$line" | cut -d":" -f2-)
        if [ "$cmd" = "# Section" ]; then
            c_echo ${mode} B "${desc}"
        else
            # there is no substring operation in 'sh' so use 'cut'
            local dash_line="$(echo "${line_str}" | cut -c ${#cmd}-)" #  = "${line_str:${#cmd}}"
            c_echo ${mode} Y "   ${cmd}" w " ${dash_line} ${desc}"
        fi
        # use <<EOF, not '<<<"$cmd_lines"' to be executable in sh
    done <<EOF
$cmd_lines
EOF
}

my_cat_prefix() {
    local IFS
    local prefix="$1"
    local line
    while IFS= read -r line; do
        echo "${prefix}${line}" # -e option doesn't work in 'sh' so disallow escaped characters
    done <&0
}

c_str() {
    local old_color=39
    local old_attr=0
    local color=39
    local attr=0
    local text=""
    local mode="color"
    if [ "${1:-}" = "color" ]; then
        mode="color"
        shift
    elif [ "${1:-}" = "nocolor" ]; then
        mode="nocolor"
        shift
    fi

    for i in "$@"; do
        case "$i" in
        r | R)
            color=31
            ;;
        g | G)
            color=32
            ;;
        y | Y)
            color=33
            ;;
        b | B)
            color=34
            ;;
        p | P)
            color=35
            ;;
        c | C)
            color=36
            ;;
        w | W)
            color=37
            ;;

        z | Z)
            color=0
            ;;
        esac
        case "$i" in
        l | L | R | G | Y | B | P | C | W)
            attr=1
            ;;
        n | N | r | g | y | b | p | c | w)
            attr=0
            ;;
        z | Z)
            attr=0
            ;;
        *)
            text="${text}$i"
            ;;
        esac
        if [ "${mode}" = "color" ]; then
            if [ ${old_color} -ne ${color} ] || [ ${old_attr} -ne ${attr} ]; then
                text="${text}\033[${attr};${color}m"
                old_color=$color
                old_attr=$attr
            fi
        fi
    done
    /bin/echo -en "$text"
}

c_echo() {
    # Select color/nocolor based on the first argument
    local mode="color"
    if [ "${1:-}" = "color" ]; then
        mode="color"
        shift
    elif [ "${1:-}" = "nocolor" ]; then
        mode="nocolor"
        shift
    else
        if [ ! -t 1 ]; then
            mode="nocolor"
        fi
    fi

    local old_opt="$(shopt -op xtrace)" # save old xtrace option
    set +x                              # unset xtrace

    if [ "${mode}" = "color" ]; then
        local text="$(c_str color "$@")"
        /bin/echo -e "$text\033[0m"
    else
        local text="$(c_str nocolor "$@")"
        /bin/echo -e "$text"
    fi
    eval "${old_opt}" # restore old xtrace option
}

echo_err() {
    echo >&2 "$@"
}

c_echo_err() {
    c_echo >&2 "$@"
}

fatal() {
    if [ -n "$*" ]; then
        c_echo_err R "$(date -u '+%Y-%m-%d %H:%M:%S') [FATAL] " Z "$@"
    fi
    if [ -n "${SCRIPT_DIR}" ]; then
        exit 1
    else
        kill -INT $$ # kill the current process instead of exit in shell environment.
    fi
}

run_command() {
    local status=0
    local cmd="$*"

    if [ "${DO_DRY_RUN}" != "true" ]; then
        c_echo_err B "$(date -u '+%Y-%m-%d %H:%M:%S') " W "\$ " G "${cmd}"
    else
        c_echo_err B "$(date -u '+%Y-%m-%d %H:%M:%S') " C "[dryrun] " W "\$ " G "${cmd}"
    fi

    [ "$(echo -n "$@")" = "" ] && return 1 # return 1 if there is no command available

    if [ "${DO_DRY_RUN}" != "true" ]; then
        "$@"
        status=$?
    fi

    return $status
}

run_docker() {
    $(./run docker_cmd "-u $(id -u):$(id -g)") "$@"
}

get_group_id() {
    local group=$1
    cat /etc/group | grep $group | cut -d: -f3
}

get_ucx_options() {
    local prefix="${1:- }"
    local postfix="${2:- }"

    local ucx_opt="${prefix}--ipc=host${postfix}"
    ucx_opt+="${prefix}--cap-add=CAP_SYS_PTRACE${postfix}"
    ucx_opt+="${prefix}--ulimit=memlock=-1${postfix}"
    ucx_opt+="${prefix}--ulimit=stack=67108864" # last item doesn't need postfix

    echo "${ucx_opt}"
}

get_conditional_options() {
    local use_tini=$1
    local persistent=$2
    local prefix="${3:- }"
    local postfix="${4:- }"
    local conditional_opt=""


    if [ -e /usr/lib/libvideomasterhd.so ]; then
        conditional_opt+="${prefix}--volume=/usr/lib/libvideomasterhd.so:/usr/lib/libvideomasterhd.so${postfix}"
    fi
    if [ -d /opt/deltacast/videomaster/Include ]; then
        conditional_opt+="${prefix}--volume=/opt/deltacast/videomaster/Include:/usr/local/deltacast/Include${postfix}"
    fi
    if [ -d /opt/yuan/qcap/include ]; then
        conditional_opt+="${prefix}--volume=/opt/yuan/qcap/include:/opt/yuan/qcap/include${postfix}"
    fi
    if [ -d /opt/yuan/qcap/lib ]; then
        conditional_opt+="${prefix}--volume=/opt/yuan/qcap/lib:/opt/yuan/qcap/lib${postfix}"
    fi

    # add tegra directory
    if [ -d /usr/lib/aarch64-linux-gnu/tegra ]; then
        conditional_opt+="${prefix}--volume=/usr/lib/aarch64-linux-gnu/tegra:/usr/lib/aarch64-linux-gnu/tegra${postfix}"
    fi

    # add user in container to group video to access video devices, e.g. /dev/video0
    video_id=$(get_group_id video)
    if [ -n "$video_id" ]; then
        conditional_opt+="${prefix}--group-add=${video_id}${postfix}"
    fi
    render_id=$(get_group_id render)
    if [ -n "$render_id" ]; then
        conditional_opt+="${prefix}--group-add=${render_id}${postfix}"
    fi

    # Add docker group to enable DooD
    conditional_opt+="${prefix}--group-add=$(get_group_id docker)${postfix}"

    if [[ $nsys_profile == true ]]; then
        # enable usage of the the perf_event_open call, see
        # https://docs.nvidia.com/nsight-systems/UserGuide/index.html#docker-profiling
        conditional_opt+="${prefix}--cap-add=SYS_ADMIN${postfix}"
    fi

    # Allow X11 forwarding over SSH
    if [[ $ssh_x11 -gt 0 ]]; then
        XAUTH=/tmp/.docker.xauth
        xauth nlist $DISPLAY | sed -e 's/^..../ffff/' | xauth -f $XAUTH nmerge -
        chmod 777 $XAUTH

        conditional_opt+="${prefix}--volume=$XAUTH:$XAUTH${postfix}"
        conditional_opt+="${prefix}-e XAUTHORITY=$XAUTH${postfix}"
    fi

    # Use tini
    if [[ $use_tini -gt 0 ]]; then
        conditional_opt+="${prefix}--init${postfix}"
    fi

    # Persistent container
    if [[ $persistent == 0 ]]; then
        conditional_opt+="${prefix}--rm${postfix}"
    fi

    # if running on AGX Orin (iGPU only) or on IGX Orin in iGPU mode
    if lsmod | grep -q nvgpu ; then
        if [ -e /dev/nvgpu/igpu0/nvsched ]; then
            conditional_opt+="${prefix}--device=/dev/nvgpu/igpu0/nvsched${postfix}"
        fi
        if [ -e /dev/nvhost-ctrl-nvdec ]; then
            conditional_opt+="${prefix}--device=/dev/nvhost-ctrl-nvdec${postfix}"
        fi
        if [ -e /dev/nvhost-ctxsw-gpu ]; then
            conditional_opt+="${prefix}--device=/dev/nvhost-ctxsw-gpu${postfix}"
        fi
        if [ -e /dev/nvhost-nvsched-gpu ]; then
            conditional_opt+="${prefix}--device=/dev/nvhost-nvsched-gpu${postfix}"
        fi
        if [ -e /dev/nvhost-sched-gpu ]; then
            conditional_opt+="${prefix}--device=/dev/nvhost-sched-gpu${postfix}"
        fi
        if [ -e /dev/nvidia0 ]; then
            conditional_opt+="${prefix}--device=/dev/nvidia0${postfix}"
        fi
        if [ -e /dev/nvidia-modeset ]; then
            conditional_opt+="${prefix}--device=/dev/nvidia-modeset${postfix}"
        fi
    fi

    # Define path for cupy's kernel cache, needed since $HOME does
    # not exist when running with `-u id:group`
    conditional_opt+="${prefix}-e CUPY_CACHE_DIR=/workspace/holohub/.cupy/kernel_cache" # last item doesn't need postfix

    echo "${conditional_opt}"
}

get_mount_device_options() {
    local prefix="${1:- }"
    local postfix="${2:- }"

    local mount_device_opt=""
    for video_dev in $(find /dev -regex '/dev/video[0-9]+'); do
        mount_device_opt+="${prefix}--device=$video_dev${postfix}"
    done

    for capture_dev in $(find /dev -regex '/dev/capture-vi-channel[0-9]+'); do
        mount_device_opt+="${prefix}--device=$capture_dev${postfix}"
    done

    for video_dev in $(find /dev -regex '/dev/ajantv2[0-9]+'); do
        mount_device_opt+="${prefix}--device=$video_dev:$video_dev${postfix}"
    done

    # Deltacast capture boards and Videomaster SDK
    for i in 0 1 2 3; do
        # Deltacast SDI capture board
        if [ -e /dev/delta-x380${i} ]; then
            mount_device_opt+="${prefix}--device=/dev/delta-x380${i}:/dev/delta-x380${i}${postfix}"
        fi
        # Deltacast HDMI capture board
        if [ -e /dev/delta-x350${i} ]; then
            mount_device_opt+="${prefix}--device=/dev/delta-x350${i}:/dev/delta-x350${i}${postfix}"
        fi
    done

    # Find all audio devices
    if [ -d /dev/snd ]; then
        audio_devices=$(find /dev/snd -type c)

        # Mount all found audio devices
        for audio_dev in $audio_devices; do
            mount_device_opt+="${prefix}--device=$audio_dev${postfix}"
        done
    fi

    # mounts the ALSA configuration for the user running the script
    if [ -e /etc/asound.conf ]; then
        mount_device_opt+="${prefix}--mount=source=/etc/asound.conf,target=/etc/asound.conf,readonly,type=bind${postfix}"
    fi

    # Mount ConnectX device nodes
    if [ -e /dev/infiniband/rdma_cm ]; then
        mount_device_opt+="${prefix}--device=/dev/infiniband/rdma_cm${postfix}"
    fi
    for uverbs_dev in $(find /dev -regex '/dev/infiniband/uverbs[0-9]+'); do
        mount_device_opt+="${prefix}--device=${uverbs_dev}${postfix}"
    done

    # grants access to the sound group
    sound_group_gid=$(getent group audio | cut -d: -f3)
    mount_device_opt+="${prefix}--group-add=$sound_group_gid" # last item does not need postfix

    echo "${mount_device_opt}"
}

check_nvidia_ctk() {
    # Check NVIDIA CTK version
    min_ctk_version="1.12.0"
    recommended_ctk_version="1.14.1"
    if ! command -v nvidia-ctk >/dev/null; then
        fatal G "nvidia-ctk" W " not found. Please install the NVIDIA Container Toolkit."
    fi
    ctk_version_output=$(nvidia-ctk --version | grep version)
    ctk_version_pattern="([0-9]+\.[0-9]+\.[0-9]+)"
    if [[ "$ctk_version_output" =~ $ctk_version_pattern ]]; then
        ctk_version="${BASH_REMATCH[1]}"
        if [[ "$(echo -e "$ctk_version\n$min_ctk_version" | sort -V | head -n1)" == "$ctk_version" ]]; then
            fatal "Found nvidia-ctk Version $ctk_version. Version $min_ctk_version+ is required ($recommended_ctk_version+ recommended)."
        fi
    else
        c_echo_err R "Could not extract nvidia-ctk version number."
        c_echo_err "  Version $min_ctk_version+ required."
        c_echo_err "  Version $recommended_ctk_version+ recommended."
    fi
}

#===============================================================================
# Section: Build
#===============================================================================

get_host_gpu() {
    if ! command -v nvidia-smi >/dev/null; then
        echo "Could not find any GPU drivers on host. Defaulting build to target dGPU/CPU stack."
        echo -n "dgpu";
    elif [[ ! $(nvidia-smi --query-gpu=name --format=csv,noheader) ]] || \
         [[ $(nvidia-smi --query-gpu=name --format=csv,noheader -i 0) =~ "Orin (nvgpu)" ]]; then
        echo -n "igpu";
    else
        echo -n "dgpu";
    fi
}

get_default_base_img() {
    echo -n "nvcr.io/nvidia/clara-holoscan/holoscan:v2.6.0-"$(get_host_gpu)
}

get_default_img() {
    echo -n "holohub:ngc-v2.6.0-"$(get_host_gpu)
}

# This function returns the compute capacity of the system's GPU (8.6, 8.9, etc.)
# Compute capacity is a version number that represents the GPU's features & specs
get_compute_capacity() {
    echo -n $(nvidia-smi --query-gpu=compute_cap --format=csv,noheader | head -n 1)
}

build_desc() { c_echo 'Build dev image
Usage: ./dev_container build [options]
    --base_img : Fully qualified base image name, e.g. holoscan-sdk-dev:latest
    --docker_file :  path to Dockerfile to use for building container.
    --img : Specify fully qualified container name
    --verbose : Print variables passed to docker build command
    --no-cache : Do not use cache when building the image
    --build-args : provides extra arguments to docker build command
    '
}

build() {

    # Choose NGC Holoscan SDK base image based on local platform, default is dGPU
    local docker_file_path="${HOLOHUB_ROOT}/Dockerfile"
    local gpu_type=$(get_host_gpu)
    local base_img=$(get_default_base_img)
    local img=$(get_default_img)
    local compute_capacity=$(get_compute_capacity)
    local print_verbose=0
    local no_cache=
    local extra_args=

    # Check if buildx exists
    if ! $(docker buildx version &>/dev/null); then
        c_echo_err G "docker buildx plugin" W " is missing. Please install " G "docker-buildx-plugin" W ":

        https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository"
        exit
    fi

    # Parse CLI arguments next
    while [[ $# -gt 0 ]]; do
        case "$1" in
        --base_img)
            if [[ -n "$2" && ! "$2" =~ ^-- ]]; then
                base_img="$2"
                shift 2
            else
                echo "Error: --base_img requires a value"
                build_desc
                exit 1
            fi
            ;;
        --docker_file)
            if [[ -n "$2" && ! "$2" =~ ^-- ]]; then
                docker_file_path="$2"
                shift 2
            else
                echo "Error: --docker_file requires a value"
                build_desc
                exit 1
            fi
            ;;
        --img)
            if [[ -n "$2" && ! "$2" =~ ^-- ]]; then
                img="$2"
                shift 2
            else
                echo "Error: --img requires a value"
                build_desc
                exit 1
            fi
            ;;
        --build-args)
            if [[ -n "$2" ]]; then
                extra_args+=" $2"
                shift 2
            else
                echo "Error: --build-args requires a value"
                build_desc
                exit 1
            fi
            ;;
        --no-cache)
            no_cache="--no-cache"
            shift
            ;;
        --verbose)
            print_verbose=1
            shift
            ;;
        *)
            echo "Error: Invalid argument '$1'"
            build_desc
            exit 1
            ;;
        esac
    done

    if [[ $print_verbose -gt 0 ]]; then
        c_echo W "Build (HOLOHUB_ROOT:" G "${HOLOHUB_ROOT}" W ")..."
        c_echo W "Build (gpu_type_type:" G "${gpu_type}" W ")..."
        c_echo W "Build (base_img:" G "${base_img}" W ")..."
        c_echo W "Build (docker_file_path:" G "${docker_file_path}" W ")..."
        c_echo W "Build (img:" G "${img}" W ")..."
    fi

    # Docker build
    run_command export DOCKER_BUILDKIT=1
    run_command docker build \
        --build-arg BUILDKIT_INLINE_CACHE=1 \
        --build-arg BASE_IMAGE=${base_img} \
        --build-arg GPU_TYPE=${gpu_type} \
        --build-arg COMPUTE_CAPACITY=${compute_capacity} \
        --network=host \
        ${extra_args} \
        ${no_cache} \
        -f ${docker_file_path} \
        -t ${img} \
        ${HOLOHUB_ROOT}
}

#===============================================================================
# Section: Launch
#===============================================================================

launch_desc() { c_echo 'Launch Docker container
Usage: ./dev_container launch [options]
    --img   : Fully qualified image name, e.g.  holohub:sdk-local-latest-dgpu
    --local_sdk_root : Path to Holoscan SDK used for building local Holoscan SDK container
    --ssh_x11 : Enable X11 forwarding of graphical HoloHub applications over SSH
    --nsys_profile : Support Nsight Systems profiling in container
    --init : Support tini entry point
    --persistent : Does not delete container after it is run
    --verbose : Print variables passed to docker run command
    --add-volume : Mount additional volume
    --as_root  : Run the container with root permissions
    --docker_opts : Additional options to pass to the Docker launch
    -- : Any trailing arguments after "--" are forwarded to `docker run`
    '
}
launch() {
    local build_path="${CMAKE_BUILD_PATH:-build}"
    local working_dir=${1:-${build_path}}
    local mount_device_opt=""
    local conditional_opt=""
    local print_verbose=0
    local local_sdk_root="undefined"
    local ssh_x11=0
    local use_tini=0
    local persistent=0
    local nsys_profile=false
    local as_root=false
    local docker_opts=""

    # Choose NGC Holoscan SDK base image based on local platform, default is dGPU
    local gpu_type=$(get_host_gpu)
    local img=$(get_default_img)

    # Initialize an empty string to hold additional volume mounts
    additional_volumes=""

    # Parse CLI arguments
    while [[ $# -gt 0 ]]; do
        case "$1" in
        --img)
            if [[ -n "$2" && ! "$2" =~ ^-- ]]; then
                img="$2"
                shift 2
            else
                echo "Error: --img requires a value"
                launch_desc
                exit 1
            fi
            ;;
        --verbose)
            print_verbose=1
            shift
            ;;
        --local_sdk_root)
            if [[ -n "$2" && ! "$2" =~ ^-- ]]; then
                local_sdk_root="$2"
                shift 2
            else
                echo "Error: --local_sdk_root requires a value"
                launch_desc
                exit 1
            fi
            ;;
        --ssh_x11)
            ssh_x11=1
            shift
            ;;
        --init)
            use_tini=1
            shift
            ;;
        --persistent)
            persistent=1
            shift
            ;;
        --nsys_profile)
            nsys_profile=true
            shift
            ;;
        --as_root)
            as_root=true
            shift
            ;;
        --add-volume)
            if [[ -n "$2" && ! "$2" =~ ^-- ]]; then
                path="$2"
                base=$(basename "$path")
                additional_volumes+="-v $path:/workspace/volumes/$base "
                shift 2
            else
                echo "Error: --add-volume requires a value"
                launch_desc
                exit 1
            fi
            ;;
        --docker_opts)
            if [[ -n "$2" ]]; then
                docker_opts="$2"
                shift 2
            else
                echo "Error: --docker_opts requires a value"
                launch_desc
                exit 1
            fi
            ;;
        --)
            # Any trailing arguments after "--" are forwarded to `docker run`
            shift
            break
            ;;
        *)
            echo "Error: Invalid argument '$1'"
            launch_desc
            exit 1
            ;;
        esac
    done

    check_nvidia_ctk

    mount_device_opt="$(get_mount_device_options)"
    conditional_opt="$(get_conditional_options $use_tini $persistent)"

    # display - use XDG_SESSION_TYPE to detect X11 or Wayland
    # see https://www.freedesktop.org/software/systemd/man/latest/pam_systemd.html#%24XDG_SESSION_TYPE
    display_server_opt=""
    if [ -n "${XDG_SESSION_TYPE-}" ]; then
        display_server_opt+=" -e XDG_SESSION_TYPE"
        if [ "${XDG_SESSION_TYPE}" == "wayland" ]; then
            display_server_opt+=" -e WAYLAND_DISPLAY"
        fi
    fi
    if [ -n "${XDG_RUNTIME_DIR-}" ]; then
        display_server_opt+=" -e XDG_RUNTIME_DIR"
        if [ -d ${XDG_RUNTIME_DIR} ]; then
            display_server_opt+=" -v ${XDG_RUNTIME_DIR}:${XDG_RUNTIME_DIR}"
        fi
    fi
    # if XDG_SESSION_TYPE is not set or set to tty or x11, use X11
    if [ -z "${XDG_SESSION_TYPE-}" ] || [ "${XDG_SESSION_TYPE}" == "x11" ] || [ "${XDG_SESSION_TYPE}" == "tty" ]; then
        # Allow the docker group to access X11.
        # Note: not necessary for WSL2 (`SI:localuser:wslg` is added by default)
        if [ -v DISPLAY ] && command -v xhost >/dev/null; then
            run_command xhost +local:docker
        fi
        display_server_opt+=" -v /tmp/.X11-unix:/tmp/.X11-unix"
        display_server_opt+=" -e DISPLAY"
    fi

    # when using a locally built Holoscan SDK container it is necessary to provide the path to Holoscan SDK
    # to map Holoscan SDK into the container for building Holohub
    local local_sdk_opt=""

    local pythonpath="/workspace/holohub/benchmarks/holoscan_flow_benchmarking"
    if [ -d "$local_sdk_root" ]; then
        local_sdk_opt+=" -v $local_sdk_root:/workspace/holoscan-sdk"
        #   Define paths needed by the python applications
        local_sdk_opt+=" -e HOLOSCAN_LIB_PATH=/workspace/holoscan-sdk/build/lib"
        local_sdk_opt+=" -e HOLOSCAN_SAMPLE_DATA_PATH=/workspace/holoscan-sdk/data"
        local_sdk_opt+=" -e HOLOSCAN_TESTS_DATA_PATH=/workspace/holoscan-sdk/tests/data"
        pythonpath="/workspace/holoscan-sdk/build/python/lib:${pythonpath}"
    else
        pythonpath="/opt/nvidia/holoscan/python/lib:${pythonpath}"
    fi
    local_sdk_opt+=" -e PYTHONPATH=${pythonpath}"

    # Find the nvoptix.bin file
    if [ -f "/usr/share/nvidia/nvoptix.bin" ]; then
        mount_nvoptix_bin="-v /usr/share/nvidia/nvoptix.bin:/usr/share/nvidia/nvoptix.bin:ro"
    fi

    # Options needed for UCX
    local ucx_opt="$(get_ucx_options)"

    # Only enable TTY if supported
    local use_tty=""
    if [ -t 1 ]; then
        use_tty="--tty"
    fi

    # Allow root permissions on request
    if [ $as_root == true ]; then
        user="root"
    else
        user="$(id -u):$(id -g)"
    fi

    # DOCKER PARAMETERS
    #
    # --interactive (-i)
    #   The container needs to be interactive to be able to interact with the X11 windows
    #
    # -u $(id -u):$(id -g)
    # -v /etc/group:/etc/group:ro
    # -v /etc/passwd:/etc/passwd:ro
    #   Ensures the generated files (build, install...) are owned by $USER and not root,
    #   and provide the configuration files to avoid warning for user and group names
    #
    # -v ${HOLOHUB_ROOT}:/workspace/holohub
    #   Mount the source directory
    #
    # -w /workspace/holohub/${working_dir}
    #   Start in the build or install directory
    #
    # --runtime=nvidia \
    # -e NVIDIA_DRIVER_CAPABILITIES=graphics,video,compute,utility,display
    #   Enable GPU acceleration
    #
    # ${mount_nvoptix_bin}
    #   Mount Optix denoiser weights.

    if [[ $print_verbose -gt 0 ]]; then
        c_echo W "Launch (HOLOHUB_ROOT: " G "${HOLOHUB_ROOT}" W ")..."
        c_echo W "Launch (gpu_type: " G "${gpu_type}" W ")..."
        c_echo W "Launch (mount_device_opt: " G "${mount_device_opt}" W ")..."
        c_echo W "Launch (conditional_opt: " G "${conditional_opt}" W ")..."
        c_echo W "Launch (display_server_opt: " G "${display_server_opt}" W ")..."
        c_echo W "Launch (local_sdk_opt: " G "${local_sdk_opt}" W ")..."
        c_echo W "Launch (ucx_opt: " G "${ucx_opt}" W ")..."
        c_echo W "Launch (docker_opts: " G "${docker_opts}" W ")..."
        c_echo W "Launch (image: " G "${img}" W ")..."
        c_echo W "Launch (trailing args: " G "$@" W ")..."
    fi

    run_command ${HOLOSCAN_DOCKER_EXE} run --net host \
        --interactive ${use_tty} \
        -u $user \
        -v /etc/group:/etc/group:ro \
        -v /etc/passwd:/etc/passwd:ro \
        -v ${HOLOHUB_ROOT}:/workspace/holohub \
        -w /workspace/holohub \
        --runtime=nvidia \
        --gpus all \
        --cap-add CAP_SYS_PTRACE \
        --ipc=host \
        -v /dev:/dev \
        --device-cgroup-rule "c 81:* rmw" \
        --device-cgroup-rule "c 189:* rmw" \
        -e NVIDIA_DRIVER_CAPABILITIES=graphics,video,compute,utility,display \
        -e HOME=/workspace/holohub \
        ${additional_volumes} \
        ${mount_device_opt} \
        ${conditional_opt} \
        ${display_server_opt} \
        ${local_sdk_opt} \
        ${mount_nvoptix_bin} \
        ${ucx_opt} \
        ${docker_opts} \
        ${img} \
        "$@"
}

build_and_install_desc() { c_echo 'Build the application and install the binaries into install/<app> if supported
    Usage: ./dev_container build_and_install <application_name>
    View options with `./dev_container build_and_run_desc`
    '
}

build_and_install() {
    build_and_run --install --no_run "$@"
}

build_and_run_desc() { c_echo 'Build and run a requested application in a Docker container
    Usage: ./dev_container build_and_run <application_name> [options]
    Options:
        --base_img : Fully qualified base image name, e.g. holoscan-sdk-dev:latest
        --docker_file : Path to Dockerfile to use for building container.
            Defaults to:
            - Application-provided "Dockerfile", if it exists;
            - Otherwise the top-level HoloHub "Dockerfile"
            If `--img` is not specified then a custom image tag will be defined.
        --img : Specify fully qualified output container name
        --container_args: Additional Docker run args.
        --language : Specify the app language implementation to run.
                     Some applications provide both `cpp` and `python` implementations.
        --no_build : Launch the app without attempting to build it first
        --no_run : Skip launching the application.
        --install: Install the application into install/<app> if supported.
        --build_args : Build the app with additional CMake configuration arguments
        --build_docker_args : additional docker arguments when building the container
        --build_with : List of optional operators that should be built separated by semicolons (;)
        --run_args : Run the app with additional args
        --verbose : Print extra output to console
        --dryrun : View build and run commands without doing anything
        --parallel_jobs: Set the # of parallel build jobs, e.g. $(($(nproc)-1))
    '
}

build_and_run() {
    local app_name=""
    local app_language=""
    local container_build_args=()
    local container_launch_args=""
    local docker_opts="--entrypoint=bash"
    local build_app=1
    local image_name=""
    local docker_file=""
    local extra_build_args=""
    local extra_build_with=""
    local extra_run_args=""
    local run_app=1
    local install=""
    local configure_args=""
    local parallel_jobs=""

    # Parse CLI arguments next
    while [[ $# -gt 0 ]]; do
        case "$1" in
        --base_img)
            if [[ -n "$2" && ! "$2" =~ ^-- ]]; then
                container_build_args+=(--base_img $2)
                shift 2
            else
                echo "Error: --base_img requires a value"
                build_and_run_desc
                exit 1
            fi
            ;;
        --docker_file)
            if [[ -n "$2" && ! "$2" =~ ^-- ]]; then
                docker_file="$2"
                shift 2
            else
                echo "Error: --docker_file requires a value"
                build_and_run_desc
                exit 1
            fi
            ;;
        --img)
            if [[ -n "$2" && ! "$2" =~ ^-- ]]; then
                image_name="$2"
                shift 2
            else
                echo "Error: --img requires a value"
                build_and_run_desc
                exit 1
            fi
            ;;
        --no_build)
            build_app=0
            shift
            ;;
        --run_args)
            if [[ -n "$2" ]]; then
                extra_run_args="--extra_args '$2'"
                shift 2
            else
                echo "Error: --run_args requires a value"
                build_and_run_desc
                exit 1
            fi
            ;;
        --build_with)
            if [[ -n "$2" ]]; then
                extra_build_with="--with '$2'"
                shift 2
            else
                echo "Error: --build_with requires a value"
                build_and_run_desc
                exit 1
            fi
            ;;
        --build_args)
            if [[ -n "$2" ]]; then
                configure_args="${configure_args} '$2'"
                shift 2
            else
                echo "Error: --build_args requires a value"
                build_and_run_desc
                exit 1
            fi
            ;;
        --build_docker_args)
            if [[ -n "$2" ]]; then
                container_build_args+=(--build-args)
                container_build_args+=("$2")
                shift 2
            else
                echo "Error: --build_docker_args requires a value"
                build_and_run_desc
                exit 1
            fi
            ;;
        --run_args)
            if [[ -n "$2" ]]; then
                extra_run_args="--extra_args '$2'"
                shift 2
            else
                echo "Error: --run_args requires a value"
                build_and_run_desc
                exit 1
            fi
            ;;
        --no_run)
            run_app=0
            shift 1
            ;;
        --install)
            install="--install"
            shift 1
            ;;
        --verbose)
            container_build_args+=(--verbose)
            container_launch_args+=" --verbose"
            shift
            ;;
        --language)
            if [[ -n "$2" && ! "$2" =~ ^-- ]]; then
                app_language="$2"
                shift 2
            else
                echo "Error: --language requires a value"
                build_and_run_desc
                exit 1
            fi
            ;;
        --container_args)
            if [[ -n "$2" ]]; then
                docker_opts+="$2"
                shift 2
            else
                echo "Error: --container_args requires a value"
                build_and_run_desc
                exit 1
            fi
            ;;
        --parallel_jobs)
            if [[ -n "$2" ]]; then
                parallel_jobs="--parallel '$2'"
                shift 2
            else
                echo "Error: --parallel_jobs requires a value"
                build_and_run_desc
                exit 1
            fi
            ;;
        *)
            if [[ -z "$app_name" ]]; then
                app_name="$1"
                shift
            else
                echo "Error: Invalid argument '$1'"
                build_and_run_desc
                exit 1
            fi
            ;;
        esac
    done

    if [ -z "${app_name}" ]; then
        fatal "Must specify a HoloHub application to build and run."
    fi

    if [[ -z "${docker_file}" ]]; then
        docker_file=$(./run get_app_dockerfile ${app_name} ${app_language:-cpp})
    fi
    if [[ -n "${docker_file}" ]]; then
        container_build_args+=(--docker_file ${docker_file})
    fi

    if [[ -n "${configure_args}" ]]; then
        extra_build_args+=" --configure-args ${configure_args}"
    fi

    if [[ -z "$image_name" ]]; then
        if [[ -n "${docker_file}" ]] && [[ "${docker_file}" != "${SCRIPT_DIR}/Dockerfile" ]]; then
            image_name="holohub:${app_name}"
        else
            image_name=$(get_default_img)
        fi
    fi

    container_build_args+=(--img $image_name)
    container_launch_args+=" --img $image_name"

    c_echo "Building application container..."
    run_command  ${SCRIPT_DIR}/dev_container build "${container_build_args[@]}"
    if [[ $build_app == 1 ]]; then
        c_echo "Building application..."
        run_command ${SCRIPT_DIR}/dev_container launch $container_launch_args --docker_opts "$docker_opts" -- -c "./run build $app_name $install $parallel_jobs $extra_build_with $extra_build_args"
    fi

    if [[ $run_app == 1 ]]; then
        c_echo "Launching application..."
        run_command ${SCRIPT_DIR}/dev_container launch $container_launch_args --docker_opts "$docker_opts" -- -c "./run launch $app_name $app_language $extra_run_args"
    fi
}

vscode_desc() { c_echo 'Launch VS Code in Dev Container

Launch a VS Code instance in a Docker container with the development environment.
    Usage: ./dev_container vscode <application_name> [options]

Options:
  application_name: Name of an existing Holohub application found in the applications folder.
      If specified and exists: the application-provided Dev Container configuration is used.
      Otherwise, the top-level Dev Container configuration.
  --base_img: Fully qualified base image name, e.g. holoscan-sdk-dev:latest
  --docker_file: Path to Dockerfile to use for building container.
      Defaults to:
      - Application-provided "Dockerfile", if it exists;
      - Otherwise the top-level HoloHub "Dockerfile"
  --language : Specify the app language implementation to run.
      Some applications provide both `cpp` and `python` implementations.
  --docker_opts : Additional options to pass to the Docker launch
  --verbose : Print variables passed to docker run command
'
}

# The vscode() function builds a Dev Container in the following stages:
# 1. builds a container image using the default (or user-provided) base image and Dockerfile.
# 2. prepares the devcontainer.json by adding the necessary devices and volumes and injects
#    the list into the devcontainer.json file.
# 3. launches a new VS Code instance using the devcontainer.json file from step 2 which builds
#    the actual dev container using the image from step 1 as the base image.
vscode() {
    check_nvidia_ctk
    local app_name=""
    local app_language=""
    local base_img=$(get_default_base_img)
    local docker_file=""
    local dev_container_tag="holohub-dev-container"
    local docker_opts=""
    local print_verbose=0

    if ! command -v code >/dev/null; then
        fatal R "Please install " W "VS Code" R " to use VS Code Dev Container. Follow the instructions in https://code.visualstudio.com/Download"
    fi

    # Parse CLI arguments next
    while [[ $# -gt 0 ]]; do
        case "$1" in
        --base_img)
            if [[ -n "$2" && ! "$2" =~ ^-- ]]; then
                base_img=$2
                shift 2
            else
                echo "Error: --base_img requires a value"
                vscode_desc
                exit 1
            fi
            ;;
        --docker_file)
            if [[ -n "$2" && ! "$2" =~ ^-- ]]; then
                docker_file="$2"
                shift 2
            else
                echo "Error: --docker_file requires a value"
                vscode_desc
                exit 1
            fi
            ;;
        --docker_opts)
            if [[ -n "$2" ]]; then
                docker_opts="$2"
                shift 2
            else
                echo "Error: --docker_opts requires a value"
                launch_desc
                exit 1
            fi
            ;;
        --language)
            if [[ -n "$2" && ! "$2" =~ ^-- ]]; then
                app_language="$2"
                shift 2
            else
                echo "Error: --language requires a value"
                build_and_run_desc
                exit 1
            fi
            ;;
        --verbose)
            print_verbose=1
            shift
            ;;
        *)
            if [[ -z "$app_name" ]]; then
                app_name="$1"
                shift
            else
                echo "Error: Invalid argument '$1'"
                vscode_desc
                exit 1
            fi
            ;;
        esac
    done

    if [[ -z "${docker_file}" ]]; then
        docker_file=$(./run get_app_dockerfile ${app_name} ${app_language:-cpp})
        c_echo W "Using Dockerfile: " G "${docker_file}"
    fi
    if [[ -n "${docker_file}" ]]; then
        container_build_args+=" --docker_file ${docker_file}"
    fi

    if [ -n "${app_name}" ]; then
        dev_container_tag+="-${app_name}"
    fi
    dev_container_tag+=":dev"
    container_build_args+=" --base_img ${base_img} --img ${dev_container_tag}"


    if [[ $print_verbose -gt 0 ]]; then
        c_echo W "Base container build parameters (container_build_args: " G "${container_build_args}" W ")..."
    fi

    c_echo W "Building base Dev Container ${dev_container_tag}..."
    run_command ${SCRIPT_DIR}/dev_container build $container_build_args

    # Prepare for the DevContainer with an environment file to be passed into runArgs
    local devcontainer_env_file=""
    devcontainer_env_file+="$(get_mount_device_options "\"" "\",")"
    devcontainer_env_file+="\","
    devcontainer_env_file+="$(get_conditional_options 0 0 "\"" "\",")"
    devcontainer_env_file+="\","

    # Find the nvoptix.bin file
    if [ -f "/usr/share/nvidia/nvoptix.bin" ]; then
        devcontainer_env_file+="\"--volume=/usr/share/nvidia/nvoptix.bin:/usr/share/nvidia/nvoptix.bin:ro\","
    fi

    # User provided Docker options
    if [ -n "${docker_opts}" ]; then
        devcontainer_env_file+="\"$docker_opts\","
    fi

    # Options needed for UCX
    devcontainer_env_file+="$(get_ucx_options "\"" "\",")"
    devcontainer_env_file+="\","

    devcontainer_env_file+="\"--device-cgroup-rule=c 81:* rmw\","
    devcontainer_env_file+="\"--device-cgroup-rule=c 189:* rmw\","
    devcontainer_env_file+="\"--runtime=nvidia\","
    devcontainer_env_file+="\"--gpus=all\","

    if [[ -n "${app_name}" ]]; then
        devcontainer_env_file+="\"--hostname=holohub-${app_name}\""
    else
        devcontainer_env_file+="\"--hostname=holohub\""
    fi

    export HOLOHUB_BASE_IMAGE=$dev_container_tag
    export HOLOHUB_APP_NAME=$app_name

    # Allow connecting from docker. This is not needed for WSL2 (`SI:localuser:wslg` is added by default)
    run_command xhost +local:docker

    # Install the VSCode Remote Development extension.
    # Checking if the extension is already installed is slow, so we always install it so that
    # the installation can be skipped if the extension is already installed.
    run_command code --force --install-extension ms-vscode-remote.vscode-remote-extensionpack

    # Top-level Dev Container template
    local dev_container_path="$SCRIPT_DIR/.devcontainer/"

    # If an application name is provided, use the application-specific DevContainer configuration when available.
    # Otherwise, fall back to the top-level DevContainer configuration.
    if [ -n "${app_name}" ] && [ -f "$SCRIPT_DIR/.devcontainer/${app_name}/devcontainer.json" ]; then
        dev_container_path="$SCRIPT_DIR/.devcontainer/${app_name}"
        c_echo "Using application-specific DevContainer configuration: ${dev_container_path}"
    else
        c_echo "Using top-level DevContainer configuration: ${dev_container_path}"
    fi


    if [[ $print_verbose -gt 0 ]]; then
        c_echo W "Dev Container run args (devcontainer_env_file: " G "${devcontainer_env_file}" W ")..."
        c_echo W "Dev Container base image (HOLOHUB_BASE_IMAGE: " G "${HOLOHUB_BASE_IMAGE}" W ")..."
    fi

    # Since Visual Studio Code allows only one instance per `devcontainer.json`,
    # this code prepares a unique temporary directory structure for each launch of a devcontainer.
    # By doing so, it ensures that multiple instances of the same environment can be run
    # simultaneously. The script replicates the `devcontainer.json` from the desired application
    # into this temporary directory, adjusting paths to ensure the correct workspace is loaded.
    # A special URL is then generated to instruct VSCode to launch the development container
    # using this temporary configuration.
    local workspace="$(basename "$(pwd)")"
    local tmpdir="$(mktemp -d)/${workspace}"
    mkdir -p "${tmpdir}"
    mkdir -p "${tmpdir}/.devcontainer"
    cp -arL "${dev_container_path}/devcontainer.json" "${tmpdir}/.devcontainer"
    sed -i -e "s|\${localWorkspaceFolder}|$(pwd)|g" "${tmpdir}/.devcontainer/devcontainer.json" \
        -e "s|//\"<env>\"|$devcontainer_env_file|g" "${tmpdir}/.devcontainer/devcontainer.json"

    local hash="$(echo -n "${tmpdir}" | xxd -pu - | tr -d '[:space:]')"
    local url="vscode://vscode-remote/dev-container+${hash}/workspace/holohub"

    local launch=""
    if type open >/dev/null 2>&1; then
        launch="open"
    elif type xdg-open >/dev/null 2>&1; then
        launch="xdg-open"
    fi

    if [ -n "${launch}" ]; then
        c_echo "Launching VSCode Dev Container from : ${tmpdir}"
        code --new-window "$tmpdir"
        c_echo "Connecting to ${url}..."
        exec "${launch}" "${url}" >/dev/null 2>&1
    else
        fatal "Error launching VSCode Dev Container. Please ensure that $(open) or $(xdg-open) is installed."
    fi
}

#===============================================================================

parse_args() {
    local OPTIND
    while getopts 'yh' option; do
        case "${option}" in
        y)
            ALWAYS_YES="true"
            ;;
        h)
            print_usage
            if [ -n "${SCRIPT_DIR}" ]; then
                exit 1
            fi
            ;;
        *) ;;
        esac
    done
    shift $((OPTIND - 1))
    CMD="$1"
    if [ -z "$CMD" ]; then
        print_usage
    fi
    shift

    ARGS=("$@")
    # Check if the command has `--help`, `-h`, or `--dryrun`, and override the CMD
    local i
    local arg
    local unset_pos
    for i in "${!ARGS[@]}"; do
        arg="${ARGS[i]}"
        if [ "$arg" = "--help" ] || [ "$arg" = "-h" ]; then
            ARGS=("$CMD")
            CMD="help"
            break
        fi
        if [ "$arg" = "--dryrun" ]; then
            unset_pos=$i
            DO_DRY_RUN="true" # set to true to print commands to screen without running
        fi
    done
    if [ "${unset_pos}" ]; then
        unset 'ARGS[unset_pos]'
    fi
}

print_usage() {
    set +x
    echo_err
    echo_err "USAGE: $0 [command] [arguments]..."
    echo_err ""
    c_echo_err W "Global Arguments"
    c_echo_err "  --help, -h      : Print help messages for [command]"
    c_echo_err "  --dryrun        : Print commands to screen without running"
    echo_err
    c_echo_err W "Command List"
    c_echo_err Y "    help  " w "----------------------------  Print detailed description for a given argument (command name)"
    echo_err "$(get_list_of_available_commands color "${RUN_SCRIPT_FILE}" | my_cat_prefix " ")"
    echo_err
}

print_cmd_help_messages() {
    local cmd="$1"
    if [ -n "${cmd}" ]; then
        if type ${cmd}_desc >/dev/null 2>&1; then
            ${cmd}_desc
            exit 0
        else
            c_echo_err R "Command '${cmd}' doesn't exist!"
            exit 1
        fi
    fi
    print_usage
    return 0
}

main() {
    local ret=0
    parse_args "$@"

    case "$CMD" in
    help)
        print_cmd_help_messages "${ARGS[@]}"
        exit 0
        ;;
    '' | main)
        print_usage
        ;;
    *)
        if type ${CMD} >/dev/null 2>&1; then
            "$CMD" "${ARGS[@]}"
        else
            print_usage
            exit 1
        fi
        ;;
    esac
    ret=$?
    if [ -n "${SCRIPT_DIR}" ]; then
        exit $ret
    fi
}

init_globals

if [ -n "${SCRIPT_DIR}" ]; then
    main "$@"
fi

#===============================================================================
# Description template
#===============================================================================
# Globals:
#   HOLOSCAN_OS
#   HOLOSCAN_TARGET
#   HOLOSCAN_USER (used if HOLOSCAN_OS is "linux")
#   HOLOSCAN_HOST (used if HOLOSCAN_OS is "linux")
# Arguments:
#   Command line to execute
# Returns:
#   Outputs print messages during the execution (stdout->stdout, stderr->stderr).
#
#   Note:
#     This command removes "\r" characters from stdout.
#
#   Exit code:
#     exit code returned from executing a given command
