{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Holoscan Reference Applications","text":""},{"location":"#holoscan-reference-applications","title":"Holoscan Reference Applications","text":"<p>Holoscan Reference Applications is a central repository for the NVIDIA Holoscan AI sensor processing community to share reference applications, operators, tutorials and benchmarks. The repository hosts a variety of applications that demonstrate how to use Holoscan for streaming, imaging, and other AI-driven tasks across embedded, edge, and cloud environments. These applications serve as reference implementations, providing developers with examples of best practices and efficient coding techniques to build high-performance, low-latency AI applications. The repository is open to contributions from the community, encouraging developers to share their own applications and extensions to enhance the Holoscan ecosystem.</p> <ul> <li> <p> Workflows (1)</p> <p>Reference workflows demonstrate how capabilities from applications and operators can be combined to achieve complex tasks.</p> <p>Browse Workflows</p> </li> <li> <p> Applications (81)</p> <p>Reference applications demonstrate a specific capability of Holoscan or how a specific operator can be used to perform an optimize task.</p> <p>Browse Applications</p> </li> <li> <p> Operators (60)</p> <p>Operators perform a specific task.</p> <p>Browse Operators</p> </li> <li> <p> Tutorials (15)</p> <p>Tutorials provide hands-on experience.</p> <p>Browse Tutorials</p> </li> <li> <p> Benchmarks (4)</p> <p>Benchmarks provide tools for assessing performance of Holoscan pipelines as well as reference benchmarks for specific releases</p> <p>Browse Benchmarks</p> </li> </ul>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#tag:3d-slicer","title":"3D Slicer","text":"<ul> <li>            Medical Imaging Segmentation with NVIDIA Vista-3D NIM (latest)          </li> <li>            OpenIGTLink 3D Slicer: Bidirectional Video Streaming with AI Segmentation (latest)          </li> <li>            openigtlink (latest)          </li> </ul>"},{"location":"tags/#tag:ai","title":"AI","text":"<ul> <li>            Medical Imaging (latest)          </li> </ul>"},{"location":"tags/#tag:aja","title":"AJA","text":"<ul> <li>            AJA Video Capture (latest)          </li> <li>            Colonoscopy Polyp Segmentation (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            aja_source (latest)          </li> </ul>"},{"location":"tags/#tag:asr","title":"ASR","text":"<ul> <li>            EHR Agent Framework (latest)          </li> <li>            FM Radio Automatic Speech Recognition (latest)          </li> <li>            Real-time Riva ASR to local-LLM (latest)          </li> <li>            Speech-to-text + Large Language Model (latest)          </li> </ul>"},{"location":"tags/#tag:acceleration","title":"Acceleration","text":"<ul> <li>            CUDA MPS Tutorial (latest)          </li> </ul>"},{"location":"tags/#tag:anonymization","title":"Anonymization","text":"<ul> <li>            Pixelator (latest)          </li> </ul>"},{"location":"tags/#tag:asynchronous-queues","title":"Asynchronous Queues","text":"<ul> <li>            Distributed Endoscopy Tool Tracking with gRPC Streaming (latest)          </li> <li>            Distributed H.264 Endoscopy Tool Tracking with gRPC Streaming (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> </ul>"},{"location":"tags/#tag:audio","title":"Audio","text":"<ul> <li>            EHR Agent Framework (latest)          </li> <li>            FM Radio Automatic Speech Recognition (latest)          </li> <li>            Real-time Riva ASR to local-LLM (latest)          </li> <li>            Software Defined Radio FM Demodulation (latest)          </li> <li>            Speech-to-text + Large Language Model (latest)          </li> </ul>"},{"location":"tags/#tag:auth-and-api","title":"Auth and API","text":"<ul> <li>            Chat with NVIDIA NIM (latest)          </li> <li>            FHIR Client for Retrieving and Posting FHIR Resources (latest)          </li> <li>            NVIDIA NV-CLIP NIM (latest)          </li> <li>            Speech-to-text + Large Language Model (latest)          </li> </ul>"},{"location":"tags/#tag:backprojection","title":"Backprojection","text":"<ul> <li>            Streaming Synthetic Aperture Radar (latest)          </li> </ul>"},{"location":"tags/#tag:base","title":"Base","text":"<ul> <li>            InferenceOperator (latest)          </li> </ul>"},{"location":"tags/#tag:bayer-rgb-pipeline-optimization","title":"Bayer RGB Pipeline Optimization","text":"<ul> <li>            High-Speed Endoscopy (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> </ul>"},{"location":"tags/#tag:beamforming","title":"Beamforming","text":"<ul> <li>            Ultrasound Beamforming with MATLAB GPU Coder (latest)          </li> </ul>"},{"location":"tags/#tag:benchmarking","title":"Benchmarking","text":"<ul> <li>            CUDA MPS Tutorial (latest)          </li> <li>            Exclusive Display Benchmark (latest)          </li> <li>            Holoscan Flow Benchmarking (latest)          </li> <li>            Holoscan Release Benchmarking (latest)          </li> <li>            Holoviz sRGB (latest)          </li> <li>            Model Benchmarking (latest)          </li> </ul>"},{"location":"tags/#tag:bridge","title":"Bridge","text":"<ul> <li>            Isaac Holoscan Bridge (latest)          </li> </ul>"},{"location":"tags/#tag:bundle","title":"Bundle","text":"<ul> <li>            MonaiBundleInferenceOperator (latest)          </li> </ul>"},{"location":"tags/#tag:cuda","title":"CUDA","text":"<ul> <li>            CUDA MPS Tutorial (latest)          </li> <li>            Deploying Llama-2 70b model on the edge with IGX Orin (latest)          </li> </ul>"},{"location":"tags/#tag:cuda-holoviz-integration","title":"CUDA Holoviz Integration","text":"<ul> <li>            Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks (latest)          </li> </ul>"},{"location":"tags/#tag:cv-cuda","title":"CV CUDA","text":"<ul> <li>            Endoscopy Depth Estimation (latest)          </li> <li>            Integrate External Libraries into a Holoscan Pipeline (latest)          </li> <li>            Laser Detection (latest)          </li> <li>            Simple CV-CUDA (latest)          </li> <li>            cvcuda_holoscan_interop (latest)          </li> </ul>"},{"location":"tags/#tag:camera","title":"Camera","text":"<ul> <li>            EVT Camera Calibration (latest)          </li> <li>            Intel RealSense Camera Visualizer (latest)          </li> <li>            Laser Detection (latest)          </li> <li>            SAM 2: Segment Anything in Images and Videos (latest)          </li> <li>            Stereo Vision (latest)          </li> <li>            USB Camera Calibration (latest)          </li> <li>            VPI Stereo Vision (latest)          </li> <li>            WebRTC Video Client (latest)          </li> <li>            Yolo Object Detection (latest)          </li> <li>            aja_source (latest)          </li> <li>            apriltag_detector (latest)          </li> <li>            emergent_source (latest)          </li> <li>            videomaster (latest)          </li> <li>            yuan_qcap (latest)          </li> </ul>"},{"location":"tags/#tag:cardiac-keypoints-detection","title":"Cardiac Keypoints Detection","text":"<ul> <li>            Multi-AI Ultrasound (latest)          </li> <li>            Multi-AI Ultrasound (latest)          </li> </ul>"},{"location":"tags/#tag:clara-viz","title":"Clara Viz","text":"<ul> <li>            ClaraVizOperator (latest)          </li> </ul>"},{"location":"tags/#tag:classification","title":"Classification","text":"<ul> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Multi-AI Ultrasound (latest)          </li> <li>            Multi-AI Ultrasound (latest)          </li> </ul>"},{"location":"tags/#tag:cloud","title":"Cloud","text":"<ul> <li>            Holoscan Playground on AWS (latest)          </li> </ul>"},{"location":"tags/#tag:colonoscopy","title":"Colonoscopy","text":"<ul> <li>            Polyp Detection (latest)          </li> </ul>"},{"location":"tags/#tag:communications","title":"Communications","text":"<ul> <li>            Basic Networking Ping (latest)          </li> <li>            WebRTC Video Server (latest)          </li> </ul>"},{"location":"tags/#tag:computer-vision-and-perception","title":"Computer Vision and Perception","text":"<ul> <li>            AJA Video Capture (latest)          </li> <li>            Body Pose Estimation (latest)          </li> <li>            Depth Anything V2 (latest)          </li> <li>            Distributed Endoscopy Tool Tracking with gRPC Streaming (latest)          </li> <li>            EVT Camera Calibration (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks (latest)          </li> <li>            GPU-Accelerated Orthorectification with NVIDIA OptiX (latest)          </li> <li>            Holoviz HDR (latest)          </li> <li>            Holoviz UI (latest)          </li> <li>            Holoviz YUV (latest)          </li> <li>            Holoviz sRGB (latest)          </li> <li>            Holoviz vsync (latest)          </li> <li>            Image Processing with MATLAB GPU Coder (latest)          </li> <li>            Intel RealSense Camera Visualizer (latest)          </li> <li>            Laser Detection (latest)          </li> <li>            NVIDIA NV-CLIP NIM (latest)          </li> <li>            Object Detection using PyTorch Faster R-CNN (latest)          </li> <li>            Orsi Academy In-Out Body Detection and Surgical Video Anonymization (latest)          </li> <li>            PVA-Accelerated Image Sharpening (latest)          </li> <li>            Qt Video Replayer (latest)          </li> <li>            Real-Time Face and Text Deidentification (latest)          </li> <li>            SAM 2: Segment Anything in Images and Videos (latest)          </li> <li>            Self-Supervised Contrastive Learning for Surgical videos (latest)          </li> <li>            Simple CV-CUDA (latest)          </li> <li>            Stereo Vision (latest)          </li> <li>            TAO PeopleNet Detection Model on V4L2 Video Stream (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            USB Camera Calibration (latest)          </li> <li>            VILA Live (latest)          </li> <li>            VPI Stereo Vision (latest)          </li> <li>            WebRTC Holoviz Server (latest)          </li> <li>            Yolo Object Detection (latest)          </li> <li>            cvcuda_holoscan_interop (latest)          </li> </ul>"},{"location":"tags/#tag:connectx","title":"ConnectX","text":"<ul> <li>            0.1          </li> </ul>"},{"location":"tags/#tag:container","title":"Container","text":"<ul> <li>            Interactively Debugging a Holoscan Application (latest)          </li> </ul>"},{"location":"tags/#tag:conversion","title":"Conversion","text":"<ul> <li>            STLConversionOperator (latest)          </li> </ul>"},{"location":"tags/#tag:convert","title":"Convert","text":"<ul> <li>            Convert Depth to Screen Space (latest)          </li> </ul>"},{"location":"tags/#tag:dds","title":"DDS","text":"<ul> <li>            Body Pose Estimation (latest)          </li> <li>            DDS Video: Real-time Video Streaming with RTI Connext (latest)          </li> <li>            dds_base (latest)          </li> <li>            dds_shapes (latest)          </li> <li>            dds_video_publisher (latest)          </li> </ul>"},{"location":"tags/#tag:dicom","title":"DICOM","text":"<ul> <li>            DICOMDataLoaderOperator (latest)          </li> <li>            DICOMEncapsulatedPDFWriterOperator (latest)          </li> <li>            DICOMSegmentationWriterOperator (latest)          </li> <li>            DICOMSeriesSelectorOperator (latest)          </li> <li>            DICOMSeriesToVolumeOperator (latest)          </li> <li>            DICOMTextSRWriterOperator (latest)          </li> </ul>"},{"location":"tags/#tag:dpdk","title":"DPDK","text":"<ul> <li>            0.1          </li> <li>            Achieving High Performance Networking with Holoscan (latest)          </li> <li>            Advanced Networking Benchmark (latest)          </li> <li>            Radar Signal Processing over Network (latest)          </li> <li>            advanced_network (latest)          </li> </ul>"},{"location":"tags/#tag:data-loading","title":"Data Loading","text":"<ul> <li>            NiftiDataLoader (latest)          </li> </ul>"},{"location":"tags/#tag:deidentification","title":"Deidentification","text":"<ul> <li>            Pixelator (latest)          </li> <li>            Real-Time Face and Text Deidentification (latest)          </li> </ul>"},{"location":"tags/#tag:deltacast","title":"Deltacast","text":"<ul> <li>            Deltacast Videomaster Transmitter (latest)          </li> <li>            videomaster (latest)          </li> </ul>"},{"location":"tags/#tag:depth","title":"Depth","text":"<ul> <li>            Convert Depth to Screen Space (latest)          </li> <li>            Intel RealSense Camera Visualizer (latest)          </li> <li>            Stereo Vision (latest)          </li> <li>            VPI Stereo Vision (latest)          </li> </ul>"},{"location":"tags/#tag:depth-conversion","title":"Depth Conversion","text":"<ul> <li>            Medical Image Viewer in XR (latest)          </li> </ul>"},{"location":"tags/#tag:detection","title":"Detection","text":"<ul> <li>            EVT Camera Calibration (latest)          </li> <li>            Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks (latest)          </li> <li>            Laser Detection (latest)          </li> <li>            Medical Image Viewer in XR (latest)          </li> <li>            Object Detection using PyTorch Faster R-CNN (latest)          </li> <li>            Orsi Academy In-Out Body Detection and Surgical Video Anonymization (latest)          </li> <li>            Polyp Detection (latest)          </li> <li>            Real-Time AI End-to-End Surgical Video Workflow (latest)          </li> <li>            Real-Time Face and Text Deidentification (latest)          </li> <li>            SSD Detection for Endoscopy Tools (latest)          </li> <li>            Simple Radar Pipeline (latest)          </li> <li>            Simple Radar Pipeline (latest)          </li> <li>            TAO PeopleNet Detection Model on V4L2 Video Stream (latest)          </li> <li>            USB Camera Calibration (latest)          </li> </ul>"},{"location":"tags/#tag:distributed","title":"Distributed","text":"<ul> <li>            Advanced Networking Benchmark (latest)          </li> <li>            Basic Networking Ping (latest)          </li> <li>            Body Pose Estimation (latest)          </li> <li>            Creating Multi-Node Holoscan Applications (latest)          </li> <li>            Distributed Endoscopy Tool Tracking with gRPC Streaming (latest)          </li> <li>            Distributed H.264 Endoscopy Tool Tracking (latest)          </li> <li>            Distributed H.264 Endoscopy Tool Tracking with gRPC Streaming (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            FHIR Client for Retrieving and Posting FHIR Resources (latest)          </li> <li>            Power Spectral Density with cuNumeric (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> </ul>"},{"location":"tags/#tag:drone","title":"Drone","text":"<ul> <li>            GPU-Accelerated Orthorectification with NVIDIA OptiX (latest)          </li> </ul>"},{"location":"tags/#tag:edge-accelerated-inference","title":"Edge Accelerated Inference","text":"<ul> <li>            Depth Anything V2 (latest)          </li> </ul>"},{"location":"tags/#tag:encoder","title":"Encoder","text":"<ul> <li>            video_encoder_request (latest)          </li> </ul>"},{"location":"tags/#tag:endoscopy","title":"Endoscopy","text":"<ul> <li>            Colonoscopy Polyp Segmentation (latest)          </li> <li>            Deltacast Videomaster Transmitter (latest)          </li> <li>            Distributed H.264 Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Depth Estimation (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Endoscopy Tool Segmentation from MONAI Model Zoo (latest)          </li> <li>            H.264 Endoscopy Tool Tracking (latest)          </li> <li>            H.264 Video Decode (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> <li>            Hyperspectral Image Segmentation (latest)          </li> <li>            Multi AI SSD Detection and MONAI Endoscopic Tool Segmentation (latest)          </li> <li>            OpenIGTLink 3D Slicer: Bidirectional Video Streaming with AI Segmentation (latest)          </li> <li>            Orsi Academy Surgical Tool Segmentation and AR Overlay (latest)          </li> <li>            PVA-Accelerated Image Sharpening (latest)          </li> <li>            ProHawk Video Replayer (latest)          </li> <li>            Simple CV-CUDA (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> </ul>"},{"location":"tags/#tag:extended-reality","title":"Extended Reality","text":"<ul> <li>            Holoscan XR (latest)          </li> <li>            Medical Image Viewer in XR (latest)          </li> <li>            Orsi Academy Multi AI and AR Visualization (latest)          </li> <li>            Orsi Academy Surgical Tool Segmentation and AR Overlay (latest)          </li> <li>            XR + Holoviz (latest)          </li> <li>            XR Basic Rendering Operator (latest)          </li> <li>            XR Demo (latest)          </li> <li>            XrBeginFrameOp (latest)          </li> <li>            XrEndFrameOp (latest)          </li> <li>            XrFrameOp (latest)          </li> <li>            XrTransformControlOp (latest)          </li> <li>            XrTransformOp (latest)          </li> <li>            XrTransformRenderOp (latest)          </li> </ul>"},{"location":"tags/#tag:fm-demodulation","title":"FM demodulation","text":"<ul> <li>            FM Radio Automatic Speech Recognition (latest)          </li> </ul>"},{"location":"tags/#tag:gpudirect","title":"GPUDirect","text":"<ul> <li>            0.1          </li> <li>            Achieving High Performance Networking with Holoscan (latest)          </li> <li>            Advanced Networking Benchmark (latest)          </li> <li>            Deltacast Videomaster Transmitter (latest)          </li> <li>            GPU Direct Storage on IGX (latest)          </li> <li>            Radar Signal Processing over Network (latest)          </li> <li>            VITA 49 Power Spectral Density (PSD) (latest)          </li> <li>            advanced_network (latest)          </li> </ul>"},{"location":"tags/#tag:gpunetio","title":"GPUNetIO","text":"<ul> <li>            0.1          </li> <li>            Achieving High Performance Networking with Holoscan (latest)          </li> </ul>"},{"location":"tags/#tag:hpc","title":"HPC","text":"<ul> <li>            0.1          </li> <li>            Achieving High Performance Networking with Holoscan (latest)          </li> </ul>"},{"location":"tags/#tag:hardware-accelerated-decode","title":"Hardware Accelerated Decode","text":"<ul> <li>            H.264 Video Decode (latest)          </li> </ul>"},{"location":"tags/#tag:healthcare-ai","title":"Healthcare AI","text":"<ul> <li>            Colonoscopy Polyp Segmentation (latest)          </li> <li>            Deltacast Videomaster Transmitter (latest)          </li> <li>            Distributed Endoscopy Tool Tracking with gRPC Streaming (latest)          </li> <li>            Distributed H.264 Endoscopy Tool Tracking (latest)          </li> <li>            Distributed H.264 Endoscopy Tool Tracking with gRPC Streaming (latest)          </li> <li>            EHR Agent Framework (latest)          </li> <li>            Endoscopy Depth Estimation (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Endoscopy Tool Segmentation from MONAI Model Zoo (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            FHIR Client for Retrieving and Posting FHIR Resources (latest)          </li> <li>            H.264 Endoscopy Tool Tracking (latest)          </li> <li>            H.264 Video Decode (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> <li>            Hyperspectral Image Segmentation (latest)          </li> <li>            Imaging AI Whole Body Segmentation (latest)          </li> <li>            Medical Imaging Segmentation with NVIDIA Vista-3D NIM (latest)          </li> <li>            Multi AI SSD Detection and MONAI Endoscopic Tool Segmentation (latest)          </li> <li>            Multi-AI Ultrasound (latest)          </li> <li>            Multi-AI Ultrasound (latest)          </li> <li>            OpenIGTLink 3D Slicer: Bidirectional Video Streaming with AI Segmentation (latest)          </li> <li>            Orsi Academy In-Out Body Detection and Surgical Video Anonymization (latest)          </li> <li>            Orsi Academy Multi AI and AR Visualization (latest)          </li> <li>            Orsi Academy Surgical Tool Segmentation and AR Overlay (latest)          </li> <li>            ProHawk Video Replayer (latest)          </li> <li>            Real-time Riva ASR to local-LLM (latest)          </li> <li>            SSD Detection for Endoscopy Tools (latest)          </li> <li>            Self-Supervised Contrastive Learning for Surgical videos (latest)          </li> <li>            Speech-to-text + Large Language Model (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            Ultrasound Beamforming with MATLAB GPU Coder (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            Volume rendering using ClaraViz (latest)          </li> </ul>"},{"location":"tags/#tag:healthcare-interop","title":"Healthcare Interop","text":"<ul> <li>            DICOM to OpenUSD mesh segmentation with MONAI Deploy and Holoscan (latest)          </li> <li>            EHR Agent Framework (latest)          </li> <li>            EHR Query LLM (latest)          </li> <li>            FHIR Client for Retrieving and Posting FHIR Resources (latest)          </li> <li>            FhirClientOperator (latest)          </li> <li>            FhirResourceSanitizerOp (latest)          </li> <li>            Imaging AI Whole Body Segmentation (latest)          </li> <li>            Medical Imaging (latest)          </li> <li>            ZeroMQPublisherOp (latest)          </li> <li>            ZeroMQSubscriberOp (latest)          </li> </ul>"},{"location":"tags/#tag:holoscan","title":"Holoscan","text":"<ul> <li>            Integrate External Libraries into a Holoscan Pipeline (latest)          </li> </ul>"},{"location":"tags/#tag:holoviz","title":"Holoviz","text":"<ul> <li>            Colonoscopy Polyp Segmentation (latest)          </li> <li>            Depth Anything V2 (latest)          </li> <li>            Distributed H.264 Endoscopy Tool Tracking (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> <li>            Holoviz HDR (latest)          </li> <li>            Holoviz UI (latest)          </li> <li>            Holoviz YUV (latest)          </li> <li>            Holoviz sRGB (latest)          </li> <li>            Holoviz vsync (latest)          </li> <li>            Image Processing with MATLAB GPU Coder (latest)          </li> <li>            Intel RealSense Camera Visualizer (latest)          </li> <li>            Medical Image Viewer in XR (latest)          </li> <li>            Object Detection using PyTorch Faster R-CNN (latest)          </li> <li>            ProHawk Video Replayer (latest)          </li> <li>            Streaming Synthetic Aperture Radar (latest)          </li> <li>            TAO PeopleNet Detection Model on V4L2 Video Stream (latest)          </li> <li>            USB Camera Calibration (latest)          </li> <li>            WebRTC Holoviz Server (latest)          </li> <li>            XR Demo (latest)          </li> </ul>"},{"location":"tags/#tag:hounsfield-scale-transfer-functions","title":"Hounsfield Scale Transfer Functions","text":"<ul> <li>            Volume rendering using ClaraViz (latest)          </li> </ul>"},{"location":"tags/#tag:huggingface","title":"Huggingface","text":"<ul> <li>            Deploying Llama-2 70b model on the edge with IGX Orin (latest)          </li> </ul>"},{"location":"tags/#tag:human-body-pose-estimation","title":"Human Body Pose Estimation","text":"<ul> <li>            Body Pose Estimation (latest)          </li> </ul>"},{"location":"tags/#tag:hyperspectral-imaging","title":"Hyperspectral Imaging","text":"<ul> <li>            Hyperspectral Image Segmentation (latest)          </li> </ul>"},{"location":"tags/#tag:ip","title":"IP","text":"<ul> <li>            advanced_network (latest)          </li> <li>            basic_network (latest)          </li> </ul>"},{"location":"tags/#tag:image-conversion","title":"Image Conversion","text":"<ul> <li>            PNGConverterOperator (latest)          </li> </ul>"},{"location":"tags/#tag:image-processing","title":"Image Processing","text":"<ul> <li>            EVT Camera Calibration (latest)          </li> <li>            GPU-Accelerated Orthorectification with NVIDIA OptiX (latest)          </li> <li>            Holoviz YUV (latest)          </li> <li>            Holoviz vsync (latest)          </li> <li>            Hyperspectral Image Segmentation (latest)          </li> <li>            Image Processing with MATLAB GPU Coder (latest)          </li> <li>            NVIDIA NV-CLIP NIM (latest)          </li> <li>            Pixelator (latest)          </li> <li>            Real-Time Face and Text Deidentification (latest)          </li> <li>            Stereo Vision (latest)          </li> <li>            USB Camera Calibration (latest)          </li> </ul>"},{"location":"tags/#tag:imaging","title":"Imaging","text":"<ul> <li>            Imaging AI Whole Body Segmentation (latest)          </li> <li>            Medical Imaging Segmentation with NVIDIA Vista-3D NIM (latest)          </li> <li>            Streaming Synthetic Aperture Radar (latest)          </li> </ul>"},{"location":"tags/#tag:inference","title":"Inference","text":"<ul> <li>            InferenceOperator (latest)          </li> <li>            MonaiBundleInferenceOperator (latest)          </li> <li>            MonaiSegInferenceOperator (latest)          </li> </ul>"},{"location":"tags/#tag:isaac-sim","title":"Isaac Sim","text":"<ul> <li>            Isaac Holoscan Bridge (latest)          </li> </ul>"},{"location":"tags/#tag:llm","title":"LLM","text":"<ul> <li>            Chat with NVIDIA NIM (latest)          </li> <li>            Deploying Llama-2 70b model on the edge with IGX Orin (latest)          </li> <li>            EHR Query LLM (latest)          </li> <li>            FHIR Client for Retrieving and Posting FHIR Resources (latest)          </li> <li>            FhirClientOperator (latest)          </li> <li>            FhirResourceSanitizerOp (latest)          </li> <li>            HoloChat (latest)          </li> <li>            Real-time Riva ASR to local-LLM (latest)          </li> <li>            Speech-to-text + Large Language Model (latest)          </li> <li>            VILA Live (latest)          </li> <li>            ZeroMQPublisherOp (latest)          </li> <li>            ZeroMQSubscriberOp (latest)          </li> </ul>"},{"location":"tags/#tag:lstm","title":"LSTM","text":"<ul> <li>            Distributed Endoscopy Tool Tracking with gRPC Streaming (latest)          </li> <li>            Distributed H.264 Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            lstm_tensor_rt_inference (latest)          </li> </ul>"},{"location":"tags/#tag:large-vision-model","title":"Large Vision Model","text":"<ul> <li>            VILA Live (latest)          </li> </ul>"},{"location":"tags/#tag:learning","title":"Learning","text":"<ul> <li>            Self-Supervised Contrastive Learning for Surgical videos (latest)          </li> </ul>"},{"location":"tags/#tag:lidar","title":"Lidar","text":"<ul> <li>            Velodyne VLP-16 Lidar Operator (latest)          </li> <li>            Velodyne VLP-16 Lidar Viewer (latest)          </li> </ul>"},{"location":"tags/#tag:loader","title":"Loader","text":"<ul> <li>            DICOMDataLoaderOperator (latest)          </li> </ul>"},{"location":"tags/#tag:monai","title":"MONAI","text":"<ul> <li>            DICOM to OpenUSD mesh segmentation with MONAI Deploy and Holoscan (latest)          </li> <li>            Medical Imaging (latest)          </li> <li>            MonaiBundleInferenceOperator (latest)          </li> <li>            MonaiSegInferenceOperator (latest)          </li> <li>            Real-Time AI End-to-End Surgical Video Workflow (latest)          </li> </ul>"},{"location":"tags/#tag:mps","title":"MPS","text":"<ul> <li>            CUDA MPS Tutorial (latest)          </li> </ul>"},{"location":"tags/#tag:medical-imaging","title":"Medical Imaging","text":"<ul> <li>            ClaraVizOperator (latest)          </li> <li>            DICOMDataLoaderOperator (latest)          </li> <li>            DICOMEncapsulatedPDFWriterOperator (latest)          </li> <li>            DICOMSegmentationWriterOperator (latest)          </li> <li>            DICOMSeriesSelectorOperator (latest)          </li> <li>            DICOMSeriesToVolumeOperator (latest)          </li> <li>            DICOMTextSRWriterOperator (latest)          </li> <li>            Imaging AI Whole Body Segmentation (latest)          </li> <li>            InferenceOperator (latest)          </li> <li>            Medical Imaging (latest)          </li> <li>            Medical Imaging Segmentation with NVIDIA Vista-3D NIM (latest)          </li> <li>            MonaiBundleInferenceOperator (latest)          </li> <li>            MonaiSegInferenceOperator (latest)          </li> <li>            NiftiDataLoader (latest)          </li> <li>            PNGConverterOperator (latest)          </li> <li>            PublisherOperator (latest)          </li> <li>            Real-time Riva ASR to local-LLM (latest)          </li> <li>            STLConversionOperator (latest)          </li> <li>            Speech-to-text + Large Language Model (latest)          </li> </ul>"},{"location":"tags/#tag:monocular-depth-estimation","title":"Monocular Depth Estimation","text":"<ul> <li>            Depth Anything V2 (latest)          </li> <li>            Endoscopy Depth Estimation (latest)          </li> </ul>"},{"location":"tags/#tag:multimodal-model","title":"Multimodal Model","text":"<ul> <li>            Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks (latest)          </li> <li>            NVIDIA NV-CLIP NIM (latest)          </li> <li>            VILA Live (latest)          </li> </ul>"},{"location":"tags/#tag:nic","title":"NIC","text":"<ul> <li>            0.1          </li> </ul>"},{"location":"tags/#tag:nifti","title":"NIfTI","text":"<ul> <li>            NiftiDataLoader (latest)          </li> </ul>"},{"location":"tags/#tag:npp","title":"NPP","text":"<ul> <li>            npp_filter (latest)          </li> </ul>"},{"location":"tags/#tag:nrrd-processing","title":"NRRD processing","text":"<ul> <li>            Medical Imaging Segmentation with NVIDIA Vista-3D NIM (latest)          </li> </ul>"},{"location":"tags/#tag:natural-language-and-conversational-ai","title":"Natural Language and Conversational AI","text":"<ul> <li>            Chat with NVIDIA NIM (latest)          </li> <li>            Deploying Llama-2 70b model on the edge with IGX Orin (latest)          </li> <li>            HoloChat (latest)          </li> </ul>"},{"location":"tags/#tag:networking","title":"Networking","text":"<ul> <li>            0.1          </li> </ul>"},{"location":"tags/#tag:networking-and-distributed-computing","title":"Networking and Distributed Computing","text":"<ul> <li>            Achieving High Performance Networking with Holoscan (latest)          </li> <li>            Advanced Networking Benchmark (latest)          </li> <li>            Basic Networking Ping (latest)          </li> <li>            DDS Video: Real-time Video Streaming with RTI Connext (latest)          </li> <li>            Endoscopy Depth Estimation (latest)          </li> <li>            OpenIGTLink 3D Slicer: Bidirectional Video Streaming with AI Segmentation (latest)          </li> <li>            Radar Signal Processing over Network (latest)          </li> <li>            VITA 49 Power Spectral Density (PSD) (latest)          </li> <li>            WebRTC Holoviz Server (latest)          </li> <li>            WebRTC Video Client (latest)          </li> <li>            WebRTC Video Server (latest)          </li> <li>            advanced_network (latest)          </li> <li>            basic_network (latest)          </li> </ul>"},{"location":"tags/#tag:opencv","title":"OpenCV","text":"<ul> <li>            Integrate External Libraries into a Holoscan Pipeline (latest)          </li> </ul>"},{"location":"tags/#tag:openusd","title":"OpenUSD","text":"<ul> <li>            DICOM to OpenUSD mesh segmentation with MONAI Deploy and Holoscan (latest)          </li> <li>            mesh_to_usd (latest)          </li> </ul>"},{"location":"tags/#tag:optimization","title":"Optimization","text":"<ul> <li>            Holoscan SDK Response Time Analysis (latest)          </li> <li>            Image Processing with MATLAB GPU Coder (latest)          </li> <li>            PVA-Accelerated Image Sharpening (latest)          </li> </ul>"},{"location":"tags/#tag:optix","title":"Optix","text":"<ul> <li>            GPU-Accelerated Orthorectification with NVIDIA OptiX (latest)          </li> </ul>"},{"location":"tags/#tag:pdf","title":"PDF","text":"<ul> <li>            DICOMEncapsulatedPDFWriterOperator (latest)          </li> </ul>"},{"location":"tags/#tag:png","title":"PNG","text":"<ul> <li>            PNGConverterOperator (latest)          </li> </ul>"},{"location":"tags/#tag:psd","title":"PSD","text":"<ul> <li>            Power Spectral Density with cuNumeric (latest)          </li> <li>            VITA 49 Power Spectral Density (PSD) (latest)          </li> </ul>"},{"location":"tags/#tag:pva","title":"PVA","text":"<ul> <li>            PVA-Accelerated Image Sharpening (latest)          </li> </ul>"},{"location":"tags/#tag:performance","title":"Performance","text":"<ul> <li>            Holoscan Release Benchmarking (latest)          </li> <li>            Holoscan SDK Response Time Analysis (latest)          </li> </ul>"},{"location":"tags/#tag:point-cloud","title":"Point Cloud","text":"<ul> <li>            Velodyne VLP-16 Lidar Operator (latest)          </li> <li>            Velodyne VLP-16 Lidar Viewer (latest)          </li> </ul>"},{"location":"tags/#tag:polyphase-resampling","title":"Polyphase Resampling","text":"<ul> <li>            FM Radio Automatic Speech Recognition (latest)          </li> </ul>"},{"location":"tags/#tag:processing","title":"Processing","text":"<ul> <li>            Isaac Holoscan Bridge (latest)          </li> </ul>"},{"location":"tags/#tag:publisher","title":"Publisher","text":"<ul> <li>            PublisherOperator (latest)          </li> </ul>"},{"location":"tags/#tag:qt","title":"Qt","text":"<ul> <li>            Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks (latest)          </li> <li>            Qt Video Replayer (latest)          </li> <li>            qt_video (latest)          </li> </ul>"},{"location":"tags/#tag:quantized-model-inference","title":"Quantized Model Inference","text":"<ul> <li>            Real-time Riva ASR to local-LLM (latest)          </li> </ul>"},{"location":"tags/#tag:quantum-computing","title":"Quantum Computing","text":"<ul> <li>            CUDA Quantum Variational Quantum Eigensolver (VQE) (latest)          </li> </ul>"},{"location":"tags/#tag:rag","title":"RAG","text":"<ul> <li>            EHR Agent Framework (latest)          </li> <li>            HoloChat (latest)          </li> </ul>"},{"location":"tags/#tag:rdma","title":"RDMA","text":"<ul> <li>            0.1          </li> <li>            Achieving High Performance Networking with Holoscan (latest)          </li> <li>            Deltacast Videomaster Transmitter (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> <li>            advanced_network (latest)          </li> </ul>"},{"location":"tags/#tag:rt-detr","title":"RT-DETR","text":"<ul> <li>            Polyp Detection (latest)          </li> </ul>"},{"location":"tags/#tag:rti-connext","title":"RTI Connext","text":"<ul> <li>            Body Pose Estimation (latest)          </li> <li>            DDS Video: Real-time Video Streaming with RTI Connext (latest)          </li> <li>            dds_base (latest)          </li> <li>            dds_shapes (latest)          </li> <li>            dds_video_publisher (latest)          </li> </ul>"},{"location":"tags/#tag:real-time","title":"Real-Time","text":"<ul> <li>            Holoscan SDK Response Time Analysis (latest)          </li> </ul>"},{"location":"tags/#tag:rendering","title":"Rendering","text":"<ul> <li>            Endoscopy Depth Estimation (latest)          </li> <li>            Holoviz UI (latest)          </li> <li>            Holoviz sRGB (latest)          </li> <li>            Holoviz vsync (latest)          </li> <li>            XR + Holoviz (latest)          </li> <li>            volume_renderer (latest)          </li> </ul>"},{"location":"tags/#tag:rivermax","title":"Rivermax","text":"<ul> <li>            0.1          </li> </ul>"},{"location":"tags/#tag:robotics","title":"Robotics","text":"<ul> <li>            Isaac Holoscan Bridge (latest)          </li> </ul>"},{"location":"tags/#tag:sdp-exchange","title":"SDP Exchange","text":"<ul> <li>            WebRTC Video Client (latest)          </li> </ul>"},{"location":"tags/#tag:sr","title":"SR","text":"<ul> <li>            DICOMTextSRWriterOperator (latest)          </li> </ul>"},{"location":"tags/#tag:ssd","title":"SSD","text":"<ul> <li>            Multi AI SSD Detection and MONAI Endoscopic Tool Segmentation (latest)          </li> <li>            Real-Time AI End-to-End Surgical Video Workflow (latest)          </li> <li>            SSD Detection for Endoscopy Tools (latest)          </li> </ul>"},{"location":"tags/#tag:st2084","title":"ST2084","text":"<ul> <li>            Holoviz HDR (latest)          </li> </ul>"},{"location":"tags/#tag:stl","title":"STL","text":"<ul> <li>            DICOM to OpenUSD mesh segmentation with MONAI Deploy and Holoscan (latest)          </li> <li>            Medical Imaging (latest)          </li> <li>            STLConversionOperator (latest)          </li> <li>            mesh_to_usd (latest)          </li> </ul>"},{"location":"tags/#tag:segmentation","title":"Segmentation","text":"<ul> <li>            Colonoscopy Polyp Segmentation (latest)          </li> <li>            DICOMSegmentationWriterOperator (latest)          </li> <li>            Endoscopy Tool Segmentation from MONAI Model Zoo (latest)          </li> <li>            Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks (latest)          </li> <li>            Hyperspectral Image Segmentation (latest)          </li> <li>            Medical Imaging Segmentation with NVIDIA Vista-3D NIM (latest)          </li> <li>            MonaiSegInferenceOperator (latest)          </li> <li>            Multi AI SSD Detection and MONAI Endoscopic Tool Segmentation (latest)          </li> <li>            OpenIGTLink 3D Slicer: Bidirectional Video Streaming with AI Segmentation (latest)          </li> <li>            Orsi Academy Multi AI and AR Visualization (latest)          </li> <li>            Orsi Academy Surgical Tool Segmentation and AR Overlay (latest)          </li> <li>            Real-Time AI End-to-End Surgical Video Workflow (latest)          </li> <li>            SAM 2: Segment Anything in Images and Videos (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            Volume rendering using ClaraViz (latest)          </li> </ul>"},{"location":"tags/#tag:selector","title":"Selector","text":"<ul> <li>            DICOMSeriesSelectorOperator (latest)          </li> </ul>"},{"location":"tags/#tag:sensor","title":"Sensor","text":"<ul> <li>            Isaac Holoscan Bridge (latest)          </li> </ul>"},{"location":"tags/#tag:signal-processing","title":"Signal Processing","text":"<ul> <li>            Basic Pulse Description Word (PDW) Generator (latest)          </li> <li>            FM Radio Automatic Speech Recognition (latest)          </li> <li>            Power Spectral Density with cuNumeric (latest)          </li> <li>            Radar Signal Processing over Network (latest)          </li> <li>            Simple Radar Pipeline (latest)          </li> <li>            Simple Radar Pipeline (latest)          </li> <li>            Software Defined Radio FM Demodulation (latest)          </li> <li>            Streaming Synthetic Aperture Radar (latest)          </li> <li>            Ultrasound Beamforming with MATLAB GPU Coder (latest)          </li> <li>            VITA 49 Power Spectral Density (PSD) (latest)          </li> <li>            Velodyne VLP-16 Lidar Viewer (latest)          </li> <li>            data_writer (latest)          </li> <li>            fft (latest)          </li> <li>            high_rate_psd (latest)          </li> <li>            low_rate_psd (latest)          </li> <li>            vita49_psd_packetizer (latest)          </li> </ul>"},{"location":"tags/#tag:stereo-vision","title":"Stereo Vision","text":"<ul> <li>            Stereo Vision (latest)          </li> <li>            VPI Stereo Vision (latest)          </li> <li>            XR + Holoviz (latest)          </li> <li>            XR Demo (latest)          </li> </ul>"},{"location":"tags/#tag:streaming","title":"Streaming","text":"<ul> <li>            AJA Video Capture (latest)          </li> <li>            openigtlink (latest)          </li> </ul>"},{"location":"tags/#tag:surgical-ai","title":"Surgical AI","text":"<ul> <li>            Distributed H.264 Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Segmentation from MONAI Model Zoo (latest)          </li> <li>            H.264 Endoscopy Tool Tracking (latest)          </li> <li>            Multi AI SSD Detection and MONAI Endoscopic Tool Segmentation (latest)          </li> <li>            Orsi Academy Surgical Tool Segmentation and AR Overlay (latest)          </li> <li>            Self-Supervised Contrastive Learning for Surgical videos (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> </ul>"},{"location":"tags/#tag:synthetic-aperture-beamforming","title":"Synthetic Aperture Beamforming","text":"<ul> <li>            Streaming Synthetic Aperture Radar (latest)          </li> <li>            Ultrasound Beamforming with MATLAB GPU Coder (latest)          </li> </ul>"},{"location":"tags/#tag:tcp","title":"TCP","text":"<ul> <li>            basic_network (latest)          </li> </ul>"},{"location":"tags/#tag:tensor","title":"Tensor","text":"<ul> <li>            tensor_to_video_buffer (latest)          </li> </ul>"},{"location":"tags/#tag:tensor-optimization","title":"Tensor Optimization","text":"<ul> <li>            Body Pose Estimation (latest)          </li> <li>            Simple CV-CUDA (latest)          </li> <li>            Yolo Object Detection (latest)          </li> </ul>"},{"location":"tags/#tag:tensorrt","title":"TensorRT","text":"<ul> <li>            Distributed H.264 Endoscopy Tool Tracking (latest)          </li> <li>            lstm_tensor_rt_inference (latest)          </li> </ul>"},{"location":"tags/#tag:text","title":"Text","text":"<ul> <li>            DICOMTextSRWriterOperator (latest)          </li> </ul>"},{"location":"tags/#tag:tools-and-other-specialized-applications","title":"Tools And Other Specialized Applications","text":"<ul> <li>            CUDA Quantum Variational Quantum Eigensolver (VQE) (latest)          </li> </ul>"},{"location":"tags/#tag:totalsegmentator","title":"TotalSegmentator","text":"<ul> <li>            Imaging AI Whole Body Segmentation (latest)          </li> <li>            Volume rendering using ClaraViz (latest)          </li> </ul>"},{"location":"tags/#tag:udp","title":"UDP","text":"<ul> <li>            Advanced Networking Benchmark (latest)          </li> <li>            Basic Networking Ping (latest)          </li> <li>            Basic Pulse Description Word (PDW) Generator (latest)          </li> <li>            VITA 49 Power Spectral Density (PSD) (latest)          </li> <li>            Velodyne VLP-16 Lidar Viewer (latest)          </li> <li>            advanced_network (latest)          </li> <li>            basic_network (latest)          </li> </ul>"},{"location":"tags/#tag:ui","title":"UI","text":"<ul> <li>            Adding a GUI to Holoscan Python Applications (latest)          </li> <li>            Qt Video Replayer (latest)          </li> <li>            qt_video (latest)          </li> </ul>"},{"location":"tags/#tag:ultrasound","title":"Ultrasound","text":"<ul> <li>            Multi-AI Ultrasound (latest)          </li> <li>            Multi-AI Ultrasound (latest)          </li> <li>            OpenIGTLink 3D Slicer: Bidirectional Video Streaming with AI Segmentation (latest)          </li> <li>            Ultrasound Beamforming with MATLAB GPU Coder (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> </ul>"},{"location":"tags/#tag:vs-code","title":"VS Code","text":"<ul> <li>            VSCode Dev Container for Holoscan (latest)          </li> </ul>"},{"location":"tags/#tag:vtk","title":"VTK","text":"<ul> <li>            Orsi Academy Multi AI and AR Visualization (latest)          </li> <li>            Orsi Academy Surgical Tool Segmentation and AR Overlay (latest)          </li> </ul>"},{"location":"tags/#tag:vector-database","title":"Vector Database","text":"<ul> <li>            EHR Agent Framework (latest)          </li> <li>            HoloChat (latest)          </li> </ul>"},{"location":"tags/#tag:video","title":"Video","text":"<ul> <li>            AJA Video Capture (latest)          </li> <li>            Body Pose Estimation (latest)          </li> <li>            Colonoscopy Polyp Segmentation (latest)          </li> <li>            DDS Video: Real-time Video Streaming with RTI Connext (latest)          </li> <li>            Deltacast Videomaster Transmitter (latest)          </li> <li>            Depth Anything V2 (latest)          </li> <li>            Distributed Endoscopy Tool Tracking with gRPC Streaming (latest)          </li> <li>            Distributed H.264 Endoscopy Tool Tracking (latest)          </li> <li>            Distributed H.264 Endoscopy Tool Tracking with gRPC Streaming (latest)          </li> <li>            Endoscopy Depth Estimation (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Endoscopy Tool Segmentation from MONAI Model Zoo (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks (latest)          </li> <li>            H.264 Endoscopy Tool Tracking (latest)          </li> <li>            H.264 Video Decode (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> <li>            Holoviz YUV (latest)          </li> <li>            Image Processing with MATLAB GPU Coder (latest)          </li> <li>            Laser Detection (latest)          </li> <li>            Multi AI SSD Detection and MONAI Endoscopic Tool Segmentation (latest)          </li> <li>            Multi-AI Ultrasound (latest)          </li> <li>            Multi-AI Ultrasound (latest)          </li> <li>            Object Detection using PyTorch Faster R-CNN (latest)          </li> <li>            OpenIGTLink 3D Slicer: Bidirectional Video Streaming with AI Segmentation (latest)          </li> <li>            Orsi Academy In-Out Body Detection and Surgical Video Anonymization (latest)          </li> <li>            PVA-Accelerated Image Sharpening (latest)          </li> <li>            ProHawk Video Replayer (latest)          </li> <li>            Qt Video Replayer (latest)          </li> <li>            Real-Time Face and Text Deidentification (latest)          </li> <li>            SAM 2: Segment Anything in Images and Videos (latest)          </li> <li>            SSD Detection for Endoscopy Tools (latest)          </li> <li>            Self-Supervised Contrastive Learning for Surgical videos (latest)          </li> <li>            Simple CV-CUDA (latest)          </li> <li>            Stereo Vision (latest)          </li> <li>            TAO PeopleNet Detection Model on V4L2 Video Stream (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            VILA Live (latest)          </li> <li>            VPI Stereo Vision (latest)          </li> <li>            WebRTC Video Client (latest)          </li> <li>            WebRTC Video Server (latest)          </li> <li>            Yolo Object Detection (latest)          </li> <li>            dds_video_publisher (latest)          </li> <li>            prohawk_video_processing (latest)          </li> <li>            qt_video (latest)          </li> <li>            tensor_to_video_buffer (latest)          </li> <li>            video_encoder_request (latest)          </li> <li>            visualizer_icardio (latest)          </li> <li>            webrtc_client (latest)          </li> <li>            webrtc_server (latest)          </li> </ul>"},{"location":"tags/#tag:visualization","title":"Visualization","text":"<ul> <li>            Body Pose Estimation (latest)          </li> <li>            ClaraVizOperator (latest)          </li> <li>            Colonoscopy Polyp Segmentation (latest)          </li> <li>            DDS Video: Real-time Video Streaming with RTI Connext (latest)          </li> <li>            Depth Anything V2 (latest)          </li> <li>            Distributed Endoscopy Tool Tracking with gRPC Streaming (latest)          </li> <li>            EVT Camera Calibration (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Exclusive Display Benchmark (latest)          </li> <li>            GPU-Accelerated Orthorectification with NVIDIA OptiX (latest)          </li> <li>            H.264 Endoscopy Tool Tracking (latest)          </li> <li>            Holoscan XR (latest)          </li> <li>            Holoviz HDR (latest)          </li> <li>            Holoviz UI (latest)          </li> <li>            Holoviz YUV (latest)          </li> <li>            Holoviz sRGB (latest)          </li> <li>            Holoviz vsync (latest)          </li> <li>            Hyperspectral Image Segmentation (latest)          </li> <li>            Image Processing with MATLAB GPU Coder (latest)          </li> <li>            Intel RealSense Camera Visualizer (latest)          </li> <li>            Medical Image Viewer in XR (latest)          </li> <li>            Multi-AI Ultrasound (latest)          </li> <li>            Multi-AI Ultrasound (latest)          </li> <li>            Object Detection using PyTorch Faster R-CNN (latest)          </li> <li>            OpenIGTLink 3D Slicer: Bidirectional Video Streaming with AI Segmentation (latest)          </li> <li>            Orsi Academy In-Out Body Detection and Surgical Video Anonymization (latest)          </li> <li>            PVA-Accelerated Image Sharpening (latest)          </li> <li>            Qt Video Replayer (latest)          </li> <li>            SAM 2: Segment Anything in Images and Videos (latest)          </li> <li>            SSD Detection for Endoscopy Tools (latest)          </li> <li>            Simple CV-CUDA (latest)          </li> <li>            Stereo Vision (latest)          </li> <li>            USB Camera Calibration (latest)          </li> <li>            Ultrasound Beamforming with MATLAB GPU Coder (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            VPI Stereo Vision (latest)          </li> <li>            Velodyne VLP-16 Lidar Viewer (latest)          </li> <li>            Volume rendering using ClaraViz (latest)          </li> <li>            WebRTC Holoviz Server (latest)          </li> <li>            WebRTC Video Client (latest)          </li> <li>            XR + Holoviz (latest)          </li> <li>            XR Demo (latest)          </li> <li>            Yolo Object Detection (latest)          </li> <li>            grpc_operators (latest)          </li> <li>            orsi_visualizer (latest)          </li> <li>            tool_tracking_postprocessor (latest)          </li> <li>            vtk_renderer (latest)          </li> </ul>"},{"location":"tags/#tag:volume","title":"Volume","text":"<ul> <li>            DICOMSeriesToVolumeOperator (latest)          </li> <li>            Medical Image Viewer in XR (latest)          </li> <li>            Volume rendering using ClaraViz (latest)          </li> <li>            volume_loader (latest)          </li> <li>            volume_renderer (latest)          </li> </ul>"},{"location":"tags/#tag:volumetric-reconstruction","title":"Volumetric Reconstruction","text":"<ul> <li>            Imaging AI Whole Body Segmentation (latest)          </li> </ul>"},{"location":"tags/#tag:vulkan","title":"Vulkan","text":"<ul> <li>            Holoscan XR (latest)          </li> </ul>"},{"location":"tags/#tag:webrtc","title":"WebRTC","text":"<ul> <li>            WebRTC Holoviz Server (latest)          </li> <li>            WebRTC Video Client (latest)          </li> <li>            WebRTC Video Server (latest)          </li> <li>            webrtc_client (latest)          </li> <li>            webrtc_server (latest)          </li> </ul>"},{"location":"tags/#tag:writer","title":"Writer","text":"<ul> <li>            DICOMEncapsulatedPDFWriterOperator (latest)          </li> <li>            DICOMSegmentationWriterOperator (latest)          </li> </ul>"},{"location":"tags/#tag:yolo-detection","title":"YOLO Detection","text":"<ul> <li>            Yolo Object Detection (latest)          </li> </ul>"},{"location":"tags/#tag:zeromq","title":"ZeroMQ","text":"<ul> <li>            EHR Agent Framework (latest)          </li> <li>            FHIR Client for Retrieving and Posting FHIR Resources (latest)          </li> </ul>"},{"location":"tags/#tag:bounding-box","title":"bounding box","text":"<ul> <li>            Polyp Detection (latest)          </li> </ul>"},{"location":"tags/#tag:color-space-conversion","title":"color space conversion","text":"<ul> <li>            Holoviz HDR (latest)          </li> </ul>"},{"location":"tags/#tag:converter","title":"converter","text":"<ul> <li>            orsi_format_converter (latest)          </li> </ul>"},{"location":"tags/#tag:filter-presets","title":"filter presets","text":"<ul> <li>            ProHawk Video Replayer (latest)          </li> </ul>"},{"location":"tags/#tag:format-conversion","title":"format conversion","text":"<ul> <li>            AJA Video Capture (latest)          </li> </ul>"},{"location":"tags/#tag:frame-rate-synchronization","title":"frame rate synchronization","text":"<ul> <li>            Holoviz vsync (latest)          </li> </ul>"},{"location":"tags/#tag:grpc","title":"gRPC","text":"<ul> <li>            Distributed Endoscopy Tool Tracking with gRPC Streaming (latest)          </li> <li>            Distributed H.264 Endoscopy Tool Tracking with gRPC Streaming (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            FM Radio Automatic Speech Recognition (latest)          </li> <li>            grpc_operators (latest)          </li> </ul>"},{"location":"applications/","title":"Applications","text":"<p>Holohub features a curated collection of reference applications that demonstrate the platform's capabilities across various domains, from medical imaging to industrial automation. Each application is designed to showcase best practices for integrating Holoscan's optimized libraries and microservices, ensuring high performance and low latency. Whether you are looking to streamline data processing workflows, enhance real-time analytics, or develop cutting-edge AI models, the applications in Holohub provide valuable examples and templates to accelerate your development process. </p>"},{"location":"applications/adv_networking_bench/","title":"Advanced Networking Benchmark","text":"<p> Authors: Cliff Burdick (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>[!TIP] Review the High Performance Networking tutorial for guided instructions to configure your system and test the Advanced Network library.</p> <p>This is a sample application to measure a lower bound on performance for the Advanced Network library by receiving packets, optionally doing work on them, and freeing the buffers. While only freeing the packets is an unrealistic workload, it's useful to see at a high level whether the application is able to keep up with the bare minimum amount of work to do. The application contains both a transmitter and receiver that are designed to run on different systems, and may be configured independently.</p> <p>The performance of this application depends heavily on a properly-configured system and choosing the best tuning parameters that are acceptable for the workload. To configure the system please see the documentation here. With the system tuned, the application performance will be dictated by batching size and whether GPUDirect is enabled.</p> <p>At this time both the transmitter and receiver are written to handle an Ethernet+IP+UDP packet with a configurable payload. Other modes may be added in the future. Also, for simplicity, the transmitter and receiver are configured to a single packet size.</p>","tags":["Networking and Distributed Computing","Distributed","UDP","GPUDirect","DPDK"]},{"location":"applications/adv_networking_bench/#transmit","title":"Transmit","text":"<p>The transmitter sends a UDP packet with an incrementing sequence of bytes after the UDP header. The batch size configured dictates how many packets the benchmark operator sends to the NIC in each tick. Typically with the same number of CPU cores the transmitter will run faster than the receiver, so this parameter may be used to throttle the sender somewhat by making the batches very small.</p>","tags":["Networking and Distributed Computing","Distributed","UDP","GPUDirect","DPDK"]},{"location":"applications/adv_networking_bench/#receiver","title":"Receiver","text":"<p>The receiver receives the UDP packets in either CPU-only mode, header-data split mode, or GPU-only mode. - CPU-only mode will receive the packets in CPU memory, copy the payload contents to a host-pinned staging buffer,   and free the buffers. - Header-data split mode: the user may configure separate memory regions for the header and data. The header is   sent to the CPU, and all bytes afterwards are sent to the GPU. Header-data split should achieve higher   rates than CPU mode since the amount of data to the CPU can be orders of magnitude lower compared to running   in CPU-only mode. - GPU-only mode: all bytes of the packets are received in GPU memory.</p>","tags":["Networking and Distributed Computing","Distributed","UDP","GPUDirect","DPDK"]},{"location":"applications/adv_networking_bench/#configuration","title":"Configuration","text":"<p>The application can be configured to do either Rx, Tx, or both, using different configuration files, found in this directory.</p>","tags":["Networking and Distributed Computing","Distributed","UDP","GPUDirect","DPDK"]},{"location":"applications/adv_networking_bench/#receive-configuration","title":"Receive Configuration","text":"<ul> <li><code>header_data_split</code>: bool   Turn on GPUDirect header-data split mode</li> <li><code>batch_size</code>: integer   Size in packets for a single batch. This should be a multiple of the advanced_network queue batch size.   A larger batch size consumes more memory since any work will not start unless this batch size is filled. Consider   reducing this value if errors are occurring.</li> <li><code>max_packet_size</code>: integer   Maximum packet size expected. This value includes all headers up to and including UDP.</li> </ul>","tags":["Networking and Distributed Computing","Distributed","UDP","GPUDirect","DPDK"]},{"location":"applications/adv_networking_bench/#transmit-configuration","title":"Transmit Configuration","text":"<ul> <li><code>batch_size</code>: integer   Size in packets for a single batch. This batch size is used to send to the NIC, and   will loop sending that many packets for each burst.</li> <li><code>payload_size</code>: integer   Size of the payload to send after all L2-L4 headers</li> </ul>","tags":["Networking and Distributed Computing","Distributed","UDP","GPUDirect","DPDK"]},{"location":"applications/adv_networking_bench/#requirements","title":"Requirements","text":"<p>This application requires all configuration and requirements from the Advanced Network library.</p>","tags":["Networking and Distributed Computing","Distributed","UDP","GPUDirect","DPDK"]},{"location":"applications/adv_networking_bench/#build-instructions","title":"Build Instructions","text":"<p>Please refer to the top level Holohub README.md file for information on how to build this application.</p> <pre><code>./holohub run adv_networking_bench\n</code></pre>","tags":["Networking and Distributed Computing","Distributed","UDP","GPUDirect","DPDK"]},{"location":"applications/adv_networking_bench/#run-instructions","title":"Run Instructions","text":"<p>First, go in your <code>build</code> or <code>install</code> directory, then:</p> <pre><code>./build/applications/adv_networking_bench/cpp/adv_networking_bench adv_networking_bench_default_tx_rx.yaml\n</code></pre> <p>With DOCA:</p> <pre><code>./build/applications/adv_networking_bench/cpp/adv_networking_bench adv_networking_bench_gpunetio_tx_rx.yaml\n</code></pre> <p>With RIVERMAX RX:</p> <pre><code>./build/applications/adv_networking_bench/cpp/adv_networking_bench adv_networking_bench_rmax_rx.yaml\n</code></pre>","tags":["Networking and Distributed Computing","Distributed","UDP","GPUDirect","DPDK"]},{"location":"applications/aja_video_capture/","title":"AJA Video Capture","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 3.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3, 2.0.0, 2.1.0, 2.2.0 Contribution metric: Level 0 - Core Stable</p> <p>Minimal example to demonstrate the use of the aja source operator to capture device input and stream to holoviz operator.</p> <p>Visit the SDK User Guide to setup the AJA Card.</p>","tags":["Computer Vision and Perception","Video","format conversion","Streaming","AJA"]},{"location":"applications/aja_video_capture/#quick-start","title":"Quick Start","text":"<pre><code>holohub run aja_video_capture --language &lt;cpp/python&gt;\n</code></pre>","tags":["Computer Vision and Perception","Video","format conversion","Streaming","AJA"]},{"location":"applications/aja_video_capture/#settings","title":"Settings","text":"<p>To evaluate the AJA example using alternative resolutions, you may modify the aja_capture.yaml configuration file as needed. For instance, to test a resolution format of 1280 x 720 at 60 Hz, you can specify the following parameters in the aja section of the configuration :</p> <pre><code>```bash\n  aja:\n    width: 1280\n    height: 720\n    framerate: 60\n```\n</code></pre>","tags":["Computer Vision and Perception","Video","format conversion","Streaming","AJA"]},{"location":"applications/aja_video_capture/#migration-notes","title":"Migration Notes","text":"<p>Holoscan SDK AJA support is migrated from the core Holoscan SDK library to the HoloHub community repository in Holoscan SDK v3.0.0. Projects depending on AJA support should accordingly update include and linking paths to reference HoloHub.</p> <p>C++/CMake projects should update <code>holoscan::ops::aja</code> to <code>holoscan::aja</code></p> <p>Python projects should update <code>import holoscan.operators.AJASourceOp</code> to <code>import holohub.aja_source.AJASourceOp</code></p>","tags":["Computer Vision and Perception","Video","format conversion","Streaming","AJA"]},{"location":"applications/asr_to_llm/","title":"Real-time Riva ASR to local-LLM","text":"<p> Authors: Nigel Nelson (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 4 - Experimental</p> <p>This application streams microphone input to NVIDIA Riva Automatic Speech Recognition (ASR), which once the user specifies they are done speaking, passes the transcribed text to an LLM running locally that then summarizes this text.</p> <p>While this workflow in principle could be used for a number of domains, the app is currently configured to be healthcare specific. The current LLM prompt is created for radiology interpretation, but this can be easily changed in the YAML file to tailor the LLM's output to a wide array of potential use cases.</p>","tags":["Healthcare AI","Audio","ASR","Quantized Model Inference","LLM","Medical Imaging"]},{"location":"applications/asr_to_llm/#example-output","title":"Example output","text":"<p>Example output can be found at example_output.md  Description of output fields: Final Transcription: Riva's transcription of the provided mic input LLM Summary:* The LLM's output summarization</p>","tags":["Healthcare AI","Audio","ASR","Quantized Model Inference","LLM","Medical Imaging"]},{"location":"applications/asr_to_llm/#yaml-configuration","title":"YAML Configuration","text":"<p>The directions for the LLM are determined by the <code>stt_to_nlp.yaml</code> file. As you see from our example, the directions for the LLM are made via natural language, and can result in very different applications.</p> <p>With the current YAML configuration, the resulting prompt to the LLM is:</p> <pre><code>&lt;|system|&gt;\nYou are a veteran radiologist, who can answer any medical related question.\n\n&lt;|user|&gt;\nTranscript from Radiologist:\n{**transcribed text inserted here**}\nRequest(s):\nMake a summary of the transcript (and correct any transcription errors in CAPS).\nCreate a Patient Summary with no medical jargon.\nCreate a full radiological report write-up.\nGive likely ICD-10 Codes.\nSuggest follow-up steps.\n\n&lt;|assistant|&gt;\n</code></pre>","tags":["Healthcare AI","Audio","ASR","Quantized Model Inference","LLM","Medical Imaging"]},{"location":"applications/asr_to_llm/#setup-instructions","title":"Setup Instructions","text":"","tags":["Healthcare AI","Audio","ASR","Quantized Model Inference","LLM","Medical Imaging"]},{"location":"applications/asr_to_llm/#install-riva","title":"Install Riva:","text":"<p>First, you must follow the Riva local deployment quickstart guide. For x86 and ARM64 devices with dGPU follow the \"Data Center\" instructions, for ARM64 devices with iGPU follow the \"Embedded\" instructions.</p> <ul> <li>Note: to minimize the Riva install size you can change the <code>config.sh</code> file in the <code>riva_quickstart_vX.XX.X</code> directory such that it specifies to only install the ASR models (Riva has more features but only ASR is needed for this app). To do this, find the <code>sevice_enabled_*</code> variables and set them as shown below: <pre><code>service_enabled_asr=true\nservice_enabled_nlp=false\nservice_enabled_tts=false\nservice_enabled_nmt=false\n</code></pre></li> </ul> <p>\u26a0\ufe0f Note: If you are using ARM64 w/ iGPU or an x86 platform the quick-start scripts should work as intended. However, if you are using ARM64 w/ dGPU, you will need to make the following modifications to the Riva Quick-start scripts:</p> <p>In <code>riva_init.sh</code> make the following changes to ensure the ARM64 version of NGC-CLI is downloaded and your dGPU is used to run the container: <pre><code># download required models\n-if [[ $riva_target_gpu_family == \"tegra\" ]]; then\n-    docker run -it -d --rm -v $riva_model_loc:/data \\\n+if [[ $riva_target_gpu_family == \"non-tegra\" ]]; then\n+            docker run -it -d --rm --gpus '\"'$gpus_to_use'\"' -v $riva_model_loc:/data \\\n                -e \"NGC_CLI_API_KEY=$NGC_API_KEY\" \\\n</code></pre> Then in <code>riva_start.sh</code> make the changes below to ensure your Riva server has access to your sound devices: <pre><code>docker rm $riva_daemon_speech &amp;&gt; /dev/null\n-if [[ $riva_target_gpu_family == \"tegra\" ]]; then\n+if [[ $riva_target_gpu_family == \"non-tegra\" ]]; then\n    docker_run_args=\"-p 8000:8000 -p 8001:8001 -p 8002:8002 -p 8888:8888 --device /dev/bus/usb --device /dev/snd\"\n</code></pre></p>","tags":["Healthcare AI","Audio","ASR","Quantized Model Inference","LLM","Medical Imaging"]},{"location":"applications/asr_to_llm/#setup-instructions_1","title":"Setup Instructions:","text":"<p>Download the quantized Mistral 7B LLM from HugginFace.co: <pre><code>wget -nc -P &lt;your_model_dir&gt; https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GGUF/resolve/main/mistral-7b-openorca.Q8_0.gguf\n</code></pre></p>","tags":["Healthcare AI","Audio","ASR","Quantized Model Inference","LLM","Medical Imaging"]},{"location":"applications/asr_to_llm/#run-instructions","title":"Run instructions","text":"<p>Build and launch the <code>holohub:asr_to_llm</code> container: <pre><code>./holohub run-container asr_to_llm --add-volume &lt;your_model_dir&gt;\n</code></pre> Run the application and use the <code>--list-devices</code> arg to determine which microphone to use: <pre><code>python &lt;streaming_asr_to_llm_dir&gt;/asr_to_llm.py --list-devices\n</code></pre> Then run the application with the <code>--input-device</code> arg to specify the correct microphone: <pre><code>python &lt;streaming_asr_to_llm_dir&gt;/asr_to_llm.py --input-device &lt;device-index&gt;\n</code></pre></p> <p>Once <code>asr_to_llm.py</code> is running, you will see output from ALSA for loading the selected audio device and also from llama_cpp for loading the LLM onto GPU memory. Once this is complete it will immediately begin printing out the transcribed text. To signal that the audio you wish to transcribe is complete, enter <code>x</code> on the keyboard. This will terminate the ASR and microphone instance, and feed the complete transcribed text into the LLM for summarization.</p>","tags":["Healthcare AI","Audio","ASR","Quantized Model Inference","LLM","Medical Imaging"]},{"location":"applications/asr_to_llm/#stopping-instructions","title":"Stopping Instructions","text":"<p>Note: The <code>python asr_to_llm.py</code> command will complete on its own once the LLM is finished summarizing the transcription * Stopping Riva services: <pre><code>bash &lt;Riva_install_dir&gt;riva_stop.sh\n</code></pre></p>","tags":["Healthcare AI","Audio","ASR","Quantized Model Inference","LLM","Medical Imaging"]},{"location":"applications/asr_to_llm/#asr_to_llm-application-arguments","title":"ASR_To_LLM Application arguments","text":"<p>The <code>asr_to_llm.py</code> can receive several cli arguments:</p> <p><code>--input-device</code>: The index of the input audio device to use. <code>--list-devices</code>: List input audio device indices. <code>--sample-rate-hz</code>: The number of frames per second in audio streamed from the selected microphone. <code>--file-streaming-chunk</code>: A maximum number of frames in a audio chunk sent to server.</p>","tags":["Healthcare AI","Audio","ASR","Quantized Model Inference","LLM","Medical Imaging"]},{"location":"applications/asr_to_llm/#implementation-details","title":"Implementation Details","text":"<p>This application adapted the speech_to_text_llm Holohub application to transcribe audio in real-time using Riva ASR, as well as ensure that the complete app runs 100% locally.</p> <p>The LLM currently used in this application is Mistral-7B-OpenOrca-GGUF, which is a quantized Mistal 7B model that is finetuned on the OpenOrca dataset. However, any model in the GGUF file format will work as long as it can fit within your device's VRAM constraints.</p> <p>The inference engine used to run the LLM is llama-cpp-python, which is a Python binding for llama.cpp. The reason for this is that the underlying llama.cpp library is hardware agnostic, dependency free, and it runs quantized LLMs with very high throughput.</p> <p>The RivaStreamingOp is a Holoscan SDK adaptation of the transcribe_mic.py script that is part of the Riva python-clients repository.</p>","tags":["Healthcare AI","Audio","ASR","Quantized Model Inference","LLM","Medical Imaging"]},{"location":"applications/asr_to_llm/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode asr_to_llm\n</code></pre>","tags":["Healthcare AI","Audio","ASR","Quantized Model Inference","LLM","Medical Imaging"]},{"location":"applications/asr_to_llm/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"<p>There are two launch profiles configured for this application:</p> <ol> <li>(debugpy) asr_to_llm/python: Launch asr_to_llm using a launch profile that enables debugging of Python code.</li> <li>(pythoncpp) asr_to_llm/python: Launch asr_to_llm using a launch profile that enables debugging of Python and C++ code.</li> </ol>","tags":["Healthcare AI","Audio","ASR","Quantized Model Inference","LLM","Medical Imaging"]},{"location":"applications/basic_networking_ping/","title":"Basic Networking Ping","text":"<p> Authors: Cliff Burdick (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0, 2.1.0, 2.2.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application takes the existing ping example that runs over Holoscan ports and instead uses the basic network operator to run over a UDP socket.</p> <p>The basic network operator allows users to send and receive UDP messages over a standard Linux socket. Separate transmit and receive operators are provided so they can run independently and better suit the needs of the application.</p>","tags":["Networking and Distributed Computing","Distributed","UDP","Communications"]},{"location":"applications/basic_networking_ping/#configuration","title":"Configuration","text":"<p>The application is configured using the file basic_networking_ping_rx.yaml or basic_networking_ping_tx.yaml, where RX will receive packets and TX will transmit. Depending on how the machine is configured, the IP and UDP port likely need to be configured. All other settings do not need to be changed.</p> <p>Please refer to the basic network operator documentation for more configuration information.</p>","tags":["Networking and Distributed Computing","Distributed","UDP","Communications"]},{"location":"applications/basic_networking_ping/#requirements","title":"Requirements","text":"<p>This application requires: 1. Linux</p>","tags":["Networking and Distributed Computing","Distributed","UDP","Communications"]},{"location":"applications/basic_networking_ping/#quick-start","title":"Quick Start","text":"<p>Use the following to build and run the application:</p> <pre><code># Start the receiver\n./holohub run basic_networking_ping --language &lt;cpp|python&gt; --run-args basic_networking_ping_rx.yaml\n# Start the transmitter\n./holohub run basic_networking_ping --language &lt;cpp|python&gt; --run-args basic_networking_ping_tx.yaml\n</code></pre>","tags":["Networking and Distributed Computing","Distributed","UDP","Communications"]},{"location":"applications/basic_networking_ping/#build-instructions","title":"Build Instructions","text":"<p>Please refer to the top level Holohub README.md file for information on how to build this application.</p>","tags":["Networking and Distributed Computing","Distributed","UDP","Communications"]},{"location":"applications/basic_networking_ping/#run-instructions","title":"Run Instructions","text":"<p>Running the sample uses the standard HoloHub <code>run</code> script:</p> <pre><code># Start the receiver\n./run launch basic_networking_ping &lt;language&gt; --extra_args basic_networking_ping_rx.yaml\n# Start the transmitter\n./run launch basic_networking_ping &lt;language&gt; --extra_args basic_networking_ping_tx.yaml\n</code></pre> <p>Language can be either C++ or Python.</p>","tags":["Networking and Distributed Computing","Distributed","UDP","Communications"]},{"location":"applications/basic_networking_ping/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode\n</code></pre>","tags":["Networking and Distributed Computing","Distributed","UDP","Communications"]},{"location":"applications/basic_networking_ping/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Networking and Distributed Computing","Distributed","UDP","Communications"]},{"location":"applications/basic_networking_ping/#c","title":"C++","text":"<p>There are three launch profiles configured for this application:</p> <ol> <li>(gdb) basic_networking_ping/cpp RX: Launch Basic Networking Ping with the RX configurations.</li> <li>(gdb) basic_networking_ping/cpp TX: Launch Basic Networking Ping with the TX configurations.</li> <li>(compound) basic_networking_ping/cpp TX &amp; RX: Launch both 1 and 2 in parallel.    This launch profile launches the receiver follow by the transmitter.</li> </ol>","tags":["Networking and Distributed Computing","Distributed","UDP","Communications"]},{"location":"applications/basic_networking_ping/#python","title":"Python","text":"<p>There are several launch profiles configured for this application:</p> <ol> <li>(debugpy) basic_networking_ping/python RX: Launch Basic Networking Ping with the RX configurations.    This launch profile enables debugging of Python code.</li> <li>(debugpy) basic_networking_ping/python TX: Launch Basic Networking Ping with the TX configurations.    This launch profile enables debugging of Python code.</li> <li>(pythoncpp) basic_networking_ping/python TX: Launch Basic Networking Ping with the RX configurations.    This launch profile enables debugging of Python and C++ code.</li> <li>(pythoncpp) basic_networking_ping/python TX: Launch Basic Networking Ping with the TX configurations.    This launch profile enables debugging of Python and C++ code.</li> <li>(compound) basic_networking_ping/python TX &amp; RX: Launch both 1 and 2 in parallel.    This launch profile launches the receiver follow by the transmitter.</li> </ol>","tags":["Networking and Distributed Computing","Distributed","UDP","Communications"]},{"location":"applications/body_pose_estimation/","title":"Body Pose Estimation","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 2 - Trusted</p> <p>Body pose estimation is a computer vision task that involves recognizing specific points on the human body in images or videos. A model is used to infer the locations of keypoints from the source video which is then rendered by the visualizer. </p>","tags":["Computer Vision and Perception","Distributed","Tensor Optimization","Human Body Pose Estimation","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/body_pose_estimation/#model","title":"Model","text":"<p>This application uses YOLO11 pose model from Ultralytics for body pose estimation. The model is downloaded when building the application.</p>","tags":["Computer Vision and Perception","Distributed","Tensor Optimization","Human Body Pose Estimation","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/body_pose_estimation/#data","title":"Data","text":"<p>This application downloads a pre-recorded video from Pexels when the application is built for use with this application.  Please review the license terms from Pexels.</p> <p>NOTE: The user is responsible for checking if the dataset license is fit for the intended purpose.</p>","tags":["Computer Vision and Perception","Distributed","Tensor Optimization","Human Body Pose Estimation","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/body_pose_estimation/#input","title":"Input","text":"<p>This app supports three different input options.  If you have a v4l2 compatible device plugged into your machine such as a webcam, you can run this application with option 1.  Otherwise you can run this application using a pre-recorded video with option 2.  For more advanced use cases, option 3 shows how to publish/subscribe to a DDS video stream to for interprocess applications.</p> <ol> <li>v4l2 compatible input device (default, see V4L2 Support below)</li> <li>pre-recorded video (see Video Replayer Support below)</li> <li>DDS video stream (see DDS Support below)</li> </ol> <p>To see the list of v4l2 devices connected to your machine, install <code>v4l-utils</code> if it's not already installed:</p> <pre><code>sudo apt-get install v4l-utils\n</code></pre> <p>Then run:</p> <pre><code>v4l2-ctl --list-devices\n</code></pre>","tags":["Computer Vision and Perception","Distributed","Tensor Optimization","Human Body Pose Estimation","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/body_pose_estimation/#run-instructions","title":"Run Instructions","text":"","tags":["Computer Vision and Perception","Distributed","Tensor Optimization","Human Body Pose Estimation","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/body_pose_estimation/#v4l2-support","title":"V4L2 Support","text":"<p>This application supports v4l2 compatible devices as input.  To run this application with your v4l2 compatible device, please plug in your input device and run: <pre><code>./holohub run body_pose_estimation\n</code></pre></p> <p>By default, this application expects the input device to be mounted at <code>/dev/video0</code>.  If this is not the case, please update <code>applications/body_pose_estimation/body_pose_estimation.yaml</code> and set it to use the corresponding input device before running the application.  You can also override the default input device on the command line by running: <pre><code>./holohub run body_pose_estimation --run-args=\"--video_device /dev/video0\"\n</code></pre></p>","tags":["Computer Vision and Perception","Distributed","Tensor Optimization","Human Body Pose Estimation","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/body_pose_estimation/#video-replayer-support","title":"Video Replayer Support","text":"<p>If you don't have a v4l2 compatible device plugged in, you may also run this application on a pre-recorded video. To launch the application using the Video Stream Replayer as the input source, run:</p> <pre><code>./holohub run body_pose_estimation --run-args=\"--source replayer\"\n</code></pre>","tags":["Computer Vision and Perception","Distributed","Tensor Optimization","Human Body Pose Estimation","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/body_pose_estimation/#dds-support","title":"DDS Support","text":"<p>This application supports using a DDS video stream as the input as well as publishing the output video stream back to DDS. To enable DDS, the application must first be built with the DDS operators enabled. Only the subscriber or publisher operators need to be enabled for the sake of input or output video streams, respectively, but to enable both use the following:</p> <pre><code>./holohub build --local body_pose_estimation --build-with \"dds_video_subscriber;dds_video_publisher\"\n</code></pre> <p>Note that building these operators requires RTI Connext be installed. See the DDS Operator Documentation for more information on how to build the operators. If using a development container, see the additional instructions below.</p> <p>To use a DDS video stream as the input to the application, use the <code>-s=dds</code> argument when running the application:</p> <pre><code>./holohub launch --local body_pose_estimation --run-args=\"-s=dds\n</code></pre> <p>To publish the output result to DDS, edit the <code>body_pose_estimation.yaml</code> configuration file so that the <code>dds_publisher</code> <code>enable</code> option is <code>true</code>:</p> <pre><code>dds_publisher:\n  enable: true\n</code></pre> <p>Note that the default DDS video stream IDs use by the application are <code>0</code> for the input and <code>1</code> for the output. These can be changed using the <code>stream_id</code> settings in the <code>dds_source</code> and <code>dds_publisher</code> sections of the configuration file, respectively.</p> <p>To produce the DDS input stream or to view the output stream generated by this application, the dds_video application can be used. For example, the following will use the <code>dds_video</code> application to capture video from the default V4L2 device and publish it to DDS so that it can be received as input by this application:</p> <pre><code>./holohub run --local dds_video --run-args=\"-p -i 0\"\n</code></pre> <p>And the following will use the <code>dds_video</code> application to receive and render the output published by this application:</p> <pre><code>./holohub run --local dds_video --run-args=\"-s -i 1\"\n</code></pre>","tags":["Computer Vision and Perception","Distributed","Tensor Optimization","Human Body Pose Estimation","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/body_pose_estimation/#using-a-development-container-with-dds-support","title":"Using a Development Container with DDS Support","text":"<p>Installing RTI Connext into the development container is not currently supported, so enabling DDS support with this application requires RTI Connext be installed onto the host and then mounted into the container at runtime. To mount RTI Connext into the container, ensure that the <code>NDDSHOME</code> and <code>CONNEXTDDS_ARCH</code> environment variables are set (which can be done using the RTI <code>setenv</code> script) then use the following:</p> <pre><code>./holohub run-container body_pose_estimation --docker-opts=\"-v $NDDSHOME:/opt/dds -e NDDSHOME=/opt/dds -e CONNEXTDDS_ARCH=$CONNEXTDDS_ARCH\"\n</code></pre>","tags":["Computer Vision and Perception","Distributed","Tensor Optimization","Human Body Pose Estimation","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/body_pose_estimation/#known-issues","title":"Known Issues","text":"<p>There is a known issue running this application on IGX w/ iGPU and on Jetson AGX (see #500). The workaround is to update the device to avoid picking up the libnvv4l2.so library.</p> <pre><code>cd /usr/lib/aarch64-linux-gnu/\nls -l libv4l2.so.0.0.999999\nsudo rm libv4l2.so.0.0.999999\nsudo ln -s libv4l2.so.0.0.0.0  libv4l2.so.0.0.999999\n</code></pre>","tags":["Computer Vision and Perception","Distributed","Tensor Optimization","Human Body Pose Estimation","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/colonoscopy_segmentation/","title":"Colonoscopy Polyp Segmentation","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>Full workflow including a generic visualization of segmentation results from a polyp segmentation models.</p>","tags":["Healthcare AI","Visualization","AJA","Endoscopy","Segmentation","Holoviz","Video"]},{"location":"applications/colonoscopy_segmentation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8+</li> <li>The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the colonoscopy data (replayer). Follow the setup instructions from the user guide to use the AJA capture card.</li> </ul>","tags":["Healthcare AI","Visualization","AJA","Endoscopy","Segmentation","Holoviz","Video"]},{"location":"applications/colonoscopy_segmentation/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI Colonoscopy Segmentation of Polyps</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Healthcare AI","Visualization","AJA","Endoscopy","Segmentation","Holoviz","Video"]},{"location":"applications/colonoscopy_segmentation/#run-instructions","title":"Run Instructions","text":"<p>To run this application, you'll need to configure your PYTHONPATH environment variable to locate the necessary python libraries based on your Holoscan SDK installation type.</p> <p>You should refer to the glossary for the terms defining specific locations within HoloHub.</p> <p>If your Holoscan SDK installation type is:</p> <ul> <li>python wheels:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <ul> <li>otherwise:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOSCAN_INSTALL_DIR&gt;/python/lib:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <p>Next, run the commands of your choice:</p> <ul> <li> <p>Using a pre-recorded video     <pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;/applications/colonoscopy_segmentation\npython3 colonoscopy_segmentation.py --source=replayer --data=&lt;DATA_DIR&gt;/colonoscopy_segmentation --no-contours\n</code></pre></p> </li> <li> <p>Using an AJA card     <pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;/applications/colonoscopy_segmentation\npython3 colonoscopy_segmentation.py --source=aja --no-contours\n</code></pre> Note that segmentation contours can be shown (instead of segmentation masks) by changing <code>--no-contours</code> to <code>--contours</code>.</p> </li> </ul>","tags":["Healthcare AI","Visualization","AJA","Endoscopy","Segmentation","Holoviz","Video"]},{"location":"applications/colonoscopy_segmentation/#holoscan-sdk-version","title":"Holoscan SDK version","text":"<p>Colonoscopy segmentation application in HoloHub requires version 0.6+ of the Holoscan SDK. If the Holoscan SDK version is 0.5 or lower, following code changes must be made in the application:</p> <ul> <li>In python/CMakeLists.txt: update the holoscan SDK version from <code>0.6</code> to <code>0.5</code></li> <li>In python/multiai_ultrasound.py: <code>InferenceOp</code> is replaced with <code>MultiAIInferenceOp</code></li> </ul>","tags":["Healthcare AI","Visualization","AJA","Endoscopy","Segmentation","Holoviz","Video"]},{"location":"applications/colonoscopy_segmentation/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode\n</code></pre>","tags":["Healthcare AI","Visualization","AJA","Endoscopy","Segmentation","Holoviz","Video"]},{"location":"applications/colonoscopy_segmentation/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"<p>There are two launch profiles configured for this application:</p> <ol> <li>(debugpy) colonoscopy_segmentation/python: Launch colonoscopy_segmentation using a launch profile that enables debugging of Python code.</li> <li>(pythoncpp) colonoscopy_segmentation/python: Launch colonoscopy_segmentation using a launch profile that enables debugging of Python and C++ code.</li> </ol> <p>Note: the launch profile starts the application with Video Replayer. To adjust the arguments of the application, open launch.json, find the launch profile named <code>(debugpy) colonoscopy_segmentation/python</code>, and adjust the <code>args</code> field as needed.</p>","tags":["Healthcare AI","Visualization","AJA","Endoscopy","Segmentation","Holoviz","Video"]},{"location":"applications/cuda_quantum/","title":"CUDA Quantum Variational Quantum Eigensolver (VQE)","text":"<p> Authors: Sean Huver (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 4 - Experimental</p>","tags":["Tools And Other Specialized Applications","Quantum Computing"]},{"location":"applications/cuda_quantum/#variational-quantum-eigensolver-vqe","title":"Variational Quantum Eigensolver (VQE)","text":"<p>The Variational Quantum Eigensolver (VQE) is a quantum algorithm designed to approximate the ground state energy of quantum systems. This energy, represented by what is called the Hamiltonian of the system, is central to multiple disciplines, including drug discovery, material science, and condensed matter physics. The goal of VQE is to find the state that minimizes the expectation value of this Hamiltonian, which corresponds to the ground state energy.</p> <p>At its core, VQE is a lighthouse example of the synergy between classical and quantum computing, requiring them both to tackle problems traditionally deemed computationally intractable. Even in the current landscape where fault-tolerant quantum computing\u2014a stage where quantum computers are resistant to errors\u2014is not yet realized, VQE is seen as a practical tool. This is due to its design as a 'near-term' algorithm, built to operate on existing noisy quantum hardware. </p>","tags":["Tools And Other Specialized Applications","Quantum Computing"]},{"location":"applications/cuda_quantum/#key-components-of-vqe","title":"Key Components of VQE","text":"<ol> <li> <p>Hamiltonian: This represents the total energy of the quantum system, which is known ahead of time. In VQE, we aim to find the lowest eigenvalue (ground state energy) of this Hamiltonian.</p> </li> <li> <p>Ansatz (or trial wavefunction): The ansatz is the initial guess for the state of the quantum system, represented by a parameterized quantum circuit. It's crucial for this state to be a good representation, as the quality of the ansatz can heavily influence the final results. VQE iteratively refines the parameters of this ansatz to approximate the true ground state of the Hamiltonian.</p> </li> </ol>","tags":["Tools And Other Specialized Applications","Quantum Computing"]},{"location":"applications/cuda_quantum/#vqe-mechanism","title":"VQE Mechanism","text":"<p>The VQE operates by employing a hybrid quantum-classical approach:</p> <ol> <li>Quantum Circuit Parameterization: VQE begins with a parameterized quantum circuit, effectively serving as an initial guess or representation of the system's state.</li> <li>Evaluation and Refinement: The quantum system's energy is evaluated using the current quantum circuit parameters. Classical optimization algorithms then adjust these parameters in a quest to minimize the energy.</li> <li>Iterative Process: The combination of quantum evaluation and classical refinement is iterative. Over multiple cycles, the parameters are tuned to get increasingly closer to the true ground state energy.</li> </ol>","tags":["Tools And Other Specialized Applications","Quantum Computing"]},{"location":"applications/cuda_quantum/#integration-with-holoscan-and-cuda-quantum","title":"Integration with Holoscan and CUDA Quantum","text":"<ul> <li>NVIDIA Holoscan SDK: The Holoscan SDK is designed for efficient handling of high-throughput, low-latency GPU tasks. Within the context of VQE, the Holoscan SDK facilitates the rapid classical computations necessary for parameter adjustments and optimization. The <code>ClassicalComputeOp</code> in the provided code sample is an example of this SDK in action, preparing the quantum circuits efficiently.</li> <li>CUDA Quantum: CUDA Quantum is a framework that manages hybrid quantum-classical workflows. For VQE, CUDA Quantum processes quantum data and executes quantum operations. The <code>QuantumComputeOp</code> operator in the code uses the cuQuantum simulator backend, but the user may optionally switch out the simulator for a real quantum cloud backend provided by either IonQ or Quantinuum (see CUDA Quantum backend documentation).</li> </ul> <p>Holoscan ensures swift and efficient classical computations, while CUDA Quantum manages the quantum components with precision.</p>","tags":["Tools And Other Specialized Applications","Quantum Computing"]},{"location":"applications/cuda_quantum/#usage","title":"Usage","text":"<p>To run the application, you need to have CUDA Quantum, Qiskit, and Holoscan installed. You also need an IBM Quantum account to use their quantum backends.</p> <ol> <li> <p>Clone the repository and navigate to the <code>cuda_quantum</code> directory containing.</p> </li> <li> <p>Install the requirements <code>pip install -r requirements.txt</code></p> </li> <li> <p>Either use or replace the <code>'hamiltonian'</code> in <code>cuda_quantum.yaml</code> dependent on the physical system you wish to model.</p> </li> <li> <p>Run the application with the command <code>python cuda_quantum.py</code>.</p> </li> </ol>","tags":["Tools And Other Specialized Applications","Quantum Computing"]},{"location":"applications/cuda_quantum/#operators","title":"Operators","text":"<p>The application uses three types of operators:</p> <ul> <li> <p><code>ClassicalComputeOp</code>: This operator performs classical computations. It also creates a quantum kernel representing the initial ansatz, or guess of the state of the system, and a Hamiltonian.</p> </li> <li> <p><code>QuantumComputeOp</code>: This operator performs quantum computations. It uses the quantum kernel and Hamiltonian from <code>ClassicalComputeOp</code> to iterate towards the ground state energy and parameter using VQE.</p> </li> <li> <p><code>PrintOp</code>: This operator prints the result from <code>QuantumComputeOp</code>.</p> </li> </ul>","tags":["Tools And Other Specialized Applications","Quantum Computing"]},{"location":"applications/cuda_quantum/#operator-connections","title":"Operator Connections","text":"<p>The operators are connected as follows:</p> <pre><code>flowchart LR\n    ClassicalComputeOp --&gt; QuantumComputeOp\n    QuantumComputeOp --&gt; PrintOp\n</code></pre> <p><code>ClassicalComputeOp</code> sends the quantum kernel and Hamiltonian to <code>QuantumComputeOp</code>, which computes the energy and parameter and sends the result to <code>PrintOp</code>.</p>","tags":["Tools And Other Specialized Applications","Quantum Computing"]},{"location":"applications/cuda_quantum/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode cuda_quantum\n</code></pre>","tags":["Tools And Other Specialized Applications","Quantum Computing"]},{"location":"applications/cuda_quantum/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"<p>There are two launch profiles configured for this application:</p> <ol> <li>(debugpy) cuda_quantum/python: Launch cuda_quantum using a launch profile that enables debugging of Python code.</li> <li>(pythoncpp) cuda_quantum/python: Launch cuda_quantum using a launch profile that enables debugging of Python and C++ code.</li> </ol> <p>Note: to adjust the arguments of the application, open launch.json, find the launch profile named <code>(debugpy) cuda_quantum/python</code>, and adjust the <code>args</code> field as needed.</p>","tags":["Tools And Other Specialized Applications","Quantum Computing"]},{"location":"applications/cunumeric_integration/","title":"Power Spectral Density with cuNumeric","text":"<p> Authors: Adam Thompson (NVIDIA) Supported platforms: x86_64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 2 - Trusted</p> <p>cuNumeric is an drop-in replacement for NumPy that aims to provide a distributed and accelerated drop-in replacement for the NumPy API on top of the Legion runtime. It works best for programs that have very large arrays of data that can't fit in the the memory of a single GPU or node.</p> <p>In this example application, we are using the cuNumeric library within a Holoscan application graph to determine the Power Spectral Density (PSD) of an incoming signal waveform. Notably, this is simply achieved by taking the absolute value of the FFT of a data array.</p> <p>The main objectives of this demonstration are to: - Highlight developer productivity in building an end-to-end streaming application with Holoscan and cuNumeric - Demonstrate how to scale a given workload to multiple GPUs using cuNumeric</p>","tags":["Signal Processing","Distributed","PSD"]},{"location":"applications/cunumeric_integration/#running-the-application","title":"Running the Application","text":"<p>Prior to running the application, the user needs to install the necessary dependencies. This is most easily done in an Anaconda environment.</p> <pre><code>conda create --name holoscan-cunumeric-demo python=3.9\nconda activate holoscan-cunumeric-demo\nconda install -c nvidia -c conda-forge -c legate cunumeric cupy\npip install holoscan\n</code></pre> <p>The cuNumeric PSD processing pipeline example can then be run via <pre><code>legate --gpus 2 applications/cunumeric_integration/cunumeric_psd.py\n</code></pre></p> <p>While running the application, you can confirm multi GPU utilization via watching <code>nvidia-smi</code> or using another GPU utilization tool</p> <p>To run the same application without cuNumeric, simply change <code>import cunumeric as np</code> to <code>import cupy as np</code> in the code and run <pre><code>python applications/cunumeric_integration/cunumeric_psd.py\n</code></pre></p>","tags":["Signal Processing","Distributed","PSD"]},{"location":"applications/cvcuda_basic/","title":"Simple CV-CUDA","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 20, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates seamless interoperability between Holoscan tensors and CV-CUDA tensors. The image processing pipeline is just a simple flip of the video orientation.</p> <p>Note that the C++ version of this application currently requires extra code to handle conversion back and forth between CV-CUDA and Holoscan tensor types. On the Python side, the conversion is trivial due to the support for the DLPack Python specification in both CV-CUDA and Holoscan. We provide two operators to handle the interoperability between CVCUDA and Holoscan tensors.</p>","tags":["Computer Vision and Perception","Video","Tensor Optimization","CV CUDA","Endoscopy","Visualization"]},{"location":"applications/cvcuda_basic/#quick-start-with-holohub-cli-container","title":"Quick Start with HoloHub CLI Container","text":"<p>Run the following command to build and run the CV-CUDA sample application in a Docker container:</p> <pre><code>./holohub run cvcuda_basic\n</code></pre>","tags":["Computer Vision and Perception","Video","Tensor Optimization","CV CUDA","Endoscopy","Visualization"]},{"location":"applications/cvcuda_basic/#advanced-build-steps","title":"Advanced Build Steps","text":"","tags":["Computer Vision and Perception","Video","Tensor Optimization","CV CUDA","Endoscopy","Visualization"]},{"location":"applications/cvcuda_basic/#using-the-docker-file","title":"Using the docker file","text":"<p>This application requires a compiled version of CV-CUDA. For simplicity a DockerFile is available. To generate the container run:</p> <pre><code>./holohub build-container cvcuda_basic\n</code></pre> <p>The C++ version of the application can then be built by launching this container and using the provided <code>holohub</code> CLI.</p> <pre><code>./holohub run-container cvcuda_basic\n./holohub build cvcuda_basic\n</code></pre>","tags":["Computer Vision and Perception","Video","Tensor Optimization","CV CUDA","Endoscopy","Visualization"]},{"location":"applications/cvcuda_basic/#running-the-application","title":"Running the Application","text":"<p>This application uses the endoscopy dataset as an example. The build command above will automatically download it. This application is then run inside the container.</p> <pre><code>./holohub run-container cvcuda_basic\n</code></pre> <p>The Python version of the simple CV-CUDA pipeline example can be run via <pre><code>python applications/cvcuda_basic/python/cvcuda_basic.py --data=/workspace/holohub/data/endoscopy\n</code></pre></p> <p>or using the HoloHub CLI entrypoint:</p> <pre><code>./holohub run cvcuda_basic --language=python --local\n</code></pre> <p>The C++ version of the simple CV-CUDA pipeline example can then be run via <pre><code>./build/applications/cvcuda_basic/cpp/cvcuda_basic --data=/workspace/holohub/data/endoscopy\n</code></pre></p> <p>or using the run script</p> <pre><code>./holohub run cvcuda_basic --language=cpp --local\n</code></pre>","tags":["Computer Vision and Perception","Video","Tensor Optimization","CV CUDA","Endoscopy","Visualization"]},{"location":"applications/cvcuda_basic/#vs-code-dev-container","title":"VS Code Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode cvcuda_basic\n</code></pre>","tags":["Computer Vision and Perception","Video","Tensor Optimization","CV CUDA","Endoscopy","Visualization"]},{"location":"applications/cvcuda_basic/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Computer Vision and Perception","Video","Tensor Optimization","CV CUDA","Endoscopy","Visualization"]},{"location":"applications/cvcuda_basic/#c","title":"C++","text":"<p>Use the <code>**(gdb) cvcuda_basic/cpp**</code> launch profile configured for this application to debug the application.</p>","tags":["Computer Vision and Perception","Video","Tensor Optimization","CV CUDA","Endoscopy","Visualization"]},{"location":"applications/cvcuda_basic/#python","title":"Python","text":"<p>There are two launch profiles configured for this Python application:</p> <ol> <li>(debugpy) cvcuda_basic/python: Launch cvcuda_basic using a launch profile that enables debugging of Python code.</li> <li>(pythoncpp) cvcuda_basic/python: Launch cvcuda_basic using a launch profile that enables debugging of Python and C++ code.</li> </ol>","tags":["Computer Vision and Perception","Video","Tensor Optimization","CV CUDA","Endoscopy","Visualization"]},{"location":"applications/dds/dds_video/","title":"DDS Video: Real-time Video Streaming with RTI Connext","text":"<p> Authors: Ian Stewart (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>The DDS Video application demonstrates how video frames can be written to or read from a DDS databus in order to provide flexible integration between Holoscan applications and other applications (using Holoscan or not) via DDS.</p> <p>The application can be run as either a publisher or as a subscriber. In either case, it will use the VideoFrame data topic registered by the <code>DDSVideoPublisherOp</code> or <code>DDSVideoSubscriberOp</code> operators in order to write or read the video frame data to/from the DDS databus, respectively.</p> <p>When run as a publisher, the source for the input video frames will come from an attached V4L2-compatible camera via the <code>V4L2VideoCaptureOp</code> operator.</p> <p>When run as a subscriber, the application will use Holoviz to render the received video frames to the display. In addition to the video stream, the subscriber application will also subscribe to the <code>Square</code>, <code>Circle</code>, and <code>Triangle</code> topics as used by the RTI Shapes Demo. Any shapes received by this subscriber will also be overlaid on top of the Holoviz output.</p> <p></p>","tags":["Networking and Distributed Computing","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/dds/dds_video/#prerequisites","title":"Prerequisites","text":"<ul> <li>This application requires RTI Connext be installed and configured with a valid RTI Connext license prior to use. </li> <li>V4L2 capable device</li> </ul> <p>[!NOTE] Instructions below are based on the `.run' installer from RTI Connext. Refer to the Linux installation for details.</p>","tags":["Networking and Distributed Computing","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/dds/dds_video/#quick-start","title":"Quick Start","text":"<pre><code># Start the publisher\n./holohub run dds_video --docker-opts=\"-v $HOME/rti_connext_dds-7.3.0:/opt/rti.com/rti_connext_dds-7.3.0/\" --run-args=\"-p\"\n\n# Start the subscriber\n./holohub run dds_video --docker-opts=\"-v $HOME/rti_connext_dds-7.3.0:/opt/rti.com/rti_connext_dds-7.3.0/\" --run-args=\"-s\"\n</code></pre>","tags":["Networking and Distributed Computing","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/dds/dds_video/#building-the-application","title":"Building the Application","text":"<p>To build on an IGX devkit (using the <code>armv8</code> architecture), follow the instructions to build Connext DDS applications for embedded Arm targets up to, and including, step 5 (Installing Java and setting JREHOME).</p> <p>To build the application, the <code>RTI_CONNEXT_DDS_DIR</code> CMake variable must point to the installation path for RTI Connext. This can be done automatically by setting the <code>NDDSHOME</code> environment variable to the RTI Connext installation directory (such as when using the RTI <code>setenv</code> scripts), or manually at build time, e.g.:</p> <pre><code>./holohub build --local dds_video --configure-args=\"-DRTI_CONNEXT_DDS_DIR=~/rti/rti_connext_dds-7.3.0\"\n</code></pre>","tags":["Networking and Distributed Computing","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/dds/dds_video/#building-with-a-container","title":"Building with a Container","text":"<p>Due to the license requirements of RTI Connext it is not currently supported to install RTI Connext into a development container. Instead, Connext should be installed onto the host as above and then the development container can be launched with the RTI Connext folder mounted at runtime. To do so, ensure that the <code>NDDSHOME</code> and <code>CONNEXTDDS_ARCH</code> environment variables are set (which can be done using the RTI <code>setenv</code> script) and use the following:</p> <pre><code># 1. Build and launch the container\n./holohub run-container dds_video --docker-opts=\"-v $HOME/rti_connext_dds-7.3.0:/opt/rti.com/rti_connext_dds-7.3.0/\"\n# 3. Build the application\n./holohub build dds_video\n# Continue to the next section to run the application with the publisher. \n# Open a new terminal to repeat step #2 and launch a new container for the subscriber.\n</code></pre>","tags":["Networking and Distributed Computing","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/dds/dds_video/#running-the-application","title":"Running the Application","text":"<p>Both a publisher and subscriber process must be launched to see the result of writing to and reading the video stream from DDS, respectively.</p> <p>To run the publisher process, use the <code>-p</code> option:</p> <pre><code>$ ./holohub run --no-local-build dds_video --run-args=\"-p\"\n</code></pre> <p>To run the subscriber process, use the <code>-s</code> option:</p> <pre><code>$ ./holohub run --no-local-build dds_video --run-args=\"-s\"\n</code></pre> <p>If running the application generates an error about <code>RTI Connext DDS No Source for License information</code>, ensure that the RTI Connext license has either been installed system-wide or the <code>NDDSHOME</code> environment variable has been set to point to your user's RTI Connext installation path.</p> <p>Note that these processes can be run on the same or different systems, so long as they are both discoverable by the other via RTI Connext. If the processes are run on different systems then they will communicate using UDPv4, for which optimizations have been defined in the default <code>qos_profiles.xml</code> file. These optimizations include increasing the buffer size used by RTI Connext for network sockets, and so the systems running the application must also be configured to increase their maximum send and receive socket buffer sizes. This can be done by running the <code>set_socket_buffer_sizes.sh</code> script within this directory:</p> <pre><code>$ ./set_socket_buffer_sizes.sh\n</code></pre> <p>For more details, see the RTI Connext Guide to Improve DDS Network Performance on Linux Systems</p> <p>The QoS profiles used by the application can also be modified by editing the <code>qos_profiles.xml</code> file in the application directory. For more information about modifying the QoS profiles, see the RTI Connext Basic QoS tutorial or the RTI Connext QoS Reference Guide.</p>","tags":["Networking and Distributed Computing","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/dds/dds_video/#publishing-shapes-from-the-rti-shapes-demo","title":"Publishing Shapes from the RTI Shapes Demo","text":"<p>The RTI Shapes Demo can be used to publish shapes which are then read and overlaid onto the video stream by this application. However, the domain participant QoS used by this application is not compatible with the default DDS QoS settings, so the RTI Shapes Demo must be configured to use the QoS settings provided by this application.  To do this, follow these steps:</p> <ol> <li>Launch the RTI Shapes Demo</li> <li>Select <code>Controls</code>, then <code>Configuration</code> from the menu bar</li> <li>Click <code>Stop</code> to disable the default domain participant</li> <li>Click <code>Manage QoS</code></li> <li>Click <code>Add</code> then navigate to and select the <code>qos_profiles.xml</code> file in this    application's directory.</li> <li>Click <code>OK</code> to close the <code>Manage QoS</code> window.</li> <li>In the <code>Choose the profile</code> drop-down, select <code>HoloscanDDSTransport::SHMEM+LAN</code></li> <li>Click <code>Start</code> to join the domain.</li> </ol> <p>Once the Shapes Demo is running and has joined the domain of a running <code>dds_video</code> subscriber, shapes published by the application should be rendered on top of the subscriber's video stream.</p>","tags":["Networking and Distributed Computing","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/deltacast_transmitter/","title":"Deltacast Videomaster Transmitter","text":"<p> Authors: Pierre PERICK (DELTACAST) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0, 2.9.0, 3.0.0 Contribution metric: Level 2 - Trusted</p> <p>This application demonstrates the use of videomaster_transmitter to transmit a video stream through a dedicated IO device.</p>","tags":["Healthcare AI","Video","Deltacast","Endoscopy","RDMA","GPUDirect"]},{"location":"applications/deltacast_transmitter/#requirements","title":"Requirements","text":"<p>This application uses the DELTACAST.TV capture card for input stream. Contact DELTACAST.TV for more details on how access the SDK and to setup your environment.</p>","tags":["Healthcare AI","Video","Deltacast","Endoscopy","RDMA","GPUDirect"]},{"location":"applications/deltacast_transmitter/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>See instructions from the top level README on how to build this application. Note that this application requires to provide the VideoMaster_SDK_DIR if it is not located in a default location on the system. This can be done with the following command, from the top level Holohub source directory:</p> <pre><code>./holohub build --local deltacast_transmitter --configure-args=\"-DVideoMaster_SDK_DIR=&lt;Path to VideoMasterSDK&gt;\"\n</code></pre>","tags":["Healthcare AI","Video","Deltacast","Endoscopy","RDMA","GPUDirect"]},{"location":"applications/deltacast_transmitter/#run-instructions","title":"Run Instructions","text":"<p>From the build directory, run the command:</p> <pre><code>./applications/deltacast_transmitter/deltacast_transmitter --data &lt;holohub_data_dir&gt;/endoscopy\n</code></pre>","tags":["Healthcare AI","Video","Deltacast","Endoscopy","RDMA","GPUDirect"]},{"location":"applications/depth_anything_v2/","title":"Depth Anything V2","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.5.0 Tested Holoscan SDK versions: 2.8.0 Contribution metric: Level 2 - Trusted</p> <p>This application uses the Depth Anything V2 model for monocular depth estimation.  Monocular Depth Estimation refers to the task of predicting the distance of objects in a scene from a single 2D image captured by a standard camera.</p>","tags":["Computer Vision and Perception","Visualization","Edge Accelerated Inference","Monocular Depth Estimation","Holoviz","Video"]},{"location":"applications/depth_anything_v2/#model","title":"Model","text":"<p>This application uses the Depth Anything V2 model from DepthAnythingV2 for monocular depth estimation. The model is downloaded when building the Docker image.</p> <p>NOTE: The user is responsible for checking if the model license is suitable for the intended purpose.</p>","tags":["Computer Vision and Perception","Visualization","Edge Accelerated Inference","Monocular Depth Estimation","Holoviz","Video"]},{"location":"applications/depth_anything_v2/#data","title":"Data","text":"<p>This application downloads a pre-recorded video from Pexels when the application is built.  Please review the license terms from Pexels.</p> <p>NOTE: The user is responsible for ensuring the dataset license is suitable for the intended purpose.</p>","tags":["Computer Vision and Perception","Visualization","Edge Accelerated Inference","Monocular Depth Estimation","Holoviz","Video"]},{"location":"applications/depth_anything_v2/#input","title":"Input","text":"<p>This app supports two different input options.  If you have a v4l2 compatible device plugged into your machine such as a webcam, you can run this application with option 1.  Otherwise you can run this application using a pre-recorded video with option 2.</p> <ol> <li>v4l2 compatible input device (default, see V4L2 Support below)</li> <li>pre-recorded video (see Video Replayer Support below)</li> </ol> <p>To see the list of v4l2 devices connected to your machine, install <code>v4l-utils</code> if it's not already installed:</p> <pre><code>sudo apt-get install v4l-utils\n</code></pre> <p>Then run:</p> <pre><code>v4l2-ctl --list-devices\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Edge Accelerated Inference","Monocular Depth Estimation","Holoviz","Video"]},{"location":"applications/depth_anything_v2/#run-instructions","title":"Run Instructions","text":"","tags":["Computer Vision and Perception","Visualization","Edge Accelerated Inference","Monocular Depth Estimation","Holoviz","Video"]},{"location":"applications/depth_anything_v2/#v4l2-support","title":"V4L2 Support","text":"<p>This application supports v4l2 compatible devices as input.  To run this application with your v4l2 compatible device, please plug in your input device and run: <pre><code>./holohub run depth_anything_v2\n</code></pre></p> <p>By default, this application expects the input device to be mounted at <code>/dev/video0</code>.  If this is not the case, update <code>applications/depth_anything_v2/depth_anything_v2.yaml</code> file to set the corresponding input device before running the application.  You can also override the default input device on the command line by running: <pre><code>./holohub run depth_anything_v2 --run-args=\"--video_device /dev/video0\"\n</code></pre></p>","tags":["Computer Vision and Perception","Visualization","Edge Accelerated Inference","Monocular Depth Estimation","Holoviz","Video"]},{"location":"applications/depth_anything_v2/#video-replayer-support","title":"Video Replayer Support","text":"<p>If you don't have a v4l2 compatible device plugged in, you can also run this application on a pre-recorded video. To launch the application using the Video Stream Replayer as the input source, run:</p> <pre><code>./holohub run depth_anything_v2 --run-args=\"--source replayer\"\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Edge Accelerated Inference","Monocular Depth Estimation","Holoviz","Video"]},{"location":"applications/depth_anything_v2/#display-modes","title":"Display Modes","text":"<p>This application has multiple display modes which you can toggle through using the left mouse button.</p> <ul> <li>original: output the original image from input source</li> <li>depth: output the color depthmap based on the depthmap returned from Depth Anything V2 model</li> <li>side-by-side: output a side-by-side view of the original image next to the color depthmap</li> <li>interactive: allow user </li> </ul> <p>In interactive mode, the middle or right mouse button can be used to modify the ratio of original image vs color depthmap is shown.</p>","tags":["Computer Vision and Perception","Visualization","Edge Accelerated Inference","Monocular Depth Estimation","Holoviz","Video"]},{"location":"applications/depth_anything_v2/#acknowledgement","title":"Acknowledgement","text":"<p>This project is based on the following projects: - Depth-Anything-V2 - Depth Anything V2 - depth-anything-tensorrt - Depth Anything TensorRT CLI</p>","tags":["Computer Vision and Perception","Visualization","Edge Accelerated Inference","Monocular Depth Estimation","Holoviz","Video"]},{"location":"applications/depth_anything_v2/#known-issues","title":"Known Issues","text":"<p>There is a known issue running this application on IGX w/ iGPU and on Jetson AGX (see #500). The workaround is to update the device to avoid picking up the libnvv4l2.so library.</p> <pre><code>cd /usr/lib/aarch64-linux-gnu/\nls -l libv4l2.so.0.0.999999\nsudo rm libv4l2.so.0.0.999999\nsudo ln -s libv4l2.so.0.0.0.0  libv4l2.so.0.0.999999\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Edge Accelerated Inference","Monocular Depth Estimation","Holoviz","Video"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/","title":"Distributed Endoscopy Tool Tracking with gRPC Streaming","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.7.0 Tested Holoscan SDK versions: 2.7.0 Contribution metric: Level 0 - Core Stable</p> <p>This application demonstrates how to offload heavy workloads to a remote Holoscan application using gRPC.</p>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#overview","title":"Overview","text":"<p>In this sample application, we divided the Endoscopy Tool Tracking application into a server and client application where the two communicate via gRPC.</p> <p>The client application inputs a video file and streams the video frames to the server application. The server application handles the heavy workloads of inferencing and post-processing of the video frames. It receives the video frames, processes each frame through the endoscopy tool tracking pipeline, and then streams the results to the client.</p> <p> Endoscopy Tool Tracking Application with gRPC</p> <p>From the diagram above, we can see that both the App Cloud (the server) and the App Edge (the client) are very similar to the standalone Endoscopy Tool Tracking application. This section will only describe the differences; for details on inference and post-processing, please refer to the link above.</p> <p>On the client side, we provided two examples, one using a single fragment and another one using two fragments. When comparing the client side to the standalone Endoscopy Tool Tracking application, the differences are the queues and the gRPC client. We added the following: - Outgoing Requests operator (<code>GrpcClientRequestOp</code>): It converts the video frames (GXF entities) received from the Video Stream Replayer operator into <code>EntityRequest</code> protobuf messages and queues each frame in the Request Queue. - gRPC Service &amp; Client (<code>EntityClientService</code> &amp; <code>EntityClient</code>): The gRPC Service is responsible for controlling the life cycle of the gRPC client. The client connects to the remote gRPC server and then sends the requests found in the Request Queue. When it receives a response, it converts it into a GXF entity and queues it in the Response Queue. - Incoming Responses operator (<code>GrpcClientResponseOp</code>): This operator is configured with an <code>AsynchronousCondition</code> condition to check the availability of the Response Queue. When notified of available responses in the queue, it dequeues each item and emits each to the output port.</p> <p>The App Cloud (the server) application consists of a gRPC server and a few components for managing Holoscan applications. When the server receives a new remote procedure call in this sample application, it launches a new instance of the Endoscopy Tool Tracking application. This is facilitated by the <code>ApplicationFactory</code> used for application registration.</p> <p>Under the hood, the Endoscopy Tool Tracking application here inherits a custom base class (<code>HoloscanGrpcApplication</code>) which manages the <code>Request Queue</code> and the <code>Response Queue</code> as well as the <code>GrpcServerRequestOp</code> and <code>GrpcServerResponseOp</code> operators for receiving requests and serving results, respectively. When the RPC is complete, the instance of the Endoscopy Tool Tracking application is destroyed and ready to serve the subsequent request.</p>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#requirements","title":"Requirements","text":"","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded when building the application.</p>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#building-and-running-grpc-endoscopy-tool-tracking-application","title":"Building and Running gRPC Endoscopy Tool Tracking Application","text":"","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#c","title":"C++","text":"<pre><code># Start the gRPC Server\n./holohub run grpc_endoscopy_tool_tracking --run-args=\"cloud\" [--language=cpp]\n\n# Start the gRPC Client\n./holohub run grpc_endoscopy_tool_tracking --run-args=\"edge\" [--language=cpp]\n</code></pre>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#python","title":"Python","text":"<pre><code># Start the gRPC Server\n./holohub run grpc_endoscopy_tool_tracking --language python --run-args=\"cloud\"\n\n# Start the gRPC Client\n./holohub run grpc_endoscopy_tool_tracking --language python --run-args=\"edge\"\n</code></pre>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#configurations","title":"Configurations","text":"<p>The Edge application runs in a single-fragment mode by default. However, it can be configured to run in a multi-fragment mode, as in the picture above.</p> <p>To switch to multi-fragment mode, edit the endoscopy_tool_tracking.yaml YAML file and change <code>multifragment</code> to <code>true</code>:</p> <pre><code>application:\n  multifragment: false\n  benchmarking: false\n</code></pre> <p>[!NOTE] The Python version of this application is only available in single-fragment mode with benchmarking turned on.</p> <p>Data Flow Tracking can also be enabled by editing the endoscopy_tool_tracking.yaml YAML file and changing <code>benchmarking</code> to <code>true</code>. This enables the built-in mechanism to profile the application and analyze the fine-grained timing properties and data flow between operators.</p> <p>For example, on the server side, when a client disconnects, it will output the results for that session:</p> <pre><code>Data Flow Tracking Results:\nTotal paths: 1\n\nPath 1: grpc_request_op,format_converter,lstm_inferer,tool_tracking_postprocessor,grpc_response_op\nNumber of messages: 663\nMin Latency Message No: 249\nMin end-to-end Latency (ms): 1.868\nAvg end-to-end Latency (ms): 2.15161\nMax Latency Message No: 371\nMax end-to-end Latency (ms): 4.19\n\nNumber of source messages [format: source operator-&gt;transmitter name: number of messages]:\ngrpc_request_op-&gt;output: 683\n</code></pre> <p>Similarly, on the client side, when it completes playing the video, it will print the results:</p> <pre><code>Data Flow Tracking Results:\nTotal paths: 3\n\nPath 1: incoming_responses,visualizer_op\nNumber of messages: 663\nMin Latency Message No: 249\nMin end-to-end Latency (ms): 0.214\nAvg end-to-end Latency (ms): 0.374005\nMax Latency Message No: 378\nMax end-to-end Latency (ms): 2.751\n\nPath 2: replayer,outgoing_requests\nNumber of messages: 663\nMin Latency Message No: 379\nMin end-to-end Latency (ms): 24.854\nAvg end-to-end Latency (ms): 27.1886\nMax Latency Message No: 142\nMax end-to-end Latency (ms): 28.003\n\nPath 3: replayer,visualizer_op\nNumber of messages: 663\nMin Latency Message No: 372\nMin end-to-end Latency (ms): 30.966\nAvg end-to-end Latency (ms): 33.325\nMax Latency Message No: 397\nMax end-to-end Latency (ms): 35.479\n\nNumber of source messages [format: source operator-&gt;transmitter name: number of messages]:\nincoming_responses-&gt;output: 683\nreplayer-&gt;output: 683\n</code></pre>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#development-environment","title":"Development Environment","text":"","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#dev-container","title":"Dev Container","text":"<p>To start the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode\n</code></pre>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#c_1","title":"C++","text":"<p>The following launch profiles are available:</p> <ul> <li>(compound) grpc_endoscopy_tool_tracking/cpp (cloud &amp; edge): Launch both the gRPC server and the client.</li> <li>(gdb) grpc_endoscopy_tool_tracking/cpp (cloud): Launch the gRPC server.</li> <li>(gdb) grpc_endoscopy_tool_tracking/cpp (edge): Launch the gRPC client.</li> </ul>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#python_1","title":"Python","text":"<p>The following launch profiles are available:</p> <ul> <li>(compound) grpc_endoscopy_tool_tracking/python (cloud &amp; edge): Launch both the gRPC server and the client.</li> <li>(pythoncpp) grpc_endoscopy_tool_tracking/python (cloud): Launch the gRPC server with <code>pythoncpp</code>.</li> <li>(pythoncpp) grpc_endoscopy_tool_tracking/python (edge): Launch the gRPC client with <code>pythoncpp</code>.</li> <li>(debugpy) grpc_endoscopy_tool_tracking/python (cloud): Launch the gRPC server with <code>debugpy</code>.</li> <li>(debugpy) grpc_endoscopy_tool_tracking/python (edge):Launch the gRPC client with <code>debugpy</code>.</li> </ul> <p>[!NOTE] The <code>compound</code> profile uses the <code>debugpy</code> extension due to a limitation that prevents launching the cloud and the edge apps together using <code>pythoncpp</code>.</p>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#limitations-known-issues","title":"Limitations &amp; Known Issues","text":"","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#c_2","title":"C++","text":"<ol> <li>Connection Timeout:</li> <li>The connection between the server and client is controlled by <code>rpc_timeout</code></li> <li>Default timeout is 5 seconds, configurable in endoscopy_tool_tracking.yaml</li> <li> <p>Consider increasing this value on slower networks</p> </li> <li> <p>Server Limitations:</p> </li> <li>Can only serve one request at a time</li> <li> <p>Subsequent calls receive <code>grpc::StatusCode::RESOURCE_EXHAUSTED</code> status</p> </li> <li> <p>Debugging Issues:</p> </li> <li>When using the compound profile, the server may need additional startup time</li> <li> <p>If needed, adjust the sleep value in tasks.json under <code>Build grpc_endoscopy_tool_tracking (delay 3s)</code></p> </li> <li> <p>Expected Exit Behavior:</p> </li> <li>The client will exit with the following expected error when the video completes:      <pre><code>[error] [program.cpp:614] Event notification 2 for entity [video_in__outgoing_requests] with id [33] received in an unexpected state [Origin]\n</code></pre></li> </ol>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#python_2","title":"Python","text":"<ul> <li>The client may require manual termination (CTRL+C) if errors occur during execution</li> </ul>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#containerization","title":"Containerization","text":"<p>To containerize the application:</p> <ol> <li>Install Holoscan CLI</li> <li>Build the application:    <pre><code>./holohub install grpc_endoscopy_tool_tracking\n</code></pre></li> <li>Run the appropriate packaging script:</li> <li>C++: cpp/package-app.sh</li> <li>Python: python/package-app.sh</li> <li>Follow the generated output instructions to package and run the application</li> </ol> <p>For more information about packaging Holoscan applications, refer to the Packaging Holoscan Applications section in the Holoscan User Guide.</p>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/","title":"Distributed H.264 Endoscopy Tool Tracking with gRPC Streaming","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.6.0 Tested Holoscan SDK versions: 2.6.0 Contribution metric: Level 0 - Core Stable</p> <p>This application demonstrates how to offload heavy workloads to a remote Holoscan application using gRPC.</p>","tags":["Healthcare AI","Distributed","Asynchronous Queues","gRPC","Video"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#overview","title":"Overview","text":"<p>In this sample application, we divided the h.264 Endoscopy Tool Tracking application into a server and client application where the two communicate via gRPC.</p> <p>The client application reads a pre-recorded h.264 video file and streams the encoded video frames to the server application. The server application handles the heavy workloads of inferencing and post-processing of the video frames. It receives the video frames, processes each frame through the endoscopy tool tracking pipeline, and then streams the results to the client.</p> <p> h.264 Endoscopy Tool Tracking Application with gRPC</p> <p>From the diagram above, we can see that both the App Cloud (the server) and the App Edge (the client) are very similar to the standalone Endoscopy Tool Tracking application. This section will only describe the differences; for details on inference and post-processing, please refer to the link above.</p> <p>On the client side, the differences are the queues and the gRPC client. In the Video Input Fragment, we added the following: - Outgoing Requests operator (<code>GrpcClientRequestOp</code>): It converts the video frames (GXF entities) received from the Video Read Stream operator into <code>EntityRequest</code> protobuf messages and queues each frame in the Request Queue. - gRPC Service &amp; Client (<code>EntityClientService</code> &amp; <code>EntityClient</code>): The gRPC Service is responsible for controlling the life cycle of the gRPC client. The client connects to the remote gRPC server and then sends the requests found in the Request Queue. When it receives a response, it converts it into a GXF entity and queues it in the Response Queue. - Incoming Responses operator (<code>GrpcClientResponseOp</code>): This operator is configured with an <code>AsynchronousCondition</code> condition to check the availability of the Response Queue. When notified of available responses in the queue, it dequeues each item and emits each to the output port.</p> <p> Details of App Cloud</p> <p>The App Cloud (the server) application consists of a gRPC server and a few components for managing Holoscan applications. When the server receives a new remote procedure call in this sample application, it launches a new instance of the Endoscopy Tool Tracking application. This is facilitated by the <code>ApplicationFactory</code> used for application registration.</p> <p>Under the hood, the Endoscopy Tool Tracking application here inherits a custom base class (<code>HoloscanGrpcApplication</code>) which manages the <code>Request Queue</code> and the <code>Response Queue</code> as well as the <code>GrpcServerRequestOp</code> and <code>GrpcServerResponseOp</code> operators for receiving requests and serving results, respectively. When the RPC is complete, the instance of the Endoscopy Tool Tracking application is destroyed and ready to serve the subsequent request.</p>","tags":["Healthcare AI","Distributed","Asynchronous Queues","gRPC","Video"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#requirements","title":"Requirements","text":"<p>This application is configured to use H.264 elementary stream from endoscopy sample data as input.</p>","tags":["Healthcare AI","Distributed","Asynchronous Queues","gRPC","Video"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded when building the application.</p>","tags":["Healthcare AI","Distributed","Asynchronous Queues","gRPC","Video"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#building-and-running-grpc-h264-endoscopy-tool-tracking-application","title":"Building and Running gRPC H.264 Endoscopy Tool Tracking Application","text":"<ul> <li>Building and running the application from the top level Holohub directory:</li> </ul>","tags":["Healthcare AI","Distributed","Asynchronous Queues","gRPC","Video"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#c","title":"C++","text":"<pre><code># Start the gRPC Server\n./holohub run grpc_h264_endoscopy_tool_tracking --run-args=\"cloud\" [--language cpp]\n\n# Start the gRPC Client\n./holohub run grpc_h264_endoscopy_tool_tracking --run-args=\"edge\" [--language cpp]\n</code></pre> <p>Important: on aarch64, applications also need tegra folder mounted inside the container and the <code>LD_LIBRARY_PATH</code> environment variable should be updated to include tegra folder path.</p> <p>Open and edit the Dockerfile and uncomment line 66:</p> <pre><code># Uncomment the following line for aarch64 support\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/aarch64-linux-gnu/tegra/\n</code></pre>","tags":["Healthcare AI","Distributed","Asynchronous Queues","gRPC","Video"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode h264\n</code></pre>","tags":["Healthcare AI","Distributed","Asynchronous Queues","gRPC","Video"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Healthcare AI","Distributed","Asynchronous Queues","gRPC","Video"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#c_1","title":"C++","text":"<p>The following launch profiles are available:</p> <ul> <li>(compound) grpc_h264_endoscopy_tool_tracking/cpp (cloud &amp; edge): Launch both the gRPC server and the client.</li> <li>(gdb) grpc_h264_endoscopy_tool_tracking/cpp (cloud): Launch the gRPC server.</li> <li>(gdb) grpc_h264_endoscopy_tool_tracking/cpp (edge): Launch the gRPC client.</li> </ul>","tags":["Healthcare AI","Distributed","Asynchronous Queues","gRPC","Video"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#limitations-known-issues","title":"Limitations &amp; Known Issues","text":"<ul> <li>The connection between the server and the client is controlled by <code>rpc_timeout</code>. If no data is received or sent within the configured time, it assumes the call has been completed and hangs up. The <code>rpc_timeout</code> value can be configured in the endoscopy_tool_tracking.yaml file with a default of 5 seconds. Increasing this value may help on a slow network.</li> <li>The server can serve one request at any given time. Any subsequent call receives a <code>grpc::StatusCode::RESOURCE_EXHAUSTED</code> status.</li> <li>When debugging using the compound profile, the server may not be ready to serve, resulting in errors with the client application. When this happens, open tasks.json, find <code>Build grpc_h264_endoscopy_tool_tracking (delay 3s)</code>, and adjust the <code>command</code> field with a higher sleep value.</li> <li>The client is expected to exit with the following error. It is how the client application terminates when it completes streaming and displays the entire video.   <pre><code>[error] [program.cpp:614] Event notification 2 for entity [video_in__outgoing_requests] with id [33] received in an unexpected state [Origin]\n</code></pre></li> </ul>","tags":["Healthcare AI","Distributed","Asynchronous Queues","gRPC","Video"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/cpp/","title":"UCX-based Distributed Endoscopy Tool Tracking","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: June 30, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 0 - Core Stable</p> <p>This application is similar to the Endoscopy Tool Tracking application, but the distributed version divides the application into three fragments:</p> <ol> <li>Video Input: get video input from a pre-recorded video file.</li> <li>Inference: run the inference using LSTM and run the post-processing script.</li> <li>Visualization: display input video and inference results.</li> </ol> <p>Based on an LSTM (long-short term memory) stateful model, these applications demonstrate the use of custom components for tool tracking, including composition and rendering of text, tool position, and mask (as heatmap) combined with the original video stream.</p>","tags":["Healthcare AI","Distributed","Surgical AI","Endoscopy","Video","Computer Vision and Perception","Segmentation"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/cpp/#requirements","title":"Requirements","text":"<p>The provided applications are configured to use a pre-recorded endoscopy video (replayer).</p>","tags":["Healthcare AI","Distributed","Surgical AI","Endoscopy","Video","Computer Vision and Perception","Segmentation"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/cpp/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Healthcare AI","Distributed","Surgical AI","Endoscopy","Video","Computer Vision and Perception","Segmentation"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/cpp/#run-instructions","title":"Run Instructions","text":"<pre><code># Build the Holohub container for the Distributed Endoscopy Tool Tracking application\n./holohub build-container ucx_endoscopy_tool_tracking --img holohub:ucx_endoscopy_tool_tracking\n\n# Launch the container\n./holohub run-container ucx_endoscopy_tool_tracking --no-docker-build --img holohub:ucx_endoscopy_tool_tracking\n\n# Build the Distributed Endoscopy Tool Tracking application\n./holohub build ucx_endoscopy_tool_tracking --local\n\n# Generate the TRT engine file from onnx\npython3 utilities/generate_trt_engine.py --input data/endoscopy/tool_loc_convlstm.onnx --output data/endoscopy/engines/ --fp16\n\n# Start the application with all three fragments\n./holohub run ucx_endoscopy_tool_tracking --language=cpp --local --no-local-build\n\n# Once you have completed the step to generate the TRT engine file, you may exit the container and\n#  use the following commands to run the application in distributed mode:\n\n# Start the application with the video_in fragment\n./holohub run ucx_endoscopy_tool_tracking --language=cpp --run-args=\"--driver --worker --fragments video_in --address :9999\"\n# Start the application with the inference fragment\n./holohub run ucx_endoscopy_tool_tracking --language=cpp --run-args=\"--worker --fragments inference --address :9999\"\n# Start the application with the visualization fragment\n./holohub run ucx_endoscopy_tool_tracking --language=cpp --run-args=\"--worker --fragments viz --address :9999\"\n</code></pre>","tags":["Healthcare AI","Distributed","Surgical AI","Endoscopy","Video","Computer Vision and Perception","Segmentation"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/python/","title":"UCX-based Distributed Endoscopy Tool Tracking","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application is similar to the Endoscopy Tool Tracking application, but the distributed version divides the application into three fragments:</p> <ol> <li>Video Input: get video input from a pre-recorded video file.</li> <li>Inference: run the inference using LSTM and run the post-processing script.</li> <li>Visualization: display input video and inference results.</li> </ol> <p>Based on an LSTM (long-short term memory) stateful model, these applications demonstrate the use of custom components for tool tracking, including composition and rendering of text, tool position, and mask (as heatmap) combined with the original video stream.</p>","tags":["Healthcare AI","Distributed","Surgical AI","Endoscopy","Video","Computer Vision and Perception","Segmentation"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/python/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8+</li> <li>The provided applications are configured to use a pre-recorded endoscopy video (replayer).</li> </ul>","tags":["Healthcare AI","Distributed","Surgical AI","Endoscopy","Video","Computer Vision and Perception","Segmentation"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/python/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Healthcare AI","Distributed","Surgical AI","Endoscopy","Video","Computer Vision and Perception","Segmentation"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/python/#run-instructions","title":"Run Instructions","text":"<pre><code># Start the application with all three fragments\n./holohub run ucx_endoscopy_tool_tracking --language=python\n\n# Use the following commands to run the same application three processes:\n# Start the application with the video_in fragment\n./holohub run ucx_endoscopy_tool_tracking --language=python --run-args=\"--driver --worker --fragments video_in --address :10000\"\n# Start the application with the inference fragment\n./holohub run ucx_endoscopy_tool_tracking --language=python --run-args=\"--worker --fragments inference --address :10000\"\n# Start the application with the visualization fragment\n./holohub run ucx_endoscopy_tool_tracking --language=python --run-args=\"--worker --fragments viz --address :10000\"\n</code></pre>","tags":["Healthcare AI","Distributed","Surgical AI","Endoscopy","Video","Computer Vision and Perception","Segmentation"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/","title":"Distributed H.264 Endoscopy Tool Tracking","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.6.0 Tested Holoscan SDK versions: 2.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application is similar to the H.264 Endoscopy Tool Tracking application, but this distributed version divides the application into three fragments:</p> <ol> <li>Video Input: get video input from a pre-recorded video file.</li> <li>Inference: run the inference using LSTM and run the post-processing script.</li> <li>Visualization: display input video and inference results.</li> </ol>","tags":["Healthcare AI","Video","Surgical AI","Distributed","Holoviz","LSTM","TensorRT","Endoscopy"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/#requirements","title":"Requirements","text":"<p>This application is configured to use H.264 elementary stream from endoscopy sample data as input.</p>","tags":["Healthcare AI","Video","Surgical AI","Distributed","Holoviz","LSTM","TensorRT","Endoscopy"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded when building the application.</p>","tags":["Healthcare AI","Video","Surgical AI","Distributed","Holoviz","LSTM","TensorRT","Endoscopy"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/#building-and-running-h264-endoscopy-tool-tracking-application","title":"Building and Running H.264 Endoscopy Tool Tracking Application","text":"<ul> <li>Building and running the application from the top level Holohub directory:</li> </ul>","tags":["Healthcare AI","Video","Surgical AI","Distributed","Holoviz","LSTM","TensorRT","Endoscopy"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/#c","title":"C++","text":"<pre><code># Start the application with all three fragments\n./holohub run ucx_h264_endoscopy_tool_tracking --language=cpp\n\n# Use the following commands to run the same application three processes:\n# Start the application with the video_in fragment\n./holohub run ucx_h264_endoscopy_tool_tracking --language=cpp --run-args=\"--driver --worker --fragments video_in --address :10000 --worker-address :10001\"\n# Start the application with the inference fragment\n./holohub run ucx_h264_endoscopy_tool_tracking --language=cpp --run-args=\"--worker --fragments inference --address :10000 --worker-address :10002\"\n# Start the application with the visualization fragment\n./holohub run ucx_h264_endoscopy_tool_tracking --language=cpp --run-args=\"--worker --fragments viz --address :10000 --worker-address :10003\"\n</code></pre>","tags":["Healthcare AI","Video","Surgical AI","Distributed","Holoviz","LSTM","TensorRT","Endoscopy"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/#python","title":"Python","text":"<pre><code># Start the application with all three fragments\n./holohub run ucx_h264_endoscopy_tool_tracking --language=python\n\n# Use the following commands to run the same application three processes:\n# Start the application with the video_in fragment\n./holohub run ucx_h264_endoscopy_tool_tracking --language=python --run-args=\"--driver --worker --fragments video_in --address :10000 --worker-address :10001\"\n# Start the application with the inference fragment\n./holohub run ucx_h264_endoscopy_tool_tracking --language=python --run-args=\"--worker --fragments inference --address :10000 --worker-address :10002\"\n# Start the application with the visualization fragment\n./holohub run ucx_h264_endoscopy_tool_tracking --language=python --run-args=\"--worker --fragments viz --address :10000 --worker-address :10003\"\n</code></pre> <p>Important: on aarch64, applications also need tegra folder mounted inside the container and the <code>LD_LIBRARY_PATH</code> environment variable should be updated to include tegra folder path.</p> <p>Open and edit the Dockerfile and uncomment line 66:</p> <pre><code># Uncomment the following line for aarch64 support\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/aarch64-linux-gnu/tegra/\n</code></pre>","tags":["Healthcare AI","Video","Surgical AI","Distributed","Holoviz","LSTM","TensorRT","Endoscopy"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/#dev-container","title":"Dev Container","text":"<p>To start the VS Code Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode h264\n</code></pre>","tags":["Healthcare AI","Video","Surgical AI","Distributed","Holoviz","LSTM","TensorRT","Endoscopy"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Healthcare AI","Video","Surgical AI","Distributed","Holoviz","LSTM","TensorRT","Endoscopy"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/#c_1","title":"C++","text":"<p>Use the (gdb) ucx_h264_endoscopy_tool_tracking/cpp (all fragments) launch profile to run and debug the C++ application.</p>","tags":["Healthcare AI","Video","Surgical AI","Distributed","Holoviz","LSTM","TensorRT","Endoscopy"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/#python_1","title":"Python","text":"<p>Use the (pythoncpp) ucx_h264_endoscopy_tool_tracking/python (all fragments) launch profile to run and debug the Python application.</p>","tags":["Healthcare AI","Video","Surgical AI","Distributed","Holoviz","LSTM","TensorRT","Endoscopy"]},{"location":"applications/ehr_query_llm/fhir/","title":"FHIR Client for Retrieving and Posting FHIR Resources","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.5.0 Tested Holoscan SDK versions: 2.5.0, 2.7.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This is an application to interface with a FHIR server to retrieve or post FHIR resources.</p> <p>It requires that the FHIR Server endpoint URL is provided on the command line as well as client authentication credentials if required. Currently, authentication and authorization is limited to OAuth2.0 server-to-server workflow. When authorization is required by the server, its OAuth2.0 token service URL along with client ID and secret must be provided to the application.</p> <p>This application also uses ZeroMQ to communicate with its own clients, listening on a well-known port on localhost for messages to retrieve resources of a patient, as well as publishing the retrieved resources on another well-known port. For simplicity, the listening port is defined in the code to be <code>5600</code>, and the publishing port <code>5601</code>. Messaging security, at transport or message level, is not implemented in this example.</p> <p>The message schema is simple, with a well-known topic string and topic-specific content schema in JSON format.</p> <p>The default set of FHIR resource types to retrieve are listed below, which can be overridden by the request message:</p> <ul> <li>Observation</li> <li>ImagingStudy</li> <li>FamilyMemberHistory</li> <li>Condition</li> <li>DiagnosticReport</li> <li>DocumentReference</li> </ul>","tags":["Healthcare AI","Distributed","ZeroMQ","Auth and API","Healthcare Interop","LLM"]},{"location":"applications/ehr_query_llm/fhir/#requirements","title":"Requirements","text":"<ul> <li>On a Holohub supported platform</li> <li>Python 3.10+</li> <li>Python packages from PyPI, including holoscan, fhir.resources, pyzmq, requests and their dependencies</li> </ul>","tags":["Healthcare AI","Distributed","ZeroMQ","Auth and API","Healthcare Interop","LLM"]},{"location":"applications/ehr_query_llm/fhir/#run-instructions","title":"Run Instructions","text":"<p>There are several ways to build and run this application and package it as a Holoscan Application Package, an Open Container Initiative compliant image. The following sections describe each in detail.</p> <p>It is further expected that you have read the HoloHub README, have cloned the HoloHub repository to your local system, and the current working directory is the HoloHub root, <code>holohub</code>.</p> <p>Note: The application listens for request messages to start retrieving resources from the server and then publishes the results, so another application is needed to drive this workflow (e.g., the LLM application). To help with simple testing, a Python script is provided as part of this application, and its usage is described below in this section.</p>","tags":["Healthcare AI","Distributed","ZeroMQ","Auth and API","Healthcare Interop","LLM"]},{"location":"applications/ehr_query_llm/fhir/#quick-start-using-holohub-container","title":"Quick Start Using Holohub Container","text":"<p>This is the simplest and fastest way to start the application in a Holohub dev container and get it ready to listen to request messages.</p> <p>Note: Please use your own FHIR server endpoint, as well as the OAuth2.0 authorization endpoint and client credentials as needed.</p> <pre><code>./holohub run fhir --run-args \"--fhir_url &lt;f_url&gt; --auth_url &lt;a_url&gt; --uid &lt;id&gt; --secret &lt;token&gt;\"\n</code></pre>","tags":["Healthcare AI","Distributed","ZeroMQ","Auth and API","Healthcare Interop","LLM"]},{"location":"applications/ehr_query_llm/fhir/#run-the-application-in-holohub-dev-container","title":"Run the Application in Holohub Dev Container","text":"<p>Launch the container:</p> <pre><code>./holohub run-container fhir\n</code></pre> <p>This command builds the <code>holohub:fhir</code> container based on the application-specific Dockerfile.</p> <p>Build and run the application:</p> <p>Now in the container, build and run the application:</p> <pre><code>~$ pwd\n/workspace/holohub\n\n~$ ./holohub clear-cache\n~$ ./holohub run fhir --run-args \"--fhir_url &lt;f_url&gt; --auth_url &lt;a_url&gt; --uid &lt;id&gt; --secret &lt;token&gt;\"\n</code></pre> <p>Once done, <code>exit</code> the container.</p>","tags":["Healthcare AI","Distributed","ZeroMQ","Auth and API","Healthcare Interop","LLM"]},{"location":"applications/ehr_query_llm/fhir/#run-the-application-in-the-host-dev-environment-bare-metal","title":"Run the Application in the Host Dev Environment (Bare Metal)","text":"<p>First, create and activate a Python virtual environment, followed by installing the dependencies:</p> <pre><code>python3 -m venv .testenv\nsource .testenv/bin/activate\npip install -r applications/ehr_query_llm/fhir/requirements.txt\n</code></pre> <p>Then, Set up the Holohub environment:</p> <pre><code>sudo ./holohub setup\n</code></pre> <p>Note: Although this application is implemented entirely in Python and relies on standard PyPI packages, you still may want to set up Holohub environment and use <code>./holohub</code> commandline .</p> <p>Next, build and install the application with <code>./holohub</code>:</p> <pre><code>./holohub install fhir --local\n</code></pre> <p>Now, run the application which is installed in the <code>install</code> folder, with server URLs and credentials of your own:</p> <pre><code>python install/bin/fhir/python/fhir_client.py --fhir_url &lt;f_url&gt; --auth_url &lt;a_url&gt; --uid &lt;id&gt; --secret &lt;token&gt;\n</code></pre>","tags":["Healthcare AI","Distributed","ZeroMQ","Auth and API","Healthcare Interop","LLM"]},{"location":"applications/ehr_query_llm/fhir/#test-the-running-application","title":"Test the Running Application","text":"<p>Once the FHIR application has been started with one of the above methods, a test application can be used to request and receive FHIR resources, namely <code>applications/ehr_query_llm/fhir/test_fhir_client.py</code>.</p> <p>The test application contains hard-coded patient name, patient FHIR resource ID, etc., corresponding to a specific test dataset, though it can be easily modified for another dataset.</p> <p>It is strongly recommended to run this test application in a Python virtual environment, which can be the same as that used for running the FHIR application. The following describes running it in its own environment:</p> <pre><code>echo \"Assuming venv already created with \\`python3 -m venv .testenv\\`\"\nsource .testenv/bin/activate\npip install -r applications/ehr_query_llm/fhir/requirements.txt\nexport PYTHONPATH=${PWD}\npython applications/ehr_query_llm/fhir/test_fhir_client.py\n</code></pre> <p>From the menu, pick one of the choices for the resources of interest.</p>","tags":["Healthcare AI","Distributed","ZeroMQ","Auth and API","Healthcare Interop","LLM"]},{"location":"applications/ehr_query_llm/fhir/#packaging-the-application-for-distribution-and-deployment","title":"Packaging the Application for Distribution and Deployment","text":"<p>With Holoscan CLI, applications built with Holoscan SDK can be packaged into a Holoscan Application Package (HAP), which is an Open Container Initiative compliant image. An HAP is well suited to be distributed for deployment on hosting platforms, be it Docker Compose, Kubernetes, or otherwise. Please refer to Packaging Holoscan Applications in the User Guide for more information.</p> <p>This example application provides all the necessary contents for HAP packaging. It is required to perform the packaging in a Python virtual environment, with the application's dependencies installed, before running the following script to reveal specific packaging commands.</p> <pre><code>applications/ehr_query_llm/fhir/packageHAP.sh\n</code></pre> <p>Once the HAP is created, it can then be saved and restored on the target deployment host, and run with the <code>docker run</code> command, shown below with user-specific parameters to be substituted.</p> <pre><code>docker run -it --rm --net host holohub-fhir-x64-workstation-dgpu-linux-amd64:1.0 \\\n--fhir_url &lt;f_url&gt; \\\n--auth_url &lt;a_url&gt; \\\n--uid &lt;id&gt; \\\n--secret &lt;token&gt;\n</code></pre> <p>Note: Packaging this application requires <code>holoscan-cli</code>, which can be installed using <code>pip</code>. If you are using the same Python environment for packaging as your development environment, there may be a version conflict for the <code>pydantic</code> package, as it is required by both <code>holoscan-cli</code> and <code>fhir.resources</code>. To ensure your development environment can still run the application after packaging, reinstall <code>fhir.resources</code>:</p> <pre><code>pip install fhir.resources\n</code></pre>","tags":["Healthcare AI","Distributed","ZeroMQ","Auth and API","Healthcare Interop","LLM"]},{"location":"applications/ehr_query_llm/lmm/","title":"EHR Agent Framework","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 1.0.0 Tested Holoscan SDK versions: 1.0.0, 2.5, 2.7.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The EHR Agent Framework is designed to handle and interact with EHR (Electronic Health Records) and it provides a modular and extensible system for handling various types of queries through specialized agents, with robust error handling and performance optimization features.</p>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Agent Framework overview</li> <li>Setup Instructions</li> <li>Run Instructions</li> <li>Offline Mode</li> </ul>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#agent-framework-overview","title":"Agent Framework overview","text":"<p>The <code>AgentFrameworkOp</code> orchestrates multiple specialized agents to handle different types of queries and responses, it maintains a streaming queue for responses and it handles response states through <code>ResponseHandler</code>. It tracks conversation history using ChatHistory class and updates history based on agent responses and ASR transcripts.</p> <p>Agent Lifecycle:</p> <ul> <li>Processes requests asynchronously using threads</li> <li>Controls response streaming and muting during speech</li> </ul>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#the-base-agent-class","title":"The Base Agent Class","text":"<p>This is an abstract base class implementing common agent functionality:</p> <ul> <li>Uses threading.Lock for LLM and , if needed, LMM access</li> <li>Prevents concurrent requests to language models</li> </ul> <p>It loads configuration from YAML files and it handles prompt templates, tokens limits, and model URLs. The Base Agent is designed to stream responses from LLM server and it supports both text and image-based prompts while enforcing maximum prompt token limits. Throughout the agent lifecycle it maintains conversation context and it creates conversation strings within token limits.</p>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#the-selector-agent","title":"The Selector Agent","text":"<p>The Selector Agent routes user queries to the appropriate specialized agent by analyzing user input to determine the appropriate agent and return the selected agent name and corrected input text. For response parsing, it handles JSON response format. If there are parsing failures, it logs them and it returns <code>None</code> for invalid selection.</p>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#the-ehr-builder-agent","title":"The EHR Builder Agent","text":"<p>The EHR Builder Agent handles EHR database construction on demand and it tracks and reports build time performance in the process. For response generation, it uses custom prompt templates for EHR tasks and it returns structured JSON responses. It also verifies build capability before execution and it reports success/failure status.</p>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#the-ehr-agent","title":"The EHR Agent","text":"<p>The EHR Agent handles EHR queries and data retrieval. It uses Chroma for document storage while implementing HuggingFaceBgeEmbeddings for embeddings. For the RAG (Retrieval-Augmented Generation) pipeline, it performs MMR (Maximal Marginal Relevance) search with configurable search parameters (<code>k</code>, <code>lambda_mult</code>, <code>fetch_k</code>). For prompt generation, it incorporates retrieved documents into prompts and it supports both standard and RAG-specific prompts. The EHR Agent is using CUDA for embedding computation and optimizes for cosine similarity calculations.</p>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#common-features-across-agents","title":"Common features across agents","text":"","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#configuration-management","title":"Configuration Management","text":"<ul> <li>YAML-based settings</li> <li>Configurable prompts and rules</li> <li>Extensible tool support</li> </ul>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#response-handling","title":"Response Handling","text":"<ul> <li>Streaming response support</li> <li>Mutable response states</li> <li>Structured output formatting</li> </ul>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#error-management","title":"Error Management","text":"<ul> <li>Connection retry logic</li> <li>Comprehensive error logging</li> <li>Graceful failure handling</li> </ul>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Thread-safe operations</li> <li>Token usage optimization</li> <li>Efficient resource management</li> </ul>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#setup-instructions","title":"Setup Instructions","text":"","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#speech-pipeline","title":"Speech pipeline","text":"<p>Note</p> <p>NVIDIA Riva provides speech and translation services for user interaction with the LLM. We recommend running Riva in the bare metal host environment outside of the development container to minimize demands on container resources. During test run, it was observed that Riva could take up around 8 GB of GPU memory, while the rest of the application around 12 GB of GPU memory.</p> <p>NVIDIA Riva Version Compatibility : tested with v2.13.0 / v2.14.0.</p> <p>Please adhere to the \"Data Center\" configuration specifications in the Riva Quick Start guide.</p> <p>To optimize Riva installation footprint:</p> <ul> <li>Locate the <code>config.sh</code> file in the riva_quickstart_vX.XX.X directory.</li> <li>Modify the <code>service_enabled_*</code> variables as follows:</li> </ul> <pre><code>service_enabled_asr=true\nservice_enabled_nlp=false\nservice_enabled_tts=true\nservice_enabled_nmt=false\n</code></pre>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#model-acquisition","title":"Model acquisition","text":"<p>It is recommended to create a directory called <code>/models</code> on your machine to download the LLM.</p> <p>Download the quantized Mistral 7B finetuned LLM from HugginFace.co:</p> <pre><code>wget -nc -P &lt;your_model_dir&gt; https://huggingface.co/TheBloke/openchat-3.5-0106-GGUF/resolve/main/openchat-3.5-0106.Q8_0.gguf\n</code></pre> <p>Download the BGE-large finetuned embedding model from NGC:</p> <p><code>bash  wget https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/models/bge-large-ehr-finetune</code></p> <p>Execute the following command from the Holohub root directory:</p> <pre><code>./holohub build lmm\n</code></pre>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#run-instructions","title":"Run Instructions","text":"","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#step-1-enabling-httpsssl-only-required-once","title":"Step 1: Enabling HTTPS/SSL (only required once)","text":"<p>\u26a0\ufe0f Note: This has only been tested with Chrome and Chromium</p> <p>Browsers require HTTPS to be used in order to access the client's microphone.  Hence, you'll need to create a self-signed SSL certificate and key.</p> <p>This key must be placed in <code>/applications/ehr_query_llm/lmm/ssl</code></p> <pre><code>cd &lt;holohub root&gt;/applications/ehr_query_llm/lmm/\nmkdir ssl\ncd ssl\nopenssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -sha256 -days 365 -nodes -subj '/CN=localhost'\n</code></pre> <p>When you first navigate your browser to a page that uses these self-signed certificates, it will issue you a warning since they don't originate from a trusted authority. Ignore this and proceed to the web app:</p> <p></p> <p>ehr_query_llm will use your default speaker and microphone. To change this go to your Ubuntu sound settings and choose the correct devices:</p> <p></p>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#step-2-ensure-riva-server-is-running","title":"Step 2: Ensure Riva server is running","text":"<p>The Riva server must be running to use the LLM pipeline. If it is already running you can skip this step.</p> <pre><code>cd &lt;riva install dir&gt;\nbash riva_start.sh\n</code></pre>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#step-3-launch-and-run-the-app","title":"Step 3: Launch and Run the App","text":"","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#step-31","title":"Step 3.1","text":"<p>Launch the <code>holohub:lmm</code> container:</p> <pre><code>sudo ./holohub run-container lmm --add-volume &lt;your_model_dir&gt;\n</code></pre> <ul> <li>Note, if the parent directory of  is not <code>/models</code> you must update the asr_llm_tts.yaml and ehr.yaml files with the complete path to your model inside the container. You will also need to update the run_lmm.sh so the correct directory is exported in the <code>set_transformer_cache()</code> function. (You can determine these paths by looking in <code>/workspace/volumes</code> inside the launched container) or you can use the following <code>sed</code> commands: <p><code>sed -i -e 's#^model_path:.*#model_path: /workspace/volumes/&lt;your_model_dir&gt;#' asr_llm_tts.yaml</code></p> <p><code>sed -i -e 's#^model_path:.*#model_path: /workspace/volumes/&lt;your_model_dir&gt;#' agents_configs/ehr.yaml</code></p> <p><code>sed -i -e 's#^export TRANSFORMERS_CACHE=.*#export TRANSFORMERS_CACHE=\"/workspace/volumes/&lt;your_model_dir&gt;\"#' run_lmm.sh</code></p>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#step-32","title":"Step 3.2","text":"<p>Then run the application:</p> <pre><code>./applications/ehr_query_llm/lmm/run_lmm.sh\n</code></pre> <p>This command builds ehr_query_llm/lmm, starts an LLM api server, then launches the ehr_query_llm app. Access the web interface at <code>https://127.0.0.1:8080</code>. Llama.cpp LLM server output is redirected to <code>./applications/ehr_query_llm/lmm/llama_cpp.log/</code>.</p> <p>To interact with ehr_query_llm using voice input:</p> <ul> <li>Press and hold the space bar to activate the voice recognition feature.</li> <li>Speak your query or command clearly while maintaining pressure on the space bar.</li> <li>Release the space bar when you've finished speaking to signal the end of your input.</li> <li>ehr_query_llm will then process your speech and generate a response.</li> </ul> <p>\u26a0\ufe0f Note: When running via VNC, you must have your keyboard focus on the VNC terminal that you are using to run ehr_query_llm in order to use the push-to-talk feature.</p>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#stopping-instructions","title":"Stopping Instructions","text":"<p>To stop the main app, simply use <code>ctrl+c</code></p> <p>To stop Riva server:</p> <pre><code>bash &lt;Riva_install_dir&gt;riva_stop.sh\n</code></pre>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#asr_to_llm-application-arguments","title":"ASR_To_LLM Application arguments","text":"<p>The <code>asr_llm_tts.py</code> can receive the following optional cli argument:</p> <p><code>--sample-rate-hz</code>: The number of frames per second in audio streamed from the selected microphone.</p>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#offline-mode","title":"Offline mode","text":"<p>To enable offline use (no internet connection required):</p> <ol> <li>First run the complete application as-is (This ensures all relevant models are downloaded)</li> <li>Uncomment <code>set_offline_flags</code> at line 52 of run_lmm.sh</li> </ol>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#troubleshooting","title":"Troubleshooting","text":"","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#adding-agents","title":"Adding Agents","text":"<p>An Agent is an LLM (or LMM) with a task specific \"persona\" - such as a EHRAgent, etc., each with their own specific task. They also have a specific prompt tailored to complete that task, pre-fix prompts specific to the model used, grammar to constrain output, as well as context length.</p> <p>The AgentFrameworkOp works by using a SelectorAgent to select which Agent should be called upon based on user input.</p> <p>Adding a new \"agent\" for ehr_query_llm involves creating a new agent .py and YAML file in the <code>agents</code> directory, and in the new .py inheriting the Agent base class <code>agents/base_agent.py</code>.</p> <p>When creating a new agent .py file, you will need to define:</p> <p>Agent name: A class name which will also need to be added to the selector agent YAML, so it knows the agent is available to be called. process_request: A runtime method describing the logic of how an agent should carry out its task and send a response.</p> <p>For the YAML file, the fields needed are:</p> <p>name: This is the name of the agent, as well as what is used as the ZeroMQ topic when the agent  publishes its output. So you must make sure your listener is using this as the topic.</p> <p>user_prefix, bot_prefix, bot_rule_prefix, end_token:: These are dependent on the particular llm or lmm being used, and help to set the correct template for the model to interact with.</p> <p>agent_prompt: This gives the agent its \"persona\" - how it should behave, and for what purpose. It should have as much context as possible.</p> <p>ctx_length: Context length for the model. This determines how much output the agent is capable of generating. Smaller values lead to faster to first token time, but can be at the sacrifice of detail and verbosity.</p> <p>grammar: This is the BNF grammar used to constrain the models output. It can be a bit tricky to write. ChatGPT is great at writing these grammars for you if you give an example JSON of what you want. Also helpful, is the Llama.cpp BNF grammar guide.</p> <p>publish: The only important part of this field is the \"ags\" sub-field. This should be a list of your arg names. This is important as this is used as the list of keys to pull the relevant args from the LMM's response, and thus ensure the relevant fields are complete for a given tool use.</p> <p>For a specific example, please refer to the EHR Agent YAML file below:</p> <pre><code>description: This tool is used to search the patient's EHR in order to answer questions about the patient, or general questions about the patient's medical history.\nuser_prefix: \"&lt;|im_start|&gt;user\"\nbot_prefix: \"&lt;|im_start|&gt;assistant\"\nbot_rule_prefix: \"&lt;|im_start|&gt;system\"\nend_token: \"&lt;|im_end|&gt;\"\nagent_prompt: |\n  You are NVIDIA's EHR Agent,your job is to assist surgeons as they prepare for surgery.\n  You are an expert when it comes to surgery and medical topics - and answer all questions to the best of your abilities.\n  The patient has signed consent for you to access and discuss all of their electronic records.\n  Be as concise as you can be with your answers.\n\n  You NEVER make-up information that isn't grounded in the provided medical documents.\n\n  If applicable, include the relevant date (use sparingly)\n  The following medical documents may be helpful to answer the surgeon's question:\n  {documents}\n  Use your expert knowledge and the above context to answer the surgeon.\n# This is the request that the LLM replies to, where '{text}' indicates where the transcribed\n# text is inserted into the prompt\nrequest: \"{text}\"\n\nctx_length: 256\n\ngrammar: |\n  space ::= \" \"?\n  string ::= \"\\\"\" ([^\"\\\\])* \"\\\"\" space \n  root ::= \"{\" space \"\\\"name\\\"\" space \":\" space \"\\\"EHRAgent\\\"\" space \",\" space \"\\\"response\\\"\" space \":\" space string \"}\" space\n\npublish:\n  ags:\n    - \"response\"\n</code></pre> <p>With a complete YAML file, an agent should be able to use any new tool effectively. The only remaining step is ensure you have a ZeroMQ listener in the primary app with a topic that is the same as the tool's name.</p> <p>The <code>AgentFrameworkOp</code> is based on a ZeroMQ publish/subscribe pattern to send and receive messages from the Message Bus. It uses the <code>MessageReceiver</code> and <code>MessageSender</code> classes implemented in the <code>message_handling.py</code> Python script. The <code>MessageSender</code> creates a ZeroMQ PUB socket that binds to port 5555, accepts connections from any interface, and is used to broadcast messages and commands from the agent framework. It uses <code>send_json()</code> to send JSON-encoded messages with topics. A 0.1-second sleep on initialization prevents the \"slow joiner\" problem where early messages might be lost. The `MessageReceiver`` creates a ZeroMQ SUB socket connecting to port 5560 and uses receive_json() to get messages, with configurable blocking behavior.</p>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#ehr-rag","title":"EHR RAG","text":"<p>To test new document formats for the database use test_db.py This will start the current Vector database in <code>./rag/ehr/db</code> and allow you to test different queries via the CLI to see what documents are returned.</p> <p>When changing the Vector DB, remove the previous database first:</p> <pre><code>rm -rf ./rag/ehr/db\n</code></pre>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#riva-cant-find-speaker-to-use","title":"Riva - Can't find speaker to use","text":"<p>This usually means that some process is using the speaker you wish to use. This could be a Riva process that didn't exit correctly, or even Outlook loaded in your browser using your speakers to play notification sounds.</p> <p>First see what processes are using your speakers:</p> <pre><code>pactl list sink-inputs | grep -E 'Sink Input|application.name|client|media.name|sink: '\n</code></pre> <p>Sometimes that will give you all the information you need to kill the process responsible. If not, and the process has unfamiliar name such as \"speech-dispatcher-espeak-ng\" then find the responsible process ID:</p> <pre><code>pgrep -l -f &lt;grep expression here (ex: 'speech')&gt;\n</code></pre> <p>Once you know the PID's of the responsible process, kill them :)</p> <pre><code>kill &lt;PID&gt;\n</code></pre>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/endoscopy_depth_estimation/","title":"Endoscopy Depth Estimation","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 2 - Trusted</p> <p>This application demonstrates the use of custom components for depth estimation and its rendering using Holoviz with triangle interpolation.</p> <p></p>","tags":["Healthcare AI","Networking and Distributed Computing","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_depth_estimation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8+</li> <li>OpenCV 4.8+</li> </ul>","tags":["Healthcare AI","Networking and Distributed Computing","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_depth_estimation/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for Endoscopy</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Healthcare AI","Networking and Distributed Computing","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_depth_estimation/#model","title":"Model","text":"<p>\ud83d\udce6\ufe0f (NGC) App Model for AI-based Endoscopy Depth Estimation</p> <p>The model is automatically downloaded to the same folder as the data in ONNX format.</p>","tags":["Healthcare AI","Networking and Distributed Computing","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_depth_estimation/#opencv-gpu","title":"OpenCV-GPU","text":"<p>This application uses OpenCV with GPU acceleration during the preprocessing stage when it runs with Histogram Equalization (flag <code>--clahe</code> or <code>-c</code>). Histogram equalization reduces the effect of specular reflections and improves the visual performance of the depth estimation overall. However, using regular OpenCV datatypes leads to unnecessary I/O operations to transfer data from Holoscan Tensors to the CPU and back. We show in this application how to blend together Holoscan Tensors and OpenCV's <code>GPUMat</code> datatype to get rid of this issue in the <code>CUDACLAHEOp</code> operator.  Compare it to <code>CPUCLAHEOp</code> for reference.</p> <p>To achieve an end-to-end GPU accelerated pipeline / application, the pre-processing operators shall support accessing the GPU memory (Holoscan Tensor)  directly without memory copy / movement in Holoscan SDK. This means that only libraries which implement the <code>__cuda_array_interface__</code>  and DLPack standards allow conversion from/to Holoscan Tensor, such as cuCIM. OpenCV, however, does not implement neither the <code>__cuda_array_interface__</code> nor the standard DLPack, and a little work is needed yet to use this library.</p> <p>First, we convert CuPy arrays to GPUMat using a fix in OpenCV only available from 4.8.0 on. More information here. This is done in the <code>gpumat_from_cp_array</code> function. With a <code>GPUMat</code>, we can now use any OpenCV-CUDA operations. Once the <code>GPUMat</code> processing has finished, we have to convert it back to a CuPy tensor with <code>gpumat_to_cupy</code>. </p> <p>Important: In order to run this application with CUDA acceleration, one must compile OpenCV with CUDA support. We provide a sample Dockerfile to build a container based on Holoscan v2.1.0 with the latest version of OpenCV and CUDA support. In case you use it, note that the variable <code>CUDA_ARCH_BIN</code>  must be modified according to your specific GPU configuration. Refer to this site to find out your NVIDIA GPU architecture.</p>","tags":["Healthcare AI","Networking and Distributed Computing","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_depth_estimation/#workflows","title":"Workflows","text":"<p>This application can be run with or without Histogram Equalization (CLAHE) by toggling the label <code>--clahe</code>.</p>","tags":["Healthcare AI","Networking and Distributed Computing","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_depth_estimation/#with-clahe","title":"With CLAHE","text":"<p> Fig. 1 Depth Estimation Application with CLAHE enabled</p> <p>The pipeline uses a recorded endoscopy video file (generated by <code>convert_video_to_gxf_entities</code> script) for input frames. Each input frame in the file is loaded by Video Stream Replayer and passed to the following two branches: - In the first branch (top), the input frames are passed to the <code>CUDACLAHEOp</code>,  then fed to the Format Converter to convert their data type from <code>uint8</code> to <code>float32</code>, and finally fed to the <code>InferenceOp</code>. The result is then ingested by the <code>DepthPostProcessingOp</code>, which converts the depth map to <code>uint8</code> and reorders its dimensions for rendering with Holoviz. - In the second branch (bottom), the input frames are passed to a Format Converter that resizes them. Its output is finally fed to the <code>DepthPostProcessingOp</code> for  rendering with Holoviz.</p>","tags":["Healthcare AI","Networking and Distributed Computing","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_depth_estimation/#without-clahe","title":"Without CLAHE","text":"<p> Fig. 2 Depth Estimation Application with CLAHE disabled</p> <p>The pipeline uses a recorded endoscopy video file (generated by <code>convert_video_to_gxf_entities</code> script) for input frames. Each input frame in the file is loaded by Video Stream Replayer and passed to a branch that firstly converts its data type to <code>float32</code> and resizes it with a Format Converter. Then, the preprocessed frames are fed to the <code>InferenceOp</code> and mixed with the original video in the custom <code>DepthPostProcessingOp</code> for rendering with Holoviz.</p>","tags":["Healthcare AI","Networking and Distributed Computing","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_depth_estimation/#run-instructions","title":"Run Instructions","text":"<p>To run this application, you'll need to configure your PYTHONPATH environment variable to locate the necessary python libraries based on your Holoscan SDK installation type.</p> <p>You should refer to the glossary for the terms defining specific locations within HoloHub.</p> <p>If your Holoscan SDK installation type is:</p> <ul> <li>python wheels:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <ul> <li>otherwise:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOSCAN_INSTALL_DIR&gt;/python/lib:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <p>This application should be run in the build directory of Holohub in order to load the GXF extensions. Alternatively, the relative path of the extensions in the corresponding yaml file can be modified to match path of the working directory.</p> <p>Next, run the command to run the application:</p> <pre><code>cd &lt;HOLOHUB_BUILD_DIR&gt;\npython3 &lt;HOLOHUB_SOURCE_DIR&gt;/applications/endoscopy_depth_estimation/endoscopy_depth_estimation.py --data=&lt;DATA_DIR&gt; --model=&lt;MODEL_DIR&gt; --clahe\n</code></pre>","tags":["Healthcare AI","Networking and Distributed Computing","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_depth_estimation/#container-build-run-instructions","title":"Container Build &amp; Run Instructions","text":"<p>Build container using Holoscan 2.0.0 NGC container as base image and built OpenCV with CUDA ARCH 8.6, 8.7 and 8.9 support for IGX Orin and Ampere and Ada Lovelace Architecture dGPUs. This application is currently not supported on iGPU.</p>","tags":["Healthcare AI","Networking and Distributed Computing","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_depth_estimation/#change-directory-to-holohub-source-directory","title":"Change directory to Holohub source directory","text":"<pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;\n</code></pre>","tags":["Healthcare AI","Networking and Distributed Computing","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_depth_estimation/#build-and-run-the-application-using-the-development-container","title":"Build and run the application using the development container","text":"<pre><code>./holohub run endoscopy_depth_estimation\n</code></pre>","tags":["Healthcare AI","Networking and Distributed Computing","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_depth_estimation/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode endoscopy_depth_estimation\n</code></pre> <p>This command will build and configure a Dev Container using a Dockerfile that is ready to run the application.</p>","tags":["Healthcare AI","Networking and Distributed Computing","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_depth_estimation/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"<p>There are two launch profiles configured for this application:</p> <ol> <li>(debugpy) endoscopy_depth_estimation/python: Launch endoscopy_depth_estimation using a launch profile that enables debugging of Python code.</li> <li>(pythoncpp) endoscopy_depth_estimation/python: Launch endoscopy_depth_estimation using a launch profile that enables debugging of Python and C++ code.</li> </ol>","tags":["Healthcare AI","Networking and Distributed Computing","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/","title":"Endoscopy Out of Body Detection","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.9.0 Contribution metric: Level 1 - Highly Reliable</p> <p></p>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#overview","title":"Overview","text":"<p>This application performs real-time detection of whether an endoscope is inside or outside the body during endoscopic procedures. For each input frame, the application:</p> <ul> <li>Classifies the frame as either \"in-body\" or \"out-of-body\"</li> <li>Provides a confidence score for the classification</li> <li>Outputs either to the console or to a CSV file (when analytics is enabled)</li> </ul> <p>Note: This application does not include visualization components.</p>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#quick-start","title":"Quick Start","text":"<p>Run the following command to build and launch the application on a supported Holoscan platform:</p> <pre><code>./holohub run endoscopy_out_of_body_detection --language cpp\n</code></pre>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#prerequisites","title":"Prerequisites","text":"<ul> <li>NVIDIA Holoscan SDK (version 0.5 or higher)</li> <li>A supported Holoscan platform or workstation with a CUDA-capable NVIDIA GPU</li> <li>CMake build system</li> <li>FFmpeg (for data conversion)</li> </ul>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#data-requirements","title":"Data Requirements","text":"","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#model-and-sample-data","title":"Model and Sample Data","text":"<p>The endoscopy detection model and sample datasets are available on NGC. The package includes:</p> <ul> <li>Pre-trained ONNX model for out-of-body detection: <code>out_of_body_detection.onnx</code></li> <li>Sample endoscopy video clips (MP4 format): <code>sample_clip_out_of_body_detection.mp4</code></li> </ul>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#data-preparation-optional","title":"Data Preparation (optional)","text":"<p>The application requires the input videos to be converted to GXF tensor format. This conversion happens automatically during building, but manual conversion can be done following these steps:</p> <ol> <li> <p>Download and extract the data:</p> <pre><code>unzip [NGC_DOWNLOAD].zip -d &lt;data_dir&gt;\n</code></pre> </li> <li> <p>Convert the video to GXF tensor format using the provided script:</p> <pre><code>ffmpeg -i &lt;INPUT_VIDEO_FILE&gt; -fs 900M -pix_fmt rgb24 -f rawvideo pipe:1 | \\\npython convert_video_to_gxf_entities.py --width 256 --height 256 --channels 3 --framerate 30\n</code></pre> <p>Note: The conversion script (<code>convert_video_to_gxf_entities.py</code>) is available in the Holoscan SDK repository.</p> </li> <li> <p>Organize the data directory as follows:</p> <pre><code>data/\n\u2514\u2500\u2500 endoscopy_out_of_body_detection/\n  \u251c\u2500\u2500 LICENSE.md\n  \u251c\u2500\u2500 out_of_body_detection.onnx\n  \u251c\u2500\u2500 sample_clip_out_of_body_detection.gxf_entities\n  \u251c\u2500\u2500 sample_clip_out_of_body_detection.gxf_index\n  \u2514\u2500\u2500 sample_clip_out_of_body_detection.mp4\n</code></pre> </li> </ol>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#configuration","title":"Configuration","text":"<p>The application uses <code>endoscopy_out_of_body_detection.yaml</code> for configuration. Key settings include:</p> <ul> <li>Input video parameters in the <code>replayer</code> section</li> <li>Model parameters in the <code>inference</code> section</li> <li>Analytics settings for data export</li> </ul>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#building-and-running-the-application","title":"Building and running the application","text":"<pre><code>./holohub run endoscopy_out_of_body_detection --language cpp\n</code></pre> <p>It builds and starts a Docker container, and then builds and runs the application inside the container.</p> <p>For more information, see the Holohub README.md.</p>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#running-in-development-mode","title":"Running in development mode","text":"<p>You can also run the application with customized arguments, you can use Holohub CLI for creating and starting the Holohub container, and then building and running the application inside the container as follows:</p> <ol> <li> <p>Create and start the Holohub container:</p> <pre><code>./holohub run-container endoscopy_out_of_body_detection\n</code></pre> </li> <li> <p>Build the application:</p> <p>Once in the docker container, you can build the application by running the following command:</p> <pre><code>./holohub build endoscopy_out_of_body_detection --language python\n</code></pre> </li> <li> <p>Run the application:</p> <p>After building the application, you can run it from your build directory with the following command for the basic usage and can modify the arguments as needed:</p> <pre><code>applications/endoscopy_out_of_body_detection/endoscopy_out_of_body_detection \\\n  --config endoscopy_out_of_body_detection.yaml \\\n  --data ../data/endoscopy_out_of_body_detection\n</code></pre> </li> </ol>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#analytics-mode","title":"Analytics Mode","text":"<p>To enable analytics and export results to CSV:</p> <ol> <li>Set <code>enable_analytics: true</code> in the configuration file:</li> </ol> <pre><code># endoscopy_out_of_body_detection.yaml\nenable_analytics: true\n</code></pre> <ol> <li>Configure analytics output (optional):</li> </ol> <pre><code># Set output directory (default: current directory)\nexport HOLOSCAN_ANALYTICS_DATA_DIRECTORY=\"/path/to/output\"\n\n# Set output filename (default: data.csv)\nexport HOLOSCAN_ANALYTICS_DATA_FILE_NAME=\"results.csv\"\n</code></pre> <p>The application will create:</p> <ul> <li>A directory named after the application</li> <li>Subdirectories with timestamps for each run</li> <li>CSV files containing frame-by-frame classification results</li> </ul>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#output-format","title":"Output Format","text":"<ul> <li> <p>Console Mode: Displays \"Likely in-body\" or \"Likely out-of-body\" along with confidence scores for each frame.</p> </li> <li> <p>Analytics Mode: Outputs a CSV file with frame-by-frame classification results in the following format:</p> </li> </ul> <pre><code>In-body,Out-of-body,Confidence Score\n1,0,0.972432\n1,0,0.902066\n</code></pre>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/python/","title":"Endoscopy Out of Body Detection","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.9.0 Contribution metric: Level 1 - Highly Reliable</p> <p></p>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/python/#overview","title":"Overview","text":"<p>This application performs real-time detection of whether an endoscope is inside or outside the body during endoscopic procedures. For each input frame, the application:</p> <ul> <li>Classifies the frame as either \"in-body\" or \"out-of-body\"</li> <li>Provides a confidence score for the classification</li> <li>Outputs either to the console or to a CSV file (when analytics is enabled)</li> </ul> <p>Note: This application does not include visualization components.</p>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/python/#quick-start","title":"Quick Start","text":"<p>Run the following command to build and launch the application on a supported Holoscan platform:</p> <pre><code>./holohub run endoscopy_out_of_body_detection --language python\n</code></pre>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/python/#prerequisites","title":"Prerequisites","text":"<ul> <li>NVIDIA Holoscan SDK (version 0.5 or higher)</li> <li>A supported Holoscan platform or workstation with a CUDA-capable NVIDIA GPU</li> <li>CMake build system</li> <li>FFmpeg (for data conversion)</li> </ul>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/python/#data-requirements","title":"Data Requirements","text":"","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/python/#model-and-sample-data","title":"Model and Sample Data","text":"<p>The endoscopy detection model and sample datasets are available on NGC. The package includes:</p> <ul> <li>Pre-trained ONNX model for out-of-body detection: <code>out_of_body_detection.onnx</code></li> <li>Sample endoscopy video clips (MP4 format): <code>sample_clip_out_of_body_detection.mp4</code></li> </ul>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/python/#data-preparation-optional","title":"Data Preparation (optional)","text":"<p>The application requires the input videos to be converted to GXF tensor format. This conversion happens automatically during building, but manual conversion can be done following these steps:</p> <ol> <li> <p>Download and extract the data:</p> <pre><code>unzip [NGC_DOWNLOAD].zip -d &lt;data_dir&gt;\n</code></pre> </li> <li> <p>Convert the video to GXF tensor format using the provided script:</p> <pre><code>ffmpeg -i &lt;INPUT_VIDEO_FILE&gt; -fs 900M -pix_fmt rgb24 -f rawvideo pipe:1 | \\\npython convert_video_to_gxf_entities.py --width 256 --height 256 --channels 3 --framerate 30\n</code></pre> <p>Note: The conversion script (<code>convert_video_to_gxf_entities.py</code>) is available in the Holoscan SDK repository.</p> </li> <li> <p>Organize the data directory as follows:</p> <pre><code>data/\n\u2514\u2500\u2500 endoscopy_out_of_body_detection/\n  \u251c\u2500\u2500 LICENSE.md\n  \u251c\u2500\u2500 out_of_body_detection.onnx\n  \u251c\u2500\u2500 sample_clip_out_of_body_detection.gxf_entities\n  \u251c\u2500\u2500 sample_clip_out_of_body_detection.gxf_index\n  \u2514\u2500\u2500 sample_clip_out_of_body_detection.mp4\n</code></pre> </li> </ol>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/python/#configuration","title":"Configuration","text":"<p>The application uses <code>endoscopy_out_of_body_detection.yaml</code> for configuration. Key settings include:</p> <ul> <li>Input video parameters in the <code>replayer</code> section</li> <li>Model parameters in the <code>inference</code> section</li> <li>Analytics settings for data export</li> </ul>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/python/#building-and-running-the-application","title":"Building and running the application","text":"<p>You can simply run the application with the following command:</p> <pre><code>./holohub run endoscopy_out_of_body_detection --language python\n</code></pre> <p>It builds and starts a Docker container, and then builds and runs the application inside the container.</p> <p>For more information, see the Holohub README.md.</p>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/python/#running-in-development-mode","title":"Running in development mode","text":"<p>You can also run the application with customized arguments, you can use Holohub CLI for creating and starting the Holohub container, and then building and running the application inside the container as follows:</p> <ol> <li> <p>Create and start the Holohub container:</p> <pre><code>./holohub run-container endoscopy_out_of_body_detection\n</code></pre> </li> <li> <p>Build the application:</p> <p>Once in the docker container, you can build the application by running the following command:</p> <pre><code>./holohub build endoscopy_out_of_body_detection --language python\n</code></pre> </li> <li> <p>Run the application:</p> <p>After building the application, you can run it from your build directory with the following command for the basic usage and can modify the arguments as needed:</p> <pre><code>applications/endoscopy_out_of_body_detection/endoscopy_out_of_body_detection.py \\\n  --config endoscopy_out_of_body_detection.yaml \\\n  --data ../data/endoscopy_out_of_body_detection\n</code></pre> </li> </ol>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/python/#analytics-mode","title":"Analytics Mode","text":"<p>To enable analytics and export results to CSV:</p> <ol> <li>Set <code>enable_analytics: true</code> in the configuration file:</li> </ol> <pre><code># endoscopy_out_of_body_detection.yaml\nenable_analytics: true\n</code></pre> <ol> <li>Configure analytics output (optional):</li> </ol> <pre><code># Set output directory (default: current directory)\nexport HOLOSCAN_ANALYTICS_DATA_DIRECTORY=\"/path/to/output\"\n\n# Set output filename (default: data.csv)\nexport HOLOSCAN_ANALYTICS_DATA_FILE_NAME=\"results.csv\"\n</code></pre> <p>The application will create:</p> <ul> <li>A directory named after the application</li> <li>Subdirectories with timestamps for each run</li> <li>CSV files containing frame-by-frame classification results</li> </ul>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/python/#output-format","title":"Output Format","text":"<ul> <li> <p>Console Mode: Displays \"Likely in-body\" or \"Likely out-of-body\" along with confidence scores for each frame.</p> </li> <li> <p>Analytics Mode: Outputs a CSV file with frame-by-frame classification results in the following format:</p> </li> </ul> <pre><code>In-body,Out-of-body,Confidence Score\n1,0,0.972432\n1,0,0.902066\n</code></pre>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_tool_tracking/cpp/","title":"Endoscopy Tool Tracking","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3, 2.0.0, 2.1.0, 2.2.0 Contribution metric: Level 0 - Core Stable</p> <p>Based on a LSTM (long-short term memory) stateful model, these applications demonstrate the use of custom components for tool tracking, including composition and rendering of text, tool position, and mask (as heatmap) combined with the original video stream.</p>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/cpp/#requirements","title":"Requirements","text":"<p>The provided applications are configured to either use capture cards for input stream, or a pre-recorded endoscopy video (replayer).</p> <p>Follow the setup instructions from the user guide to use the AJA capture card.</p> <p>Refer to the Deltacast documentation to use the Deltacast VideoMaster capture card.</p> <p>Refer to the Yuan documentation to use the Yuan QCap capture card.</p>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/cpp/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/cpp/#build-instructions","title":"Build Instructions","text":"<p>Please refer to the top level Holohub README.md file for information on how to build this application. In order to build with the Deltacast VideoMaster operator use <code>./run build --with deltacast_videomaster</code></p>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/cpp/#run-instructions","title":"Run Instructions","text":"<p>In your <code>build</code> directory, run the commands of your choice:</p> <ul> <li> <p>Using a pre-recorded video     <pre><code>sed -i -e 's#^source:.*#source: replayer#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\napplications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking --data &lt;data_dir&gt;/endoscopy\n</code></pre></p> </li> <li> <p>Using a vtk_renderer instead of holoviz     <pre><code>sed -i -e 's#^visualizer:.*#visualizer: \"vtk\"#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\napplications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking --data &lt;data_dir&gt;/endoscopy\n</code></pre></p> </li> <li> <p>Using a holoviz instead of vtk_renderer     <pre><code>sed -i -e 's#^visualizer:.*#visualizer: \"holoviz\"#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\napplications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking --data &lt;data_dir&gt;/endoscopy\n</code></pre></p> </li> <li> <p>Using an AJA card     <pre><code>sed -i -e 's#^source:.*#source: aja#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\napplications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking\n</code></pre></p> </li> <li> <p>Using a Deltacast card     <pre><code>sed -i -e '/^#.*deltacast_videomaster/s/^#//' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\nsed -i -e 's#^source:.*#source: deltacast#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\napplications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking\n</code></pre></p> </li> <li>Using a Yuan card     <pre><code>sed -i -e '/^#.*yuan_qcap/s/^#//' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\nsed -i -e 's#^source:.*#source: yuan#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\napplications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking\n</code></pre></li> <li>Using an AJA card with hardware keying overlay (Only specific cards support this feature)     <pre><code>./run launch endoscopy_tool_tracking cpp --extra_args -capplications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking_aja_overlay.yaml\n</code></pre></li> </ul>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/python/","title":"Endoscopy Tool Tracking","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3, 2.0.0, 2.1.0, 2.2.0 Contribution metric: Level 1 - Highly Reliable</p> <p>Based on a LSTM (long-short term memory) stateful model, these applications demonstrate the use of custom components for tool tracking, including composition and rendering of text, tool position, and mask (as heatmap) combined with the original video stream.</p>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/python/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8+</li> <li>The provided applications are configured to either use the AJA, DELTACAST or Yuan capture cards for input stream, or a pre-recorded endoscopy video (replayer).  Follow the setup instructions from the user guide to use the AJA capture card.</li> </ul>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/python/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/python/#run-instructions","title":"Run Instructions","text":"<p>To run this application, you'll need to configure your PYTHONPATH environment variable to locate the necessary python libraries based on your Holoscan SDK installation type.</p> <p>You should refer to the glossary for the terms defining specific locations within HoloHub.</p> <p>If your Holoscan SDK installation type is:</p> <ul> <li>python wheels:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <ul> <li>otherwise:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOSCAN_INSTALL_DIR&gt;/python/lib:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <p>Next, run the commands of your choice:</p> <p>This application should be run in the build directory of Holohub in order to load the GXF extensions. Alternatively, the relative path of the extensions in the corresponding yaml file can be modified to match path of the working directory.</p> <ul> <li> <p>Using a pre-recorded video     <pre><code>cd &lt;HOLOHUB_BUILD_DIR&gt;\npython3 &lt;HOLOHUB_SOURCE_DIR&gt;/applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking.py --source=replayer --data=&lt;DATA_DIR&gt;/endoscopy\n</code></pre></p> </li> <li> <p>Using an AJA card     <pre><code>./run launch endoscopy_tool_tracking python --extra_args -s=aja\n</code></pre></p> </li> <li> <p>Using a YUAN card     <pre><code>cd &lt;HOLOHUB_BUILD_DIR&gt;\npython3  &lt;HOLOHUB_SOURCE_DIR&gt;/applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking.py --source=yuan\n</code></pre></p> </li> <li>Using an AJA card with hardware keying overlay (Only specific cards support this feature)     <pre><code>./run launch endoscopy_tool_tracking python --extra_args \"-c=applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking_aja_overlay.yaml -s=aja\"\n</code></pre></li> </ul>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/florence-2-vision/","title":"Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates how to run the Florence-2 models on a live video feed with the possibility of changing the task and optional prompt via a QT UI.</p> <p> </p> <p>Note: This demo currently uses Florence-2-large-ft, but any of the Florence-2 models should work as long as the correct URLs and names are used in Dockerfile and config.yaml: - Florence-2-large-ft - Florence-2-large - Florence-2-base-ft - Florence-2-base</p>","tags":["Computer Vision and Perception","Video","CUDA Holoviz Integration","Qt","Multimodal Model","Detection","Segmentation"]},{"location":"applications/florence-2-vision/#setup-instructions","title":"\u2699\ufe0f Setup Instructions","text":"<p>The app defaults to using the video device at <code>/dev/video0</code></p> <p>Note: You can use a USB webcam as the video source, or an MP4 video by following the instructions for the V4L2_Camera example app.</p> <p>To debug if this is the correct device download <code>v4l2-ctl</code>: <pre><code>sudo apt-get install v4l-utils\n</code></pre> To check for your devices run: <pre><code>v4l2-ctl --list-devices\n</code></pre> This command will output something similar to this: <pre><code>NVIDIA Tegra Video Input Device (platform:tegra-camrtc-ca):\n        /dev/media0\n\nvi-output, lt6911uxc 2-0056 (platform:tegra-capture-vi:0):\n        /dev/video0\n\nDummy video device (0x0000) (platform:v4l2loopback-000):\n        /dev/video3\n</code></pre> Determine your desired video device and edit the source device in config.yaml</p>","tags":["Computer Vision and Perception","Video","CUDA Holoviz Integration","Qt","Multimodal Model","Detection","Segmentation"]},{"location":"applications/florence-2-vision/#build-and-run-instructions","title":"\ud83d\ude80 Build and Run Instructions","text":"<p>From the Holohub main directory run the following command: <pre><code>./holohub florence-2-vision\n</code></pre> Note: The first build will take ~1.5 hours if you're on ARM64. This is largely due to building Flash Attention 2 since pre-built wheels are not distributed for ARM64 platforms.</p>","tags":["Computer Vision and Perception","Video","CUDA Holoviz Integration","Qt","Multimodal Model","Detection","Segmentation"]},{"location":"applications/florence-2-vision/#supported-hardware","title":"\ud83d\udcbb Supported Hardware","text":"<ul> <li>IGX w/ dGPU</li> <li>x86 w/ dGPU</li> <li>IGX w/ iGPU and Jetson AGX supported with workaround   There is a known issue running this application on IGX w/ iGPU and on Jetson AGX (see #500).   The workaround is to update the device to avoid picking up the libnvv4l2.so library.</li> </ul> <pre><code>cd /usr/lib/aarch64-linux-gnu/\nls -l libv4l2.so.0.0.999999\nsudo rm libv4l2.so.0.0.999999\nsudo ln -s libv4l2.so.0.0.0.0  libv4l2.so.0.0.999999\n</code></pre>","tags":["Computer Vision and Perception","Video","CUDA Holoviz Integration","Qt","Multimodal Model","Detection","Segmentation"]},{"location":"applications/florence-2-vision/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode florence-2-vision\n</code></pre> <p>This command will build and configure a Dev Container using a Dockerfile that is ready to run the application.</p>","tags":["Computer Vision and Perception","Video","CUDA Holoviz Integration","Qt","Multimodal Model","Detection","Segmentation"]},{"location":"applications/florence-2-vision/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"<p>There are two launch profiles configured for this application:</p> <ol> <li>(debugpy) florence-2-vision/python: Launch florence-2-vision using a launch profile that enables debugging of Python code.</li> <li>(pythoncpp) florence-2-vision/python: Launch florence-2-vision using a launch profile that enables debugging of Python and C++ code.</li> </ol>","tags":["Computer Vision and Perception","Video","CUDA Holoviz Integration","Qt","Multimodal Model","Detection","Segmentation"]},{"location":"applications/fm_asr/","title":"FM Radio Automatic Speech Recognition","text":"<p> Authors: Joshua Martinez (NVIDIA) Supported platforms: x86_64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.4.1 Tested Holoscan SDK versions: 0.4.1, 0.5.0 Contribution metric: Level 3 - Developmental</p> <p>This project is proof-of-concept demo featuring the combination of real-time, low-level signal processing and deep learning inference. It currently supports the RTL-SDR. Specifically, this project demonstrates the demodulation, downsampling, and automatic transcription of live, civilian FM radio broadcasts. The pipeline architecture is shown in the figure below. </p> <p></p> <p>The primary pipeline segments are written in Python. Future improvements will introduce a fully C++ system.</p> <p>This project leverages NVIDIA's Holoscan SDK for performant GPU pipelines, cuSignal package for GPU-accelerated signal processing, and the RIVA SDK for high accuracy automatic speech recognition (ASR).</p>","tags":["Signal Processing","Audio","FM demodulation","Polyphase Resampling","ASR","gRPC"]},{"location":"applications/fm_asr/#table-of-contents","title":"Table of Contents","text":"<ul> <li>FM ASR</li> <li>Table of Contents</li> <li>Install<ul> <li>Local Sensor - Basic Configuration</li> <li>Local Jetson Container</li> <li>Remote Sensor - Network in the Loop</li> <li>Bare Metal Install</li> </ul> </li> <li>Startup<ul> <li>Scripted Launch</li> <li>Manual Launch</li> <li>Initialize and Start the Riva Service</li> </ul> </li> <li>Configuration Parameters<ul> <li>Known Issues</li> </ul> </li> </ul>","tags":["Signal Processing","Audio","FM demodulation","Polyphase Resampling","ASR","gRPC"]},{"location":"applications/fm_asr/#install","title":"Install","text":"<p>To begin installation, clone this repository using the following: <pre><code>git clone https://github.com/nvidia-holoscan/holohub.git\n</code></pre> NVIDIA Riva is required to perform the automated transcriptions. You will need to install and configure the NGC-CLI tool, if you have not done so already, to obtain the Riva container and API. The Riva installation steps may be found at this link: Riva-Install. Note that Riva performs a TensorRT build during setup and requires access to the targeted GPU.  This project has been tested with RIVA 2.10.0.</p> <p>Container-based development and deployment is supported. The supported configurations are explained in the sections that follow. </p>","tags":["Signal Processing","Audio","FM demodulation","Polyphase Resampling","ASR","gRPC"]},{"location":"applications/fm_asr/#local-sensor-basic-configuration","title":"Local Sensor - Basic Configuration","text":"<p>The Local Sensor configuration assumes that the RTL-SDR is connected directly to the GPU-enabled system via USB. I/Q samples are collected from the RTL-SDR directly, using the SoapySDR library. Specialized containers are provided for Jetson devices.</p> <p>Only two containers are used in this configuration:  - The Application Container which includes all the necessary low level libraries, radio drivers, Holoscan SDK for the core application pipeline, and the Riva client API; and - The Riva SDK container that houses the ASR transcription service.</p> <p></p> <p>For convenience, container build scripts are provided to automatically build the application containers for Jetson and x86 systems. The Dockerfiles can be readily modified for ARM based systems with a discrete GPU. To build the container for this configuration, run the following: <pre><code># Starting from FM-ASR root directory\ncd scripts\n./build_application_container.sh # builds Application Container\n</code></pre> Note that this script does not build the Riva container.</p> <p>A script for running the application container is also provided. The run scripts will start the containers and leave the user at a bash terminal for development. Separate launch scripts are provided to automatically run the application. <pre><code># Starting from FM-ASR root directory\n./scripts/run_application_container.sh\n</code></pre></p>","tags":["Signal Processing","Audio","FM demodulation","Polyphase Resampling","ASR","gRPC"]},{"location":"applications/fm_asr/#local-jetson-container","title":"Local Jetson Container","text":"<p>Helper scripts will be provided in a future release.</p>","tags":["Signal Processing","Audio","FM demodulation","Polyphase Resampling","ASR","gRPC"]},{"location":"applications/fm_asr/#remote-sensor-network-in-the-loop","title":"Remote Sensor - Network in the Loop","text":"<p>This configuration is currently in work and will be provided in a future release. Developers can modify this code base to support this configuration if desired.</p>","tags":["Signal Processing","Audio","FM demodulation","Polyphase Resampling","ASR","gRPC"]},{"location":"applications/fm_asr/#bare-metal-install","title":"Bare Metal Install","text":"<p>Will be added in the future. Not currently supported.</p>","tags":["Signal Processing","Audio","FM demodulation","Polyphase Resampling","ASR","gRPC"]},{"location":"applications/fm_asr/#startup","title":"Startup","text":"<p>After installation, the following steps are needed to launch the application: 1. Start the Riva ASR service 2. Launch the Application Container</p>","tags":["Signal Processing","Audio","FM demodulation","Polyphase Resampling","ASR","gRPC"]},{"location":"applications/fm_asr/#scripted-launch","title":"Scripted Launch","text":"<p>The above steps are automated by some helper scripts. <pre><code># Starting from FM-ASR root directory\n./scripts/lauch_application.sh # Starts Application Container and launches app using the config file defined in the script\n</code></pre></p>","tags":["Signal Processing","Audio","FM demodulation","Polyphase Resampling","ASR","gRPC"]},{"location":"applications/fm_asr/#manual-launch","title":"Manual Launch","text":"<p>As an alternative to <code>launch_application.sh</code>, the FM-ASR pipeline can be run from inside the Application Container using the following commands: <pre><code>cd /workspace\nexport CONFIG_FILE=/workspace/params/holoscan.yml # can be edited by user\npython fm_asr_app.py $CONFIG_FILE\n</code></pre></p>","tags":["Signal Processing","Audio","FM demodulation","Polyphase Resampling","ASR","gRPC"]},{"location":"applications/fm_asr/#initialize-and-start-the-riva-service","title":"Initialize and Start the Riva Service","text":"<p>Riva can be setup following the Quickstart guide (version 2.10.0 currently supported). To summarize it, run the following: <pre><code>cd &lt;riva_quickstart_download_directory&gt;\nbash riva_init.sh\nbash riva_start.sh\n</code></pre> The initialization step will take a while to complete but only needs to be done once. Riva requires a capable GPU to setup and run properly. If your system has insufficient resources, the initialization script may hang. </p> <p>When starting the service, Riva may output a few \"retrying\" messages. This is normal and not an indication that the service is frozen. You should see a message saying <code>Riva server is ready...</code> once successful. </p> <p>Note for users with multiple GPUs:</p> <p>If you want to specify which GPU Riva uses (defaults to device 0), open and edit <code>&lt;riva_quickstart_download_directory&gt;/config.sh</code>, then change line <pre><code>gpus_to_use=\"device=0\"\n</code></pre> to <pre><code>gpus_to_use=\"device=&lt;your-device-number&gt;\"\n# or, to guarantee a specific device\ngpus_to_use=\"device=&lt;your-GPU-UUID&gt;\"\n</code></pre> You can determine your GPUs' UUIDs by running <code>nvidia-smi -L</code>.</p>","tags":["Signal Processing","Audio","FM demodulation","Polyphase Resampling","ASR","gRPC"]},{"location":"applications/fm_asr/#configuration-parameters","title":"Configuration Parameters","text":"<p>A table of the configuration parameters used in this project is shown below, organized by application operator.</p> Parameter Type Description run_time int Number of seconds that pipeline will execute RtlSdrGeneratorOp sample_rate float Reception sample rate used by the radio. RTL-SDR max stable sample rate without dropping is 2.56e6. tune_frequency float Tuning frequency for the radio in Hz. gain float 40.0 PlayAudioOp play_audio bool Flag used to enable simultaneous audio playback of signal. RivaAsrOp sample_rate int Audio sample rate expected by the Riva ASR model. Riva default is to 16000, other values will incurr an additional resample operation within Riva. max_alternatives int Riva - Maximum number of alternative transcripts to return (up to limit configured on server). Setting to 1 returns only the best response. word-time-offsets bool Riva - Option to output word timestamps in transcript. automatic-punctuation bool Riva - Flag that controls if transcript should be automatically punctuated. uri str localhost:50051 no-verbatim-transcripts bool Riva - If specified, text inverse normalization will be applied boosted_lm_words str Riva - words to boost when decoding. Useful for handling jargon and acronyms. boosted_lm_score float Value by which to boost words when decoding language-code str Riva - Language code of the model to be used. US English is en-US. Check Riva docs for more options interim_transcriptions bool Riva - Flag to include interim transcriptions in the output file. ssl_cert str Path to SSL client certificates file. Not currently utilized use_ssl bool Boolean to control if SSL/TLS encryption should be used. Not currently utilized. recognize_interval int Specifies the amount of data RIVA processes per request, in time (s). TranscriptSinkOp output_file str File path to store a transcript. Existing files will be overwritten.","tags":["Signal Processing","Audio","FM demodulation","Polyphase Resampling","ASR","gRPC"]},{"location":"applications/fm_asr/#known-issues","title":"Known Issues","text":"<p>This table will be populated as issues are identified.</p> Issue Description Status","tags":["Signal Processing","Audio","FM demodulation","Polyphase Resampling","ASR","gRPC"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/","title":"H.264 Endoscopy Tool Tracking","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.6.0 Tested Holoscan SDK versions: 2.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The application showcases how to use H.264 video source as input to and output from the Holoscan pipeline. This application is a modified version of Endoscopy Tool Tracking reference application in Holoscan SDK that supports H.264 elementary streams as the input and output.</p> <p>The H.264 video decode operators do not adjust framerate as it reads the elementary stream input. As a result the video stream can be displayed as quickly as the decoding can be performed. This application uses <code>PeriodicCondition</code> to play video at the same speed as the source video.</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/#requirements","title":"Requirements","text":"<p>This application is configured to use H.264 elementary stream from endoscopy sample data as input. The recording of the output can be enabled by setting <code>record_output</code> flag in the config file to <code>true</code>. If the <code>record_output</code> flag in the config file is set to <code>true</code>, the output of the pipeline is again recorded to a H.264 elementary stream on the disk, file name / path for this can be specified in the 'h264_endoscopy_tool_tracking.yaml' file.</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded when building the application.</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/#building-and-running-h264-endoscopy-tool-tracking-application","title":"Building and Running H.264 Endoscopy Tool Tracking Application","text":"<ul> <li>Building and running the application from the top level Holohub directory:</li> </ul> <pre><code># C++ version\n./holohub run h264_endoscopy_tool_tracking --language cpp\n\n# Python version\n./holohub run h264_endoscopy_tool_tracking --language python\n</code></pre> <p>Important: on aarch64, applications also need tegra folder mounted inside the container and the <code>LD_LIBRARY_PATH</code> environment variable should be updated to include tegra folder path.</p> <p>Open and edit the Dockerfile and uncomment line 66:</p> <pre><code># Uncomment the following line for aarch64 support\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/aarch64-linux-gnu/tegra/\n</code></pre>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/#enable-recording-of-the-output","title":"Enable recording of the output","text":"<p>The recording of the output can be enabled by setting <code>record_output</code> flag in the config file <code>&lt;build_dir&gt;/applications/h264/endoscopy_tool_tracking/h264_endoscopy_tool_tracking.yaml</code> to <code>true</code>.</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode h264\n</code></pre>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/#c","title":"C++","text":"<p>Use the (gdb) h264_endoscopy_tool_tracking/cpp launch profile to run and debug the C++ application.</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/#python","title":"Python","text":"<p>There are a couple of launch profiles configured for this application:</p> <ol> <li>(debugpy) h264_endoscopy_tool_tracking/python: Launch the h.264 Endoscopy Tool Tracking application with the ability to debug Python code.</li> <li>(pythoncpp) h264_endoscopy_tool_tracking/python: Launch the h.264 Endoscopy Tool Tracking application with the ability to debug both Python and C++ code.</li> </ol>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/h264/h264_video_decode/","title":"H.264 Video Decode","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.6.0 Tested Holoscan SDK versions: 2.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This is a minimal reference application demonstrating usage of H.264 video decode operators. This application makes use of H.264 elementary stream reader operator for reading H.264 elementary stream input and uses Holoviz operator for rendering decoded data to the native window.</p> <p>The H.264 video decode operators do not adjust framerate as it reads the elementary stream input. As a result the video stream can be displayed as quickly as the decoding can be performed. This application uses <code>PeriodicCondition</code> to play video at the same speed as the source video.</p>","tags":["Healthcare AI","Video","Hardware Accelerated Decode","Endoscopy"]},{"location":"applications/h264/h264_video_decode/#requirements","title":"Requirements","text":"<p>This application is configured to use H.264 elementary stream from endoscopy sample data as input. To use any other stream, the filename / path for the input file can be specified in the 'h264_video_decode.yaml' file.</p>","tags":["Healthcare AI","Video","Hardware Accelerated Decode","Endoscopy"]},{"location":"applications/h264/h264_video_decode/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded when building the application.</p>","tags":["Healthcare AI","Video","Hardware Accelerated Decode","Endoscopy"]},{"location":"applications/h264/h264_video_decode/#building-and-running-h264-endoscopy-tool-tracking-application","title":"Building and Running H.264 Endoscopy Tool Tracking Application","text":"<ul> <li>Building and running the application from the top level Holohub directory:</li> </ul> <pre><code># C++ version\n./holohub run h264_video_decode --language=cpp\n\n# Python version\n./holohub run h264_video_decode --language=python\n</code></pre> <p>Important: on aarch64, applications also need tegra folder mounted inside the container and the <code>LD_LIBRARY_PATH</code> environment variable should be updated to include tegra folder path.</p> <p>Open and edit the Dockerfile and uncomment line 66:</p> <pre><code># Uncomment the following line for aarch64 support\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/aarch64-linux-gnu/tegra/\n</code></pre>","tags":["Healthcare AI","Video","Hardware Accelerated Decode","Endoscopy"]},{"location":"applications/h264/h264_video_decode/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode h264\n</code></pre>","tags":["Healthcare AI","Video","Hardware Accelerated Decode","Endoscopy"]},{"location":"applications/h264/h264_video_decode/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Healthcare AI","Video","Hardware Accelerated Decode","Endoscopy"]},{"location":"applications/h264/h264_video_decode/#c","title":"C++","text":"<p>Use the (gdb) h264_video_decode/cpp launch profile to run and debug the C++ application.</p>","tags":["Healthcare AI","Video","Hardware Accelerated Decode","Endoscopy"]},{"location":"applications/h264/h264_video_decode/#python","title":"Python","text":"<p>There are a couple of launch profiles configured for this application:</p> <ol> <li>(debugpy) h264_video_decode/python: Launch the h.264 Video Decode application with the ability to debug Python code.</li> <li>(pythoncpp) h264_video_decode/python: Launch the h.264 Video Decode application with the ability to debug both Python and C++ code.</li> </ol>","tags":["Healthcare AI","Video","Hardware Accelerated Decode","Endoscopy"]},{"location":"applications/high_speed_endoscopy/cpp/","title":"High-Speed Endoscopy","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The application showcases how high resolution cameras can be used to capture the scene, post-processed on GPU and displayed at high frame rate.</p>","tags":["Healthcare AI","Video","Bayer RGB Pipeline Optimization","Endoscopy","RDMA","Holoviz"]},{"location":"applications/high_speed_endoscopy/cpp/#requirements","title":"Requirements","text":"<p>This application requires: 1. an Emergent Vision Technologies camera (see setup instructions 2. a NVIDIA ConnectX SmartNIC with Rivermax SDK and drivers installed (see prerequisites) 3. a display with high refresh rate to keep up with the camera's framerate 4. additional setups to reduce latency</p>","tags":["Healthcare AI","Video","Bayer RGB Pipeline Optimization","Endoscopy","RDMA","Holoviz"]},{"location":"applications/high_speed_endoscopy/cpp/#build-instructions","title":"Build Instructions","text":"<p>Please refer to the top level Holohub README.md file for information on how to build this application.</p> <p>\u26a0\ufe0f At this time, camera controls are hardcoded within the <code>gxf_emergent_source</code> extension. To update them at the application level, the GXF extension, and the application need to be rebuilt. For more information on the controls, refer to the EVT Camera Attributes Manual</p>","tags":["Healthcare AI","Video","Bayer RGB Pipeline Optimization","Endoscopy","RDMA","Holoviz"]},{"location":"applications/high_speed_endoscopy/cpp/#run-instructions","title":"Run Instructions","text":"<p>First, go in your <code>build</code> or <code>install</code> directory. Then, run the commands of your choice:</p> <ul> <li> <p>RDMA disabled     <pre><code># C++\nsed -i -e 's#rdma:.*#rdma: false#' ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy.yaml \\\n    &amp;&amp; sudo ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy\n</code></pre></p> </li> <li> <p>RDMA enabled     <pre><code># C++\nsed -i -e 's#rdma:.*#rdma: true#' ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy.yaml \\\n    &amp;&amp; sudo MELLANOX_RINGBUFF_FACTOR=14 ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy\n</code></pre></p> </li> </ul> <p>\u2139\ufe0f The <code>MELLANOX_RINGBUFF_FACTOR</code> is used by the EVT driver to decide how much BAR1 size memory would be used on the dGPU. It can be changed to different number based on different use cases.</p>","tags":["Healthcare AI","Video","Bayer RGB Pipeline Optimization","Endoscopy","RDMA","Holoviz"]},{"location":"applications/high_speed_endoscopy/python/","title":"High-Speed Endoscopy","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The application showcases how high resolution cameras can be used to capture the scene, post-processed on GPU and displayed at high frame rate.</p>","tags":["Healthcare AI","Video","Bayer RGB Pipeline Optimization","Endoscopy","RDMA","Holoviz"]},{"location":"applications/high_speed_endoscopy/python/#requirements","title":"Requirements","text":"<p>This application requires: 1. an Emergent Vision Technologies camera (see setup instructions 2. a NVIDIA ConnectX SmartNIC with Rivermax SDK and drivers installed (see prerequisites) 3. a display with high refresh rate to keep up with the camera's framerate 4. additional setups to reduce latency</p>","tags":["Healthcare AI","Video","Bayer RGB Pipeline Optimization","Endoscopy","RDMA","Holoviz"]},{"location":"applications/high_speed_endoscopy/python/#run-instructions","title":"Run Instructions","text":"<p>TODO</p>","tags":["Healthcare AI","Video","Bayer RGB Pipeline Optimization","Endoscopy","RDMA","Holoviz"]},{"location":"applications/holochat/","title":"HoloChat","text":"<p> Authors: Nigel Nelson (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 0.2.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 4 - Experimental</p>","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Hardware Requirements</li> <li>Run Instructions</li> <li>Intended Use</li> <li>Known Limitations</li> <li>Best Practices</li> </ul> <p>HoloChat is an AI-driven chatbot, built on top of a locally hosted Code-Llama model OR a remote NIM API for Llama-3-70b, which acts as developer's copilot in Holoscan development. The LLM leverages a vector database comprised of the Holoscan SDK repository and user guide, enabling HoloChat to answer general questions about Holoscan, as well act as a Holoscan SDK coding assistant.</p> <p> </p>","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#hardware-requirements","title":"Hardware Requirements: \ud83d\udc49\ud83d\udcbb","text":"<ul> <li>Processor: x86/Arm64</li> </ul> <p>If running local LLM: - GPU: NVIDIA dGPU w/ &gt;= 28 GB VRAM - Memory: &gt;= 28 GB of available disk memory   - Needed to download fine-tuned Code Llama 34B and BGE-Large embedding model</p> <p>*Tested using NVIDIA IGX Orin w/ RTX A6000 and Dell Precision 5820 Workstation w/ RTX A6000</p>","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#running-holochat","title":"Running HoloChat: \ud83c\udfc3\ud83d\udca8","text":"<p>When running HoloChat, you have two LLM options: - Local: Uses Phind-CodeLlama-34B-v2 running on your local machine using Llama.cpp - Remote: Uses Llama-3-70b-Instruct using the NVIDIA NIM API</p> <p>You can also run HoloChat in MCP mode: - MCP: Runs as a Model Context Protocol server that provides Holoscan documentation and code context to upstream LLMs like Claude</p>","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#tldr","title":"TLDR; \ud83e\udd71","text":"<p>To run locally: <pre><code>./holohub run holochat --run-args=--local\n</code></pre> To run using the NVIDIA NIM API: <pre><code>echo \"NVIDIA_API_KEY=&lt;api_key_here&gt;\" &gt; ./applications/holochat/.env\n\n./holohub run holochat\n</code></pre> To run as an MCP server: <pre><code>./holohub run holochat --run-args=--mcp\n</code></pre> See MCP_MODE.md for more details on using MCP mode.</p>","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#build-notes","title":"Build Notes: \u2699\ufe0f","text":"<p>Build Time: - HoloChat uses a PyTorch container from NGC and may also download the ~23 GB Phind LLM from HuggingFace. As such, the first time building this application will likely take ~45 minutes depending on your internet speeds. However, this is a one-time set-up and subsequent runs of HoloChat should take seconds to launch.</p> <p>Build Location:</p> <ul> <li>If running locally: HoloChat downloads ~28 GB of model data to the <code>holochat/models</code> directory. As such, it is recommended to only run this application on a disk drive with ample storage (ex: the 500 GB SSD included with NVIDIA IGX Orin).</li> </ul>","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#running-instructions","title":"Running Instructions:","text":"<p>If connecting to your machine via SSH, be sure to forward the appropriate ports: - For chatbot UI: 7860 - For local LLM: 8080 - For MCP server: 8090</p> <pre><code>ssh &lt;user_name&gt;@&lt;IP address&gt; -L 7860:localhost:7860 -L 8080:localhost:8080 -L 8090:localhost:8090\n</code></pre>","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#running-w-local-llm","title":"Running w/ Local LLM \ud83d\udcbb","text":"<p>To build and start the app: <pre><code>./holohub run holochat --run-args=--local\n</code></pre> Once the LLM is loaded on the GPU and the Gradio app is running, HoloChat should be available at http://127.0.0.1:7860/.</p>","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#running-w-nim-api","title":"Running w/ NIM API \u2601\ufe0f","text":"<p>To use the NIM API you must create a .env file at: <pre><code>./applications/holochat/.env\n</code></pre> This is where you should place your NVIDIA API key. <pre><code>NVIDIA_API_KEY=&lt;api_key_here&gt;\n</code></pre></p> <p>To build and run the app: <pre><code>./holohub run holochat\n</code></pre> Once the Gradio app is running, HoloChat should be available at http://127.0.0.1:7860/.</p>","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#usage-notes","title":"Usage Notes: \ud83d\uddd2\ufe0f","text":"","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#intended-use","title":"Intended use: \ud83c\udfaf","text":"<p>HoloChat is developed to accelerate and assist Holoscan developers\u2019 learning and development. HoloChat serves as an intuitive chat interface, enabling users to pose natural language queries related to the Holoscan SDK. Whether seeking general information about the SDK or specific coding insights, users can obtain immediate responses thanks to the underlying Large Language Model (LLM) and vector database.</p> <p>HoloChat is given access to the Holoscan SDK repository, the HoloHub repository, and the Holoscan SDK user guide. This essentially allows users to engage in natural language conversations with these documents, gaining instant access to the information they need, thus sparing them the task of sifting through vast amounts of documentation themselves.</p>","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#known-limitations","title":"Known Limitations: \u26a0\ufe0f\ud83d\udea7","text":"<p>Before diving into how to make the most of HoloChat, it's crucial to understand and acknowledge its known limitations. These limitations can guide you in adopting the best practices below, which will help you navigate and mitigate these issues effectively. * Hallucinations: Occasionally, HoloChat may provide responses that are not entirely accurate. It's advisable to approach answers with a healthy degree of skepticism. * Memory Loss: LLM's limited attention window may lead to the loss of previous conversation history. To mitigate this, consider restarting the application to clear the chat history when necessary. * Limited Support for Stack Traces: HoloChat's knowledge is based on the Holoscan repository and the user guide, which lack large collections of stack trace data. Consequently, HoloChat may face challenges when assisting with stack traces.</p>","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#best-practices","title":"Best Practices: \u2705\ud83d\udc4d","text":"<p>While users should be aware of the above limitations, following the recommended tips will drastically minimize these possible shortcomings. In general, the more detailed and precise a question is, the better the results will be. Some best practices when asking questions are: * Be Verbose: If you want to create an application, specify which operators should be used if possible (HolovizOp, V4L2VideoCaptureOp, InferenceOp, etc.). * Be Specific: The less open-ended a question is the less likely the model will hallucinate. * Specify Programming Language: If asking for code, include the desired language (Python or C++). * Provide Code Snippets: If debugging errors include as much relevant information as possible. Copy and paste the code snippet that produces the error, the abbreviated stack trace, and describe any changes that may have introduced the error.</p> <p>In order to demonstrate how to get the most out of HoloChat two example questions are posed below. These examples illustrate how a user can refine their questions and as a result, improve the responses they receive:</p> <p>Worst\ud83d\udc4e: \u201cCreate an app that predicts the labels associated with a video\u201d</p> <p>Better\ud83d\udc4c: \u201cCreate a Python app that takes video input and sends it through a model for inference.\u201d</p> <p>Best\ud83d\ude4c: \u201cCreate a Python Holoscan application that receives streaming video input, and passes that video input into a pytorch classification model for inference. Then, collect the model\u2019s predicted class and use Holoviz to display the class label on each video frame.\u201d</p> <p>Worst\ud83d\udc4e: \u201cWhat os can I use?\u201d</p> <p>Better\ud83d\udc4c: \u201cWhat operating system can I use with Holoscan?\u201d</p> <p>Best\ud83d\ude4c: \u201cCan I use MacOS with the Holoscan SDK?\u201d</p>","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#appendix","title":"Appendix:","text":"","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#meta-terms-of-use","title":"Meta Terms of Use:","text":"<p>By using the Code-Llama model, you are agreeing to the terms and conditions of the license, acceptable use policy and Meta\u2019s privacy policy.</p>","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#implementation-details","title":"Implementation Details:","text":"<p>HoloChat operates by taking user input and comparing it to the text stored within the vector database, which is comprised of Holoscan SDK information. The most relevant text segments from SDK code and the user guide are then appended to the user's query. This approach allows the chosen LLM to answer questions about the Holoscan SDK, without being explicitly trained on SDK data.</p> <p>However, there is a drawback to this method - the most relevant documentation is not always found within the vector database. Since the user's question serves as the search query, queries that are too simplistic or abbreviated may fail to extract the most relevant documents from the vector database. As a consequence, the LLM will then lack the necessary context, leading to poor and potentially inaccurate responses. This occurs because LLMs strive to provide the most probable response to a question, and without adequate context, they hallucinate to fill in these knowledge gaps.</p>","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holoviz/holoviz_hdr/","title":"Holoviz HDR","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.5 Tested Holoscan SDK versions: 2.5 Contribution metric: Level 1 - Highly Reliable</p> <p> This application demonstrates displaying HDR images using the Holoviz operator. The application creates image data in HDR10 (BT2020 color space) with SMPTE ST2084 Perceptual Quantizer (PQ) EOTF and displays the image on the screen.</p> <p>Note that the screenshot above does not show the real HDR image on the display since it's not possible to take screenshots of HDR images.</p> <p>The Holoviz operator parameter <code>display_color_space</code> is used to set the color space. This allows HDR output on Linux distributions and displays supporting that feature. See https://docs.nvidia.com/holoscan/sdk-user-guide/visualization.html#hdr for more information.</p> <pre><code>    auto holoviz = make_operator&lt;ops::HolovizOp&gt;(\n        \"holoviz\",\n        // select the HDR10 ST2084 display color space\n        Arg(\"display_color_space\", ops::HolovizOp::ColorSpace::HDR10_ST2084));\n</code></pre>","tags":["Computer Vision and Perception","Visualization","color space conversion","ST2084","Holoviz"]},{"location":"applications/holoviz/holoviz_hdr/#run-instructions","title":"Run Instructions","text":"<p>To build and start the application:</p> <pre><code>./holohub run holoviz_hdr\n</code></pre>","tags":["Computer Vision and Perception","Visualization","color space conversion","ST2084","Holoviz"]},{"location":"applications/holoviz/holoviz_srgb/","title":"Holoviz sRGB","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.3 Tested Holoscan SDK versions: 2.3 Contribution metric: Level 1 - Highly Reliable</p> <p> This application demonstrates the handling of the sRGB color space supported by the Holoviz operator.</p> <p>The Holoviz operator can convert sRGB input images to linear color space before rendering and also can convert from linear color space to sRGB before writing to the frame buffer.</p> <p>sRGB color space can be enabled for input images and for the frame buffer independently. By default, the sRGB color space is disabled for both.</p> <p>By default, the Holoviz operator is auto detecting the input image format. Auto detection always assumes linear color space for input images. To change this to sRGB color space explicitly set the <code>image_format_</code> member of the input spec for that input image to a format ending with <code>SRGB</code>:</p> <pre><code>    // By default the image format is auto detected. Auto detection assumes linear color space,\n    // but we provide an sRGB encoded image. Create an input spec and change the image format to\n    // sRGB.\n    ops::HolovizOp::InputSpec input_spec(\"image\", ops::HolovizOp::InputType::COLOR);\n    input_spec.image_format_ = ops::HolovizOp::ImageFormat::R8G8B8_SRGB;\n\n    auto holoviz = make_operator&lt;ops::HolovizOp&gt;(\n        \"holoviz\",\n        Arg(\"tensors\", std::vector&lt;ops::HolovizOp::InputSpec&gt;{input_spec}));\n</code></pre> <p>By default, the frame buffer is using linear color space. To use the sRGB color space, set the <code>framebuffer_srbg</code> argument of the Holoviz operator to <code>true</code>:</p> <pre><code>    auto holoviz = make_operator&lt;ops::HolovizOp&gt;(\n        \"holoviz\",\n        // enable the sRGB frame buffer\n        Arg(\"framebuffer_srbg\", true));\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Benchmarking","Holoviz","Rendering"]},{"location":"applications/holoviz/holoviz_srgb/#run-instructions","title":"Run Instructions","text":"<p>To build and start the application:</p> <pre><code>./holohub run holoviz_srgb\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Benchmarking","Holoviz","Rendering"]},{"location":"applications/holoviz/holoviz_ui/","title":"Holoviz UI","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.5 Tested Holoscan SDK versions: 2.5 Contribution metric: Level 1 - Highly Reliable</p> <p> This application uses the layer callback provided by the Holoviz operator and leverages the Holoviz module API to add an UI layer with <code>Dear ImGui</code> elements and a geometry layer dynamically changing based on user input.</p>","tags":["Computer Vision and Perception","Visualization","Holoviz","Rendering"]},{"location":"applications/holoviz/holoviz_ui/#run-instructions","title":"Run Instructions","text":"<p>To build and start the application:</p> <pre><code>./holohub run holoviz_ui\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Holoviz","Rendering"]},{"location":"applications/holoviz/holoviz_vsync/","title":"Holoviz vsync","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.3 Tested Holoscan SDK versions: 2.3 Contribution metric: Level 1 - Highly Reliable</p> <p> This application demonstrates the capability of the Holoviz operator to wait for the vertical blank of the display before updating the current image. It prints the displayed frames per second to the console, if sync to vertical blank is enabled the frames per second are capped to the display refresh rate.</p> <p>To enable syncing to vertical blank set the <code>vsync</code> parameter of the Holoviz operator to <code>true</code>:</p> <pre><code>    auto holoviz = make_operator&lt;ops::HolovizOp&gt;(\n        \"holoviz\",\n        // enable synchronization to vertical blank\n        Arg(\"vsync\", true));\n</code></pre> <p>By default, the Holoviz operator is not syncing to the vertical blank of the display.</p>","tags":["Computer Vision and Perception","Visualization","frame rate synchronization","Image Processing","Holoviz","Rendering"]},{"location":"applications/holoviz/holoviz_vsync/#run-instructions","title":"Run Instructions","text":"<p>To build and start the application:</p> <pre><code>./holohub run holoviz_vsync\n</code></pre>","tags":["Computer Vision and Perception","Visualization","frame rate synchronization","Image Processing","Holoviz","Rendering"]},{"location":"applications/holoviz/holoviz_yuv/","title":"Holoviz YUV","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.4 Tested Holoscan SDK versions: 2.4 Contribution metric: Level 1 - Highly Reliable</p> <p> This application demonstrates the capability of the Holoviz operator to display images in YUV (aka YCbCr) format.</p> <p>Holoviz supports multiple YUV formats including 420 and 422, 8 and 16 bit, single plane and multi plane. It supports BT.601, BT.709 and BT.2020 color conversions, narrow and full range and cosited even and midpoint chroma downsample positions.</p> <p>The application creates a GXF video buffer containing YUV 420 BT.601 extended range data.</p> <p>The YUV image properties are specified using a input spec structure:</p> <pre><code>    ops::HolovizOp::InputSpec input_spec(\"image\", ops::HolovizOp::InputType::COLOR);\n\n    // Set the YUV image format, model conversion and range for the input tensor.\n    input_spec.image_format_ = ops::HolovizOp::ImageFormat::Y8_U8V8_2PLANE_420_UNORM;\n    input_spec.yuv_model_conversion_ = ops::HolovizOp::YuvModelConversion::YUV_601;\n    input_spec.yuv_range_ = ops::HolovizOp::YuvRange::ITU_FULL;\n\n    auto holoviz = make_operator&lt;ops::HolovizOp&gt;(\n        \"holoviz\",\n        Arg(\"tensors\", std::vector&lt;ops::HolovizOp::InputSpec&gt;{input_spec}));\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Video","Holoviz","Image Processing"]},{"location":"applications/holoviz/holoviz_yuv/#run-instructions","title":"Run Instructions","text":"<p>To build and start the application:</p> <pre><code>./holohub run holoviz_yuv\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Video","Holoviz","Image Processing"]},{"location":"applications/hyperspectral_segmentation/","title":"Hyperspectral Image Segmentation","text":"<p> Authors: Lars Doorenbos (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 2 - Trusted</p> <p></p> <p>This application segments endoscopic hyperspectral cubes into 20 organ classes. It visualizes the result together with the RGB image corresponding to the cube.</p>","tags":["Healthcare AI","Visualization","Hyperspectral Imaging","Segmentation","Endoscopy","Image Processing"]},{"location":"applications/hyperspectral_segmentation/#data-and-models","title":"Data and Models","text":"<p>The data is a subset of the HeiPorSPECTRAL dataset. The application loops over the 84 cubes selected. The model is the <code>2022-02-03_22-58-44_generated_default_model_comparison</code> checkpoint from this repository, converted to ONNX with the script in <code>utils/convert_to_onnx.py</code>.</p> <p>\ud83d\udce6\ufe0f (NGC) App Data and Model for Hyperspectral Segmentation.  This resource is automatically downloaded when building the application.</p>","tags":["Healthcare AI","Visualization","Hyperspectral Imaging","Segmentation","Endoscopy","Image Processing"]},{"location":"applications/hyperspectral_segmentation/#run-instructions","title":"Run Instructions","text":"<p>This application requires some python modules to be installed. You can simply use Holohub CLI to build and run the application.</p> <pre><code>./holohub run hyperspectral_segmentation\n</code></pre> <p>This single command builds and runs a Docker container, then inside that container, it builds and runs the application.</p> <p>To build and run the container without building the application, you can use the following command:</p> <pre><code>./holohub run-container hyperspectral_segmentation\n</code></pre>","tags":["Healthcare AI","Visualization","Hyperspectral Imaging","Segmentation","Endoscopy","Image Processing"]},{"location":"applications/hyperspectral_segmentation/#viewing-results","title":"Viewing Results","text":"<p>With the default settings, the results of this application are saved to <code>result.png</code> file in the hyperspectral segmentation app directory. Each time a new image is processed, it overwrites <code>result.png</code>.  By opening this image while the application is running, you can see the results as the updates are made (may depend on your image viewer).</p>","tags":["Healthcare AI","Visualization","Hyperspectral Imaging","Segmentation","Endoscopy","Image Processing"]},{"location":"applications/imaging_ai_segmentator/","title":"Imaging AI Whole Body Segmentation","text":"<p> Authors: Ming Qin (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: June 2, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.3.0 Tested Holoscan SDK versions: 2.3.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates the use of medical imaging operators to build and package an application that parses DICOM images and performs inference using a MONAI model (TotalSegmentator).</p> <p> Fig. 1: 3D volume rendering of segmentation results in NIfTI format</p>","tags":["Healthcare AI","Imaging","Volumetric Reconstruction","Healthcare Interop","TotalSegmentator","Medical Imaging"]},{"location":"applications/imaging_ai_segmentator/#overview","title":"Overview","text":"<p>This application uses a MONAI re-trained TotalSegmentator model to segment 104 body parts from a DICOM CT series. It is implemented using Holohub DICOM processing operators and PyTorch inference operators.</p> <p>The input is a DICOM CT series, and the segmentation results are saved as both DICOM Segmentation (Part 10 storage format) and NIfTI format. The workflow includes:</p> <ul> <li>Loading DICOM studies</li> <li>Selecting series with application-defined rules</li> <li>Converting DICOM pixel data to a 3D volume image</li> <li>Using the MONAI SDK to transform input/output and perform inference</li> <li>Writing results as a DICOM Segmentation OID instance, re-using study-level metadata from the original DICOM study</li> </ul> <p> Fig. 2: A slice of the segmentation saved in a DICOM segmentation instance (without color coding the segments)</p>","tags":["Healthcare AI","Imaging","Volumetric Reconstruction","Healthcare Interop","TotalSegmentator","Medical Imaging"]},{"location":"applications/imaging_ai_segmentator/#requirements","title":"Requirements","text":"<ul> <li>On a Holohub supported platform</li> <li>Python 3.8+</li> <li>Python packages from PyPI, including:</li> <li>torch</li> <li>monai</li> <li>nibabel</li> <li>pydicom</li> <li>highdicom</li> <li>Other dependencies as specified in requirements.txt</li> <li>NVIDIA GPU with at least 14GB memory (for a 200-slice CT series)</li> </ul>","tags":["Healthcare AI","Imaging","Volumetric Reconstruction","Healthcare Interop","TotalSegmentator","Medical Imaging"]},{"location":"applications/imaging_ai_segmentator/#data","title":"Data","text":"<p>The input for this application is a folder of DICOM image files from a CT series. For testing, CT scan images can be downloaded from The Cancer Imaging Archive, subject to Data Usage Policies and Restrictions.</p> <p>One such dataset, a CT Abdomen series described as <code>ABD/PANC_3.0_B31f</code>, was used in testing the application. Other DICOM CT Abdomen series can be downloaded from TCIA as test inputs.</p>","tags":["Healthcare AI","Imaging","Volumetric Reconstruction","Healthcare Interop","TotalSegmentator","Medical Imaging"]},{"location":"applications/imaging_ai_segmentator/#data-citation","title":"Data Citation","text":"<p>National Cancer Institute Clinical Proteomic Tumor Analysis Consortium (CPTAC). (2018). The Clinical Proteomic Tumor Analysis Consortium Cutaneous Melanoma Collection (CPTAC-CM) (Version 11) [Dataset]. The Cancer Imaging Archive. https://doi.org/10.7937/K9/TCIA.2018.ODU24GZE</p>","tags":["Healthcare AI","Imaging","Volumetric Reconstruction","Healthcare Interop","TotalSegmentator","Medical Imaging"]},{"location":"applications/imaging_ai_segmentator/#model","title":"Model","text":"<p>This application uses the MONAI whole-body segmentation model, which can segment 104 body parts from CT scans.</p>","tags":["Healthcare AI","Imaging","Volumetric Reconstruction","Healthcare Interop","TotalSegmentator","Medical Imaging"]},{"location":"applications/imaging_ai_segmentator/#build-and-run-instructions","title":"Build and Run Instructions","text":"","tags":["Healthcare AI","Imaging","Volumetric Reconstruction","Healthcare Interop","TotalSegmentator","Medical Imaging"]},{"location":"applications/imaging_ai_segmentator/#quick-start-using-holohub-container","title":"Quick Start Using Holohub Container","text":"<p>This is the simplest and fastest way to run the application:</p> <pre><code>./holohub run imaging_ai_segmentator\n</code></pre> <p>Note: It takes quite a few minutes when this command is run the first time. This command pulls the latest Holoscan SDK docker image, create Holohub docker image and set up requirement for this application, run the container, and finally build and run the application.</p> <p>The output will be available in the <code>\"&lt;LOCAL_HOLOHUB_PATH&gt;/build/imaging_ai_segmentator/output\"</code> directory, where <code>&lt;LOCAL_HOLOHUB_PATH&gt;</code> refers to where you have cloned your Holohub repository and running the <code>./holohub</code> command.</p> <pre><code>output\n\u251c\u2500\u2500 1.2.826.0.1.3680043.10.511.3.57591117750107235783166330094310669.dcm\n\u2514\u2500\u2500 saved_images_folder\n    \u2514\u2500\u2500 1.3.6.1.4.1.14519.5.2.1.7085.2626\n        \u251c\u2500\u2500 1.3.6.1.4.1.14519.5.2.1.7085.2626.nii\n        \u2514\u2500\u2500 1.3.6.1.4.1.14519.5.2.1.7085.2626_seg.nii\n</code></pre>","tags":["Healthcare AI","Imaging","Volumetric Reconstruction","Healthcare Interop","TotalSegmentator","Medical Imaging"]},{"location":"applications/imaging_ai_segmentator/#development-environment-setup","title":"Development Environment Setup","text":"<p>You can run the application either in your local development environment or inside the Holohub development container.</p> <ol> <li>Set up the environment:</li> </ol> <p>A. Holohub Container:</p> <ul> <li> <p>Build and launch the Holohub Container:</p> <pre><code>./holohub run-container imaging_ai_segmentator\n</code></pre> </li> </ul> <p>B. Bare Metal (not using Holohub/Holoscan container):</p> <pre><code>- Install Python dependencies:  \n  It is strongly recommended a Python virtual environment is used for running the application in dev environment.\n\n   ```bash\n   pip install -r applications/imaging_ai_segmentator/requirements.txt\n   ```\n\n- Set up the Holohub environment:  \n  Although this application is implemented entirely in Python and relies on standard PyPI packages, you still may want to set up Holohub environment and use `./holohub build` to help organize the Python code and automatically download the required segmentation model.\n\n   ```bash\n   sudo ./holohub setup\n   ```\n\n- Set environment variables for the application:\n\n   ```bash\n   source applications/imaging_ai_segmentator/env_settings.sh\n   ```\n</code></pre> <ol> <li>Download test data (if not already done):</li> <li>Download CT series from TCIA</li> <li> <p>Save DICOM files under <code>$HOLOSCAN_INPUT_PATH</code></p> </li> <li> <p>Build the application:</p> </li> </ol> <pre><code>./holohub build imaging_ai_segmentator\n</code></pre> <ol> <li>Run the application:</li> </ol> <pre><code>rm -fr $HOLOSCAN_OUTPUT_PATH  # Optional\nexport PYTHONPATH=$PYTHONPATH:/workspace/holohub/build/imaging_ai_segmentator/python/lib:/workspace/holohub\npython applications/imaging_ai_segmentator/app.py\n</code></pre> <p>Tip:    You can override the default input, output, and model directories by specifying them as command-line arguments. For example:</p> <pre><code>python applications/imaging_ai_segmentator/app.py -m /path/to/model -i /path/to/input -o /path/to/output\n</code></pre> <ol> <li> <p>Check output:</p> <pre><code>ls $HOLOSCAN_OUTPUT_PATH\n</code></pre> </li> </ol>","tags":["Healthcare AI","Imaging","Volumetric Reconstruction","Healthcare Interop","TotalSegmentator","Medical Imaging"]},{"location":"applications/imaging_ai_segmentator/#output","title":"Output","text":"<p>The application generates two types of outputs:</p> <ol> <li>DICOM Segmentation file (Part10 storage format)</li> <li>NIfTI format files in the <code>saved_images_folder</code>:</li> <li>Original CT scan in NIfTI format</li> <li>Segmentation results in NIfTI format</li> </ol>","tags":["Healthcare AI","Imaging","Volumetric Reconstruction","Healthcare Interop","TotalSegmentator","Medical Imaging"]},{"location":"applications/imaging_ai_segmentator/#packaging-the-application-for-distribution","title":"Packaging the Application for Distribution","text":"<p>With Holoscan CLI, an applications built with Holoscan SDK can be packaged into a Holoscan Application Package (HAP), which is essentially a Open Container Initiative compliant container image. An HAP is well suited to be distributed for deployment on hosting platforms, be a Docker Compose, Kubernetes, or else. Please refer to Packaging Holoscan Applications in the User Guide for more information.</p> <p>This example application includes all the necessary files for HAP packaging. First, you should install the application:</p> <pre><code>./holohub install imaging_ai_segmentator\n</code></pre> <p>Then, run the following command to see and use the specific packaging commands.</p> <pre><code>source applications/imaging_ai_segmentator/packageHAP.sh\n</code></pre>","tags":["Healthcare AI","Imaging","Volumetric Reconstruction","Healthcare Interop","TotalSegmentator","Medical Imaging"]},{"location":"applications/isaac_holoscan_bridge/","title":"Isaac Holoscan Bridge","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64 Language: Python Last modified: June 25, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 3.3.0 Tested Holoscan SDK versions: 3.3.0 Contribution metric: Level 3 - Developmental</p> <p>The Isaac Holoscan Bridge is a sophisticated application that demonstrates the integration between NVIDIA's Isaac Simulator and the Holoscan framework.</p> <p></p> <p>It creates a bridge where simulated data, including camera and robotic joint data, from Isaac Sim is processed through a Holoscan pipeline. The application features a virtual camera in Isaac Sim that captures video frames, which are then streamed to a Holoscan pipeline for real-time processing. The processed data is then again displayed in Isaac Sim. The application also streams joint positions from a robot to a Holoscan pipeline. The pipeline modifies these joint positions and feeds them back into the simulation moving the robotic arm.</p> <p>The pipeline includes several specialized operators:</p> <ul> <li>an AsyncDataPushOp for handling data streaming from Isaac Sim to Holoscan</li> <li>a CallbackOp which is handling data transfer from Holoscan to Isaac Sim</li> <li>a SobelOp for image processing (edge detection)</li> <li>a ControlOp for managing arm joint positions</li> <li>and HolovizOp for visualization.</li> </ul> <p>A unique feature of this application is its bidirectional callback system, where data flows between the simulation and Holoscan pipeline through carefully orchestrated callbacks, enabling real-time interaction between the two systems.</p>","tags":["Sensor","Processing","Isaac Sim","Bridge","Robotics"]},{"location":"applications/isaac_holoscan_bridge/#flow-diagram","title":"Flow Diagram","text":"<pre><code>flowchart LR\n    subgraph Holoscan application\n        AsyncDataPushOp --&gt;|camera_image| HolovizOp\n        AsyncDataPushOp --&gt;|camera_image| SobelOp\n        AsyncDataPushOp --&gt;|arm_joint_positions| ControlOp\n        SobelOp --&gt;|camera_image_sobel| CallbackOp\n        SobelOp --&gt;|camera_image_sobel| HolovizOp\n        ControlOp --&gt;|arm_joint_positions| CallbackOp\n    end\n    subgraph Isaac Sim\n        Simulation --&gt; push_data_callback\n        data_ready_callback --&gt; Simulation\n        CallbackOp -.-&gt;|callback| data_ready_callback\n        push_data_callback -.-&gt;|callback| AsyncDataPushOp\n    end\n</code></pre>","tags":["Sensor","Processing","Isaac Sim","Bridge","Robotics"]},{"location":"applications/isaac_holoscan_bridge/#requirements","title":"Requirements","text":"<ul> <li>On a Holohub supported platform</li> <li>Python 3.10+</li> <li>Nvidia GPU with at least 8GB memory</li> </ul>","tags":["Sensor","Processing","Isaac Sim","Bridge","Robotics"]},{"location":"applications/isaac_holoscan_bridge/#run-instructions","title":"Run Instructions","text":"<pre><code>./holohub run --docker-opts=\"-u root -e ACCEPT_EULA=Y -e PRIVACY_CONSENT=Y \\\n-v ${HOME}/docker/isaac-sim/cache/kit:/isaac-sim/kit/cache:rw \\\n-v ${HOME}/docker/isaac-sim/cache/ov:/root/.cache/ov:rw \\\n-v ${HOME}/docker/isaac-sim/cache/pip:/root/.cache/pip:rw \\\n-v ${HOME}/docker/isaac-sim/cache/glcache:/root/.cache/nvidia/GLCache:rw \\\n-v ${HOME}/docker/isaac-sim/cache/computecache:/root/.nv/ComputeCache:rw \\\n-v ${HOME}/docker/isaac-sim/logs:/root/.nvidia-omniverse/logs:rw \\\n-v ${HOME}/docker/isaac-sim/data:/root/.local/share/ov/data:rw \\\n-v ${HOME}/docker/isaac-sim/documents:/root/Documents:rw\" isaac_holoscan_bridge\n</code></pre> <p>To keep Isaac Sim configuration and data persistent when running in a container, various directories are mounted into the container.</p> <p>Note It takes quite a few minutes when this command is run the first time since shaders need to be compiled.</p>","tags":["Sensor","Processing","Isaac Sim","Bridge","Robotics"]},{"location":"applications/laser_detection_latency/evt_cam_calibration/","title":"EVT Camera Calibration","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.2.0 Contribution metric: Level 1 - Highly Reliable</p>","tags":["Computer Vision and Perception","Visualization","Image Processing","Camera","Detection"]},{"location":"applications/laser_detection_latency/evt_cam_calibration/#overview","title":"Overview","text":"<p>This application performs monitor registration using an Emergent Vision Technologies (EVT) camera. It detects April tags placed at the four corners of a monitor to establish the monitor's position and orientation in 3D space.</p>","tags":["Computer Vision and Perception","Visualization","Image Processing","Camera","Detection"]},{"location":"applications/laser_detection_latency/evt_cam_calibration/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>EVT HB-9000-G 25GE camera</li> <li>Monitor with April tags at all four corners</li> <li>Proper lighting conditions (well-lit environment without backlight)</li> </ul>","tags":["Computer Vision and Perception","Visualization","Image Processing","Camera","Detection"]},{"location":"applications/laser_detection_latency/evt_cam_calibration/#setup-instructions","title":"Setup Instructions","text":"<ol> <li>Follow the Holoscan SDK user guide to set up the EVT camera</li> <li>Place the calibration image with April tags on the monitor</li> <li>Position the camera so it can see all four corners of the monitor</li> <li>Verify camera visibility using the high_speed_endoscopy app</li> </ol>","tags":["Computer Vision and Perception","Visualization","Image Processing","Camera","Detection"]},{"location":"applications/laser_detection_latency/evt_cam_calibration/#running-the-application","title":"Running the Application","text":"<pre><code>./holohub build evt_cam_calibration --local\n./holohub run evt_cam_calibration --local --no-local-build\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Image Processing","Camera","Detection"]},{"location":"applications/laser_detection_latency/evt_cam_calibration/#output","title":"Output","text":"<p>The application generates a calibration file <code>evt-cali.npy</code> in the build directory, which contains the monitor's corner coordinates.</p>","tags":["Computer Vision and Perception","Visualization","Image Processing","Camera","Detection"]},{"location":"applications/laser_detection_latency/evt_cam_calibration/#notes","title":"Notes","text":"<ul> <li>The camera must have a clear view of all four April tags</li> <li>Avoid backlighting or glare on the monitor</li> <li>If using a different camera model, update the camera settings in the Python app or YAML configuration file</li> <li>The application requires sudo privileges to run</li> </ul>","tags":["Computer Vision and Perception","Visualization","Image Processing","Camera","Detection"]},{"location":"applications/laser_detection_latency/laser_detection/","title":"Laser Detection","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.2.0 Contribution metric: Level 1 - Highly Reliable</p>","tags":["Computer Vision and Perception","Video","CV CUDA","Camera","Detection"]},{"location":"applications/laser_detection_latency/laser_detection/#overview","title":"Overview","text":"<p>This application demonstrates the latency differences between USB and EVT cameras by detecting laser pointer positions on a monitor. It uses two camera sources to track laser positions and displays the results in real-time with different colored icons.</p>","tags":["Computer Vision and Perception","Video","CV CUDA","Camera","Detection"]},{"location":"applications/laser_detection_latency/laser_detection/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>USB camera (Logitech 4k Pro Webcam or compatible)</li> <li>EVT camera (HB-9000-G 25GE or compatible)</li> <li>Monitor with matte screen (120fps or higher refresh rate recommended)</li> <li>Safe laser pointer for viewing purposes</li> <li>Completed calibration files from both USB and EVT calibration apps</li> </ul>","tags":["Computer Vision and Perception","Video","CV CUDA","Camera","Detection"]},{"location":"applications/laser_detection_latency/laser_detection/#setup-instructions","title":"Setup Instructions","text":"<ol> <li>Complete the calibration process for both USB and EVT cameras</li> <li>Ensure both cameras are properly connected and configured</li> <li>Position the cameras to have a clear view of the monitor</li> <li>Verify the calibration files (<code>usb-cali.npy</code> and <code>evt-cali.npy</code>) are present in the build directory</li> </ol>","tags":["Computer Vision and Perception","Video","CV CUDA","Camera","Detection"]},{"location":"applications/laser_detection_latency/laser_detection/#running-the-application","title":"Running the Application","text":"<pre><code>[sudo] LD_PRELOAD=/usr/lib/aarch64-linux-gnu/nvidia/libnvjpeg.so ./holohub run laser_detection\n</code></pre>","tags":["Computer Vision and Perception","Video","CV CUDA","Camera","Detection"]},{"location":"applications/laser_detection_latency/laser_detection/#usage","title":"Usage","text":"<ul> <li>A white icon represents the USB camera's laser detection</li> <li>A green icon represents the EVT camera's laser detection</li> <li>Point the laser at the monitor to see the latency difference between the two cameras</li> <li>The icons will move to the coordinates where the laser is detected</li> </ul>","tags":["Computer Vision and Perception","Video","CV CUDA","Camera","Detection"]},{"location":"applications/laser_detection_latency/laser_detection/#notes","title":"Notes","text":"<ul> <li>Use only a matte screen monitor to avoid specular reflections</li> <li>Ensure proper lighting conditions</li> <li>Use only safe laser pointers designed for viewing purposes</li> <li>If detection is inaccurate, recalibrate both cameras</li> <li>The application requires sudo privileges to run</li> </ul>","tags":["Computer Vision and Perception","Video","CV CUDA","Camera","Detection"]},{"location":"applications/laser_detection_latency/usb_cam_calibration/","title":"USB Camera Calibration","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.2.0 Contribution metric: Level 1 - Highly Reliable</p>","tags":["Computer Vision and Perception","Visualization","Camera","Detection","Image Processing","Holoviz"]},{"location":"applications/laser_detection_latency/usb_cam_calibration/#overview","title":"Overview","text":"<p>This application performs monitor registration using a USB camera. It detects April tags placed at the four corners of a monitor to establish the monitor's position and orientation in 3D space.</p>","tags":["Computer Vision and Perception","Visualization","Camera","Detection","Image Processing","Holoviz"]},{"location":"applications/laser_detection_latency/usb_cam_calibration/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>Logitech 4k Pro Webcam or compatible USB camera</li> <li>Monitor with April tags at all four corners</li> <li>Proper lighting conditions (well-lit environment without backlight)</li> </ul>","tags":["Computer Vision and Perception","Visualization","Camera","Detection","Image Processing","Holoviz"]},{"location":"applications/laser_detection_latency/usb_cam_calibration/#setup-instructions","title":"Setup Instructions","text":"<ol> <li>Ensure the USB camera is properly connected</li> <li>Place the calibration image with April tags on the monitor</li> <li>Position the camera so it can see all four corners of the monitor</li> <li>Verify camera visibility using the v4l2_camera app</li> </ol>","tags":["Computer Vision and Perception","Visualization","Camera","Detection","Image Processing","Holoviz"]},{"location":"applications/laser_detection_latency/usb_cam_calibration/#running-the-application","title":"Running the Application","text":"<pre><code>[sudo] LD_PRELOAD=/usr/lib/aarch64-linux-gnu/nvidia/libnvjpeg.so ./holohub run usb_cam_calibration\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Camera","Detection","Image Processing","Holoviz"]},{"location":"applications/laser_detection_latency/usb_cam_calibration/#output","title":"Output","text":"<p>The application generates a calibration file <code>usb-cali.npy</code> in the build directory, which contains the monitor's corner coordinates.</p>","tags":["Computer Vision and Perception","Visualization","Camera","Detection","Image Processing","Holoviz"]},{"location":"applications/laser_detection_latency/usb_cam_calibration/#notes","title":"Notes","text":"<ul> <li>The camera must have a clear view of all four April tags</li> <li>Avoid backlighting or glare on the monitor</li> <li>If using a different camera model, update the camera settings in the Python app or YAML configuration file</li> </ul>","tags":["Computer Vision and Perception","Visualization","Camera","Detection","Image Processing","Holoviz"]},{"location":"applications/matlab_gpu_coder/matlab_beamform/","title":"Ultrasound Beamforming with MATLAB GPU Coder","text":"<p> Authors: Holoscan Team (NVIDIA), MathWorks Team (MathWorks) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 4 - Experimental</p> <p>This application does real-time ultrasound beamforming of simulated data. The beamforming algorithm is implemented in MATLAB and converted to CUDA using MATLAB GPU Coder. When the application is run, Holoviz will display the beamformed data in real time.</p>","tags":["Healthcare AI","Visualization","Synthetic Aperture Beamforming","Signal Processing","Beamforming","Ultrasound"]},{"location":"applications/matlab_gpu_coder/matlab_beamform/#folder-structure","title":"Folder Structure","text":"<pre><code>matlab_beamform\n\u251c\u2500\u2500 data  # Data is generated with generate_data.mlx\n\u2502   \u2514\u2500\u2500 ultrasound_beamforming.bin  # Simulated ultrasound data\n\u251c\u2500\u2500 matlab  # MATLAB files\n\u2502   \u251c\u2500\u2500 generate_beamform_jetson.m  # MATLAB script to generate CUDA DLLs on Jetson\n\u2502   \u251c\u2500\u2500 generate_beamform_x86.m  # MATLAB script to generate CUDA DLLs on x86\n\u2502   \u251c\u2500\u2500 generate_data.mlx  # MATLAB script to generate simulated data\n\u2502   \u2514\u2500\u2500 matlab_beamform.m  # MATLAB function that CUDA code is generated from\n\u251c\u2500\u2500 CMakeLists.txt  # CMake build file\n\u251c\u2500\u2500 main.cpp  # Ultrasound beamforming app\n\u2514\u2500\u2500 matlab_beamform.yaml  # Ultrasound beamforming config\n</code></pre>","tags":["Healthcare AI","Visualization","Synthetic Aperture Beamforming","Signal Processing","Beamforming","Ultrasound"]},{"location":"applications/matlab_gpu_coder/matlab_beamform/#generate-simulated-data","title":"Generate Simulated Data","text":"<p>The required MATLAB Toolboxes are:</p> <ul> <li>Phased Array System Toolbox</li> <li>Communications Toolbox</li> </ul> <p>Simply run the script <code>matlab/generate_data.mlx</code> from MATLAB and a binary file <code>ultrasound_beamforming.bin</code> will be written to a top-level <code>data</code> folder. The binary file contains the simulated ultrasound data, prior to beamforming.</p>","tags":["Healthcare AI","Visualization","Synthetic Aperture Beamforming","Signal Processing","Beamforming","Ultrasound"]},{"location":"applications/matlab_gpu_coder/matlab_beamform/#generate-cuda-code-with-matlab-gpu-coder","title":"Generate CUDA Code with MATLAB GPU Coder","text":"","tags":["Healthcare AI","Visualization","Synthetic Aperture Beamforming","Signal Processing","Beamforming","Ultrasound"]},{"location":"applications/matlab_gpu_coder/matlab_beamform/#x86-ubuntu","title":"x86: Ubuntu","text":"<p>In order to generate the CUDA Code, start MATLAB and <code>cd</code> to the <code>matlab</code> folder and open the <code>generate_beamform_x86.m</code> script. Run the script and a folder <code>codegen/dll/matlab_beamform</code> will be generated in the <code>matlab_beamform</code> folder.</p>","tags":["Healthcare AI","Visualization","Synthetic Aperture Beamforming","Signal Processing","Beamforming","Ultrasound"]},{"location":"applications/matlab_gpu_coder/matlab_beamform/#arm64-jetson","title":"arm64: Jetson","text":"<p>On an x86 computer with MATLAB installed, <code>cd</code> to the <code>matlab</code> folder and open the <code>generate_beamform_jetson.m</code> script. Having an <code>ssh</code> connection to the Jetson device you want to build the CUDA DLLs on, specify the parameters of that connection in the <code>hwobj</code> on line 7, also replace <code>&lt;ABSOLUTE_PATH&gt;</code> of <code>cfg.Hardware.BuildDir</code> on line 39, as the absolute path (on the Jetson device) to <code>holohub</code> folder. Run the script and a folder <code>MATLAB_ws</code> will be created in the <code>matlab_beamform</code> folder.</p>","tags":["Healthcare AI","Visualization","Synthetic Aperture Beamforming","Signal Processing","Beamforming","Ultrasound"]},{"location":"applications/matlab_gpu_coder/matlab_beamform/#configure-holoscan-for-matlab","title":"Configure Holoscan for MATLAB","text":"","tags":["Healthcare AI","Visualization","Synthetic Aperture Beamforming","Signal Processing","Beamforming","Ultrasound"]},{"location":"applications/matlab_gpu_coder/matlab_beamform/#x86-ubuntu_1","title":"x86: Ubuntu","text":"<p>Define the environment variable:</p> <pre><code>export MATLAB_ROOT=\"/usr/local/MATLAB\"\nexport MATLAB_VERSION=\"R2023b\"\n</code></pre> <p>where you, if need be, replace <code>MATLAB_ROOT</code> with the location of your MATLAB install and <code>MATLAB_VERSION</code> with the correct version.</p> <p>Next, run the HoloHub Docker container:</p> <pre><code>./holohub run-container \\\n    --add-volume ${MATLAB_ROOT}/${MATLAB_VERSION} \\\n    --docker-opts=\"-e MATLAB_ROOT=/workspace/volumes/${MATLAB_VERSION}\"\n</code></pre>","tags":["Healthcare AI","Visualization","Synthetic Aperture Beamforming","Signal Processing","Beamforming","Ultrasound"]},{"location":"applications/matlab_gpu_coder/matlab_beamform/#arm64-jetson_1","title":"arm64: Jetson","text":"<p>The folder <code>MATLAB_ws</code>, created by MATLAB, mirrors the folder structure of the host machine and is therefore different from one user to another; hence, we need to specify the path to the <code>codegen</code> folder in the <code>CMakeLists.txt</code>, in order for the build to find the required libraries. Set the variable <code>REL_PTH_MATLAB_CODEGEN</code> to the relative path where the <code>codegen</code> folder is located in the <code>MATLAB_ws</code> folder. For example, if GPU Coder created the following folder structure on the Jetson device:</p> <pre><code>matlab_beamform\n\u2514\u2500\u2500 MATLAB_ws\n    \u2514\u2500\u2500 R2023b\n        \u2514\u2500\u2500 C\n            \u2514\u2500\u2500 Users\n                \u2514\u2500\u2500 Jensen\n                    \u2514\u2500\u2500 holohub\n                        \u2514\u2500\u2500 applications\n                            \u2514\u2500\u2500 matlab_gpu_coder\n                                \u2514\u2500\u2500 matlab_beamform\n                                    \u2514\u2500\u2500 matlab\n                                        \u2514\u2500\u2500 codegen\n</code></pre> <p>the variable should be set as:</p> <pre><code>REL_PTH_MATLAB_CODEGEN=MATLAB_ws/R2023b/C/Users/Jensen/holohub/applications/matlab_gpu_coder/matlab_beamform/matlab/codegen\n</code></pre> <p>Next, run the HoloHub Docker container:</p> <pre><code>./holohub run-container\n</code></pre>","tags":["Healthcare AI","Visualization","Synthetic Aperture Beamforming","Signal Processing","Beamforming","Ultrasound"]},{"location":"applications/matlab_gpu_coder/matlab_image_processing/","title":"Image Processing with MATLAB GPU Coder","text":"<p> Authors: Holoscan Team (NVIDIA), MathWorks Team (MathWorks) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 4 - Experimental</p> <p>This application does real-time image processing of Holoscan sample data. The image processing is implemented in MATLAB and converted to CUDA using GPU Coder. When the application is run, Holoviz will display the processed data in real time.</p> <p></p>","tags":["Computer Vision and Perception","Visualization","Optimization","Image Processing","Video","Holoviz"]},{"location":"applications/matlab_gpu_coder/matlab_image_processing/#folder-structure","title":"Folder Structure","text":"<pre><code>matlab_image_processing\n\u251c\u2500\u2500 matlab  # MATLAB files\n\u2502   \u251c\u2500\u2500 generate_image_processing_jetson.m  # MATLAB script to generate CUDA DLLs on Jetson\n\u2502   \u251c\u2500\u2500 generate_image_processing_x86.m  # MATLAB script to generate CUDA DLLs on x86\n\u2502   \u251c\u2500\u2500 matlab_image_processing.m  # MATLAB function that CUDA code is generated from\n\u2502   \u2514\u2500\u2500 test_image_processing.m  # MATLAB script to test MATLAB function\n\u251c\u2500\u2500 CMakeLists.txt  # CMake build file\n\u251c\u2500\u2500 main.cpp  # Ultrasound beamforming app\n\u2514\u2500\u2500 matlab_image_processing.yaml  # Ultrasound beamforming config\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Optimization","Image Processing","Video","Holoviz"]},{"location":"applications/matlab_gpu_coder/matlab_image_processing/#generate-cuda-code-with-matlab-gpu-coder","title":"Generate CUDA Code with MATLAB GPU Coder","text":"","tags":["Computer Vision and Perception","Visualization","Optimization","Image Processing","Video","Holoviz"]},{"location":"applications/matlab_gpu_coder/matlab_image_processing/#x86-ubuntu","title":"x86: Ubuntu","text":"<p>In order to generate the CUDA Code, start MATLAB and <code>cd</code> to the <code>matlab</code> folder and open the <code>generate_image_processing_x86.m</code> script. Run the script and a folder <code>codegen/dll/matlab_image_processing</code> will be generated in the <code>matlab_image_processing</code> folder.</p>","tags":["Computer Vision and Perception","Visualization","Optimization","Image Processing","Video","Holoviz"]},{"location":"applications/matlab_gpu_coder/matlab_image_processing/#arm64-jetson","title":"arm64: Jetson","text":"<p>On an x86 computer with MATLAB installed, <code>cd</code> to the <code>matlab</code> folder and open the <code>generate_image_processing_jetson.m</code> script. Having an <code>ssh</code> connection to the Jetson device you want to build the CUDA DLLs on, specify the parameters of that connection in the <code>hwobj</code> on line 7, also replace <code>&lt;ABSOLUTE_PATH&gt;</code> of <code>cfg.Hardware.BuildDir</code> on line 39, as the absolute path (on the Jetson device) to <code>holohub</code> folder. Run the script and a folder <code>MATLAB_ws</code> will be created in the <code>matlab_image_processing</code> folder.</p>","tags":["Computer Vision and Perception","Visualization","Optimization","Image Processing","Video","Holoviz"]},{"location":"applications/matlab_gpu_coder/matlab_image_processing/#configure-holoscan-for-matlab","title":"Configure Holoscan for MATLAB","text":"","tags":["Computer Vision and Perception","Visualization","Optimization","Image Processing","Video","Holoviz"]},{"location":"applications/matlab_gpu_coder/matlab_image_processing/#x86-ubuntu_1","title":"x86: Ubuntu","text":"<p>Define the environment variable:</p> <pre><code>export MATLAB_ROOT=\"/usr/local/MATLAB\"\nexport MATLAB_VERSION=\"R2023b\"\n</code></pre> <p>where you, if need be, replace <code>MATLAB_ROOT</code> with the location of your MATLAB install and <code>MATLAB_VERSION</code> with the correct version.</p> <p>Next, run the HoloHub Docker container:</p> <pre><code>./holohub run-container \\\n    --add-volume ${MATLAB_ROOT}/${MATLAB_VERSION} \\\n    --docker-opts=\"-e MATLAB_ROOT=/workspace/volumes/${MATLAB_VERSION}\"\n</code></pre> <p>and build the endoscopy tool tracking application to download the necessary data:</p> <pre><code>./holohub build endoscopy_tool_tracking\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Optimization","Image Processing","Video","Holoviz"]},{"location":"applications/matlab_gpu_coder/matlab_image_processing/#arm64-jetson_1","title":"arm64: Jetson","text":"<p>The folder <code>MATLAB_ws</code>, created by MATLAB, mirrors the folder structure of the host machine and is therefore different from one user to another; hence, we need to specify the path to the <code>codegen</code> folder in the <code>CMakeLists.txt</code>, in order for the build to find the required libraries. Set the variable <code>REL_PTH_MATLAB_CODEGEN</code> to the relative path where the <code>codegen</code> folder is located in the <code>MATLAB_ws</code> folder. For example, if GPU Coder created the following folder structure on the Jetson device:</p> <pre><code>matlab_gpu_coder\n\u2514\u2500\u2500 MATLAB_ws\n    \u2514\u2500\u2500 R2023b\n        \u2514\u2500\u2500 C\n            \u2514\u2500\u2500 Users\n                \u2514\u2500\u2500 Jensen\n                    \u2514\u2500\u2500 holohub\n                        \u2514\u2500\u2500 applications\n                            \u2514\u2500\u2500 matlab_gpu_coder\n                                \u2514\u2500\u2500 matlab_image_processing\n                                    \u2514\u2500\u2500 matlab\n                                        \u2514\u2500\u2500 codegen\n</code></pre> <p>the variable should be set as:</p> <pre><code>REL_PTH_MATLAB_CODEGEN=MATLAB_ws/R2023b/C/Users/Jensen/holohub/applications/matlab_gpu_coder/matlab_image_processing/matlab/codegen\n</code></pre> <p>Next, run the HoloHub Docker container:</p> <pre><code>./holohub run-container\n</code></pre> <p>and build the endoscopy tool tracking application **inside the container to download the necessary data:</p> <pre><code>./holohub build endoscopy_tool_tracking\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Optimization","Image Processing","Video","Holoviz"]},{"location":"applications/monai_endoscopic_tool_seg/","title":"Endoscopy Tool Segmentation from MONAI Model Zoo","text":"<p> Authors: Jin Li (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.1 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 2 - Trusted</p> <p>This endoscopy tool segmentation application runs the MONAI Endoscopic Tool Segmentation from MONAI Model Zoo.</p> <p>This HoloHub application has been verified on the GI Genius sandbox and is currently deployable to GI Genius Intelligent Endoscopy Modules. GI Genius is Cosmo Intelligent Medical Devices\u2019 AI-powered endoscopy system. This implementation by Cosmo Intelligent Medical Devices showcases the fast and seamless deployment of HoloHub applications on products/platforms running on NVIDIA Holoscan.</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Segmentation"]},{"location":"applications/monai_endoscopic_tool_seg/#model","title":"Model","text":"<p>We will be deploying the endoscopic tool segmentation model from MONAI Model Zoo.  Note that you could also use the MONAI model zoo repo for training your own semantic segmentation model with your own data, but here we are directly deploying the downloaded MONAI model checkpoint into Holoscan. </p> <p>You can choose to  - download the MONAI Endoscopic Tool Segmentation Model on NGC directly and skip the rest of this Model section, or  - go through the following conversion steps yourself.</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Segmentation"]},{"location":"applications/monai_endoscopic_tool_seg/#model-conversion-to-onnx-optional","title":"Model conversion to ONNX (optional)","text":"<p>Before deploying the MONAI Model Zoo's trained model checkpoint in Holoscan SDK, we convert the model checkpoint into ONNX. </p> <ol> <li>Download the PyTorch model checkpoint linked in the README of endoscopic tool segmentation. We will assume its name to be <code>model.pt</code>.</li> <li> <p>Clone the MONAI Model Zoo repo.  <pre><code>cd [your-workspace]\ngit clone https://github.com/Project-MONAI/model-zoo.git\n</code></pre> and place the downloaded PyTorch model into <code>model-zoo/models/endoscopic_tool_segmentation/</code>.</p> </li> <li> <p>Pull and run the docker image for MONAI. We will use this docker image for converting the PyTorch model to ONNX.  <pre><code>docker pull projectmonai/monai\ndocker run -it --rm --gpus all -v [your-workspace]/model-zoo:/workspace/model-zoo -w /workspace/model-zoo/models/endoscopic_tool_segmentation/ projectmonai/monai\n</code></pre></p> </li> <li>Install onnxruntime within the container  <code>pip install onnxruntime onnx-graphsurgeon</code></li> <li>Convert model</li> </ol> <p>We will first export the model.pt file to ONNX by using the export_to_onnx.py file. Modify the backbone in line 122 to be efficientnet-b2: <pre><code>model = load_model_and_export(modelname, outname, out_channels, height, width, multigpu, backbone=\"efficientnet-b2\")\n</code></pre> Note that the model in the Model Zoo here was trained to have only two output channels: label 1 = tools, label 0 = everything else, but the same Model Zoo repo can be repurposed to train a model with a different dataset that has more than two classes. <pre><code>python scripts/export_to_onnx.py --model model.pt --outpath model_endoscopic_tool_seg.onnx --width 736 --height 480 --out_channels 2\n</code></pre> Fold constants in the ONNX model. <pre><code>polygraphy surgeon sanitize --fold-constants model_endoscopic_tool_seg.onnx -o model_endoscopic_tool_seg_sanitized.onnx\n</code></pre> Finally, modify the input and output channels to have shape [n, height, width, channels], [n, channels, height, width].  <pre><code>python scripts/graph_surgeon_tool_seg.py --orig_model model_endoscopic_tool_seg_sanitized.onnx --new_model model_endoscopic_tool_seg_sanitized_nhwc_in_nchw_out.onnx\n</code></pre></p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Segmentation"]},{"location":"applications/monai_endoscopic_tool_seg/#data","title":"Data","text":"<p>For this application we will use the same Endoscopy Sample Data as the Holoscan SDK reference applications.</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Segmentation"]},{"location":"applications/monai_endoscopic_tool_seg/#requirements","title":"Requirements","text":"<p>The only requirement is to make sure the model and data are accessible by the application. At runtime we will need to specify via the <code>--data</code> arg, assuming the directory specified contains two subdirectories <code>endoscopy/</code> (endoscopy video data directory) and <code>monai_tool_seg_model/</code> (model directory).</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Segmentation"]},{"location":"applications/monai_endoscopic_tool_seg/#running-the-application","title":"Running the application","text":"","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Segmentation"]},{"location":"applications/monai_endoscopic_tool_seg/#quick-start","title":"Quick start","text":"<p>The easiest way to test this application is to use Holohub CLI from the top level of Holohub</p> <pre><code>./holohub run monai_endoscopic_tool_seg\n</code></pre>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Segmentation"]},{"location":"applications/monai_endoscopic_tool_seg/#running-the-application-manually","title":"Running the application manually","text":"<p>To run this application, you'll need to configure your PYTHONPATH environment variable to locate the necessary python libraries based on your Holoscan SDK installation type.</p> <p>You should refer to the glossary for the terms defining specific locations within HoloHub.</p> <p>If your Holoscan SDK installation type is:</p> <ul> <li>python wheels:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <ul> <li>otherwise:</li> </ul> <p><pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOSCAN_INSTALL_DIR&gt;/python/lib:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> Next, run the application, where  is a directory that contains two subdirectories <code>endoscopy/</code> and <code>monai_tool_seg_model/</code>.: <p><pre><code>python3 tool_segmentation.py --data &lt;DATA_DIR&gt;\n</code></pre> If you'd like the application to run at the input framerate, change the <code>replayer</code> config in the yaml file to <code>realtime: true</code>.</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Segmentation"]},{"location":"applications/multiai_endoscopy/","title":"Multi AI SSD Detection and MONAI Endoscopic Tool Segmentation","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 2 - Trusted</p> <p>In this application, we show how to build a Multi AI application with detection and segmentation models, write postprocessing operators using CuPy and NumPy in Python tensor interop and MatX library (An efficient C++17 GPU numerical computing library with Python-like syntax) in C++ tensor interop, and pass multiple tensors from postprocessing to Holoviz.</p> <p> Fig. 1 Endoscopy (laparoscopy) image from a cholecystectomy (gallbladder removal surgery) showing tool detection and segmentation results from two concurrently executed AI models. Image courtesy of Research Group Camma, IHU Strasbourg and the University of Strasbourg (NGC Resource)</p> <p>Please refer to the README under ./app_dev_process to see the process of developing the applications.</p> <p>The application graph looks like: </p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","SSD","Segmentation"]},{"location":"applications/multiai_endoscopy/#model","title":"Model","text":"<p>We combine two models from the single model applications SSD Tool Detection and MONAI Endoscopic Tool Segmentation:</p> <ul> <li>SSD model from NGC with additional NMS op: <code>epoch24_nms.onnx</code></li> <li>MONAI tool segmentation model from NGC: <code>model_endoscopic_tool_seg_sanitized_nhwc_in_nchw_out.onnx</code></li> </ul>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","SSD","Segmentation"]},{"location":"applications/multiai_endoscopy/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","SSD","Segmentation"]},{"location":"applications/multiai_endoscopy/#requirements","title":"Requirements","text":"<p>Ensure you have installed the Holoscan SDK via one of the methods specified in the SDK user guide.</p> <p>The directory specified by <code>--data</code> at app runtime is assumed to contain three subdirectories, corresponding to the three NGC resources specified in Model and Data: <code>endoscopy</code>, <code>monai_tool_seg_model</code> and <code>ssd_model</code>.  These resources will be automatically downloaded to the holohub data directory when building the application.</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","SSD","Segmentation"]},{"location":"applications/multiai_endoscopy/#building-and-running-the-application","title":"Building and Running the Application","text":"","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","SSD","Segmentation"]},{"location":"applications/multiai_endoscopy/#python-apps","title":"Python Apps","text":"<p>To run the Python application, you can make use of the run script</p> <pre><code>./holohub run multiai_endoscopy --language python\n</code></pre> <p>Alternatively, to run this application, you'll need to configure your PYTHONPATH environment variable to locate the necessary python libraries based on your Holoscan SDK installation type.</p> <p>You should refer to the glossary for the terms defining specific locations within HoloHub.</p> <p>If your Holoscan SDK installation type is:</p> <ul> <li>python wheels:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <ul> <li>otherwise:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOSCAN_INSTALL_DIR&gt;/python/lib:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <p>Next, run the application:</p> <pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;/applications/multiai_endoscopy/python\npython3 multi_ai.py --data &lt;DATA_DIR&gt;\n</code></pre>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","SSD","Segmentation"]},{"location":"applications/multiai_endoscopy/#c-apps","title":"C++ Apps","text":"<p>There are three versions of C++ apps, with the only difference being that they implement the inference post-processing operator <code>DetectionPostprocessorOp</code> in different ways:</p> <ul> <li><code>post-proc-cpu</code>: Multi-AI app running the inference post-processing operator on the CPU using <code>std</code> features only.</li> <li><code>post-proc-matx-cpu</code>: Multi-AI app running the inference post-processing operator on the CPU using the MatX library).</li> <li><code>post-proc-matx-gpu</code>: Multi-AI app running  the inference post-processing operator on the GPU using MatX (CUDA).</li> </ul> <p>To run <code>post-proc-cpu</code>, you can simply run:</p> <pre><code>./holohub run multiai_endoscopy --language cpp\n</code></pre> <p>For the other two C++ applications, you'll need to build these without the Holohub CLI as follows.</p> <p>To run <code>post-proc-matx-cpu</code> or <code>post-proc-matx-gpu</code>, first navigate to the app directory.</p> <pre><code>cd applications/multiai_endoscopy/cpp/post-proc-matx-cpu\n</code></pre> <p>Next we need to configure and build the app.</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","SSD","Segmentation"]},{"location":"applications/multiai_endoscopy/#configuring","title":"Configuring","text":"<p>First, create a build folder:</p> <pre><code>mkdir -p build\n</code></pre> <p>then run CMake configure with:</p> <pre><code>cmake -S . -B build\n</code></pre> <p>Unless you make changes to <code>CMakeLists.txt</code>, this step only needs to be done once.</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","SSD","Segmentation"]},{"location":"applications/multiai_endoscopy/#building","title":"Building","text":"<p>The app can be built with:</p> <pre><code>cmake --build build\n</code></pre> <p>or equally:</p> <pre><code>cd build\nmake\n</code></pre>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","SSD","Segmentation"]},{"location":"applications/multiai_endoscopy/#running","title":"Running","text":"<p>You can run the app with:</p> <pre><code>./build/multi_ai --data &lt;DATA_DIR&gt;\n</code></pre>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","SSD","Segmentation"]},{"location":"applications/multiai_ultrasound/cpp/","title":"Multi-AI Ultrasound","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates how to run multiple inference pipelines in a single application by leveraging the Holoscan Inference module, a framework that facilitates designing and executing inference applications in the Holoscan SDK.</p> <p>The Inference and the Processing operators use APIs from the Holoscan Inference module to extract data, initialize and execute the inference workflow, process, and transmit data for visualization.</p> <p>The applications uses models and echocardiogram data from iCardio.ai. The models include: - a Plax chamber model, that identifies four critical linear measurements of the heart - a Viewpoint Classifier model, that determines confidence of each frame to known 28 cardiac anatomical view as defined by the guidelines of the American Society of Echocardiography - an Aortic Stenosis Classification model, that provides a score which determines likeability for the presence of aortic stenosis</p> <p>The default configuration (<code>multiai_ultrasound.yaml</code>) runs on default GPU (GPU-0). Multi-AI Ultrasound application can be executed on multiple GPUs with the Holoscan SDK version 0.6 onwards. A sample configuration file for multi GPU configuration for multi-AI ultrasound application (<code>mgpu_multiai_ultrasound.yaml</code>) is present in both <code>cpp</code> and <code>python</code> applications. The multi-GPU configuration file is designed for a system with at least 2 GPUs connected to the same PCIE network.</p>","tags":["Healthcare AI","Visualization","Cardiac Keypoints Detection","Ultrasound","Video","Classification"]},{"location":"applications/multiai_ultrasound/cpp/#requirements","title":"Requirements","text":"<p>The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the echocardiogram (replayer). Follow the setup instructions from the user guide to use the AJA capture card.</p>","tags":["Healthcare AI","Visualization","Cardiac Keypoints Detection","Ultrasound","Video","Classification"]},{"location":"applications/multiai_ultrasound/cpp/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for Multi-AI Ultrasound Pipeline</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Healthcare AI","Visualization","Cardiac Keypoints Detection","Ultrasound","Video","Classification"]},{"location":"applications/multiai_ultrasound/cpp/#build-instructions","title":"Build Instructions","text":"<p>Please refer to the top level Holohub README.md file for information on how to build this application.</p>","tags":["Healthcare AI","Visualization","Cardiac Keypoints Detection","Ultrasound","Video","Classification"]},{"location":"applications/multiai_ultrasound/cpp/#run-instructions","title":"Run Instructions","text":"<p>In your <code>build</code> directory, run the commands of your choice:</p> <ul> <li> <p>Using a pre-recorded video     <pre><code>sed -i -e 's#^source:.*#source: replayer#' applications/multiai_ultrasound/cpp/multiai_ultrasound.yaml\napplications/multiai_ultrasound/cpp/multiai_ultrasound --data &lt;DATA_DIR&gt;/multiai_ultrasound\n</code></pre></p> </li> <li> <p>Using a pre-recorded video on multi-GPU system     <pre><code>sed -i -e 's#^source:.*#source: replayer#' applications/multiai_ultrasound/cpp/mgpu_multiai_ultrasound.yaml\napplications/multiai_ultrasound/cpp/multiai_ultrasound applications/multiai_ultrasound/cpp/mgpu_multiai_ultrasound.yaml --data &lt;DATA_DIR&gt;/multiai_ultrasound\n</code></pre></p> </li> <li> <p>Using an AJA card     <pre><code>sed -i -e 's#^source:.*#source: aja#' applications/multiai_ultrasound/cpp/multiai_ultrasound.yaml\napplications/multiai_ultrasound/cpp/multiai_ultrasound\n</code></pre></p> </li> </ul>","tags":["Healthcare AI","Visualization","Cardiac Keypoints Detection","Ultrasound","Video","Classification"]},{"location":"applications/multiai_ultrasound/operators/visualizer_icardio/","title":"Visualizer iCardio","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>visualizer_icardio</code> extension generates the visualization components from the processed results of the plax chamber model.</p>","tags":["Video"]},{"location":"applications/multiai_ultrasound/operators/visualizer_icardio/#nvidiaholoscanmultiaivisualizericardio","title":"<code>nvidia::holoscan::multiai::VisualizerICardio</code>","text":"<p>Visualizer iCardio extension ingests the processed results of the plax chamber model and generates the key points, the key areas and the lines that are transmitted to the HoloViz codelet.</p>","tags":["Video"]},{"location":"applications/multiai_ultrasound/operators/visualizer_icardio/#parameters","title":"Parameters","text":"<ul> <li><code>in_tensor_names_</code>: Input tensor names</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>out_tensor_names_</code>: Output tensor names</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>allocator_</code>: Memory allocator</li> <li>type: <code>gxf::Handle&lt;gxf::Allocator&gt;</code></li> <li><code>receivers_</code>: Vector of input receivers. Multiple receivers supported.</li> <li>type: <code>HoloInfer::GXFReceivers</code></li> <li><code>transmitter_</code>: Output transmitter. Single transmitter supported.</li> <li>type: <code>HoloInfer::GXFTransmitters</code></li> </ul>","tags":["Video"]},{"location":"applications/multiai_ultrasound/python/","title":"Multi-AI Ultrasound","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates how to run multiple inference pipelines in a single application by leveraging the Holoscan Inference module, a framework that facilitates designing and executing inference applications in the Holoscan SDK.</p> <p>The Multi AI operators (inference and postprocessor) use APIs from the Holoscan Inference module to extract data, initialize and execute the inference workflow, process, and transmit data for visualization.</p> <p>The applications uses models and echocardiogram data from iCardio.ai. The models include: - a Plax chamber model, that identifies four critical linear measurements of the heart - a Viewpoint Classifier model, that determines confidence of each frame to known 28 cardiac anatomical view as defined by the guidelines of the American Society of Echocardiography - an Aortic Stenosis Classification model, that provides a score which determines likeability for the presence of aortic stenosis</p> <p>The default configuration (<code>multiai_ultrasound.yaml</code>) runs on default GPU (GPU-0). Multi-AI Ultrasound application can be executed on multiple GPUs with the Holoscan SDK version 0.6 onwards. A sample configuration file for multi GPU configuration for multi-AI ultrasound application (<code>mgpu_multiai_ultrasound.yaml</code>) is present in both <code>cpp</code> and <code>python</code> applications. The multi-GPU configuration file is designed for a system with at least 2 GPUs connected to the same PCIE network.</p>","tags":["Healthcare AI","Visualization","Cardiac Keypoints Detection","Ultrasound","Video","Classification"]},{"location":"applications/multiai_ultrasound/python/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8+</li> <li>The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the echocardiogram (replayer). Follow the setup instructions from the user guide to use the AJA capture card.</li> </ul>","tags":["Healthcare AI","Visualization","Cardiac Keypoints Detection","Ultrasound","Video","Classification"]},{"location":"applications/multiai_ultrasound/python/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for Multi-AI Ultrasound Pipeline</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Healthcare AI","Visualization","Cardiac Keypoints Detection","Ultrasound","Video","Classification"]},{"location":"applications/multiai_ultrasound/python/#run-instructions","title":"Run Instructions","text":"<p>To run this application, you'll need to configure your PYTHONPATH environment variable to locate the necessary python libraries based on your Holoscan SDK installation type.</p> <p>You should refer to the glossary for the terms defining specific locations within HoloHub.</p> <p>If your Holoscan SDK installation type is:</p> <ul> <li>python wheels:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <ul> <li>otherwise:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOSCAN_INSTALL_DIR&gt;/python/lib:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <p>Next, run the commands of your choice:</p> <ul> <li> <p>Using a pre-recorded video     <pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;/applications/multiai_ultrasound/python\npython3 multiai_ultrasound.py --source=replayer --data &lt;DATA_DIR&gt;/multiai_ultrasound\n</code></pre></p> </li> <li> <p>Using a pre-recorded video on multi-GPU system     <pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;/applications/multiai_ultrasound/python\npython3 multiai_ultrasound.py --config mgpu_multiai_ultrasound.yaml --source=replayer --data &lt;DATA_DIR&gt;/multiai_ultrasound\n</code></pre></p> </li> <li> <p>Using an AJA card     <pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;/applications/multiai_ultrasound/python\npython3 multiai_ultrasound.py --source=aja\n</code></pre></p> </li> </ul>","tags":["Healthcare AI","Visualization","Cardiac Keypoints Detection","Ultrasound","Video","Classification"]},{"location":"applications/network_radar_pipeline/","title":"Radar Signal Processing over Network","text":"<p> Authors: Dylan Eustice (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.1 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 2 - Trusted</p> <p>The Network Radar application demonstrates signal processing on data streamed via packets over a network. It showcases the use of both the Advanced Network Operator and Basic Network Operator to send or receive data, combined with the signal processing operators implemented in the Simple Radar Pipeline application.</p> <p>Using the GPUDirect capabilities afforded by the Advanced Network Operator, this pipeline has been tested up to 100 Gbps (Tx/Rx) using a ConnectX-7 NIC and A30 GPU.</p> <p>The motivation for building this application is to demonstrate how data arrays can be assembled from packet data in real-time for low-latency, high-throughput sensor processing applications. The main components of this work are defining a message format and writing code connecting the network operators to the signal processing operators.</p> <p>This application supports the Advanced Network Operator DPDK and DOCA GPUNetIO transport layers.</p>","tags":["Signal Processing","Networking and Distributed Computing","GPUDirect","DPDK"]},{"location":"applications/network_radar_pipeline/#prerequisites","title":"Prerequisites","text":"<p>See the README for the Advanced Network Operator for requirements and system tuning needed to enable high-throughput GPUDirect capabilities.</p>","tags":["Signal Processing","Networking and Distributed Computing","GPUDirect","DPDK"]},{"location":"applications/network_radar_pipeline/#environment","title":"Environment","text":"<p>Note: Dockerfile should be cross-compatible, but has only been tested on x86. Needs to be edited if different versions / architectures are required.</p>","tags":["Signal Processing","Networking and Distributed Computing","GPUDirect","DPDK"]},{"location":"applications/network_radar_pipeline/#build","title":"Build","text":"<p>Please refer to the top level Holohub README.md file for information on how to build this application: <code>./holohub build network_radar_pipeline</code>.</p>","tags":["Signal Processing","Networking and Distributed Computing","GPUDirect","DPDK"]},{"location":"applications/network_radar_pipeline/#run","title":"Run","text":"<p>Note: must properly configure YAML files before running. To run with DPDK as ANO transport layer: - On Tx machine: <code>./build/applications/network_radar_pipeline/cpp/network_radar_pipeline source.yaml</code> - On Rx machine: <code>./build/applications/network_radar_pipeline/cpp/network_radar_pipeline process.yaml</code></p> <p>To run with DOCA GPUNetIO as ANO transport layer: - On Tx machine: <code>./build/applications/network_radar_pipeline/cpp/network_radar_pipeline source_doca.yaml</code> - On Rx machine: <code>./build/applications/network_radar_pipeline/cpp/network_radar_pipeline process_doca.yaml</code></p>","tags":["Signal Processing","Networking and Distributed Computing","GPUDirect","DPDK"]},{"location":"applications/network_radar_pipeline/#network-operator-connectors","title":"Network Operator Connectors","text":"<p>See each operators' README before using / for more detailed information.</p>","tags":["Signal Processing","Networking and Distributed Computing","GPUDirect","DPDK"]},{"location":"applications/network_radar_pipeline/#basic-network-operator-connector","title":"Basic Network Operator Connector","text":"<p>Implementation in <code>basic_network_connectors</code>. Only supports CPU packet receipt / transmit. Uses cudaMemcpy to move data between network operator and MatX tensors.</p>","tags":["Signal Processing","Networking and Distributed Computing","GPUDirect","DPDK"]},{"location":"applications/network_radar_pipeline/#advanced-network-operator-connector","title":"Advanced Network Operator Connector","text":"<p>Implementation in <code>advanced_network_connectors</code>. RX connector is only configured to run with GPUDirect enabled, in header-data split (HDS) mode. TX connector supports both GPUDirect/HDS or CPU-only.</p>","tags":["Signal Processing","Networking and Distributed Computing","GPUDirect","DPDK"]},{"location":"applications/network_radar_pipeline/#testing-rx-on-generic-packet-data","title":"Testing RX on generic packet data","text":"<p>When using the Advanced network operator, the application supports testing the radar processing component in a \"spoof packets\" mode. This functionality allows for easier benchmarking of the application by ingesting generic packet data and writing in header fields such that the full radar pipeline will still be exercised. When \"SPOOF_PACKET_DATA\" (adv_networking_rx.h) is set to \"true\", the index of the packet will be used to set fields appropriately. This functionality is currently unsupported using the basic network operator connectors.</p>","tags":["Signal Processing","Networking and Distributed Computing","GPUDirect","DPDK"]},{"location":"applications/network_radar_pipeline/#message-format","title":"Message format","text":"<p>The message format is defined by <code>RFPacket</code>. It is a byte array, represented by <code>RFPacket::payload</code>, where the first 16 bytes are reserved for metadata and the rest are used for representing complex I/Q samples. The metadata is: - Sample index: The starting index for a single pulse/channel of the transmitted samples (2 bytes) - Waveform ID: Index of the transmitted waveform (2 bytes) - Channel index: Index of the channel (2 bytes) - Pulse index: Index of the pulse (2 bytes) - Number samples: Number of I/Q samples transmitted (2 bytes) - End of array: Boolean - true if this is the last message for the waveform (2 bytes)</p>","tags":["Signal Processing","Networking and Distributed Computing","GPUDirect","DPDK"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/","title":"Chat with NVIDIA NIM","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3, 2.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This is a sample application that shows how to use the OpenAI SDK with NVIDIA Inference Microservice (NIM). Whether you are using a NIM from build.nvidia.com/ or a self-hosted NIM, this sample application will work for both.</p>","tags":["Natural Language and Conversational AI","Auth and API","LLM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#quick-start","title":"Quick Start","text":"<ol> <li>Add API key in <code>nvidia_nim.yaml</code></li> <li><code>./holohub run nvidia_nim_chat</code></li> </ol>","tags":["Natural Language and Conversational AI","Auth and API","LLM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#configuring-the-sample-application","title":"Configuring the sample application","text":"<p>Use the <code>nvidia_nim.yaml</code> configuration file to configure the sample application:</p>","tags":["Natural Language and Conversational AI","Auth and API","LLM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#connection-information","title":"Connection Information","text":"<pre><code>nim:\n  base_url: https://integrate.api.nvidia.com/v1\n  api_key:\n</code></pre> <p><code>base_url</code>: The URL of your NIM instance. Defaults to NVIDIA hosted NIMs. <code>api_key</code>: Your API key to access NVIDIA hosted NIMs.</p>","tags":["Natural Language and Conversational AI","Auth and API","LLM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#model-information","title":"Model Information","text":"<p>The <code>models</code> section in the YAML file is configured with multiple NVIDIA hosted models by default. This allows you to switch between different models easily within the application by sending the prompt <code>/m</code> to the application.</p> <p>Model parameters may be added or adjusted in the <code>models</code> section as well per model.</p>","tags":["Natural Language and Conversational AI","Auth and API","LLM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#run-the-sample-application","title":"Run the sample application","text":"<p>There are a couple of options to run the sample application:</p>","tags":["Natural Language and Conversational AI","Auth and API","LLM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#run-using-docker","title":"Run using Docker","text":"<p>To run the sample application with Docker, you must first build and run a Docker image that includes the sample application and its dependencies:</p> <pre><code># Build and run the Docker images from the root directory of Holohub\n./holohub run-container nvidia_nim\n</code></pre> <p>Continue to the Start the Application section once inside the Docker container.</p>","tags":["Natural Language and Conversational AI","Auth and API","LLM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#run-the-application-without-docker","title":"Run the Application without Docker","text":"<p>Install all dependencies from the <code>requirements.txt</code> file:</p> <pre><code># optionally create a virtual environment and activate it\npython3 -m venv .venv\nsource .venv/bin/activate\n\n# install the required packages\npip install -r applications/nvidia_nim/chat/requirements.txt\n</code></pre>","tags":["Natural Language and Conversational AI","Auth and API","LLM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#start-the-application","title":"Start the Application","text":"<p>To use the NIMs on build.nvidia.com/, configure your API key in the <code>nvidia_nim.yaml</code> configuration file and run the sample app as follows:</p> <p>note: you may also configure your api key using an environment variable. E.g., <code>export API_KEY=...</code></p> <pre><code># To use NVIDIA hosted NIMs available on build.nvidia.com, export your API key first\nexport API_KEY=[enter your api key here]\n\n./holohub run nvidia_nim_chat\n</code></pre> <p>Have fun!</p>","tags":["Natural Language and Conversational AI","Auth and API","LLM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#connecting-with-locally-hosted-nims","title":"Connecting with Locally Hosted NIMs","text":"<p>To use a locally hosted NIM, first download and start the NIM. Then configure the <code>base_url</code> parameter in the <code>nvidia_nim.yaml</code> configuration file to point to your local NIM instance.</p> <p>The following example shows a NIM running locally and serving its APIs and the <code>meta-llama3-8b-instruct</code> model from <code>http://0.0.0.0:8000/v1</code>.</p> <pre><code>nim:\n  base_url: http://0.0.0.0:8000/v1/\n\nmodels:\n  llama3-8b-instruct:\n    model: meta-llama3-8b-instruct # name of the model serving by the NIM\n    # add/update/remove the following key/value pairs to configure the parameters for the model\n    top_p: 1\n    n: 1\n    max_tokens: 1024\n    frequency_penalty: 1.0\n</code></pre>","tags":["Natural Language and Conversational AI","Auth and API","LLM"]},{"location":"applications/nvidia_nim/nvidia_nim_imaging/","title":"Medical Imaging Segmentation with NVIDIA Vista-3D NIM","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.0 Tested Holoscan SDK versions: 1.0.0, 2.0.2 Contribution metric: Level 1 - Highly Reliable</p> <p>Vista-3D is a specialized interactive foundation model for segmenting and annotating human anatomies. This sample application demonstrates using the Vista-3D NVIDIA Inference Microservice (NIM) in a Holoscan pipeline.</p> <p>The application instructs the Vista-3D NIM API to process the given dataset and downloads and extracts the results of a segmentation NRRD file onto a local directory.</p> <p>Visit build.nvidia.com to learn more about Vista-3D and generate an API key to use with this application.</p>","tags":["Healthcare AI","Imaging","Segmentation","NRRD processing","3D Slicer","Medical Imaging"]},{"location":"applications/nvidia_nim/nvidia_nim_imaging/#quick-start","title":"Quick Start","text":"<ol> <li>Add API key in <code>nvidia_nim.yaml</code></li> <li><code>./holohub run nvidia_nim_imaging</code></li> </ol>","tags":["Healthcare AI","Imaging","Segmentation","NRRD processing","3D Slicer","Medical Imaging"]},{"location":"applications/nvidia_nim/nvidia_nim_imaging/#configuring-the-sample-application","title":"Configuring the sample application","text":"<p>Use the <code>nvidia_nim.yaml</code> configuration file to configure the sample application:</p>","tags":["Healthcare AI","Imaging","Segmentation","NRRD processing","3D Slicer","Medical Imaging"]},{"location":"applications/nvidia_nim/nvidia_nim_imaging/#connection-information","title":"Connection Information","text":"<pre><code>nim:\n base_url: https://integrate.api.nvidia.com/v1\n api_key:\n</code></pre> <ul> <li><code>base_url</code>: The URL of your NIM instance. Defaults to NVIDIA-hosted NIMs.</li> <li><code>api_key</code>: Your API key to access NVIDIA-hosted NIMs.</li> </ul>","tags":["Healthcare AI","Imaging","Segmentation","NRRD processing","3D Slicer","Medical Imaging"]},{"location":"applications/nvidia_nim/nvidia_nim_imaging/#build-and-run-the-sample-application","title":"Build and Run the sample application","text":"<pre><code># This first builds and runs the Docker images, then builds and runs the application.\n./holohub run nvidia_nim_imaging\n</code></pre>","tags":["Healthcare AI","Imaging","Segmentation","NRRD processing","3D Slicer","Medical Imaging"]},{"location":"applications/nvidia_nim/nvidia_nim_imaging/#display-the-results","title":"Display the Results","text":"<p>In this section, we will show how to view the sample data and segmentation results returned from Vista-3D.</p> <ol> <li>Download 3D Slicer: https://download.slicer.org/</li> <li>Decompress and launch 3D Slicer    <pre><code>tar -xvzf Slicer-5.6.2-linux-amd64.tar.gz\n</code></pre></li> <li>Locate the sample data volume and the segmentation results in <code>build/nvidia_nim_imaging/applications/nvidia_nim/nvidia_nim_imaging</code> <pre><code>drwxr-xr-x 3 user domain-users \u00a0 \u00a0 4096 Jul \u00a03 11:41 ./\ndrwxr-xr-x 4 user domain-users \u00a0 \u00a0 4096 Jul \u00a03 11:40 ../\n-rw-r--r-- 1 user user         27263336 Jul 23 14:22 example-1_seg.nrrd\n-rw-r--r-- 1 user user         33037057 Jul 23 14:21 sample.nii.gz\n</code></pre></li> <li>In 3D Slicer, click File, Add Data and click Choose File(s) to Add.    From the Add Data into the scene dialog, find and add the <code>sample.nii.gz</code> file and the <code>example-1_seg.nrrd</code> file.    For the <code>sample.nrrd</code> file, select Segmentation and click Ok.    </li> <li>3D Slicer shall display the volume and the segmentation results as shown below:    </li> </ol>","tags":["Healthcare AI","Imaging","Segmentation","NRRD processing","3D Slicer","Medical Imaging"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/","title":"NVIDIA NV-CLIP NIM","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3, 2.1.0, 2.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>NV-CLIP is a multimodal embeddings model for image and text, and this is a sample application that shows how to use the OpenAI SDK with NVIDIA Inference Microservice (NIM). Whether you are using a NIM from build.nvidia.com/ or a self-hosted NIM, this sample application will work for both.</p>","tags":["Computer Vision and Perception","Multimodal Model","Auth and API","Image Processing"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/#quick-start","title":"Quick Start","text":"<p>Get your API Key and start the sample application.</p> <ol> <li>Enter your API key in <code>nvidia_nim.yaml</code></li> <li><code>./holohub run nvidia_nim_nvclip</code></li> </ol>","tags":["Computer Vision and Perception","Multimodal Model","Auth and API","Image Processing"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/#advanced","title":"Advanced","text":"","tags":["Computer Vision and Perception","Multimodal Model","Auth and API","Image Processing"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/#configuring-the-sample-application","title":"Configuring the sample application","text":"<p>Use the <code>nvidia_nim.yaml</code> configuration file to configure the sample application:</p>","tags":["Computer Vision and Perception","Multimodal Model","Auth and API","Image Processing"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/#nvidia-hosted-nv-clip-nim","title":"NVIDIA-Hosted NV-CLIP NIM","text":"<p>By default, the application is configured to use NVIDIA-hosted NV-CLIP NIM.</p> <pre><code>nim:\n base_url: https://integrate.api.nvidia.com/v1\n api_key:\n</code></pre> <p><code>base_url</code>: The URL of your NIM instance. Defaults to NVIDIA-hosted NIMs. <code>api_key</code>: Your API key to access NVIDIA-hosted NIMs.</p> <p>Note: you may also configure your API key using an environment variable. E.g., <code>export API_KEY=...</code></p> <pre><code># To use NVIDIA hosted NIMs available on build.nvidia.com, export your API key first\nexport API_KEY=[enter your API key here]\n</code></pre>","tags":["Computer Vision and Perception","Multimodal Model","Auth and API","Image Processing"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/#self-hosted-nims","title":"Self-Hosted NIMs","text":"<p>To use a self-hosted NIM, refer to the NV-CLIP NIM documentation to configure and start the NIM.</p> <p>Then, comment out the NVIDIA-hosted section and uncomment the self-hosted configuration section in the <code>nvidia_nim.yaml</code> file.</p> <pre><code>nim:\n  base_url: http://0.0.0.0:8000/v1/\n  encoding_format: float\n  api_key: NA\n  model: nvidia/nvclip-vit-h-14\n</code></pre>","tags":["Computer Vision and Perception","Multimodal Model","Auth and API","Image Processing"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/#build-the-application","title":"Build The Application","text":"<p>To run the sample application, you must first build a Docker image that includes the sample application and its dependencies:</p> <pre><code># Build and run the Docker images from the root directory of Holohub\n./holohub run-container nvidia_nim_nvclip\n</code></pre>","tags":["Computer Vision and Perception","Multimodal Model","Auth and API","Image Processing"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/#run-the-application","title":"Run the Application","text":"<p>To use the NIMs on build.nvidia.com/, configure your API key in the <code>nvidia_nim.yaml</code> configuration file and run the sample app as follows:</p> <pre><code>./holohub run nvidia_nim_nvclip\n</code></pre>","tags":["Computer Vision and Perception","Multimodal Model","Auth and API","Image Processing"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/#using-the-application","title":"Using the Application","text":"<p>Once the application is ready, it will prompt you to input URLs to the images you want to perform inference.</p> <pre><code>Enter a URL to an image: https://domain.to/my/image-cat.jpg\nDownloading image...\n\nEnter a URL to another image or hit ENTER to continue: https://domain.to/my/image-rabbit.jpg\nDownloading image...\n\nEnter a URL to another image or hit ENTER to continue: https://domain.to/my/image-dog.jpg\nDownloading image...\n</code></pre> <p>If there are no more images that you want to use, hit ENTER to continue and then enter a prompt:</p> <pre><code>Enter a URL to another image or hit ENTER to continue:\n\nEnter a prompt: Which image contains a rabbit?\n</code></pre> <p>The application will connect to the NIM to generate an answer and then calculate the cosine similarity between the images and the prompt:</p> <pre><code>\u2827 Generating...\nPrompt: Which image contains a rabbit?\nOutput:\nImage 1: 3.0%\nImage 2: 52.0%\nImage 3: 46.0%\n</code></pre>","tags":["Computer Vision and Perception","Multimodal Model","Auth and API","Image Processing"]},{"location":"applications/object_detection_torch/","title":"Object Detection using PyTorch Faster R-CNN","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application performs object detection using frcnn resnet50 model from torchvision. The inference is executed using <code>torch</code> backend in <code>holoinfer</code> module in Holoscan SDK.</p> <p><code>object_detection_torch.yaml</code> is the configuration file. Input video file is converted into GXF tensors and the name and location of the GXF tensors are updated in the <code>basename</code> and the <code>directory</code> field in <code>replayer</code>.</p> <p>This application need <code>Libtorch</code> for inferencing. Ensure that the Holoscan SDK is build with <code>build_libtorch</code> flag as true. If not, then rebuild the SDK with following: <code>./holohub build --build_libtorch true</code> before running this application.</p>","tags":["Computer Vision and Perception","Video","Detection","Visualization","Holoviz"]},{"location":"applications/object_detection_torch/#data","title":"Data","text":"<p>To run this application, you will need the following:</p> <ul> <li>Model name: frcnn_resnet50_t.pt<ul> <li>The model should be converted to torchscript format.  The original pytorch model can be downloaded from pytorch model. <code>frcnn_resnet50_t.pt</code> is used</li> </ul> </li> <li>Model configuration file: frcnn_resnet50_t.yaml<ul> <li>Model config documents input and output nodes, their dimensions and respective datatype.</li> </ul> </li> <li>Labels file: labels.txt<ul> <li>Labels for identified objects.</li> </ul> </li> <li>Postprocessor configuration file: postprocessing.yaml<ul> <li>This configuration stores the number and type of objects to be identified. By default, the application detects and generates bounding boxes for <code>car</code> (max 50), <code>person</code> (max 50), <code>motorcycle</code> (max 10) in the input frame. All remaining identified objects are tagged with label <code>object</code> (max 50).</li> <li>Additionally, color of the bounding box for each identified object can be set.</li> <li>Threshold of scores can be set in the <code>params</code>. Default value is 0.75.</li> </ul> </li> </ul> <p>Sample dataset can be any video file freely available for testing on the web. E.g. Traffic video</p> <p>Once the video is downloaded, it must be converted into GXF entities. As shown in the command below, width and height is set to 1920x1080 by default. To reduce the size of generated tensors a lower resolution can be used. Generated entities must be saved at /object_detection_torch folder. <pre><code>ffmpeg -i &lt;downloaded_video&gt; -pix_fmt rgb24 -f rawvideo pipe:1 | python utilities/convert_video_to_gxf_entities.py --width 1920 --height 1080 --channels 3 --framerate 30\n</code></pre> <p>If resolution is updated in entity generation, it must be updated in the following config files as well: /object_detection_torch/frcnn_resnet50_t.yaml /object_detection_torch/postprocessing.yaml","tags":["Computer Vision and Perception","Video","Detection","Visualization","Holoviz"]},{"location":"applications/object_detection_torch/#quick-start","title":"Quick start","text":"<p>If you want to quickly run this application, you can use the <code>./holohub run</code> command.</p> <pre><code>./holohub run object_detection_torch\n</code></pre> <p>Otherwise, you can build and run the application using the commands below.</p>","tags":["Computer Vision and Perception","Video","Detection","Visualization","Holoviz"]},{"location":"applications/object_detection_torch/#building-the-application","title":"Building the application","text":"<p>The best way to run this application is inside the container, as it would provide all the required third-party packages:</p> <pre><code># Create and launch the container image for this application\n./holohub run-container object_detection_torch\n# Build the application inside the container. Note that this downloads the video data as well\n./holohub build object_detection_torch\n# Generate the pytorch model\npython3 applications/object_detection_torch/generate_resnet_model.py  data/object_detection_torch/frcnn_resnet50_t.pt\n# Run the application\n./holohub run object_detection_torch\n</code></pre> <p>Please refer to the top level Holohub README.md file for more information on how to build this application.</p>","tags":["Computer Vision and Perception","Video","Detection","Visualization","Holoviz"]},{"location":"applications/object_detection_torch/#running-the-application","title":"Running the application","text":"<pre><code># ensure the current working directory contains the &lt;data_dir&gt;.\n&lt;build_dir&gt;/object_detection_torch\n</code></pre> <p>If application is executed from within the holoscan sdk container and is not able to find <code>libtorch.so</code>, update <code>LD_LIBRARY_PATH</code> as below:</p> <pre><code>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/libtorch/1.13.1/lib\n</code></pre> <p>On aarch64, if application is executed from within the holoscan sdk container and libtorch throws linking errors, update the <code>LD_LIBRARY_PATH</code> as below:</p> <pre><code>export LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:/opt/hpcx/ompi/lib\"\n</code></pre>","tags":["Computer Vision and Perception","Video","Detection","Visualization","Holoviz"]},{"location":"applications/object_detection_torch/#containerize-the-application","title":"Containerize the application","text":"<p>To containerize the application using Holoscan CLI, first build the application using <code>./holohub install object_detection_torch</code>, run the <code>package-app.sh</code> script and then follow the generated output to package and run the application.</p> <p>Refer to the Packaging Holoscan Applications section of the Holoscan User Guide to learn more about installing the Holoscan CLI or packaging your application using Holoscan CLI.</p>","tags":["Computer Vision and Perception","Video","Detection","Visualization","Holoviz"]},{"location":"applications/openigtlink_3dslicer/","title":"OpenIGTLink 3D Slicer: Bidirectional Video Streaming with AI Segmentation","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: June 4, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 2 - Trusted</p> <p>This application demonstrates how to interface Holoscan SDK with 3D Slicer, using the OpenIGTLink protocol. The application is shown in the application graph below.</p> <p></p> <p>In summary, the <code>openigtlink</code> transmit and receive operators are used in conjunction with an AI segmentation pipeline to:</p> <ol> <li>Send Holoscan sample video data from a node running Holoscan SDK, using <code>OpenIGTLinkTxOp</code>, to 3D Slicer running on a different node (simulating a video source connected to 3D Slicer):<ul> <li>For <code>cpp</code> application, the ultrasound sample data is sent.</li> <li>For <code>python</code> application, the colonoscopy sample data is sent.</li> </ul> </li> <li>Transmit the video data back to Holoscan SDK using OpenIGTLinkIF Module, and receive the data with the <code>OpenIGTLinkRxOp</code> operator.</li> <li>Perform an AI segmentation pipeline in Holoscan:<ul> <li>For <code>cpp</code> application, the ultrasound segmentation model is deployed.</li> <li>For <code>python</code> application, the colonoscopy segmentation model is deployed.</li> </ul> </li> <li>Use Holoviz in <code>headless</code> mode to render image and segmentation and then send the data back to 3D Slicer using the <code>OpenIGTLinkTxOp</code> operator.</li> </ol> <p>This workflow allows for sending image data from 3D Slicer over network to Holoscan SDK (running on either <code>x86</code> or <code>arm</code>), do some compute task (e.g., AI inference), and send the results back to 3D Slicer for visualization. Nodes can run distributed; for example, Holoscan SDK can run on an IGX Orin (Node A) sending the video data, 3D Slicer on a Windows laptop (Node B) and the AI inference pipeline on yet another machine (Node C). Also, note that the <code>openigtlink</code> operators can connect to any software/library that supports the OpenIGTLink protocol; here, 3D Slicer is used as it is a popular open source software package for image analysis and scientific visualization.</p> <p>For the <code>cpp</code> application, which does ultrasound segmentations the results look like</p> <p></p> <p>and for the <code>python</code> application, which does colonoscopy segmentation, the results look like</p> <p></p> <p>where the image data before Holoscan processing is shown in the left slice view, and the image data with segmentation overlay (after Holoscan processing) is shown in the right slice view.</p>","tags":["Healthcare AI","Networking and Distributed Computing","Video","Visualization","3D Slicer","Segmentation","Ultrasound","Endoscopy"]},{"location":"applications/openigtlink_3dslicer/#run-instructions","title":"Run Instructions","text":"","tags":["Healthcare AI","Networking and Distributed Computing","Video","Visualization","3D Slicer","Segmentation","Ultrasound","Endoscopy"]},{"location":"applications/openigtlink_3dslicer/#machine-running-3d-slicer","title":"Machine running 3D Slicer","text":"<p>On the machine running 3D Slicer:</p> <ol> <li>In 3D Slicer, open the Extensions Manager and install the <code>SlicerOpenIGTLink</code> extension.</li> <li>Next, load the scene <code>openigtlink_3dslicer/scene/openigtlink_3dslicer.mrb</code> into 3D Slicer.</li> <li>Go to the <code>OpenIGTLinkIF</code> module and make sure that the <code>SendToHoloscan</code> connector has the IP address of the machine running Holoscan SDK in the Hostname input box (under Properties).</li> <li>Then activate the two connectors <code>ReceiveFromHoloscan</code> and <code>SendToHoloscan</code> (click Active check box under Properties).</li> </ol>","tags":["Healthcare AI","Networking and Distributed Computing","Video","Visualization","3D Slicer","Segmentation","Ultrasound","Endoscopy"]},{"location":"applications/openigtlink_3dslicer/#machine-running-holoscan-sdk","title":"Machine running Holoscan SDK","text":"<p>On the machine running Holoscan SDK:</p> <ol> <li>Configure the connection: Update the <code>host_name</code> parameters in the configuration files for both <code>OpenIGTLinkRxOp</code> operators:</li> <li><code>openigtlink_tx_slicer_img</code></li> <li><code>openigtlink_tx_slicer_holoscan</code></li> </ol> <p>Set these to the IP address of the machine running 3D Slicer.</p> <pre><code>&gt; **Note**: This application requires [OpenIGTLink](http://openigtlink.org/) to be installed.\n</code></pre> <ol> <li> <p>Run the application: Use the Holohub CLI to launch the application.</p> </li> <li> <p>For the <code>python</code> application:</p> <pre><code>./holohub run openigtlink_3dslicer --language python\n</code></pre> </li> <li> <p>For the <code>cpp</code> application:</p> <pre><code>./holohub run openigtlink_3dslicer --language cpp\n</code></pre> </li> </ol>","tags":["Healthcare AI","Networking and Distributed Computing","Video","Visualization","3D Slicer","Segmentation","Ultrasound","Endoscopy"]},{"location":"applications/orsi/orsi_in_out_body/","title":"Orsi Academy In-Out Body Detection and Surgical Video Anonymization","text":"<p> Authors: Jasper Hofman (Orsi Academy) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 3 - Developmental</p>  Fig. 1: Example of anonymized result after inference  <p></p>","tags":["Healthcare AI","Video","Computer Vision and Perception","Detection","Visualization"]},{"location":"applications/orsi/orsi_in_out_body/#introduction","title":"Introduction","text":"<p>In robotic surgery, anonymization of video is necessary to ensure privacy and protect patient data. During e.g. swapping of robotic tools or lens cleaning the endoscope is removed from the body, possibly capturing faces, recognizable tattoos or written patient data. Therefore, going out of body is a suiting definition for an anonymization boundary.</p>","tags":["Healthcare AI","Video","Computer Vision and Perception","Detection","Visualization"]},{"location":"applications/orsi/orsi_in_out_body/#pipeline","title":"Pipeline","text":"Fig. 2: Schematic overview of Holoscan application  <p>Towards realtime anonymization, a binary out-of-body classifier was trained and deployed using Holoscan platform. Figure 2 shows a schematic overview of the application. After capturing the frame, the alpha channel is dropped by the Format Converter operator. Additionally, the black padding borders added by the robotic system are removed, the tensor is resized to 512x512 pixels and the tensor type is converted from int [0, 255] to float [0, 1]. In the Anonymization Preprocessor operator the tensor pixel values of every collor channel are normalized using the corresponding means and standard deviations of the anonymization dataset. After model inference with the Multi-AI inference operator, the result frame is anonymized in the Orsi Visualizer operator according to the model output. The blurring is applied using a glsl program.</p>","tags":["Healthcare AI","Video","Computer Vision and Perception","Detection","Visualization"]},{"location":"applications/orsi/orsi_in_out_body/#controls","title":"Controls","text":"Action Control Enable anonymization B","tags":["Healthcare AI","Video","Computer Vision and Perception","Detection","Visualization"]},{"location":"applications/orsi/orsi_in_out_body/#build-and-launch-app","title":"Build and Launch app","text":"<p>C++</p> <pre><code>./holohub run orsi_in_out_body --language cpp\n</code></pre> <p>Python</p> <pre><code>./holohub run orsi_in_out_body --language python\n</code></pre>","tags":["Healthcare AI","Video","Computer Vision and Perception","Detection","Visualization"]},{"location":"applications/orsi/orsi_multi_ai_ar/","title":"Orsi Academy Multi AI and AR Visualization","text":"<p> Authors: Jasper Hofman (Orsi Academy) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 3 - Developmental</p>  Fig. 1: Application screenshots   <p></p>","tags":["Healthcare AI","Extended Reality","Segmentation","VTK"]},{"location":"applications/orsi/orsi_multi_ai_ar/#introduction","title":"Introduction","text":"<p>3D models are used in surgery to improve patient outcomes. They provide information on patient specific anatomies that are not visible in the present surgical scene. Especially in robotic surgery these 3D models give great insights because they can be projected and aligned directly onto the endoscopic video. This augmented reality supports navigation for the surgeon in the console. The downside of the 3D model projection is that it occludes the surgical instruments, creating a possible hazardous situation for the patient. This application uses a deep learning segmentation model to identify non-organic items such as tools, clips and wires and projects them on top of the 3D model. This solves the occlusion problem and adds a sense of depth to the AR application. Figure 1 shows in-app screenshots where the 3D model is used to support navigation. The large tumor in green is visible as anatomical landmark.  In addition, the application features an out-of-body detector. Anonymization of video is necessary to ensure privacy and protect patient data. During e.g. swapping of robotic tools or lens cleaning the endoscope is removed from the body, possibly capturing faces, recognizable tattoos or written patient data. Therefore, going out of body is a suiting definition for an anonymization boundary.</p>  Fig. 2: 3D model of kidney tumor case  <p></p> <p>The application was successfully used to remove a kidney tumor during a partial nephrectomy. Figure 2 shows the kidney 3D model where the tumor is colored in yellow.</p>","tags":["Healthcare AI","Extended Reality","Segmentation","VTK"]},{"location":"applications/orsi/orsi_multi_ai_ar/#pipeline","title":"Pipeline","text":"Fig. 3: Schematic overview of Holoscan application  <p>Towards realtime segmentation, AR overlay and anonymization, a binary segmentation model for non-organic items and a binary out-of-body detector was trained and deployed using Holoscan platform. Figure 3 shows a schematic overview of the application. After capturing the frame, the alpha channel is dropped by the Format Converter operator. Additionally, the black padding borders added by the robotic system are removed, the tensor is resized to 512x512 pixels and the tensor type is converted from int [0, 255] to float [0, 1]. In the Segmentation Preprocessor and Anonymization Preprocessor operators the tensor pixel values of every color channel are normalized using the corresponding means and standard deviations of the segmentation and anonymization dataset respectively. After model inference with the Multi-AI inference operator, a sigmoid layer is applied to the model predictions by the Segmentation Postprocessor resulting in a binary segmentation mask. Additionally, the resizing and cropping operations are inverted by first resizing and then adding black padding borders as to not compromise resolution. In the Orsi Visualizer operator the 3D model is rendered using the VTK library and composited onto the endoscopic image using OpenGL. The pixels corresponding to non-organic items are passed through the 3D model layer using a glsl shader program. Finally, the frame is anonymized according to the out-of-body detector output.</p>","tags":["Healthcare AI","Extended Reality","Segmentation","VTK"]},{"location":"applications/orsi/orsi_multi_ai_ar/#controls","title":"Controls","text":"Action Control Enable/Disable anonymization B Enable/Disable manipulations T Load 3D model orientation preset CTRL + L Save current 3D model orientation as preset (will overwrite default preset) CTRL + S Rotate 3D model (3 degrees of freedom) Left Click + Drag Rotate 3D model (1 degree of freedom) CTRL + Left Click + Drag Zoom 3D model Right Click + Drag Translate 3D  model SHIFT + Left Click + Drag Enable/Disable 3D model E Enable/Disable segmentation overlay O Increase opacity 3D model + Decrease opacity 3D model - Toggle arterial tree 1 Toggle venous tree 2 Toggle ureter 4 Toggle parenchyma 5 Toggle tumor 6","tags":["Healthcare AI","Extended Reality","Segmentation","VTK"]},{"location":"applications/orsi/orsi_multi_ai_ar/#build-and-launch-app","title":"Build and Launch app","text":"<pre><code>./holohub run orsi_multi_ai_ar --language cpp\n</code></pre> <p>or</p> <pre><code>./holohub run orsi_multi_ai_ar --language python\n</code></pre>","tags":["Healthcare AI","Extended Reality","Segmentation","VTK"]},{"location":"applications/orsi/orsi_segmentation_ar/","title":"Orsi Academy Surgical Tool Segmentation and AR Overlay","text":"<p> Authors: Jasper Hofman (Orsi Academy) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 3 - Developmental</p>  Fig. 1: Application screenshot   <p></p>","tags":["Healthcare AI","Extended Reality","Surgical AI","VTK","Segmentation","Endoscopy"]},{"location":"applications/orsi/orsi_segmentation_ar/#introduction","title":"Introduction","text":"<p>3D models are used in surgery to improve patient outcomes. They provide information on patient specific anatomies that are not visible in the present surgical scene. Especially in robotic surgery these 3D models give great insights because they can be projected and aligned directly onto the endoscopic video. This augmented reality supports navigation for the surgeon in the console. The downside of the 3D model projection is that it occludes the surgical instruments, creating a possible hazardous situation for the patient. This application uses a deep learning segmentation model to identify non-organic items such as tools, clips and wires and projects them on top of the 3D model. This solves the occlusion problem and adds a sense of depth to the AR application. Figure 1 shows an in-app screenshot where the 3D model is aligned with the anatomy and the tools are segmented on top.</p>  Fig. 2: 3D model of nutcracker case  <p></p> <p>The application was successfully used to verify stent location during a nutcracker syndrome stent removal. Nutcracker syndrome is a rare vein compression disorder where the left renal vein is squeezed between the superior mesenteric artery and abdominal aorta, obstructing blood drainage and even risking blood to flow backwards, causing pain and blood in the urine. Typically, blood flow is restored through endovascular stenting. Although effective, over time the stent had migrated beyond the compression site, causing the symptoms to return and in addition posing an incidental obstruction for blood flow. The stent ofcourse had to be removed. Figure 2 shows a 3D model depicting the current situation.</p>","tags":["Healthcare AI","Extended Reality","Surgical AI","VTK","Segmentation","Endoscopy"]},{"location":"applications/orsi/orsi_segmentation_ar/#pipeline","title":"Pipeline","text":"Fig. 3: Schematic overview of Holoscan application  <p>Towards realtime segmentation and AR overlay, a binary segmentation model for non-organic items was trained and deployed using Holoscan platform. Figure 3 shows a schematic overview of the application. After capturing the frame, the alpha channel is dropped by the Format Converter operator. Additionally, the black padding borders added by the robotic system are removed, the tensor is resized to 512x512 pixels and the tensor type is converted from int [0, 255] to float [0, 1]. In the Segmentation Preprocessor operator the tensor pixel values of every color channel are normalized using the corresponding means and standard deviations of the segmentation dataset. After model inference with the Multi-AI inference operator, a sigmoid layer is applied to the model predictions by the Segmentation Postprocessor resulting in a binary segmentation mask. Additionally, the resizing and cropping operations are inverted by first resizing and then adding black padding borders as to not compromise resolution. In the Orsi Visualizer operator the 3D model is rendered using the VTK library and composited onto the endoscopic image using OpenGL. The pixels corresponding to non-organic items are passed through the 3D model layer using a glsl shader program.</p>","tags":["Healthcare AI","Extended Reality","Surgical AI","VTK","Segmentation","Endoscopy"]},{"location":"applications/orsi/orsi_segmentation_ar/#controls","title":"Controls","text":"Action Control Enable/Disable anonymization B Enable/Disable manipulations T Load 3D model orientation preset CTRL + L Save current 3D model orientation as preset (will overwrite default preset) CTRL + S Rotate 3D model (3 degrees of freedom) Left Click + Drag Rotate 3D model (1 degree of freedom) CTRL + Left Click + Drag Zoom 3D model Right Click + Drag Translate 3D  model SHIFT + Left Click + Drag Enable/Disable 3D model E Enable/Disable segmentation overlay O Increase opacity 3D model + Decrease opacity 3D model - Toggle venous tree 0 Toggle venous stent zone 1 Toggle stent 2","tags":["Healthcare AI","Extended Reality","Surgical AI","VTK","Segmentation","Endoscopy"]},{"location":"applications/orsi/orsi_segmentation_ar/#build-and-launch-app","title":"Build and Launch app","text":"<pre><code>./holohub run orsi_segmentation_ar --language cpp\n</code></pre> <p>or</p> <pre><code>./holohub run orsi_segmentation_ar --language python\n</code></pre>","tags":["Healthcare AI","Extended Reality","Surgical AI","VTK","Segmentation","Endoscopy"]},{"location":"applications/orthorectification_with_optix/","title":"GPU-Accelerated Orthorectification with NVIDIA OptiX","text":"<p> Authors: Brent Bartlett (NVIDIA) Supported platforms: x86_64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 4 - Experimental</p> <p>This application is an example of utilizing the nvidia OptiX SDK via the PyOptix bindings to create per-frame orthorectified imagery. In this example, one can create a visualization of mapping frames from a drone mapping mission processed with Open Drone Map. A typical output of a mapping mission is a single merged mosaic. While this product is useful for GIS applications, it is difficult to apply algorithms on a such a large single image without incurring additional steps like image chipping. Additionally, the mosaic process introduces image artifacts which can negativley impact algorithm performance. </p> <p>Since this holoscan pipeline processes each frame individually, it opens the door for one to apply an algorithm to the original un-modififed imagery and then map the result. If custom image processing is desired, it is recommended to insert custom operators before the Ray Trace Ortho operator in the application flow. </p> <p> Fig. 1 Orthorectification sample application workflow</p> <p>Steps for running the application:</p> <p>a) Download and Prep the ODM Dataset 1. Download the Lafayette Square Dataset and place into ~/Data.</p> <ol> <li>Process the dataset with ODM via docker command:  <code>docker run -ti --rm -v ~/Data/lafayette_square:/datasets/code opendronemap/odm --project-path /datasets --camera-lens perspective --dsm</code></li> </ol> <p>If you run out of memory add the following argument to preserve some memory: <code>--feature-quality medium</code></p> <p>b) Clone holohub and navigate to this application directory</p> <p>c) Download OptiX SDK 7.4.0 and extract the package in the same directory as the source code (i.e. applications/orthorectification_with_optix).</p> <p>d) Build development container  1. <code>DOCKER_BUILDKIT=1 docker build -t holohub-ortho-optix:latest .</code></p> <p>You can now run the docker container by:  1. <code>xhost +local:docker</code> 2. <code>nvidia_icd_json=$(find /usr/share /etc -path '*/vulkan/icd.d/nvidia_icd.json' -type f 2&gt;/dev/null | grep .) || (echo \"nvidia_icd.json not found\" &gt;&amp;2 &amp;&amp; false)</code> 3. <code>docker run -it --rm --net host --runtime=nvidia -v ~/Data:/root/Data  -v .:/work/ -v /tmp/.X11-unix:/tmp/.X11-unix  -v $nvidia_icd_json:$nvidia_icd_json:ro  -e NVIDIA_DRIVER_CAPABILITIES=graphics,video,compute,utility,display -e DISPLAY=$DISPLAY  holohub-ortho-optix</code></p> <p>Finish prepping the input data:  1. <code>gdal_translate -tr 0.25 0.25 -r cubic ~/Data/lafayette_square/odm_dem/dsm.tif ~/Data/lafayette_square/odm_dem/dsm_small.tif</code> 2. <code>gdal_fillnodata.py -md 0 ~/Data/lafayette_square/odm_dem/dsm_small.tif ~/Data/lafayette_square/odm_dem/dsm_small_filled.tif</code></p> <p>Finally run the application:  1. <code>python ./python/ortho_with_pyoptix.py</code></p> <p>You can modify the applications settings in the file \"ortho_with_pyoptix.py\" </p> <pre><code>sensor_resize = 0.25 # resizes the raw sensor pixels\nncpu = 8 # how many cores to use to load sensor simulation\ngsd = 0.25 # controls how many pixels are in the rendering\niterations = 425 # how many frames to render from the source images (in this case 425 is max)\nuse_mosaic_bbox = True # render to a static bounds on the ground as defined by the DEM\nwrite_geotiff = False \nnb=3 # how many bands to write to the GeoTiff\nrender_scale = 0.5 # scale the holoview window up or down\nfps = 8.0 # rate limit the simulated sensor feed to this many frames per second\n</code></pre> <p> Fig. 2 Running the orthorectification sample application</p>","tags":["Computer Vision and Perception","Visualization","Optix","Drone","Image Processing"]},{"location":"applications/polyp_detection/","title":"Polyp Detection","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: June 16, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 3.3.0 Tested Holoscan SDK versions: 3.3.0 Contribution metric: Level 2 - Trusted</p> <p>This application demonstrates how to run polyp detection models on live video in real-time.</p> <p></p> <p>The model: RT-DETR v2 is trained on the REAL-Colon dataset.</p> <p>Compared to the <code>SSD</code> object detection model described in the paper, <code>RT-DETR</code> demonstrates improvements. The table below shows metrics for SSD obtained from Table 3 of the paper, and metrics for RT-DETR calculated on the same test set (using all test images from the <code>REAL-Colon</code> dataset).</p> Method MAP@0.5 MAP@0.5:0.95 SSD 0.338 0.216 RT-DETR 0.452 0.301","tags":["Colonoscopy","Detection","RT-DETR","bounding box"]},{"location":"applications/polyp_detection/#run-instructions","title":"Run Instructions","text":"<p>For simplicity a DockerFile is available. To run this application:</p> <pre><code>./holohub run polyp_detection\n</code></pre> <p>The (NGC) Sample App Model for AI Polyp Detection and the (NGC) Sample App Data are automatically downloaded and converted to the correct format when first run the application.</p> <p>If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p> <p>If you want to run the <code>polyp_detection.py</code> script directly with the built docker container, please refer to the following instructions:</p> <ul> <li>Ensure that the <code>rtdetrv2_timm_r50_nvimagenet_pretrained_neg_finetune_bhwc.onnx</code> file is located in the directory specified by the <code>data</code> argument.</li> <li>Verify that the generated video files (<code>.gxf_index</code> and <code>.gxf_entities</code> files) are in the directory specified by the <code>video_dir</code> argument.</li> <li>Specify the correct video width and height using the <code>video_size</code> argument.</li> </ul> <p>For example: <pre><code>python polyp_detection.py --data /path-to-onnx-model/ --video_dir /path-to-video/ --video_size \"(width, height)\"\n</code></pre></p>","tags":["Colonoscopy","Detection","RT-DETR","bounding box"]},{"location":"applications/prohawk_video_replayer/","title":"ProHawk Video Replayer","text":"<p> Authors: Nigel Nelson (NVIDIA) Supported platforms: aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 4 - Experimental</p> <p>This application utilizes the ProHawk restoration operator along with Holoscan's Video Replayer and Holoviz operators to enhance and restore medical imagery in real-time, offering superior image quality. The user-friendly interface of the application provides a range of filter options, enabling users to dynamically select the most suitable filter for optimal results.</p> <p></p>","tags":["Healthcare AI","Video","filter presets","Endoscopy","Holoviz"]},{"location":"applications/prohawk_video_replayer/#prohawk-vision-restoration-operator","title":"ProHawk Vision Restoration Operator","text":"<p>The ProHawk Vision Operator is a groundbreaking solution that is transforming both healthcare and manufacturing industries by revolutionizing computer vision technology with its patented restoration capabilities. It seamlessly integrates into the NVIDIA Holoscan full-stack infrastructure, fundamentally altering the way healthcare professionals diagnose and treat patients, while also optimizing manufacturing processes. In healthcare, the ProHawk Vision Operator Plugin automatically interprets medical imaging frames, identifies real-world conditions, and employs precise control over ProHawk Vision Restoration algorithms, all driven by an objective mathematical model using quantitative measurements to enhance accuracy in diagnoses and treatments by restoring degraded frames. In manufacturing, the ProHawk Vision Operator Plugin algorithms reveal manufacturing line defects ensuring product quality.</p>","tags":["Healthcare AI","Video","filter presets","Endoscopy","Holoviz"]},{"location":"applications/prohawk_video_replayer/#application-controls","title":"Application Controls","text":"<p>The operator can be controlled with keyboard shortcuts:</p> <ul> <li>AFS (0) - Automatic Filter Selection (AFS) \u2013 Automatically and continuously adjust the image pixel values to maximize image detail and visibility.</li> <li>LowLight (1) - Lowlight preset filter that corrects lighting compromised imagery.</li> <li>Vascular Detail (2) - Vasculature Detail Preset Filter that reveals the fine details of vasculature structures without the need for dye or contrast agents.</li> <li>Vapor (3) - Vapor Preset Filter that removes vapor, smoke, and stream from the video.</li> <li>Disable Restoration (d) - Disable ProHawk Vision computer vision restoration.</li> <li>Side-by-Side View (v) - Display Side-by-Side (restored/non-restores) Video.</li> <li>Display Menu Items (m) - Display menus control items.</li> <li>Quit (q) - Exit the application</li> </ul>","tags":["Healthcare AI","Video","filter presets","Endoscopy","Holoviz"]},{"location":"applications/prohawk_video_replayer/#data","title":"Data","text":"<p>The following dataset is used by this application: \ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking.</p>","tags":["Healthcare AI","Video","filter presets","Endoscopy","Holoviz"]},{"location":"applications/prohawk_video_replayer/#quick-start","title":"Quick Start","text":"<p>To build this application within a container and run it, please use the following command:</p> <pre><code>./holohub run prohawk_video_replayer\n</code></pre> <p>For a separate build and run, please see the following instructions:</p>","tags":["Healthcare AI","Video","filter presets","Endoscopy","Holoviz"]},{"location":"applications/prohawk_video_replayer/#step-by-step-build-and-run","title":"Step by step build and run","text":"<p>From the Holohub main directory run the following command:</p> <pre><code>./holohub build-container prohawk_video_replayer\n</code></pre> <p>Then launch the container to build the application:</p> <pre><code>./holohub run-container prohawk_video_replayer --no-docker-build\n</code></pre> <p>Inside the container build the application:</p> <pre><code>./holohub build prohawk_video_replayer\n</code></pre> <p>Inside the container run the application:</p> <ul> <li>C++:     <pre><code>./holohub run prohawk_video_replayer --language=cpp --no-local-build\n</code></pre></li> <li>Python:     <pre><code>export PYTHONPATH=$PYTHONPATH:/workspace/holohub/build/python/lib/\npython &lt;prohawk_app_dir&gt;/python/prohawk_video_replayer.py\n</code></pre></li> </ul> <p>For more information about this application and operator please visit https://prohawk.ai/prohawk-vision-operator/#learn For technical support or other assistance, please don't hesitate to visit us at https://prohawk.ai/contact</p>","tags":["Healthcare AI","Video","filter presets","Endoscopy","Holoviz"]},{"location":"applications/psd_pipeline/","title":"VITA 49 Power Spectral Density (PSD) (latest)","text":"","tags":["Signal Processing","Networking and Distributed Computing","PSD","GPUDirect","UDP"]},{"location":"applications/psd_pipeline/#vita-49-power-spectral-density-psd","title":"VITA 49 Power Spectral Density (PSD)","text":"<p> Authors: John Moon john.moon@vts-i.com (Valley Tech Systems, Inc.), Eric Ferrara eric.ferrara@vts-i.com (Valley Tech Systems, Inc.), Matthew Luckenbihl matthew.luckenbihl@vts-i.com (Valley Tech Systems, Inc.) Supported platforms: x86_64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.5.0 Tested Holoscan SDK versions: 2.5.0, 2.6.0, 2.7.0, 2.8.0, 2.9.0, 3.0.0, 3.1.0 Contribution metric: Level 3 - Developmental</p>","tags":["Signal Processing","Networking and Distributed Computing","PSD","GPUDirect","UDP"]},{"location":"applications/psd_pipeline/#overview","title":"Overview","text":"<p>The VITA 49 Power Spectral Density (PSD) application takes in a VITA49 data stream from the advanced network operator, then performs an FFT, PSD, and averaging operation before generating a VITA 49.2 spectral data packet which gets sent to a destination UDP socket.</p> <p></p>","tags":["Signal Processing","Networking and Distributed Computing","PSD","GPUDirect","UDP"]},{"location":"applications/psd_pipeline/#acronyms","title":"Acronyms","text":"Acronym Meaning FFT Fast Fourier Transform NIC Network Interface Card PSD Power Spectral Display VITA 49 Standard for interoperability between RF (radio frequency) devices VRT VITA Radio Tansport (transport-layer protocol)","tags":["Signal Processing","Networking and Distributed Computing","PSD","GPUDirect","UDP"]},{"location":"applications/psd_pipeline/#requirements","title":"Requirements","text":"<ul> <li>ConnectX 6 or 7 NIC for GPUDirect RDMA with packet size steering</li> <li>MatX (dependency - assumed to be installed on system)</li> <li>vita49-rs (dependency)</li> </ul>","tags":["Signal Processing","Networking and Distributed Computing","PSD","GPUDirect","UDP"]},{"location":"applications/psd_pipeline/#configuration","title":"Configuration","text":"<p>[!IMPORTANT] The settings in <code>config.yaml</code> need to be tailored to your system/radio.</p> <p>Each operator in the pipeline has its own configuration section. The specific options and their meaning are defined in each operator's own README:</p> <ol> <li><code>advanced_network</code></li> <li><code>vita_connector</code></li> <li><code>fft</code></li> <li><code>high_rate_psd</code></li> <li><code>low_rate_psd</code></li> <li><code>vita49_psd_packetizer</code></li> </ol> <p>There is also one option specific to this application:</p> <ol> <li><code>num_psds</code>: Number of PSDs to produce out of the pipeline before exiting.                Passing <code>-1</code> here will cause the pipeline to run indefinitely.</li> </ol>","tags":["Signal Processing","Networking and Distributed Computing","PSD","GPUDirect","UDP"]},{"location":"applications/psd_pipeline/#metadata","title":"Metadata","text":"<p>This pipeline leverages Holoscan's operator metadata dictionaries to pass VITA 49-adjacent metadata through the pipeline.</p> <p>Each operator in the pipeline adds its own metadata to the dictionary. At the end of the pipeline, the packetizer operator uses the metadata to construct VITA 49 context packets to send alongside the spectral data.</p>","tags":["Signal Processing","Networking and Distributed Computing","PSD","GPUDirect","UDP"]},{"location":"applications/psd_pipeline/#memory-layout","title":"Memory Layout","text":"<p>The ANO operates using memory regions that it directs data to. Since VITA49 is somewhat unusual in that signal data packets and context packets arrive at the same IP/port, we want to use the ANO's packet length steering feature to drop packets in the appropriate memory region.</p> <p>First, we want to define our memory regions:</p> <ol> <li>A region for any packets that don't match any of our flows [CPU]</li> <li>A region for frame headers (i.e. Ethernet + IP + UDP) [CPU]</li> <li>These headers are not currently used, so this memory region is      essentially acting as a <code>/dev/null</code> sink.</li> <li>A region for each channel's VRT headers [CPU]</li> <li>We need these headers to grab things like stream ID and      timestamp, but don't need that information in the GPU processing,      so make this a CPU region.</li> <li>A region for each channel's VRT signal data [GPU]</li> <li>These are the raw IQ samples from our radio - we want these to      land in GPU memory via GPUDirect RDMA.</li> <li>A region for all channels' VRT context data [CPU]</li> <li>We need the whole packet in the CPU to fill out our metadata      map for downstream processing/packet assembly.</li> </ol> <p>When an individual packet comes in, the ANO will try to match it against the defined flows. So, for our data packets, we want to define a queue like this:</p> <pre><code>            flows:\n              - name: \"Data packets\"\n                id: 0\n                action:\n                  type: queue\n                  id: 1\n                match:\n                  # Match with the port your SDR is sending to and the\n                  # length of the signal data packets\n                  udp_src: 4991\n                  udp_dst: 4991\n                  ipv4_len: 4148\n</code></pre> <p>This is saying \"if a UDP packet with IPv4 length 4,148 comes in on port 4991, send it to the queue with ID 1\". Now, if we look at our queue with ID 1, we see:</p> <pre><code>              - name: \"Data\"\n                id: 1\n                cpu_core: 5\n                batch_size: 12500\n                memory_regions:\n                  - \"Headers_RX_CPU\"\n                  - \"VRT_Headers_RX_CPU\"\n                  - \"Data_RX_GPU\"\n</code></pre> <p>When multiple <code>memory_regions</code> are specified like this, it means that each packet should be split based on the memory region size. In this case, <code>Headers_RX_CPU</code> has <code>buf_size: 42</code> (the size of the frame header), <code>VRT_Headers_RX_CPU</code> has <code>buf_size: 20</code> (the size of the VRT header), and <code>Data_RX_GPU</code> has <code>buf_size: 4100</code> (the remaining size of the data packet). These numbers may be different depending on the packet size of your radio!</p> <p><code>batch_size: 12500</code> tells the ANO to batch up 12,500 packets before sending the data to downstream operators. In our case, 12,500 packets represents 100ms worth of data and that's how much we want to process on each run of the pipeline.</p>","tags":["Signal Processing","Networking and Distributed Computing","PSD","GPUDirect","UDP"]},{"location":"applications/psd_pipeline/#multiple-channels","title":"Multiple Channels","text":"<p>When working with multiple channels, this pipeline expects all context packets (from every channel) to flow to one queue, but each data channel flows to its own queue.</p> <p>The connector operator also makes the following assumptions:</p> <ol> <li>All context packets flow to queue <code>id: 0</code>.</li> <li>All context packets flow ID matches its channel (e.g., flow ID <code>1</code>    is for context packets from channel <code>1</code>).</li> <li>All data packets arrive on a queue ID one greater than its channel    (e.g., queue ID <code>1</code> is for channel <code>0</code> data).</li> <li>The <code>batch_size</code> of the context queue is equal to the number of    channels.</li> </ol>","tags":["Signal Processing","Networking and Distributed Computing","PSD","GPUDirect","UDP"]},{"location":"applications/psd_pipeline/#ingest-nic","title":"Ingest NIC","text":"<p>The PCIe address of your ingest NIC needs to be specified in <code>config.yaml</code>.</p> <pre><code>    interfaces:\n      - name: sdr_data\n        address: 0000:17:00.0\n</code></pre> <p>You can find the addresses of your devices with: <code>lshw -c network -businfo</code>:</p> <pre><code># lshw -c network -businfo\nBus info          Device     Class          Description\n=======================================================\npci@0000:03:00.0  eth0       network        I210 Gigabit Network Connection\npci@0000:06:00.0  eno1       network        Aquantia Corp.\npci@0000:51:00.0  ens3f0np0  network        MT2910 Family [ConnectX-7]\npci@0000:51:00.1  ens3f1np1  network        MT2910 Family [ConnectX-7]\nusb@1:14.2        usb0       network        Ethernet interface\n</code></pre> <p>In this example, if you wanted to use the <code>ens3f1np1</code> interface, you'd pass <code>0000:51:00.1</code>.</p>","tags":["Signal Processing","Networking and Distributed Computing","PSD","GPUDirect","UDP"]},{"location":"applications/psd_pipeline/#build-run","title":"Build &amp; Run","text":"<ol> <li>Build the development container in two steps:    <pre><code># Build the ANO dev container\n./holohub build-container advanced_network --docker-file ./operators/advanced_network/Dockerfile\n\n# Add the psd-pipeline deps\n./holohub build-container psd_pipeline --base-img holohub:ngc-v3.1.0-dgpu --img holohub-psd-pipeline:ngc-v3.1.0-dgpu\n</code></pre></li> <li>Launch the development container with the command:    <pre><code>./holohub run-container psd_pipeline --no-docker-build --docker-opts=\"-u root --privileged\" --img holohub-psd-pipeline:ngc-v3.1.0-dgpu\n</code></pre></li> </ol> <p>Once you are in the dev container: 1. Build the application using:     <pre><code>./holohub build psd_pipeline\n</code></pre> 2. Run the application using:     <pre><code>./holohub run psd_pipeline --local --no-local-build --run-args=\"config.yaml\"\n</code></pre></p>","tags":["Signal Processing","Networking and Distributed Computing","PSD","GPUDirect","UDP"]},{"location":"applications/psd_pipeline/data_writer/","title":"data_writer (latest)","text":"","tags":["Signal Processing"]},{"location":"applications/psd_pipeline/data_writer/#data-writer-operator","title":"Data Writer Operator","text":"<p> Authors: John Moon john.moon@vts-i.com (Valley Tech Systems, Inc.), Eric Ferrara eric.ferrara@vts-i.com (Valley Tech Systems, Inc.), Matthew Luckenbihl matthew.luckenbihl@vts-i.com (Valley Tech Systems, Inc.) Supported platforms: x86_64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.5.0 Tested Holoscan SDK versions: 2.5.0, 2.6.0, 2.7.0, 2.8.0, 2.9.0, 3.0.0, 3.1.0 Contribution metric: Level 4 - Experimental</p>","tags":["Signal Processing"]},{"location":"applications/psd_pipeline/data_writer/#overview","title":"Overview","text":"<p>Writes binary data from its input to an output file. This operator is intened for use as a debugging aid.</p>","tags":["Signal Processing"]},{"location":"applications/psd_pipeline/data_writer/#description","title":"Description","text":"<p>The data writer operator takes in a <code>std::tuple&lt;tensor_t&lt;complex, 2&gt;, cuda_stream_t&gt;</code>, copies the data to a host tensor, then writes the data out to a binary file.</p> <p>The file path is determined based on input metadata with the following keys:</p> <ol> <li><code>channel_number</code> (default <code>0</code>)</li> <li><code>bandwidth_hz</code> (default <code>0.0</code>)</li> <li><code>rf_ref_freq_hz</code> (default <code>0.0</code>)</li> </ol> <p>With this, it creates: <code>data_writer_out_ch{channel_number}_bw{bandwidth_hz}_freq{rf_ref_freq_hz}.dat</code>.</p>","tags":["Signal Processing"]},{"location":"applications/psd_pipeline/data_writer/#requirements","title":"Requirements","text":"<ul> <li>MatX (dependency - assumed to be installed on system)</li> </ul>","tags":["Signal Processing"]},{"location":"applications/psd_pipeline/data_writer/#configuration","title":"Configuration","text":"<p>The data writer operator takes in a few parameters:</p> <pre><code>data_writer:\n  burst_size: 1280\n  num_bursts: 625\n</code></pre> <ul> <li><code>burst_size</code>: Number of samples contained in each burst</li> <li><code>num_bursts</code>: Number of bursts to process at once</li> </ul>","tags":["Signal Processing"]},{"location":"applications/psd_pipeline/data_writer/#example-usage","title":"Example Usage","text":"<p>For an example of how to use this operator, see the <code>psd_pipeline</code> application.</p> <p>Usually, you'd just want to write one burst of data to a file. To do that, you could use a <code>CountCondition</code> to limit the number of times this operator runs:</p> <pre><code>auto dataWriterOp = make_operator&lt;ops::DataWriter&gt;(\n    \"dataWriterOp\",\n    make_condition&lt;CountCondition&gt;(1));\n</code></pre>","tags":["Signal Processing"]},{"location":"applications/pva_video_filter/","title":"PVA-Accelerated Image Sharpening","text":"<p> Authors: Soham Sinha (NVIDIA), Mehmet Umut Demircin (NVIDIA), Wendell Hom (NVIDIA) Supported platforms: aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates the usage of Programmable Vision Accelerator (PVA) within a Holoscan application. It reads a video stream, applies a 2D unsharp mask filter and renders it via the visualizer. The unsharp mask filtering operation is done in PVA. Since the PVA is used for this operation, the GPU workload is minimized. This example is a demonstration of how pre-processing, post-processing, and image processing tasks can be offloaded from a GPU, allowing it to concentrate on more compute-intensive machine learning and artificial intelligence tasks.</p> <p>This example application processes a video stream, displaying two visualizer windows: one for the original stream and another for the stream enhanced with image sharpening via PVA.</p>","tags":["Computer Vision and Perception","Video","Optimization","PVA","Endoscopy","Visualization"]},{"location":"applications/pva_video_filter/#about-pva","title":"About PVA","text":"<p>PVA is a highly power-efficient VLIW processor integrated into NVIDIA Tegra platforms, specifically designed for advanced image processing and computer vision algorithms. The Compute Unified Programmable Vision Accelerator (CUPVA) SDK offers a comprehensive and unified programming model for PVA, enabling developers to create and optimize their own algorithms. For access to the SDK and further development opportunities, please contact NVIDIA.</p>","tags":["Computer Vision and Perception","Video","Optimization","PVA","Endoscopy","Visualization"]},{"location":"applications/pva_video_filter/#content","title":"Content","text":"<ul> <li><code>main.cpp</code>: This file contains a C++ Holoscan application that demonstrates the use of an operator for loading and executing a pre-compiled PVA library dedicated to performing the unsharp masking algorithm on images. CUPVA SDK and license are not required to run this Holohub application.</li> <li><code>pva_unsharp_mask/</code>: This directory houses the <code>pva_unsharp_mask.hpp</code> header file, which declares the <code>PvaUnsharpMask</code> class. The <code>PvaUnsharpMask</code> class includes an <code>init</code> API, invoked for the initial tensor, and a <code>process</code> API, used for processing input tensors. Pre-compiled algorithm library file, <code>libpva_unsharp_mask.a</code>, and the corresponding allowlist file, <code>cupva_allowlist_pva_unsharp_mask</code>, are automatically downloaded by the CMake scripts. Please note that only PVA executables with signatures included in a secure allowlist database are permitted to execute on the PVA. This ensures that only verified and trusted executables are run, enhancing the security and integrity of the system.</li> </ul>","tags":["Computer Vision and Perception","Video","Optimization","PVA","Endoscopy","Visualization"]},{"location":"applications/pva_video_filter/#algorithm-overview","title":"Algorithm Overview","text":"<p>The PVAVideoFilterExecutor operator performs an image sharpening operation in three steps:</p> <ol> <li>Convert the input RGB image to the NV24 color format.</li> <li>Apply a 5x5 unsharp mask filter on the luminance color plane.</li> <li>Convert the enhanced image back to the RGB format.</li> </ol> <p>Numerous algorithm examples leveraging the PVA can be found in the Vision Programming Interface (VPI) library. VPI enables computer vision software developers to utilize multiple compute engines simultaneously\u2014including CPU, GPU, PVA, VIC, NVENC, and OFA\u2014through a unified interface. For comprehensive details, please refer to the VPI Documentation.</p>","tags":["Computer Vision and Perception","Video","Optimization","PVA","Endoscopy","Visualization"]},{"location":"applications/pva_video_filter/#compiling-the-application","title":"Compiling the application","text":"<p>Build the application inside docker</p> <pre><code>$ ./holohub build-container pva_video_filter --base-img nvcr.io/nvidia/clara-holoscan/holoscan:v2.1.0-dgpu \n# Check which version of CUPVA is installed on your platform at /opt/nvidia\n$ ./holohub run-container pva_video_filter --no-docker-build --docker_opts \"-v /opt/nvidia/cupva-&lt;version&gt;:/opt/nvidia/cupva-&lt;version&gt; --device /dev/nvhost-ctrl-pva0:/dev/nvhost-ctrl-pva0 --device /dev/nvmap:/dev/nvmap --device /dev/dri/renderD129:/dev/dri/renderD129\"\n</code></pre> <p>Inside docker, add to your environment variable the following directories: <pre><code># inside docker\n$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/aarch64-linux-gnu/tegra/:/opt/nvidia/cupva-2.5/lib/aarch64-linux-gnu/\n</code></pre></p> <p>Build the application inside docker: <pre><code>$ ./holohub build pva_video_filter --local\n</code></pre></p>","tags":["Computer Vision and Perception","Video","Optimization","PVA","Endoscopy","Visualization"]},{"location":"applications/pva_video_filter/#running-the-application","title":"Running the application","text":"<p>The application takes an endoscopy video stream as input, applies the unsharp mask filter, and shows it in HoloViz window.</p> <p>Before running the application, deploy VPU application signature allowlist on target in your host (outside a container): <pre><code>sudo cp &lt;HOLOHUB_BUILD_DIR&gt;/applications/pva_video_filter/cpp/pva_unsharp_mask/cupva_allowlist_pva_unsharp_mask /etc/pva/allow.d/cupva_allowlist_pva_unsharp_mask\nsudo pva_allow\n</code></pre></p> <p>Run the same docker container you used to build your application</p> <pre><code>$ ./holohub run-container pva_video_filter --no-docker-build --docker_opts \"-v /opt/nvidia/cupva-&lt;version&gt;:/opt/nvidia/cupva-&lt;version&gt; --device /dev/nvhost-ctrl-pva0:/dev/nvhost-ctrl-pva0 --device /dev/nvmap:/dev/nvmap --device /dev/dri/renderD129:/dev/dri/renderD129\"\n\n# inside docker\n# don't forget the line below to export the environment variables\n$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/aarch64-linux-gnu/tegra/:/opt/nvidia/cupva-2.5/lib/aarch64-linux-gnu/\n$ ./holohub run pva_video_filter --local --no-local-build\n</code></pre> <p></p>","tags":["Computer Vision and Perception","Video","Optimization","PVA","Endoscopy","Visualization"]},{"location":"applications/qt_video_replayer/","title":"Qt Video Replayer","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p></p> <p>This application demonstrates how to integrate Holoscan with a Qt application. It support displaying the video frames output by a Holoscan operator and changing operator properties using Qt UI elements.</p> <pre><code>flowchart LR\n    subgraph Holoscan application\n        A[(VideoFile)] --&gt; VideostreamReplayerOp\n        VideostreamReplayerOp --&gt; FormatConverterOp\n        FormatConverterOp --&gt; NppFilterOp\n        NppFilterOp --&gt; QtVideoOp\n    end\n    subgraph Qt Window\n        QtVideoOp &lt;-.-&gt; QtHoloscanVideo\n    end\n</code></pre> <p>The application uses the VideostreamReplayerOp to read from a file on disk, the FormatConverterOp to convert the frames from RGB to RGBA, the NppFilterOp to apply a filter to the frame and the QtVideoOp operator to display the video stream in a Qt window.</p> <p>The QtHoloscanApp class, which extends the <code>holoscan::Application</code> class, is used to expose parameters of Holoscan operators as Qt properties.</p> <p>For example the application uses a QML Checkbox is used the set the <code>realtime</code> property of the <code>VideostreamReplayerOp</code> operator.</p> <pre><code>    CheckBox {\n        id: realtime\n        text: \"Use Video Framerate\"\n        checked: holoscanApp.realtime\n        onCheckedChanged: {\n            holoscanApp.realtime = checked;\n        }\n    }\n</code></pre> <p>The QtHoloscanVideo is a QQuickItem which can be use in the QML file. Multiple <code>QtHoloscanVideo</code> items can be placed in a Qt window.</p> <pre><code>import QtHoloscanVideo\nItem {\n    QtHoloscanVideo {\n        objectName: \"video\"\n    }\n}\n</code></pre>","tags":["Computer Vision and Perception","UI","Qt","Video","Visualization"]},{"location":"applications/qt_video_replayer/#run-instructions","title":"Run Instructions","text":"<p>Note: This application container is pinned to Holoscan SDK 3.2 since it requires CUDA Driver 550+ for running with Holoscan SDK 3.3+.</p> <p>This application requires Qt.</p> <p>For simplicity a DockerFile is available. To run this application:</p> <pre><code>./holohub run qt_video_replayer\n</code></pre>","tags":["Computer Vision and Perception","UI","Qt","Video","Visualization"]},{"location":"applications/realsense_visualizer/","title":"Intel RealSense Camera Visualizer","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 0 - Core Stable</p> <p>Visualizes frames captured from an Intel RealSense camera. </p>","tags":["Computer Vision and Perception","Visualization","Camera","Depth","Holoviz"]},{"location":"applications/realsense_visualizer/#build-and-run","title":"Build and Run","text":"<p>This application requires an Intel RealSense camera.</p> <p>At the top level of the holohub run the following command:</p> <pre><code>./holohub run realsense_visualizer\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Camera","Depth","Holoviz"]},{"location":"applications/sam2/","title":"SAM 2: Segment Anything in Images and Videos","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates how to run SAM2 models on live video feed with the possibility of changing query points in real-time.</p> <p> </p> <p>The application currently uses a single query point as a foreground point that moves on the perimeter of a circle with a configured angular speed. The models returns three masks, the best mask is selected based on the model scores. For visualization, two options exist. Select between \"logits\" or \"masks\". - \"logits\": predictions of the network, mapped onto a colorscale that matches matplotlib.pyplot's \"viridis\" - \"masks\": binarized predictions</p> <p>SAM2, recently announced by Meta, is the next iteration of the Segment Anything Model (SAM). This new version expands upon its predecessor by adding the capability to segment both videos and images. This sample application wraps the ImageInference class, and applies it on a live video feed.</p> <p>Note: This demo currently uses \"sam2_hiera_l.yaml\", but any of the sam2 models work. You only need to adjust segment_one_thing.yaml.</p>","tags":["Computer Vision and Perception","Video","Segmentation","Visualization","Camera"]},{"location":"applications/sam2/#setup-instructions","title":"\u2699\ufe0f Setup Instructions","text":"<p>The app defaults to using the video device at <code>/dev/video0</code></p> <p>To debug if this is the correct device download <code>v4l2-ctl</code>: <pre><code>sudo apt-get install v4l-utils\n</code></pre> To check for your devices run: <pre><code>v4l2-ctl --list-devices\n</code></pre> This command will output something similar to this: <pre><code>NVIDIA Tegra Video Input Device (platform:tegra-camrtc-ca):\n        /dev/media0\n\nvi-output, lt6911uxc 2-0056 (platform:tegra-capture-vi:0):\n        /dev/video0\n\nDummy video device (0x0000) (platform:v4l2loopback-000):\n        /dev/video3\n</code></pre> Determine your desired video device and edit the source device in segment_one_thing.yaml</p>","tags":["Computer Vision and Perception","Video","Segmentation","Visualization","Camera"]},{"location":"applications/sam2/#build-and-run-instructions","title":"\ud83d\ude80 Build and Run Instructions","text":"","tags":["Computer Vision and Perception","Video","Segmentation","Visualization","Camera"]},{"location":"applications/sam2/#arm64-and-x86","title":"ARM64 and x86","text":"<p>OBS: If you are building on a Clara AGX Dev Kit, replace the <code>Dockerfile</code> below with <code>./alternative_docker/Dockerfile_cagx</code>.</p> <p>This application uses a custom Dockerfile based on a pytorch container. Build and run the application using <pre><code> ./holohub run sam2\n</code></pre> Or first build the container, then launch it and run.</p> <p><pre><code> ./holohub build-container sam2\n</code></pre> <pre><code>./holohub run-container sam2 --no-docker-build\n</code></pre> <pre><code>./holohub run sam2 --local --no-local-build\n</code></pre></p>","tags":["Computer Vision and Perception","Video","Segmentation","Visualization","Camera"]},{"location":"applications/sam2/#x86-only","title":"x86 only","text":"<p>If you are only using an x86 system, you may use a Dockerfile based on the Holoscan container. Replace the Dockerfile with this alternative Dockerfile. Then, from the Holohub main directory run the following command: <pre><code>./holohub run sam2\n</code></pre></p> <p>Alternatively build and run: <pre><code>./holohub vscode sam2\n</code></pre> Run the application in debug mode from vscode, or execute it by <pre><code>python applications/sam2/segment_one_thing.py\n</code></pre></p> <p>You can choose to output \"logits\" or \"masks\" in the configuration of the postprocessor and holoviz operator segment_one_thing.yaml</p>","tags":["Computer Vision and Perception","Video","Segmentation","Visualization","Camera"]},{"location":"applications/sam2/#supported-hardware","title":"\ud83d\udcbb Supported Hardware","text":"<ul> <li>x86 w/ dGPU</li> <li>IGX Dev Kit w/ dGPU</li> <li>Clara AGX Dev Kit w/ dGPU</li> </ul>","tags":["Computer Vision and Perception","Video","Segmentation","Visualization","Camera"]},{"location":"applications/sam2/#acknowledgements","title":"\ud83d\ude4c Acknowledgements","text":"<ul> <li>Meta, SAM2: for providing these models and inference infrastructure</li> </ul>","tags":["Computer Vision and Perception","Video","Segmentation","Visualization","Camera"]},{"location":"applications/sdr_fm_demodulation/","title":"Software Defined Radio FM Demodulation","text":"<p> Authors: Adam Thompson (NVIDIA) Supported platforms: x86_64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.4.0 Tested Holoscan SDK versions: 0.4.0 Contribution metric: Level 2 - Trusted</p> <p>As the \"Hello World\" application of software defined radio developers, this demonstration highlights real-time FM demodulation, resampling, and playback on GPU with NVIDIA's Holoscan SDK. In this example, we are using an inexpensive USB-based RTL-SDR dongle to feed complex valued Radio Frequency (RF) samples into GPU memory and use cuSignal functions to perform the relevant signal processing. The main objectives of this demonstration are to: - Highlight developer productivity in building an end-to-end streaming application with Holoscan and existing GPU-Accelerated Python libraries - Demonstrate how to construct and connect isolated units of work via Holoscan operators - Emphasize that operators created for this application can be re-used in other ones doing similar tasks</p>","tags":["Signal Processing","Audio"]},{"location":"applications/sdr_fm_demodulation/#running-the-application","title":"Running the Application","text":"<p>Prior to running the application, the user needs to install the necessary dependencies (and, of course, plug in a SDR into your computer). This is most easily done in an Anaconda environment.</p> <pre><code>conda create --name holoscan-sdr-demo python=3.8\nconda activate holoscan-sdr-demo\nconda install -c conda-forge -c rapidsai -c nvidia cusignal soapysdr soapysdr-module-rtlsdr pyaudio\npip install holoscan\n</code></pre> <p>The FM demodulation example can then be run via <pre><code>python applications/sdr_fm_demodulation/sdr_fm_demodulation.py\n</code></pre></p>","tags":["Signal Processing","Audio"]},{"location":"applications/simple_pdw_pipeline/","title":"Basic Pulse Description Word (PDW) Generator","text":"<p> Authors: Joshua Anderson (GTRI), Christopher Jones (GTRI) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 4 - Experimental</p> <p>This is a Holoscan pipeline that shows the possibility of using Holoscan as a Pulse Description Word (PDW) generator. This is a process that takes in IQ samples (signals represented using time-series complex numbers) and picks out peaks in the signal that may be transmissions from another source. These PDW processors are used to see what is transmitting in your area, be they radio towers or radars.</p> <p>siggen.c a signal generator written in C that will transmit the input to this pipeline.</p>","tags":["Signal Processing","UDP"]},{"location":"applications/simple_pdw_pipeline/#basicnetworkoprx","title":"BasicNetworkOpRx","text":"<p>This uses the Basic Network Operator to read udp packets this operator is documented elsewhere.</p>","tags":["Signal Processing","UDP"]},{"location":"applications/simple_pdw_pipeline/#packettotensorop","title":"PacketToTensorOp","text":"<p>This converts the bytes from the Basic Network Operator into the packets used in the rest of the pipeline. The format of the incoming packets is a 16-bit id followed by 8192 IQ samples each sample has the following format: 16 bits (I) 16 bits (Q)</p>","tags":["Signal Processing","UDP"]},{"location":"applications/simple_pdw_pipeline/#fftop","title":"FFTOp","text":"<p>Does what it says on the tin. Takes an FFT of the input data. Also shifts data so that 0 Hz is centered.</p>","tags":["Signal Processing","UDP"]},{"location":"applications/simple_pdw_pipeline/#thresholdingop","title":"ThresholdingOp","text":"<p>Detects samples over a threshold and then packetizes the runs of samples that are above the threshold as a \"pulse\".</p>","tags":["Signal Processing","UDP"]},{"location":"applications/simple_pdw_pipeline/#pulsedescriptiorop","title":"PulseDescriptiorOp","text":"<p>Takes simple statistics of input pulses. This is where I am most excited for future work, but that is not the point of this particular project.</p>","tags":["Signal Processing","UDP"]},{"location":"applications/simple_pdw_pipeline/#pulseprinterop","title":"PulsePrinterOp","text":"<p>Prints the pulse to screen. Also optionally sends packets to a BasicNetworkOpTx. The transmitted network packets have the following format: Each of the following fields are 16bit unsigned integers   id   low bin   high bin   zero bin   sum power   max amplitude   average amplitude</p>","tags":["Signal Processing","UDP"]},{"location":"applications/simple_radar_pipeline/cpp/","title":"Simple Radar Pipeline","text":"<p> Authors: Cliff Burdick (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.1 Minimum Holoscan SDK version: 2.6.0 Tested Holoscan SDK versions: 2.6.0 Contribution metric: Level 2 - Trusted</p> <p>This demonstration walks the developer through building a simple radar signal processing pipeline, targeted towards detecting objects, with Holoscan. In this example, we generate random radar and waveform data, passing both through: 1. Pulse Compression 2. Moving Target Indication (MTI) Filtering 3. Range-Doppler Map 4. Constant False Alarm Rate (CFAR) Analysis</p> <p>While this example generates 'offline' complex-valued data, it could be extended to accept streaming data from a phased array system or simulation via modification of the <code>SignalGeneratorOperator</code>.</p> <p>The output of this demonstration is a measure of the number of pulses per second processed on GPU.</p> <p>The main objectives of this demonstration are to: - Highlight developer productivity in building an end-to-end streaming application with Holoscan and existing GPU-Accelerated Python libraries - Demonstrate how to construct and connect isolated units of work via Holoscan operators, particularly with handling multiple inputs and outputs into an Operator - Emphasize that operators created for this application can be re-used in other ones doing similar tasks</p>","tags":["Signal Processing","Detection"]},{"location":"applications/simple_radar_pipeline/cpp/#building-the-application","title":"Building the application","text":"<p>Make sure CMake (https://www.cmake.org) is installed on your system (minimum version 3.20)</p> <ul> <li> <p>Holoscan Debian Package - Follow the instructions in the link to install the latest version of Holoscan Debian package from NGC.</p> </li> <li> <p>Create a build directory:   <pre><code>mkdir -p &lt;build_dir&gt; &amp;&amp; cd &lt;build_dir&gt;\n</code></pre></p> </li> <li>Configure with CMake:</li> </ul> <p>Make sure CMake can find your installation of the Holoscan SDK. For example, setting <code>holoscan_ROOT</code> to its install directory during configuration:</p> <pre><code>cmake -S &lt;source_dir&gt; -B &lt;build_dir&gt; -DAPP_simple_radar_pipeline=1 \n</code></pre> <p>Notes: If the error <code>No CMAKE_CUDA_COMPILER could be found</code> is encountered, make sure that the :code:<code>nvcc</code> executable can be found by adding the CUDA runtime location to your <code>PATH</code> variable:</p> <pre><code>export PATH=$PATH:/usr/local/cuda/bin\n</code></pre> <ul> <li>Build:</li> </ul> <pre><code>cmake --build &lt;build_dir&gt;\n</code></pre>","tags":["Signal Processing","Detection"]},{"location":"applications/simple_radar_pipeline/cpp/#running-the-application","title":"Running the application","text":"<pre><code>&lt;build_dir&gt;/simple_radar_pipeline\n</code></pre>","tags":["Signal Processing","Detection"]},{"location":"applications/simple_radar_pipeline/python/","title":"Simple Radar Pipeline","text":"<p> Authors: Cliff Burdick (NVIDIA) Supported platforms: x86_64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.4.0 Tested Holoscan SDK versions: 0.4.0 Contribution metric: Level 2 - Trusted</p> <p>This demonstration walks the developer through building a simple radar signal processing pipeline, targeted towards detecting objects, with Holoscan. In this example, we generate random radar and waveform data, passing both through: 1. Pulse Compression 2. Moving Target Indication (MTI) Filtering 3. Range-Doppler Map 4. Constant False Alarm Rate (CFAR) Analysis</p> <p>While this example generates 'offline' complex-valued data, it could be extended to accept streaming data from a phased array system or simulation via modification of the <code>SignalGeneratorOperator</code>.</p> <p>The output of this demonstration is a measure of the number of pulses per second processed on GPU.</p> <p>The main objectives of this demonstration are to: - Highlight developer productivity in building an end-to-end streaming application with Holoscan and existing GPU-Accelerated Python libraries - Demonstrate how to construct and connect isolated units of work via Holoscan operators, particularly with handling multiple inputs and outputs into an Operator - Emphasize that operators created for this application can be re-used in other ones doing similar tasks</p>","tags":["Signal Processing","Detection"]},{"location":"applications/simple_radar_pipeline/python/#running-the-application","title":"Running the Application","text":"<p>Prior to running the application, the user needs to install the necessary dependencies. This is most easily done in an Anaconda environment.</p> <pre><code>conda create --name holoscan-sdr-demo python=3.8\nconda activate holoscan-sdr-demo\nconda install -c conda-forge -c rapidsai -c nvidia cusignal\npip install holoscan\n</code></pre> <p>The simple radar signal processing pipeline example can then be run via <pre><code>python applications/simple_radar_pipeline/simple_radar_pipeline.py\n</code></pre></p>","tags":["Signal Processing","Detection"]},{"location":"applications/speech_to_text_llm/","title":"Speech-to-text + Large Language Model","text":"<p> Authors: Sean Huver (NVIDIA) Supported platforms: x86_64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 2 - Trusted</p> <p>This application transcribes an audio file using a speech-to-text model (STT), then uses a large language model (LLM) to summarize and generate new relevant information.</p> <p>While this workflow in principle could be used for a number of domains, here we provide a healthcare specific example. A <code>sample.wav</code> file is provided which is an example of a radiology interpretation. An OpenAI whisper model is used to transform the audio into text, then an API call is made to either the GPT3.5-turbo or GPT4 LLM.</p>","tags":["Healthcare AI","Audio","ASR","LLM","Auth and API","Medical Imaging"]},{"location":"applications/speech_to_text_llm/#yaml-configuration","title":"YAML Configuration","text":"<p>The input (either audio or video file), specific Whisper model (tiny, small, medium, or large), LLM model, and directions for the LLM are all determined by the <code>stt_to_nlp.yaml</code> file. As you see from our example, the directions for the LLM are made via natural language, and can result in very different applications.</p> <p>For our purposes we specify the directions as:</p> <pre><code>  context: 'Make summary of the transcript (and correct any transcription errors in CAPS).\\n Create a Patient Summary with no medical jargon. \\n \n  Create a full radiological report write-up. \\n Give likely ICD-10 Codes \\n Suggested follow-up steps.'\n</code></pre> <p>This results in the following output from the LLM:</p> <pre><code>LLM Response: \n Summary of Transcript:\nThe patient has full thickness wear on the dorsal half of the second metatarsal head with reactive bone marrow edema and capsulitis. There is also second web space bursitis and a third web space neuroma. The 51-year-old male has multiple gallbladder polyps, with the largest measuring 1.9 x 2 cm, 1.7 x 1.7 cm in the mid portion, and 1.6 x 1.6 cm distally. Other smaller polyps are also present.\n\nPatient Summary (No Medical Jargon):\nThe patient has damage and inflammation in the foot, specifically in the second toe joint and surrounding areas. They also have multiple growths in their gallbladder, with the largest being about the size of a grape. The patient is a 51-year-old male with a family history of abdominal aortic aneurysm.\n\nFull Radiological Report Write-up:\nPatient: 51-year-old male\nFamily History: Abdominal aortic aneurysm\n\nFindings:\n1. Foot: Full thickness wear over the dorsal half of the second metatarsal head with reactive subchondral bone marrow edema and capsulitis. Second web space intermetatarsal bursitis and a third web space neuroma are also present.\n2. Gallbladder: Multiple gallbladder polyps are observed. The largest polyp measures 1.9 x 2 cm, with additional polyps measuring 1.7 x 1.7 cm in the mid portion and 1.6 x 1.6 cm distally. Two smaller polyps measure 0.5 x 0.4 x 0.4 cm and 0.5 x 0.3 x 0.5 cm.\n\nLikely ICD-10 Codes:\n1. M25.572 - Capsulitis, left ankle and foot\n2. M79.671 - Bursitis, right ankle and foot\n3. G57.60 - Lesion of plantar nerve, unspecified lower limb\n4. K82.8 - Other specified diseases of the gallbladder (gallbladder polyps)\n\nSuggested Follow-up Steps:\n1. For the foot issues, the patient may benefit from a consultation with a podiatrist or orthopedic specialist to discuss treatment options, which may include physical therapy, orthotics, or surgery.\n2. For the gallbladder polyps, the patient should consult with a gastroenterologist to determine the need for further evaluation, monitoring, or possible surgical intervention. Regular ultrasound examinations may be recommended to monitor the growth of the polyps.\n</code></pre>","tags":["Healthcare AI","Audio","ASR","LLM","Auth and API","Medical Imaging"]},{"location":"applications/speech_to_text_llm/#run-instructions","title":"Run Instructions","text":"<p>Note: To run this application you will need to create an OpenAI account and obtain your own API key with active credits.</p> <p>You should refer to the glossary for the terms defining specific locations within HoloHub.</p> <ul> <li>(Optional) Create and use a virtual environment:</li> </ul> <pre><code>python3 -m venv .venv\nsource .venv/bin/activate\n</code></pre> <ul> <li>Install the python packages</li> </ul> <pre><code>pip install -r applications/speech_to_text_llm/requirements.txt\n</code></pre> <ul> <li>Run the application</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOSCAN_INSTALL_DIR&gt;/python/lib:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\ncd applications/speech_to_text_llm \npython3 stt_to_nlp.py\n</code></pre>","tags":["Healthcare AI","Audio","ASR","LLM","Auth and API","Medical Imaging"]},{"location":"applications/speech_to_text_llm/#sample-audio-file","title":"Sample Audio File","text":"<p>Please note the sample audio file included is licensed as CC-BY-4.0 International, copyright NVIDIA 2023.</p>","tags":["Healthcare AI","Audio","ASR","LLM","Auth and API","Medical Imaging"]},{"location":"applications/ssd_detection_endoscopy_tools/","title":"SSD Detection for Endoscopy Tools","text":"<p> Authors: Jin Li (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.1 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 2 - Trusted</p>","tags":["Healthcare AI","Video","SSD","Detection","Visualization"]},{"location":"applications/ssd_detection_endoscopy_tools/#model","title":"Model","text":"<p>We can train the SSD model from NVIDIA DeepLearningExamples repo with any data of our choosing. Here for the purpose of demonstrating the deployment process, we will use a SSD model checkpoint that is only trained for the demo video clip.</p> <p>Please download the models at this NGC Resource for <code>epoch_24.pt</code>, <code>epoch24_nms.onnx</code> and <code>epoch24.onnx</code>. You can go through the next steps of Model Conversion to ONNX to convert <code>epoch_24.pt</code> into <code>epoch24_nms.onnx</code> and <code>epoch24.onnx</code>, or use the downloaded ONNX models directly.</p>","tags":["Healthcare AI","Video","SSD","Detection","Visualization"]},{"location":"applications/ssd_detection_endoscopy_tools/#model-conversion-to-onnx","title":"Model Conversion to ONNX","text":"<p>The scripts we need to export the model from .pt checkpoint to the ONNX format are all within this dir <code>./scripts</code>. It is a two step process.</p> <p>Step 1: Export the trained checkpoint to ONNX.  We use <code>export_to_onnx_ssd.py</code> if we want to use the model as is without NMS, or <code>export_to_onnx_ssd_nms.py</code> to prepare the model with NMS.  Let's assume the re-trained SSD model checkpoint from the repo is saved as <code>epoch_24.pt</code>.  The export process is <pre><code># For exporting the original ONNX model\n python export_to_onnx_ssd.py --model epoch_24.pt  --outpath epoch24_temp.onnx\n</code></pre> <pre><code># For preparing to add the NMS step to ONNX model\npython export_to_onnx_ssd_nms.py --model epoch_24.pt  --outpath epoch24_nms_temp.onnx\n</code></pre> Step 2: modify input shape.  Step 1 produces a onnx model with input shape <code>[1, 3, 300, 300]</code>, but we will want to modify the input node to have shape <code>[1, 300, 300, 3]</code> or in general <code>[batch_size, height, width, channels]</code> for compatibility and easy of deployment in the Holoscan SDK. If we want to incorporate the NMS operation in the the ONNX model, we could add a <code>EfficientNMS_TRT</code> op, which is documented in <code>graph_surgeon_ssd.py</code>'s nms related block. <pre><code># For exporting the original ONNX model\npython graph_surgeon_ssd.py --orig_model epoch24_temp.onnx --new_model epoch24.onnx\n</code></pre> <pre><code># For adding the NMS step to ONNX model, use --nms\npython graph_surgeon_ssd.py --orig_model epoch24_nms_temp.onnx --new_model epoch24_nms.onnx --nms\n</code></pre></p> <p>Note that  - <code>epoch24.onnx</code> is used in <code>ssd_step1.py</code> and <code>ssd_step2_route1.py</code>  - <code>epoch24_nms.onnx</code> is used in <code>ssd_step2_route2.py</code> and <code>ssd_step2_route2_render_labels.py</code></p>","tags":["Healthcare AI","Video","SSD","Detection","Visualization"]},{"location":"applications/ssd_detection_endoscopy_tools/#data","title":"Data","text":"<p>For this application we will use the same Endoscopy Sample Data as the Holoscan SDK reference applications.</p>","tags":["Healthcare AI","Video","SSD","Detection","Visualization"]},{"location":"applications/ssd_detection_endoscopy_tools/#requirements","title":"Requirements","text":"<p>There are two requirements 1. To run <code>ssd_step1.py</code> and <code>ssd_step2_route1.py</code> with the original exported model, we need the installation of PyTorch and CuPy.  To run <code>ssd_step2_route2.py</code> and <code>ssd_step2_route2_render_labels.py</code> with the exported model with additional NMS layer in ONNX, we need the installation of CuPy.  If you're using the dGPU on the devkit, since there are no prebuilt PyTorch wheels for aarch64 dGPU, the simplest way is to modify the Dockerfile and build from source; if you're on x86 or using the iGPU on the devkit, there should be existing prebuilt PyTorch wheels.  If you choose to build the SDK from source, you can find the modified Dockerfile here to replace the SDK repo Dockerfile to satisfy the installation requirements.  The main changes in Dockerfile for dGPU: the base image changed to <code>nvcr.io/nvidia/pytorch:22.03-py3</code> instead of the <code>nvcr.io/nvidia/tensorrt:22.03-py3</code> as dGPU's base image; adding the installation of NVTX for optional profiling. Build the SDK container following the README instructions.   Make sure the directory containing this application and the directory containing the NGC data and models are mounted in the container. Add the <code>-v</code> mount options to the <code>docker run</code> command launched by <code>./holohub run-container</code> in the SDK repo.</p> <ol> <li>Make sure the model and data are accessible by the application.  Make sure the yaml files <code>ssd_endo_model.yaml</code> and <code>ssd_endo_model_with_NMS.yaml</code> are pointing to the right locations for the ONNX model and data. The assumption in the yaml file is that the <code>epoch24_nms.onnx</code> and <code>epoch24.onnx</code> are located at: <pre><code>model_file_path: /byom/models/endo_ssd/epoch24_nms.onnx\nengine_cache_dir: /byom/models/endo_ssd/epoch24_nms_engines\n</code></pre> and / or <pre><code>model_file_path: /byom/models/endo_ssd/epoch24.onnx\nengine_cache_dir: /byom/models/endo_ssd/epoch24_engines\n</code></pre> The Endoscopy Sample Data is assumed to be at <pre><code>/workspace/holoscan-sdk/data/endoscopy\n</code></pre> Please check and modify the paths to model and data in the yaml file if needed.</li> </ol>","tags":["Healthcare AI","Video","SSD","Detection","Visualization"]},{"location":"applications/ssd_detection_endoscopy_tools/#building-the-application","title":"Building the application","text":"<p>Please refer to the README under ./app_dev_process to see the process of building the applications.</p>","tags":["Healthcare AI","Video","SSD","Detection","Visualization"]},{"location":"applications/ssd_detection_endoscopy_tools/#running-the-application","title":"Running the application","text":"<p>Run the incrementally improved Python applications by: <pre><code>python ssd_step1.py\n\npython ssd_step2_route1.py\n\npython ssd_step2_route2.py\n\npython ssd_step2_route2_render_labels.py --labelfile endo_ref_data_labels.csv\n</code></pre></p>","tags":["Healthcare AI","Video","SSD","Detection","Visualization"]},{"location":"applications/stereo_vision/","title":"Stereo Vision","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.4.0 Tested Holoscan SDK versions: 2.4.0 Contribution metric: Level 1 - Highly Reliable</p> <p> </p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Depth","Camera","Visualization","Image Processing"]},{"location":"applications/stereo_vision/#overview","title":"Overview","text":"<p>A demo pipeline showcasing stereo disparity estimation.</p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Depth","Camera","Visualization","Image Processing"]},{"location":"applications/stereo_vision/#description","title":"Description","text":"<p>This pipeline takes video from a stereo camera and estimates disparity using DNN ESS. The disparity map is displayed through Holoviz.</p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Depth","Camera","Visualization","Image Processing"]},{"location":"applications/stereo_vision/#requirements","title":"Requirements","text":"<p>This application requires a V4L2 stereo camera or recorded stereo video as input. A video acquired from a StereoLabs ZED camera is downloaded when running the <code>get_data_and_models.sh</code> script when building the application. A script for obtaining the calibration for StereoLabs cameras is also provided. Holoscan SDK &gt;=2.0,&lt;=2.5 is required for TensorRT 8.6 compatibility.</p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Depth","Camera","Visualization","Image Processing"]},{"location":"applications/stereo_vision/#camera-calibration","title":"Camera Calibration","text":"<p>The default calibration will work for the sample video. If using a stereolabs camera the calibration can be retrieved using <code>get_zed_calibration.py</code> and the devices serial number.</p> <pre><code>python3 get_zed_calibration.py -s [Serial Number]\n</code></pre>","tags":["Computer Vision and Perception","Video","Stereo Vision","Depth","Camera","Visualization","Image Processing"]},{"location":"applications/stereo_vision/#input-video","title":"Input video","text":"<p>For the input video stream, either use a v4l2 stereo camera such as those produced by stereolabs or included recorded video. The <code>stereo-plants.mp4</code> video is provided here and will be downloaded and converted to the necessary format when building the application.</p> <p>The source device in <code>stereo_vision.yaml</code> should be modified to match the device the v4l2 video is using. This can be found using <code>v4l2-ctl --list-devices</code>.</p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Depth","Camera","Visualization","Image Processing"]},{"location":"applications/stereo_vision/#models","title":"Models","text":"<p>This demo requires the ESS DNN Stereo Disparity available from the NGC catalog for disparity estimation. This model is downloaded when you build the application.</p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Depth","Camera","Visualization","Image Processing"]},{"location":"applications/stereo_vision/#ess-dnn","title":"ESS DNN","text":"<p>The ESS engine files generated in this demo application is specific to TRT8.6; make sure you build the devcontainer with a compatible <code>base_img</code> as shown in the Build and Run Instructions section.</p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Depth","Camera","Visualization","Image Processing"]},{"location":"applications/stereo_vision/#build-and-run-instructions","title":"Build and Run Instructions","text":"<p>Run the following command to build and run application using the recorded video: <pre><code>./holohub run stereo_vision --base_img nvcr.io/nvidia/clara-holoscan/holoscan:v2.4.0-dgpu\n</code></pre></p> <p>To run the application using a v4l2 compatible stereo camera, run: <pre><code>./holohub run stereo_vision --base_img nvcr.io/nvidia/clara-holoscan/holoscan:v2.4.0-dgpu --run-args=\"--source v4l2\"\n</code></pre></p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Depth","Camera","Visualization","Image Processing"]},{"location":"applications/synthetic_aperture_radar/","title":"Streaming Synthetic Aperture Radar","text":"<p> Authors: Dan Campbell (NVIDIA), Amanda Butler (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 4 - Experimental</p>","tags":["Signal Processing","Imaging","Backprojection","Synthetic Aperture Beamforming","Holoviz"]},{"location":"applications/synthetic_aperture_radar/#description","title":"Description","text":"<p>This application is a demonstration of using Holoscan to construct Synthetic Aperture Radar (SAR) imagery from a data collection.  In current form, the data is assumed to be precollected and contained in a particular binary format.  It has been tested with 2 versions of the publicly available GOTCHA volumetric SAR data collection.  Python-based converters are included to manipulate the public datasets into the binary format expected by the application.  The application implements Backprojection for image formation.</p>","tags":["Signal Processing","Imaging","Backprojection","Synthetic Aperture Beamforming","Holoviz"]},{"location":"applications/synthetic_aperture_radar/#requirements","title":"Requirements","text":"<ul> <li>Holoscan (&gt;=0.5)</li> <li>Python implementation:<ul> <li>Python3</li> <li>CuPy or Numpy</li> <li>Pillow</li> </ul> </li> <li>Scripts in <code>deploy/</code> will build and execute a docker environment that meets the requirements for systems using nvidia-docker</li> </ul>","tags":["Signal Processing","Imaging","Backprojection","Synthetic Aperture Beamforming","Holoviz"]},{"location":"applications/synthetic_aperture_radar/#obtain-and-format-gotcha-dataset","title":"Obtain and Format GOTCHA Dataset","text":"<ul> <li>Navigate to https://www.sdms.afrl.af.mil/index.php?collection=gotcha </li> <li>Click the DOWNLOAD link below the images</li> <li>Log in.  You may need to create an account to do so</li> <li>Under \"GOTCHA Volumetric SAR Data Set Challenge Problem\" download \"Disc 1 of 2\".<ul> <li>The data in \"Disc 2 of 2\" is compatible with this demo but not used</li> </ul> </li> <li>Unpack the contents of \"Disc 1 of 2\" into the <code>data/</code> directory.  This should create a subdirectry named <code>GOTCHA-CP_Disc1/</code></li> <li><code>cd data</code></li> <li><code>python3 cp-large_convert.py</code></li> <li>This should create a data file named <code>gotcha-cp-td-os.dat</code> that has a file size 2766987672 bytes, and a md5sum of 554b509c2d5c2c3de8e5643983a9748d</li> </ul>","tags":["Signal Processing","Imaging","Backprojection","Synthetic Aperture Beamforming","Holoviz"]},{"location":"applications/synthetic_aperture_radar/#build-and-use-docker-container-optional","title":"Build and Use Docker Container (Optional)","text":"<ul> <li>This demonstration is distributed with tools to build a docker container that meets the demonstration's system requirements.  This approach will only work properly with nvidia-docker</li> <li>From the demonstration root directory:</li> <li><code>cd deploy</code></li> <li><code>bash build_application_container.sh</code> - this will build the container</li> <li><code>bash run_application_container.sh</code> - this will launch a container that meets the demonstration system requirements</li> </ul>","tags":["Signal Processing","Imaging","Backprojection","Synthetic Aperture Beamforming","Holoviz"]},{"location":"applications/synthetic_aperture_radar/#build-and-execute","title":"Build and Execute","text":"<ul> <li>Python: <ul> <li><code>python3 holosar.py</code></li> </ul> </li> </ul> <p>The application will create a window with the resolved SAR image, and update after each group of 100 pulses received.  The image represents the strength of reflectivity at points on the ground within the imaging window.  The text at the top of the window indicates the (X,Y) position of the collecting radar at the most recent pulse, along with the total count of pulses received.  The red line points in the direction of the collection vehicle's location at the most recent pulse.  </p> <p>A screen grab is included below for reference:</p> <p></p>","tags":["Signal Processing","Imaging","Backprojection","Synthetic Aperture Beamforming","Holoviz"]},{"location":"applications/tao_peoplenet/","title":"TAO PeopleNet Detection Model on V4L2 Video Stream","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0, 1.0.3, 2.0.0, 2.1.0 Contribution metric: Level 2 - Trusted</p> <p>Use the TAO PeopleNet available on NGC to detect faces and people in a V4L2 supported video stream. HoloViz is used to draw bounding boxes around the detections.</p>","tags":["Computer Vision and Perception","Video","Detection","Holoviz"]},{"location":"applications/tao_peoplenet/#model","title":"Model","text":"<p>This application uses the TAO PeopleNet model from NGC for face and person classification. The model is downloaded when building the application.</p>","tags":["Computer Vision and Perception","Video","Detection","Holoviz"]},{"location":"applications/tao_peoplenet/#data","title":"Data","text":"<p>This application downloads a pre-recorded video from Pexels when the application is built for use with this application.  Please review the license terms from Pexels.</p> <p>NOTE: The user is responsible for checking if the dataset license is fit for the intended purpose.</p>","tags":["Computer Vision and Perception","Video","Detection","Holoviz"]},{"location":"applications/tao_peoplenet/#input","title":"Input","text":"<p>This app supports two different input options.  If you have a v4l2 compatible device plugged into your machine such as a webcam, you can run this application with option 1.  Otherwise you can run this application using a pre-recorded video with option 2.</p> <ol> <li>v4l2 compatible input device (default, see V4L2 Support below)</li> <li>pre-recorded video (see Video Replayer Support below)</li> </ol> <p>To see the list of v4l2 devices connected to your machine, install <code>v4l-utils</code> if it's not already installed:</p> <pre><code>sudo apt-get install v4l-utils\n</code></pre> <p>Then run:</p> <pre><code>v4l2-ctl --list-devices\n</code></pre>","tags":["Computer Vision and Perception","Video","Detection","Holoviz"]},{"location":"applications/tao_peoplenet/#run-instructions","title":"Run Instructions","text":"","tags":["Computer Vision and Perception","Video","Detection","Holoviz"]},{"location":"applications/tao_peoplenet/#v4l2-support","title":"V4L2 Support","text":"<p>This application supports v4l2 compatible devices as input.  To run this application with your v4l2 compatible device, please plug in your input device and run: <pre><code>./holohub run tao_peoplenet\n</code></pre></p> <p>By default, this application expects the input device to be mounted at <code>/dev/video0</code>.  If this is not the case, please update <code>applications/tao_peoplenet/tao_peoplenet.yaml</code> and set it to use the corresponding input device before running the application.  You can also override the default input device on the command line by running: <pre><code>./holohub run tao_peoplenet --run-args=\"--video_device /dev/video0\"\n</code></pre></p>","tags":["Computer Vision and Perception","Video","Detection","Holoviz"]},{"location":"applications/tao_peoplenet/#video-replayer-support","title":"Video Replayer Support","text":"<p>If you don't have a v4l2 compatible device plugged in, you may also run this application on a pre-recorded video. To launch the application using the Video Stream Replayer as the input source, run:</p> <pre><code>./holohub run tao_peoplenet --run-args=\"--source replayer\"\n</code></pre>","tags":["Computer Vision and Perception","Video","Detection","Holoviz"]},{"location":"applications/tao_peoplenet/#known-issues","title":"Known Issues","text":"<p>There is a known issue running this application on IGX w/ iGPU and on Jetson AGX (see #500). The workaround is to update the device to avoid picking up the libnvv4l2.so library.</p> <pre><code>cd /usr/lib/aarch64-linux-gnu/\nls -l libv4l2.so.0.0.999999\nsudo rm libv4l2.so.0.0.999999\nsudo ln -s libv4l2.so.0.0.0.0  libv4l2.so.0.0.999999\n</code></pre>","tags":["Computer Vision and Perception","Video","Detection","Holoviz"]},{"location":"applications/ultrasound_segmentation/cpp/","title":"Ultrasound Bone Scoliosis Segmentation","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>Full workflow including a generic visualization of segmentation results from a spinal scoliosis segmentation model of ultrasound videos. The model used is stateless, so this workflow could be configured to adapt to any vanilla DNN model. </p>","tags":["Healthcare AI","Video","Ultrasound","Segmentation","AJA","Visualization"]},{"location":"applications/ultrasound_segmentation/cpp/#requirements","title":"Requirements","text":"<p>The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the ultrasound data (replayer). Follow the setup instructions from the user guide to use the AJA capture card.</p>","tags":["Healthcare AI","Video","Ultrasound","Segmentation","AJA","Visualization"]},{"location":"applications/ultrasound_segmentation/cpp/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Bone Scoliosis Segmentation</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Healthcare AI","Video","Ultrasound","Segmentation","AJA","Visualization"]},{"location":"applications/ultrasound_segmentation/cpp/#build-instructions","title":"Build Instructions","text":"<p>Please refer to the top level Holohub README.md file for information on how to build this application.</p>","tags":["Healthcare AI","Video","Ultrasound","Segmentation","AJA","Visualization"]},{"location":"applications/ultrasound_segmentation/cpp/#run-instructions","title":"Run Instructions","text":"<p>In your <code>build</code> directory, run the commands of your choice:</p> <ul> <li> <p>Using a pre-recorded video     <pre><code>sed -i -e 's#^source:.*#source: replayer#' applications/ultrasound_segmentation/cpp/ultrasound_segmentation.yaml\napplications/ultrasound_segmentation/cpp/ultrasound_segmentation --data &lt;data_dir&gt;/ultrasound_segmentation\n</code></pre></p> </li> <li> <p>Using an AJA card     <pre><code>sed -i -e 's#^source:.*#source: aja#' applications/ultrasound_segmentation/cpp/ultrasound_segmentation.yaml\napplications/ultrasound_segmentation/cpp/ultrasound_segmentation\n</code></pre></p> </li> </ul>","tags":["Healthcare AI","Video","Ultrasound","Segmentation","AJA","Visualization"]},{"location":"applications/ultrasound_segmentation/python/","title":"Ultrasound Bone Scoliosis Segmentation","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>Full workflow including a generic visualization of segmentation results from a spinal scoliosis segmentation model of ultrasound videos. The model used is stateless, so this workflow could be configured to adapt to any vanilla DNN model. </p>","tags":["Healthcare AI","Video","Ultrasound","Segmentation","AJA","Visualization"]},{"location":"applications/ultrasound_segmentation/python/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8+</li> <li>The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the ultrasound data (replayer). Follow the setup instructions from the user guide to use the AJA capture card.</li> </ul>","tags":["Healthcare AI","Video","Ultrasound","Segmentation","AJA","Visualization"]},{"location":"applications/ultrasound_segmentation/python/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Bone Scoliosis Segmentation</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Healthcare AI","Video","Ultrasound","Segmentation","AJA","Visualization"]},{"location":"applications/ultrasound_segmentation/python/#run-instructions","title":"Run Instructions","text":"<p>To run this application, you'll need to configure your PYTHONPATH environment variable to locate the necessary python libraries based on your Holoscan SDK installation type.</p> <p>You should refer to the glossary for the terms defining specific locations within HoloHub.</p> <p>If your Holoscan SDK installation type is:</p> <ul> <li>python wheels:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <ul> <li>otherwise:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOSCAN_INSTALL_DIR&gt;/python/lib:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <p>Next, run the commands of your choice:</p> <ul> <li> <p>Using a pre-recorded video     <pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;/applications/ultrasound_segmentation/python\npython3 ultrasound_segmentation.py --source=replayer --data &lt;DATA_DIR&gt;/ultrasound_segmentation\n</code></pre></p> </li> <li> <p>Using an AJA card     <pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;/applications/ultrasound_segmentation/python\npython3 ultrasound_segmentation.py --source=aja\n</code></pre></p> </li> </ul>","tags":["Healthcare AI","Video","Ultrasound","Segmentation","AJA","Visualization"]},{"location":"applications/velodyne_lidar_app/","title":"Velodyne VLP-16 Lidar Viewer","text":"<p> Authors: Holoscan Team (NVIDIA), nvMap Team (NVIDIA), nvMap Embedded Team (NVIDIA), Tom Birdsong (NVIDIA), Julien Jomier (NVIDIA), Jiahao Yin (NVIDIA), Marlene Wan (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 4 - Experimental</p> <p></p>","tags":["Signal Processing","Lidar","Point Cloud","UDP","Visualization"]},{"location":"applications/velodyne_lidar_app/#overview","title":"Overview","text":"<p>In this application we demonstrate how to use Holoscan SDK for low-latency lidar processing. We receive lidar packets from a Velodyne VLP-16 lidar sensor, convert packet information to a rolling Cartesian point cloud on GPU, then visualize the results with HoloViz.</p>","tags":["Signal Processing","Lidar","Point Cloud","UDP","Visualization"]},{"location":"applications/velodyne_lidar_app/#background","title":"Background","text":"<p>\"Lidar\" (LIght Detection And Ranging) is a technique by which \"light\", typically of wavelengths in the infrared spectrum, is used to determine the position of reflective surfaces surrounding a sensor. A 3D lidar sensor often employs a stacked vertical array of infrared laser emitters and sources that it spins rapidly. Similar to radar, the strength and timing of reflected lasers can be used to generate a 360-degree 3D point cloud view of the surrounding environment, with each point corresponding to an estimated point of reflection.</p> <p>For demonstration purposes we selected the Velodyne VLP-16 lidar sensor as our input source. We adapted existing packet processing code from NVIDIA DeepMap SDK into a custom Holoscan operator, <code>VelodyneLidarOp</code>, and connected it with the existing <code>BasicNetworkOp</code> and <code>HoloVizOp</code> operators to provide a complete viewing pipeline. We performed initial benchmarking on an NVIDIA IGX devkit.</p>","tags":["Signal Processing","Lidar","Point Cloud","UDP","Visualization"]},{"location":"applications/velodyne_lidar_app/#requirements","title":"Requirements","text":"<p>This application is intended to run on a Holoscan SDK support platform, namely a Linux x64 system or an NVIDIA IGX developer kit.</p> <p>To run the application you need a live or replayer source to stream Velodyne VLP-16 packet data to the application. That may be either: - A Velodyne VLP-16 lidar sensor. Review the VLP-16 user manual - A VLP-16 <code>.pcap</code> recording file and a packet replayer software.   - Visit Kitware's VeloView Velodyne Lidar collection for sample VLP-16 <code>.pcap</code> files.   - Visit the third party Wireshark wiki for a curated list of software options for generating traffic from <code>.pcap</code> files.</p>","tags":["Signal Processing","Lidar","Point Cloud","UDP","Visualization"]},{"location":"applications/velodyne_lidar_app/#running-the-application","title":"Running the Application","text":"<p>First, start your lidar stream source. If you are using a VLP-16 lidar sensor, review the VLP-16 user manual for instructions on how to properly set up your network configuration.</p> <p>Then, build and start the Holoscan lidar viewing application:</p> <pre><code>./holohub run velodyne_lidar_app\n</code></pre>","tags":["Signal Processing","Lidar","Point Cloud","UDP","Visualization"]},{"location":"applications/velodyne_lidar_app/#benchmarks","title":"Benchmarks","text":"<p>We performed benchmarking on an NVIDIA IGX developer kit with an A4000 GPU. (Note that an A6000 GPU is standard for IGX.) We used the holoscan_flow_benchmarking project to collect and summarize performance. The performance for each component in the Holoscan SDK pipeline is shown in the image below.</p> <p>Key statistics:</p> Minimum Latency 1.03 milliseconds Average Latency 1.12 milliseconds Maximum Latency 1.34 milliseconds <p>By comparison, the VLP-16 lidar publishes packets at a rate of approximately 1.33 milliseconds per packet.</p> <p></p> <p></p>","tags":["Signal Processing","Lidar","Point Cloud","UDP","Visualization"]},{"location":"applications/velodyne_lidar_app/#frequently-asked-questions-faq","title":"Frequently Asked Questions (FAQ)","text":"","tags":["Signal Processing","Lidar","Point Cloud","UDP","Visualization"]},{"location":"applications/velodyne_lidar_app/#how-does-the-application-work","title":"How does the application work?","text":"<p>The application flow is as follows:</p> <ol> <li>A UDP packet is emitted from the Velodyne VLP-16 lidar sensor and received on port 2368 in the Holoscan <code>BasicNetworkOp</code> operator.</li> <li>The packet payload is forwarded to the Holoscan <code>VelodyneLidarOp</code> operator. The operator decodes the packet according to the Velodyne lidar specification, where the VLP-16 packet defines 384 spherical points from laser firings. The operator converts the spherical points to Cartesian points on the GPU device and adds the resulting cloud to a rolling, accumulated point cloud.</li> <li>The Velodyne operator forwards the rolling point cloud to HoloViz, which renders the GPU point cloud to the screen.</li> </ol>","tags":["Signal Processing","Lidar","Point Cloud","UDP","Visualization"]},{"location":"applications/velodyne_lidar_app/#what-are-some-limitations-of-the-application","title":"What are some limitations of the application?","text":"<p>This application is intended as a simple demonstration of how a lidar sensor can be integrated for input to Holoscan SDK for low latency processing. It does not propose any novel features. Some limitations compared with more complete lidar solutions are: - No cloud filtering -- all zero-ranged points are kept in the buffer and visualized. - No advanced inference techniques -- the cloud is simply translated and visualized. - No RDMA -- VLP-16 lidar packets are received via the host ethernet interface on the IGX or x86_64 machine and then copied to the GPU device. - Monochrome visual -- HoloViz operator cloud support is currently limited to one color.</p> <p>Each of these limitations is merely a result of our scope of work, and could be overcome with additional attention.</p>","tags":["Signal Processing","Lidar","Point Cloud","UDP","Visualization"]},{"location":"applications/velodyne_lidar_app/#why-is-holoviz-not-responding","title":"Why is HoloViz not responding?","text":"<p>In most cases this indicates that the Holoscan application is not receiving UDP packets. There are several reasons that this could be the case: - The VLP-16 lidar sensor is not turned on, or the ethernet cable is disconnected.   The sensor typically takes approximately 30 seconds between powering on and transmitting packets. - The VLP-16 lidar sensor network interface is not properly configured to receive packets. You can use a tool   such as Wireshark to review live packets on the network interface. Review the VLP-16   user manual for troubleshooting. - The HoloHub application is not properly configured. Review the <code>lidar.yaml</code> configuration   and confirm that the port and IP address match the VLP-16 configuration.</p>","tags":["Signal Processing","Lidar","Point Cloud","UDP","Visualization"]},{"location":"applications/velodyne_lidar_app/#acknowledgements","title":"Acknowledgements","text":"<p>This operator was developed in part with support from the NVIDIA nvMap team and adapts portions of the NVIDIA DeepMap SDK.</p>","tags":["Signal Processing","Lidar","Point Cloud","UDP","Visualization"]},{"location":"applications/video_deidentification/","title":"Real-Time Face and Text Deidentification","text":"<p> Authors: Wendell Hom (NVIDIA), Jonathan McLeod (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 2 - Trusted</p> <p>This sample application demonstrates the use of face and text detection models to do real-time video deidentification. Regions identified to be face or text are blurred out from the final image.</p> <p>NOTE: This application is a demonstration of real-time face and text deidentification and is not meant to be used in critical applications that has zero error tolerance.  The models used in this sample application have limitations, e.g., in detecting faces and text that are partially occluded, in low lighting situations, when there is motion blur, etc.</p>","tags":["Computer Vision and Perception","Video","Deidentification","Detection","Image Processing"]},{"location":"applications/video_deidentification/#models","title":"Models","text":"<p>This application uses TAO PeopleNet model from NGC for detecting faces. The model is downloaded when building the application.</p> <p>For text detection, this application uses EasyOCR python library which uses Character Region Awareness for Text Detection (CRAFT).</p>","tags":["Computer Vision and Perception","Video","Deidentification","Detection","Image Processing"]},{"location":"applications/video_deidentification/#data","title":"Data","text":"<p>This application downloads a pre-recorded video from Pexels when the application is built for use with this application.  Please review the license terms from Pexels.</p> <p>NOTE: The user is responsible for checking if the dataset license is fit for the intended purpose.</p>","tags":["Computer Vision and Perception","Video","Deidentification","Detection","Image Processing"]},{"location":"applications/video_deidentification/#input","title":"Input","text":"<p>This app currently supports three different input options:</p> <ol> <li>v4l2 compatible input device (default, see V4L2 Support below)</li> <li>pre-recorded video (see Video Replayer Support below)</li> </ol>","tags":["Computer Vision and Perception","Video","Deidentification","Detection","Image Processing"]},{"location":"applications/video_deidentification/#run-instructions","title":"Run Instructions","text":"","tags":["Computer Vision and Perception","Video","Deidentification","Detection","Image Processing"]},{"location":"applications/video_deidentification/#v4l2-support","title":"V4L2 Support","text":"<p>This application supports v4l2 compatible devices as input.  To run this application with your v4l2 compatible device, please plug in your input device and run: <pre><code>./holohub run video_deidentification\n</code></pre></p> <p>By default, this application expects the input device to be mounted at <code>/dev/video0</code>.  If this is not the case, please update <code>applications/video_deidentification/video_deidentification.yaml</code> and set it to use the corresponding input device before running the application.  You can also override the default input device on the command line by running: <pre><code>./holohub run video_deidentification --run-args=\"--video_device /dev/video0\"\n</code></pre></p>","tags":["Computer Vision and Perception","Video","Deidentification","Detection","Image Processing"]},{"location":"applications/video_deidentification/#video-replayer-support","title":"Video Replayer Support","text":"<p>If you don't have a v4l2 compatible device plugged in, you may also run this application on a pre-recorded video. To launch the application using the Video Stream Replayer as the input source, run:</p> <pre><code>./holohub run video_deidentification --run-args=\"--source replayer\"\n</code></pre>","tags":["Computer Vision and Perception","Video","Deidentification","Detection","Image Processing"]},{"location":"applications/video_deidentification/#known-issues","title":"Known Issues","text":"<p>There is a known issue running this application on IGX w/ iGPU and on Jetson AGX (see #500). The workaround is to update the device to avoid picking up the libnvv4l2.so library.</p> <pre><code>cd /usr/lib/aarch64-linux-gnu/\nls -l libv4l2.so.0.0.999999\nsudo rm libv4l2.so.0.0.999999\nsudo ln -s libv4l2.so.0.0.0.0  libv4l2.so.0.0.999999\n</code></pre>","tags":["Computer Vision and Perception","Video","Deidentification","Detection","Image Processing"]},{"location":"applications/vila_live/","title":"VILA Live","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates how to run VILA 1.5 models on live video feed with the possibility of changing the prompt in real time.</p> <p>VILA 1.5 is a family of Vision Language Models (VLM) created by NVIDIA &amp; MIT. It uses SigLIP to encode images into tokens which are fed into an LLM with an accompanying prompt. This application collects video frames from the V4L2 operator and feeds them to an AWQ-quantized VILA 1.5 for inference using the TinyChat library. This allows users to interact with a Generative AI model that is \"watching\" a chosen video stream in real-time.</p> <p> Note: This demo currently uses Llama-3-VILA1.5-8b-AWQ, but any of the following AWQ-quantized models from the VILA 1.5 family should work as long as the file names are changed in the Dockerfile and run_vila_live.sh: - VILA1.5-3b-AWQ - VILA1.5-3b-s2-AWQ - Llama-3-VILA1.5-8b-AWQ - VILA1.5-13b-AWQ - VILA1.5-40b-AWQ</p>","tags":["Computer Vision and Perception","LLM","Large Vision Model","Multimodal Model","Video"]},{"location":"applications/vila_live/#setup-instructions","title":"\u2699\ufe0f Setup Instructions","text":"<p>The app defaults to using the video device at <code>/dev/video0</code></p> <p>Note: You can use a USB webcam as the video source, or an MP4 video by following the instructions for the V4L2_Camera example app.</p> <p>To debug if this is the correct device download <code>v4l2-ctl</code>: <pre><code>sudo apt-get install v4l-utils\n</code></pre> To check for your devices run: <pre><code>v4l2-ctl --list-devices\n</code></pre> This command will output something similar to this: <pre><code>NVIDIA Tegra Video Input Device (platform:tegra-camrtc-ca):\n        /dev/media0\n\nvi-output, lt6911uxc 2-0056 (platform:tegra-capture-vi:0):\n        /dev/video0\n\nDummy video device (0x0000) (platform:v4l2loopback-000):\n        /dev/video3\n</code></pre> Determine your desired video device and edit the source device in vila_live.yaml</p>","tags":["Computer Vision and Perception","LLM","Large Vision Model","Multimodal Model","Video"]},{"location":"applications/vila_live/#build-and-run-instructions","title":"\ud83d\ude80 Build and Run Instructions","text":"<p>From the Holohub main directory run the following command: <pre><code>./holohub run vila_live\n</code></pre> or running with a replayer: <pre><code>./holohub run vila_live --run-args=\"--source replayer\"\n</code></pre> Note: The first build will take ~1.5 hours if you're on ARM64. This is largely due to building Flash Attention 2 since pre-built wheels are not distributed for ARM64 platforms.</p> <p>Once the main LMM-based app is running, you will see a link for the app at <code>http://127.0.0.1:8050</code>. To receive the video stream, please also ensure port 49000 is open.</p>","tags":["Computer Vision and Perception","LLM","Large Vision Model","Multimodal Model","Video"]},{"location":"applications/vila_live/#supported-hardware","title":"\ud83d\udcbb Supported Hardware","text":"<ul> <li>IGX w/ dGPU</li> <li>x86 w/ dGPU</li> <li>IGX w/ iGPU and Jetson AGX supported with workaround   There is a known issue running this application on IGX w/ iGPU and on Jetson AGX (see #500).   The workaround is to update the device to avoid picking up the libnvv4l2.so library.</li> </ul> <pre><code>cd /usr/lib/aarch64-linux-gnu/\nls -l libv4l2.so.0.0.999999\nsudo rm libv4l2.so.0.0.999999\nsudo ln -s libv4l2.so.0.0.0.0  libv4l2.so.0.0.999999\n</code></pre>","tags":["Computer Vision and Perception","LLM","Large Vision Model","Multimodal Model","Video"]},{"location":"applications/vila_live/#video-options","title":"\ud83d\udcf7\u2699\ufe0f Video Options","text":"<p>There are three options to ingest video data.</p> <ol> <li>use a physical device or capture card, such as a v4l2 device as described in the Setup Instructions. Make sure the vila_live.yaml contains the v4l2_source group and specifies the device correctly (<code>pixel_format</code> may be tuned accordingly, e.g. <code>pixel_format: \"auto\"</code>).</li> <li> <p>convert a video file to a gxf-compatible format using the convert_video_to_gxf_entities.py script. See the yolo_model_deployment application for a detailed example. When using the replayer, configure the replayer_source in the yaml file and launch the application with:     <pre><code>./run_vila_live.sh --source \"replayer\"\n</code></pre> This application downloads a pre-recorded video from Pexels when the application is built.  Please review the license terms from Pexels.</p> </li> <li> <p>create a virtual video device, that mounts a video file and replays it, as detailed in the v4l2_camera examples in holoscan-sdk. This approach may require signing the v4l2loopback kernel module, when using a system with secure-boot enabled. Make sure the vila_live.yaml contains the v4l2_source group and specifies the virtual device correctly. replay the video, using for example:     <pre><code>ffmpeg -stream_loop -1 -re -i &lt;your_video_path&gt; -pix_fmt yuyv422 -f v4l2 /dev/video3\n</code></pre></p> </li> </ol>","tags":["Computer Vision and Perception","LLM","Large Vision Model","Multimodal Model","Video"]},{"location":"applications/vila_live/#acknowledgements","title":"\ud83d\ude4c Acknowledgements","text":"<ul> <li>Jetson AI Lab, Live LLaVA: for the inspiration to create this app</li> <li>Jetson-Containers repo: For the Flask web-app with WebSockets</li> <li>LLM-AWQ repo: For the example code to create AWQ-powered LLM servers</li> </ul>","tags":["Computer Vision and Perception","LLM","Large Vision Model","Multimodal Model","Video"]},{"location":"applications/volume_rendering/","title":"Volume rendering using ClaraViz","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3, 2.0.0, 2.1.0, 3.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p></p> <p>This application loads a medical CT scan and renders it in real time at interactive frame rates using ClaraViz (https://github.com/NVIDIA/clara-viz).</p> <p>The application uses the <code>VolumeLoaderOp</code> operator to load the medical volume data, the <code>VolumeRendererOp</code> operator to render the volume and the <code>HolovizOp</code> operator to display the result and handle the camera movement.</p>","tags":["Healthcare AI","Visualization","Hounsfield Scale Transfer Functions","Volume","Segmentation","TotalSegmentator"]},{"location":"applications/volume_rendering/#data","title":"Data","text":"<p>You can find CT scan datasets for use with this application from embodi3d.</p> <p>Datasets are bundled with a default ClaraViz JSON configuration file for volume rendering. See <code>VolumeRendererOp</code> documentation for details on configuration schema.</p> <p>See <code>VolumeLoaderOp</code> documentation for supported volume formats.</p>","tags":["Healthcare AI","Visualization","Hounsfield Scale Transfer Functions","Volume","Segmentation","TotalSegmentator"]},{"location":"applications/volume_rendering/#build-and-run-instructions","title":"Build and Run Instructions","text":"<p>To build and run this application, use the <code>dev_container</code> script:</p> <pre><code># C++\n ./holohub run volume_rendering --language cpp\n\n # Python\n  ./holohub run volume_rendering --language python\n</code></pre> <p>The path of the volume configuration file, volume density file and volume mask file can be passed to the application.</p> <p>You can use the following command to get more information on command line parameters for this application:</p> <pre><code>./holohub run volume_rendering --language [cpp|python] --run-args=\"--usages\"\n</code></pre>","tags":["Healthcare AI","Visualization","Hounsfield Scale Transfer Functions","Volume","Segmentation","TotalSegmentator"]},{"location":"applications/volume_rendering/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode volume_rendering\n</code></pre>","tags":["Healthcare AI","Visualization","Hounsfield Scale Transfer Functions","Volume","Segmentation","TotalSegmentator"]},{"location":"applications/volume_rendering/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Healthcare AI","Visualization","Hounsfield Scale Transfer Functions","Volume","Segmentation","TotalSegmentator"]},{"location":"applications/volume_rendering/#c","title":"C++","text":"<p>Use the (gdb) volume_rendering/cpp launch profile to run and debug the C++ application.</p>","tags":["Healthcare AI","Visualization","Hounsfield Scale Transfer Functions","Volume","Segmentation","TotalSegmentator"]},{"location":"applications/volume_rendering/#python","title":"Python","text":"<p>There are a couple of launch profiles configured for this application:</p> <ol> <li>(debugpy) python_rendering/python: Launch the Volume Rendering application with the ability to debug Python code.</li> <li>(pythoncpp) python_rendering/python: Launch the Volume Rendering application with the ability to debug both Python and C++ code.</li> </ol>","tags":["Healthcare AI","Visualization","Hounsfield Scale Transfer Functions","Volume","Segmentation","TotalSegmentator"]},{"location":"applications/volume_rendering/#command-line-arguments","title":"Command Line Arguments","text":"<pre><code>Holoscan ClaraViz volume renderer.\nUsage: ./applications/volume_rendering/volume_rendering [options]\nOptions:\n  -h,-u, --help, --usages               Display this information\n  -c &lt;FILENAME&gt;, --config &lt;FILENAME&gt;    Name of the renderer JSON configuration file to load (default '../../../data/volume_rendering/config.json')\n  -p &lt;FILENAME&gt;, --preset &lt;FILENAME&gt;    Name of the renderer JSON preset file to load. This will be merged into the settings loaded from the configuration file. Multiple presets can be specified.\n  -w &lt;FILENAME&gt;, --write_config &lt;FILENAME&gt; Name of the renderer JSON configuration file to write to (default '')\n  -d &lt;FILENAME&gt;, --density &lt;FILENAME&gt;   Name of density volume file to load (default '../../../data/volume_rendering/highResCT.mhd')\n  -i &lt;MIN&gt;, --density_min &lt;MIN&gt;         Set the minimum of the density element values. If not set this is calculated from the volume data. In practice CT volumes have a minimum value of -1024 which corresponds to the lower value of the Hounsfield scale range usually used.\n  -a &lt;MAX&gt;, --density_max &lt;MAX&gt;         Set the maximum of the density element values. If not set this is calculated from the volume data. In practice CT volumes have a maximum value of 3071 which corresponds to the upper value of the Hounsfield scale range usually used.\n  -m &lt;FILENAME&gt;, --mask &lt;FILENAME&gt;      Name of mask volume file to load (default '../../../data/volume_rendering/smoothmasks.seg.mhd')\n  -n &lt;COUNT&gt;, --count &lt;COUNT&gt;           Duration to run application (default '-1' for unlimited duration)\n  ```\n\n### Importing CT datasets\n\nThis section describes the steps to user CT datasets additionally to the dataset provided by the volume rendering application.\n\nFirst get the data in a supported format. Supported formats are:\n* [MHD](https://itk.org/Wiki/ITK/MetaIO/Documentation)\n* [NIFTI](https://nifti.nimh.nih.gov/)\n* [NRRD](https://teem.sourceforge.net/nrrd/format.html)\n\nCT Data for the example dataset is downloaded to the `data/volume_rendering` folder when the application builds.\n\nAdditionally information on lighting, transfer functions and other settings is needed for the renderer to create an image. These settings are loaded from JSON files. The JSON files for the included example dataset is here `data/volume_rendering/config.json`.\n\nThere are two options to create a config file for a new dataset. First, use the example config as a reference to create a new config and modify parameters. Or let the renderer create a config file with settings deduced from the dataset.\n\nAssuming the volume file is is named `new_volume.nrrd`. Specify the new volume file (`-d new_volume.nrrd`), set the config file option to an empty string (`-c \"\"`) to force the renderer to deduce settings and specify the name of the config file to write (`-w new_config.json`):\n\n```bash\n  ./applications/volume_rendering/cpp/volume_rendering -d new_volume.nrrd -c \"\" -w new_config.json\n</code></pre> <p>This will create a file <code>new_config.json</code>. If there is a segmentation volume present add it with <code>-m new_seg_volume.nrrd</code>.</p> <p>By default the configuration is set up for rendering still images. For interactive rendering change the <code>timeSlot</code> setting in <code>RenderSettings</code> to the desired frame time in milliseconds, e.g. <code>33.0</code> for 30 fps.</p> <p>Also by default all lights and the background are shown in the scene. To avoid this change all <code>\"show\": true,</code> values to <code>\"show\": false,</code>.</p> <p>Modify the configuration file to your needs. To display the volume with the new configuration file add the configuration with the <code>-c new_config.json</code> argument:</p> <pre><code>  ./applications/volume_rendering/cpp/volume_rendering -d new_volume.nrrd -c new_config.json\n</code></pre> <p>It's possible to load preset JSON configuration files by using the <code>--preset preset.json</code> command line option. Presets are merged into the settings loaded from the configuration file. Multiple presets can be specified.</p> <p>A preset for bones is included. To load that preset use this command:</p> <pre><code>  ./applications/volume_rendering/cpp/volume_rendering -d new_volume.nrrd -c new_config.json -p presets/bones.json\n</code></pre>","tags":["Healthcare AI","Visualization","Hounsfield Scale Transfer Functions","Volume","Segmentation","TotalSegmentator"]},{"location":"applications/volume_rendering/#transfer-functions","title":"Transfer functions","text":"<p>Usually CT datasets are stored in Hounsfield scale. The renderer maps these values in Hounsfield scale to opacity in order to display the volume. These mappings are called transfer functions. Multiple transfer functions for different input value regions can be defined. Transfer functions also include material properties like diffuse, specular and emissive color. The range of input values the transfer function is applied to is in normalized input range <code>[0, 1]</code>.</p>","tags":["Healthcare AI","Visualization","Hounsfield Scale Transfer Functions","Volume","Segmentation","TotalSegmentator"]},{"location":"applications/volume_rendering/#segmentation-volume","title":"Segmentation volume","text":"<p>Different organs often have very similar Hounsfield values, therefore additionally an segmentation volume is supported. The segmentation volume contains an integer index for each element of the volume. Transfer functions can be restricted on specific segmentation indices. The segmentation volume can, for example, be generated using TotalSegmentator.</p>","tags":["Healthcare AI","Visualization","Hounsfield Scale Transfer Functions","Volume","Segmentation","TotalSegmentator"]},{"location":"applications/volume_rendering_xr/","title":"Medical Image Viewer in XR","text":"<p> Authors: Andreas Heumann (NVIDIA), Connor Smith (NVIDIA), Cristiana Dinea (NVIDIA), Tom Birdsong (NVIDIA), Antonio Ospite (Magic Leap), Jiwen Cai (Magic Leap), Jochen Stier (Magic Leap), Korcan Hussein (Magic Leap), Robbie Bridgewater (Magic Leap) Supported platforms: x86_64, aarch64 Language: C++ Last modified: July 1, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p></p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#description","title":"Description","text":"<p>We collaborated with Magic Leap on a proof of concept mixed reality viewer for medical imagery built on the Holoscan platform.</p> <p>Medical imagery is one of the fastest-growing sources of data in any industry. When we think about typical diagnostic imaging, X-ray, CT scans, and MRIs come to mind. X-rays are 2D images, so viewing them on a lightbox or, if they\u2019re digital, a computer, is fine. But CT scans and MRIs are 3D. They\u2019re incredibly important technologies, but our way of interacting with them is flawed. This technology helps physicians in so many ways, from training and education to making more accurate diagnoses and ultimately to planning and even delivering more effective treatments.</p> <p>You can use this viewer to visualize a segmented medical volume with a mixed reality device.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#prerequisites","title":"Prerequisites","text":"","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#host-machine","title":"Host Machine","text":"<p>Review the HoloHub README document for supported platforms and software requirements.</p> <p>The application supports x86_64 or IGX dGPU platforms. IGX iGPU, AGX, and RHEL platforms are not fully tested at this time.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#magic-leap-2-device","title":"Magic Leap 2 Device","text":"<p>The following packages and applications are required to run remote rendering with a Magic Leap 2 device:</p> Requirement Platform Version Source Magic Leap Hub Windows or macOS PC latest Magic Leap Website Headset Firmware Magic Leap 2 v1.6.0 Magic Leap Hub Headset Remote Rendering Viewer (.apk) Magic Leap 2 1.11.64 Magic Leap Download Link Windrunner OpenXR Backend HoloHub Container 1.11.74 Included in Container Magic Leap 2 Pro License Magic Leap <p>Refer to the Magic Leap 2 documentation for more information: - Updating your device with Magic Leap Hub; - Installing <code>.apk</code> packages with Magic Leap Hub</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#quick-start","title":"Quick Start","text":"<p>Run the following command in the top-level HoloHub folder to build and run the host application:</p> <pre><code>./holohub run volume_rendering_xr\n</code></pre> <p>A QR code will be visible in the console log. Refer to Magic Leap 2 Remote Rendering Setup documentation to pair the host and device in preparation for remote viewing. Refer to the Remote Viewer section to regenerate the QR code as needed, or to use the local debugger GUI in place of a physical device.</p> <p>The application supports the following hand or controller interactions by default: - Translate: Reach and grab inside the volume with your hand or with the controller trigger to move the volume. - Scale: Grab any face of the bounding box and move your hand or controller to scale the volume. - Rotate: Grab any edge of the bounding box and move your hand or controller to rotate the volume. - Crop: Grab any vertex of the bounding box and move your hand or controller to translate the cropping planes.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#testing-utility","title":"Testing Utility","text":"<p>We provide a simple test application in <code>utils/xr_hello_holoscan</code> for validating basic XR functionality. This utility uses the same XR operators and configuration as the main application but with minimal rendering setup. See utils/xr_hello_holoscan/README.md for details on running the test utility.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#advanced-setup","title":"Advanced Setup","text":"<p>You can use the <code>--dryrun</code> option to see the individual commands run by the quick start option above: <pre><code>./holohub run volume_rendering_xr --dryrun\n</code></pre></p> <p>Alternatively, follow the steps below to set up the interactive container session.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#build-the-container","title":"Build the Container","text":"<p>Run the following commands to build and enter the interactive container environment: <pre><code>./holohub run-container volume_rendering_xr # Build and launch the container\n</code></pre></p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#build-the-application","title":"Build the Application","text":"<p>Inside the container environment, build the application: <pre><code>./holohub build volume_rendering_xr # Build the application\n</code></pre></p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#run-the-application","title":"Run the Application","text":"<p>Inside the container environment, start the application: <pre><code>export ML_START_OPTIONS=&lt;\"\"/\"debug\"&gt; # Defaults to \"debug\" to run XR device simulator GUI\n./holohub run volume_rendering_xr\n</code></pre></p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#deploying-as-a-standalone-application","title":"Deploying as a Standalone Application","text":"<p><code>volume_rendering_xr</code> can be packaged in a self-contained release container with datasets and binaries.</p> <p>To build the release container: <pre><code># Generate HoloHub `volume_rendering_xr` installation in the \"holohub/install\" folder\n./holohub build volume_rendering_xr --configure-args=\"-DCMAKE_INSTALL_PREFIX:PATH=/workspace/holohub/install\"\n./holohub run-container volume_rendering_xr --docker-opts=\"--entrypoint=bash\" -- -c cmake --build ./build --target install\n\n# Copy files into a release container\n./holohub build-container --img holohub:volume_rendering_xr_rel --docker-file ./applications/volume_rendering_xr/scripts/Dockerfile.rel --base-img nvcr.io/nvidia/cuda:12.4.1-runtime-ubuntu22.04\n</code></pre></p> <p>To run the release container, first create the container startup script: <pre><code>docker run --rm holohub:volume_rendering_xr_rel &gt; ./render-volume-xr\nchmod +x ./render-volume-xr\n</code></pre></p> <p>Then execute the script to start the Windrunner service and the app: <pre><code>./render-volume-xr\n</code></pre></p> <p>For more options, e.g. list available datasets or to select a different dataset, type <pre><code>./render-volume-xr --help\n</code></pre></p> <p>Options not recognized by the render-volume-xr script are forwarded to the application.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#additional-notes","title":"Additional Notes","text":"","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#supported-formats","title":"Supported Formats","text":"<p>This application loads static volume files from the local disk. See HoloHub <code>VolumeLoaderOp</code> documentation for supported volume formats and file conversion tools.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#launch-options","title":"Launch Options","text":"<p>Use the <code>--extra-args</code> to see all options, including how to specify a different dataset or configuration file to use. <pre><code>./holohub run volume_rendering_xr --run-args=\"--help\"\n...\nHoloscan OpenXR volume renderer.Usage: /workspace/holohub/build/applications/volume_rendering_xr/volume_rendering_xr [options]\nOptions:\n  -h, --help                            Display this information\n  -c &lt;FILENAME&gt;, --config &lt;FILENAME&gt;    Name of the renderer JSON configuration file to load (default '/workspace/holoscan-openxr/data/volume_rendering/config.json')\n  -d &lt;FILENAME&gt;, --density &lt;FILENAME&gt;   Name of density volume file to load (default '/workspace/holoscan-openxr/data/volume_rendering/highResCT.mhd')\n  -m &lt;FILENAME&gt;, --mask &lt;FILENAME&gt;      Name of mask volume file to load (default '/workspace/holoscan-openxr/data/volume_rendering/smoothmasks.seg.mhd')\n</code></pre></p> <p>To use a new dataset with the application, mount its volume location from the host machine when launching the container and pass all required arguments explicitly to the executable: <pre><code>./holohub run-container --docker-opts=\"-u root\" --img holohub:openxr-dev --add-volume /host/path/to/data-dir\n./build/applications/volume_rendering_xr/volume_rendering_xr \\\n      -c /workspace/holohub/data/volume_rendering/config.json \\\n      -d /workspace/volumes/path/to/data-dir/dataset.nii.gz \\\n      -m /workspace/volumes/path/to/data-dir/dataset.seg.nii.gz\n</code></pre></p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#starting-the-magic-leap-openxr-runtime","title":"Starting the Magic Leap OpenXR runtime","text":"<p>OpenXR runtimes are implementations of the OpenXR API that allow the Holoscan XR operators to create XR sessions and render content. The Magic Leap OpenXR runtime including a CLI are by default installed in the dev container. From a terminal inside the dev container you can execute the following scripts:</p> <p><pre><code>ml_start.sh\n</code></pre> starts the OpenXR runtime service. After executing this command, the remote viewer on the Magic Leap device should connect to this runtime service. If not, then you still have to pair the device with the host computer running the Holoscan application.</p> <p>For rapid iteration without a Magic Leap device, pass the argument <code>debug</code> to <code>ml_start.sh</code> i.e. <pre><code>ml_start.sh debug\n</code></pre> This will enable a debug view on your computer showing what the headset would see. You may click into this window and navigate with the keyboard and mouse to manipulate the virtual head position.</p> <p>If you connect an ML2 while the debug view is active, you can continue to view the content on the debug view but can no longer adjust the virtual position, as the real position is used instead.</p> <p><pre><code>ml_pair.sh\n</code></pre> displays a QR code used to pair the device with the host. Start the QR code reader App on the device and scan the QR code displayed in the terminal. Note that the OpenXR runtime has to have been started using the ml_start command in order for the paring script to execute correctly.</p> <p><pre><code>ml_stop.sh\n</code></pre> stops the OpenXR runtime service.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#starting-the-magic-leap-2-remote-viewer","title":"Starting the Magic Leap 2 Remote Viewer","text":"<p>When using a Magic Leap 2 device for the first time or after a software upgrade, the device must be provided with the IP address of the host running the OpenXR runtime. From a terminal inside the dev container run the</p> <pre><code>ml_pair.sh\n</code></pre> <p>command, which will bring up a QR code that has to be scanned using the QR Code App on the Magic Leap 2 device. Once paired with the host, the device  will automatically start the remote viewer which will then prompt you to start an OpenXR application on the host. Any time thereafter, start the remote viewer via the App menu.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#developing-with-a-different-openxr-backend","title":"Developing with a Different OpenXR Backend","text":"<p><code>volume_renderer_xr</code> is an OpenXR compatible application. The Magic Leap Remote Rendering runtime is installed in the application container by default, but a compatible runtime can be used if appropriate to your use case. See https://www.khronos.org/openxr/ for more information on conformant OpenXR runtimes.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#volume-rendering","title":"Volume Rendering","text":"<p>The application carries out volume rendering via the HoloHub <code>volume_renderer</code> operator, which in turn wraps the NVIDIA ClaraViz rendering project. ClaraViz JSON configurations provided in the config folder are available for specifying default scene parameters.</p> <p>See <code>volume_renderer</code> Configuration section for details on manipulating configuration values, along with how to create a new configuration file to fit custom data.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#troubleshooting","title":"Troubleshooting","text":"<p>Please verify that you are building from the latest HoloHub <code>main</code> branch before reviewing troubleshooting steps.</p> <pre><code>git checkout main\n</code></pre>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#libraries-are-missing-when-building-the-application-vulkan-openxr-etc","title":"Libraries are missing when building the application (Vulkan, OpenXR, etc)","text":"<p>This error may indicate that you are building inside the default HoloHub container instead of the expected <code>volume_rendering_xr</code> container. Review the build steps and ensure that you have launched the container with the appropriate <code>holohub run-container --img</code> option.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#unexpected-cmake-errors","title":"Unexpected CMake errors","text":"<p>You may need to clear your CMake build cache. See the HoloHub Cleaning section for instructions.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#seccomp-errors","title":"\"Seccomp\" Errors","text":"<p>The Magic Leap Windrunner OpenXR backend and remote rendering host application use seccomp to limit syscalls on Linux platforms. You can exempt individual syscalls for local development by adding them to the application syscall whitelist.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#debug-gui-does-not-appear","title":"Debug GUI does not appear","text":"<p>The <code>./holohub run volume_rendering_xr</code> command initializes the Magic Leap Windrunner debug GUI by default. If you do not see the debug GUI appear in your application, or if the application appears to stall with no further output after the pairing QR code appears, try any of the following:</p> <ol> <li> <p>Manually set the <code>ML_START_OPTIONS</code> environment variable so that <code>holohub run</code> initializes with the debug view: <pre><code>export ML_START_OPTIONS=\"debug\"\n</code></pre></p> </li> <li> <p>Follow Advanced Setup Instructions and add the <code>-u root</code> option to launch the container with root permissions. <pre><code>./holohub run --img holohub:volume_rendering_xr --docker-opts\"-u root\"\n</code></pre></p> </li> <li> <p>Clear the build cache and any home cache folders in the HoloHub workspace. <pre><code>./run clear_cache\nrm -rf .cache/ .cmake/ .config/ .local/\n</code></pre></p> </li> </ol>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/operators/XrFrameOp/","title":"XrFrame Operator","text":"<p> Authors: Magic Leap Team (Magic Leap) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>The <code>XrFrameOp</code> directory contains the <code>XrBeginFrameOp</code> and the <code>XrEndFrameOp</code>. <code>XrBeginFrameOp</code> operator initiates the rendering of a single OpenXR frame. It converts render buffers and events generated by the OpenXR runtime into Holoscan specific events to be consumed by downstream operators. Application specific operators are then expected to render left and right camera views into the given color and depth buffers. Once complete, the buffers must be passed to <code>XrEndFrameOp</code> in order to deliver the frame back to the OpenXR runtime. Note that a single connection xr_frame from <code>XrBeginFrameOp</code> to <code>XrEndFrameOp</code> is required to synchronize the OpenXR calls issued by the two operators.</p>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrFrameOp/begin_frame/","title":"XRBeginFrame Operator","text":"<p> Authors: Magic Leap Team (Magic Leap) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>The <code>XrBeginFrameOp</code> operator initiates the rendering of a single OpenXR frame. It converts render buffers and events generated by the OpenXR runtime into Holoscan specific events to be consumed by downstream operators. Application specific operators are then expected to render left and right camera views into the given color and depth buffers. Once complete, the buffers must be passed to <code>XrEndFrameOp</code> in order to deliver the frame back to the OpenXR runtime. Note that a single arc xr_frame from <code>XrBeginFrameOp</code> to <code>XrEndFrameOp</code> is required to synchronize the OpenXR calls issued by the two operators.</p>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrFrameOp/begin_frame/#holoscanopenxrxrbeginframeop","title":"<code>holoscan::openxr::XrBeginFrameOp</code>","text":"","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrFrameOp/begin_frame/#outputs","title":"Outputs","text":"<p>Output for camera state  - <code>left_camera_pose</code>: camera pose for the left eye   - type: <code>nvidia::gxf::Pose3D</code> - <code>right_camera_pose</code>: camera pose for the right eye   - type: <code>nvidia::gxf::Pose3D</code> - <code>left_camera_model</code>: camera model for the left eye   - type: <code>nvidia::gxf::CameraModel</code> - <code>right_camera_model</code>: camera model for the right eye   - type: <code>nvidia::gxf::CameraModel</code> - <code>depth_range</code>: depth range   - type: <code>nvidia::gxf::Vector2f</code></p> <p>Output for input state  - <code>trigger_click</code>: trigger click , values true/false   - type: <code>bool</code> - <code>shoulder_click</code>: shoulder click , values true/false   - type: <code>bool</code> - <code>trackpad_touch</code>: trackpad touch , values true/false   - type: <code>bool</code> - <code>trackpad</code>: trackpad values [x.y]   - type: <code>std::array&lt;float, 2&gt;</code> - <code>aim_pose</code>: aim pose for the controller specific for the right hand   - type: <code>nvidia::gxf::Pose3D</code> - <code>head_pose</code>: head pose    - type: <code>nvidia::gxf::Pose3D</code> - <code>color_buffer</code>: color buffer   - type: <code>holoscan::gxf::Entity</code> - <code>depth_buffer</code>: depth buffer   - type: <code>holoscan::gxf::Entity</code></p>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrFrameOp/begin_frame/#parameters","title":"Parameters","text":"<ul> <li><code>XrSession</code>: A class that encapsulates a single OpenXR session</li> <li>type: <code>holoscan::openxr::XrSession</code></li> </ul> <p>Note:</p> <ul> <li><code>XrCudaInteropSwapchain</code>: A class that encapsulates the Vulkan buffers of the OpenXR runtime and compatible CUDA buffer to provide interoperability between ClaraViz and OpenXR</li> </ul>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrFrameOp/convert_depth/","title":"Convert Depth To Screen Space Operator","text":"<p> Authors: Magic Leap team (Magic Leap) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 0.0 Minimum Holoscan SDK version: 0.6 Tested Holoscan SDK versions: 0.6, 2.0 Contribution metric: Level 2 - Trusted</p> <p>The <code>ConvertDepthToScreenSpaceOp</code> operator remaps the depth buffer from Clara Viz to an OpenXR specific range. The depth buffer is converted in place.</p>","tags":["Convert","Depth"]},{"location":"applications/volume_rendering_xr/operators/XrFrameOp/convert_depth/#holoscanopenxrconvertdepthtoscreenspaceop","title":"<code>holoscan::openxr::ConvertDepthToScreenSpaceOp</code>","text":"<p>Converts a depth buffer from linear world units to screen space ([0,1])</p>","tags":["Convert","Depth"]},{"location":"applications/volume_rendering_xr/operators/XrFrameOp/convert_depth/#inputs","title":"Inputs","text":"<ul> <li><code>depth_buffer_in</code>: input depth buffer to be remapped</li> <li>type: <code>holoscan::gxf::VideoBuffer</code></li> <li><code>depth_range</code>: Allocator used to allocate the volume data</li> <li>type: <code>nvidia::gxf::Vector2f</code></li> </ul>","tags":["Convert","Depth"]},{"location":"applications/volume_rendering_xr/operators/XrFrameOp/convert_depth/#outputs","title":"Outputs","text":"<ul> <li><code>depth_buffer_out</code>: output depth buffer </li> <li>type: <code>holoscan::gxf::Entity</code></li> </ul>","tags":["Convert","Depth"]},{"location":"applications/volume_rendering_xr/operators/XrFrameOp/end_frame/","title":"XrEndFrame Operator","text":"<p> Authors: Magic Leap Team (Magic Leap) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>The <code>XrEndFrameOp</code> operator completes the rendering of a single OpenXR frame by passing populated color and depth buffer for the left and right eye to the OpenXR device. Note that a single connection <code>xr_frame</code> from <code>XrBeginFrameOp</code> to <code>XrEndFrameOp</code> is required to synchronize the OpenXR calls issued by the two operators.</p>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrFrameOp/end_frame/#holoscanopenxrxrendframeop","title":"<code>holoscan::openxr::XrEndFrameOp</code>","text":"","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrFrameOp/end_frame/#parameters","title":"Parameters","text":"<ul> <li><code>XrSession</code>: A class that encapsulates a single OpenXR session</li> <li>type: <code>holoscan::openxr::XrSession</code></li> </ul>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrFrameOp/end_frame/#inputs","title":"Inputs","text":"<p>Render buffers populated by application - <code>color_buffer</code>: color buffer   - type: <code>holoscan::gxf::VideoBuffer</code> - <code>depth_buffer</code>: depth buffer   - type: <code>holoscan::gxf::VideoBuffer</code></p> <p>OpenXR synchronization - <code>XrFrame</code>: connection to synchronize <code>XrBeginFrameOp</code> and <code>XrEndFrameOp</code>   - type: <code>XrFrame</code></p> <p>Note:</p> <ul> <li><code>XrCudaInteropSwapchain</code>: A class that encapsulates the Vulkan buffers of the OpenXR runtime and compatible CUDA buffer to provide interoperability between ClaraViz and OpenXR</li> </ul>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrTransformOp/","title":"User interface Control Operator","text":"<p> Authors: Magic Leap Team (Magic Leap) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>The <code>XrTransformControlOp</code> maintains the state of the mixed reality user interface for the volumetric rendering application. It consumes controller events and produces user interface state structures as well as parameters for the volume rendering operator. The operator is application specific and will grow over time to include additional user interface widgets.</p>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrTransformOp/#holoscanopenxrxrtransformcontrolop","title":"<code>holoscan::openxr::XrTransformControlOp</code>","text":"","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrTransformOp/#inputs","title":"Inputs","text":"<p>Controller state - <code>trigger_click</code>: trigger button state   - type: <code>bool</code> - <code>shoulder_click</code>: shoulder button state   - type: <code>bool</code> - <code>trackpad_touch</code>: trackpad state   - type: <code>bool</code> - <code>trackpad</code>: trackpad values [x,y]   - type: <code>std::array&lt;float, 2&gt;</code> - <code>aim_pose</code>: world space pose of the controller tip   - type: <code>nvidia::gxf::Pose3D</code></p> <p>Device state - <code>head_pose</code>: world space head pose of the device   - type: <code>nvidia::gxf::Pose3D</code></p> <p>Volume state - <code>extent</code>: size of bounding box containing volume   - type: <code>std::array&lt;float, 3&gt;</code></p>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrTransformOp/#outputs","title":"Outputs","text":"<p>User interface widget state structures - <code>ux_box</code>: bounding box state structure   - type: <code>UxBoundingBox</code> - <code>ux_cursor</code>: cursor state structure   - type: <code>UxCursor</code></p> <p>Volume rendering parameters - <code>volume_pose</code>: world pose of dataset    - type: <code>nvidia::gxf::Pose3D</code> - <code>crop_box</code>: axis aligned cropping planes in local coordinates   - type: <code>std::array&lt;nvidia::gxf::Vector2f, 3&gt;</code></p>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrTransformOp/XrTransformControlOp/","title":"User interface Control Operator","text":"<p> Authors: Magic Leap Team (Magic Leap) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>The <code>XrTransformControlOp</code> maintains the state of the mixed reality user interface for the volumetric rendering application. It consumes controller events and produces user interface state structures as well as parameters for the volume rendering operator. The operator is application specific and will grow over time to include additional user interface widgets.</p>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrTransformOp/XrTransformControlOp/#holoscanopenxrxrtransformcontrolop","title":"<code>holoscan::openxr::XrTransformControlOp</code>","text":"","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrTransformOp/XrTransformControlOp/#inputs","title":"Inputs","text":"<p>Controller state - <code>trigger_click</code>: trigger button state   - type: <code>bool</code> - <code>shoulder_click</code>: shoulder button state   - type: <code>bool</code> - <code>trackpad_touch</code>: trackpad state   - type: <code>bool</code> - <code>trackpad</code>: trackpad values [x,y]   - type: <code>std::array&lt;float, 2&gt;</code> - <code>aim_pose</code>: world space pose of the controller tip   - type: <code>nvidia::gxf::Pose3D</code></p> <p>Device state - <code>head_pose</code>: world space head pose of the device   - type: <code>nvidia::gxf::Pose3D</code></p> <p>Volume state - <code>extent</code>: size of bounding box containing volume   - type: <code>std::array&lt;float, 3&gt;</code></p>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrTransformOp/XrTransformControlOp/#outputs","title":"Outputs","text":"<p>User interface widget state structures - <code>ux_box</code>: bounding box state structure   - type: <code>UxBoundingBox</code> - <code>ux_cursor</code>: cursor state structure   - type: <code>UxCursor</code></p> <p>Volume rendering parameters - <code>volume_pose</code>: world pose of dataset    - type: <code>nvidia::gxf::Pose3D</code> - <code>crop_box</code>: axis aligned cropping planes in local coordinates   - type: <code>std::array&lt;nvidia::gxf::Vector2f, 3&gt;</code></p>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrTransformOp/XrTransformRenderOp/","title":"User interface Render Operator","text":"<p> Authors: Magic Leap Team (Magic Leap) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>The <code>XrTransformRenderOp</code> renders the mixed reality user interface of the volumetric rendering application. It consumes interface widget state structures as well as render buffers into which to overlay the interface widgets. The operator is application specific and will grow over time to include additional user interface widgets.</p>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrTransformOp/XrTransformRenderOp/#holoscanopenxrxrtransformrenderop","title":"<code>holoscan::openxr::XrTransformRenderOp</code>","text":"","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrTransformOp/XrTransformRenderOp/#parameters","title":"Parameters","text":"<ul> <li><code>display_width</code>: pixel height of display</li> <li>type: <code>int</code></li> <li><code>display_height</code>: pixel width of display</li> <li>type: <code>int</code></li> </ul>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrTransformOp/XrTransformRenderOp/#inputs","title":"Inputs","text":"<p>Camera state for stereo view - <code>left_camera_pose</code>: world space pose of the left eye   - type: <code>nvidia::gxf::Pose3D</code> - <code>right_camera_pose</code>: world space pose of the right eye   - type: <code>nvidia::gxf::Pose3D</code> - <code>left_camera_model</code>: camera model for the left eye   - type: <code>nvidia::gxf::CameraModel</code> - <code>right_camera_model</code>: camera model for the right eye   - type: <code>nvidia::gxf::CameraModel</code> - <code>depth_range</code>: depth range</p> <p>User interface widget state structures - <code>ux_box</code>: bounding box state structure   - type: <code>UxBoundingBox</code> - <code>ux_cursor</code>: cursor state structure   - type: <code>UxCursor</code></p> <p>Render buffers to be populated - <code>Collor buffer_in</code>: color buffer   - type: <code>holoscan::gxf::VideoBuffer</code> - <code>Depth buffer_in</code>: depth buffer   - type: <code>holoscan::gxf::VideoBuffer</code></p>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrTransformOp/XrTransformRenderOp/#outputs","title":"Outputs","text":"<p>Render buffers including interface widgets - <code>color_buffer_out</code>: color buffer   - type: <code>holoscan::gxf::VideoBuffer</code> - <code>depth_buffer_out</code>: depth buffer   - type: <code>holoscan::gxf::VideoBuffer</code></p>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/utils/xr_hello_holoscan/","title":"XR Demo","text":"<p> Authors: Andreas Heumann (NVIDIA), Connor Smith (NVIDIA), Cristiana Dinea (NVIDIA), Tom Birdsong (NVIDIA), Antonio Ospite (Magic Leap), Jiwen Cai (Magic Leap), Jochen Stier (Magic Leap), Korcan Hussein (Magic Leap), Robbie Bridgewater (Magic Leap) Supported platforms: x86_64, aarch64 Language: C++ Last modified: July 1, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>This application provides a simple scene demonstrating mixed reality viewing with Holoscan SDK. It can be served as a testing tool for validating basic XR features.</p> <p></p>","tags":["Extended Reality","Visualization","Holoviz","Stereo Vision"]},{"location":"applications/volume_rendering_xr/utils/xr_hello_holoscan/#background","title":"Background","text":"<p>We created this test application as part of a collaboration between the Magic Leap and NVIDIA Holoscan teams. See the <code>volume_rendering_xr</code> application for a demonstration of medical viewing in XR with Holoscan SDK.</p>","tags":["Extended Reality","Visualization","Holoviz","Stereo Vision"]},{"location":"applications/volume_rendering_xr/utils/xr_hello_holoscan/#description","title":"Description","text":"<p>The application provides a blueprint for how to set up a mixed reality scene for viewing with Holoscan SDK and HoloHub components.</p> <p>The mixed reality demonstration scene includes: - Static components such as scene axes and cube primitives; - A primitive overlay on the tracked controller input; - A static UI showcasing sensor inputs and tracking.</p>","tags":["Extended Reality","Visualization","Holoviz","Stereo Vision"]},{"location":"applications/volume_rendering_xr/utils/xr_hello_holoscan/#getting-started","title":"Getting Started","text":"<p>Refer to the <code>volume_rendering_xr</code> README for details on hardware, firmware, and software prerequisites.</p> <p>This utility is part of the <code>volume_rendering_xr</code> application suite. </p>","tags":["Extended Reality","Visualization","Holoviz","Stereo Vision"]},{"location":"applications/volume_rendering_xr/utils/xr_hello_holoscan/#running-the-test-utility","title":"Running the Test Utility","text":"<p>Run the following command in the top-level HoloHub folder to build and run the host application:</p> <pre><code>./holohub run volume_rendering_xr --run-args=\"xr_hello_holoscan\"\n</code></pre> <p>Note that without specifying the extra arguments, it will launch the main volume rendering application by default.</p> <p>To pair your Magic Leap 2 device with the host, open the QR Reader application in the ML2 headset and scan the QR code printed in console output on the host machine.</p>","tags":["Extended Reality","Visualization","Holoviz","Stereo Vision"]},{"location":"applications/volume_rendering_xr/utils/xr_hello_holoscan/#frequently-asked-questions","title":"Frequently Asked Questions","text":"","tags":["Extended Reality","Visualization","Holoviz","Stereo Vision"]},{"location":"applications/volume_rendering_xr/utils/xr_hello_holoscan/#can-i-test-the-application-without-a-magic-leap-2-device","title":"Can I test the application without a Magic Leap 2 device?","text":"<p>Yes, a debug GUI not requiring a headset is installed inside the application container by default. Follow the steps below to launch the debug GUI and run the application:</p> <pre><code># Build and launch the container\n./holohub run-container xr_hello_holoscan\n\n# Enable the debug GUI\nexport ML_START_OPTIONS=\"debug\"\n\n# Build and run the application\n./holohub run xr_hello_holoscan\n</code></pre> <p>The ImGui debug application will launch. Click and slide the position entries to adjust your view of the scene.</p> <p></p>","tags":["Extended Reality","Visualization","Holoviz","Stereo Vision"]},{"location":"applications/volume_rendering_xr/utils/xr_hello_holoscan/xr_basic_render/","title":"XR Basic Rendering Operator","text":"<p> Authors: Magic Leap Team (Magic Leap), NVIDIA Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>The <code>XrBasicRenderOp</code> defines and renders a basic scene to demonstrate OpenXR compatibility. It provides visuals for static primitives, controller tracking, and an ImGui window.</p> <p>See the <code>xr_hello_holoscan</code> application for a complete example demonstrating <code>XrBasicRenderOp</code> in a Holoscan SDK pipeline.</p>","tags":["Extended Reality"]},{"location":"applications/vpi_stereo/","title":"VPI Stereo Vision","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.4.0 Tested Holoscan SDK versions: 2.4.0, 2.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p> </p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Camera","Depth","Visualization"]},{"location":"applications/vpi_stereo/#overview","title":"Overview","text":"<p>Demo pipeline showing stereo disparity estimation using the Vision Programming Interface VPI.</p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Camera","Depth","Visualization"]},{"location":"applications/vpi_stereo/#description","title":"Description","text":"<p>This pipeline takes video from a stereo camera and uses VPI's stereo disparity estimation algorithm. The input video and estimate disparity map are displayed using Holoviz.</p> <p>The application will select accelerator backends if available (OFA, PVA and VIC). This demonstrates how VPI can be used to offload stereo disparity processing from the GPU on supported devices such as NVIDIA IGX, AGX, or NX platforms.</p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Camera","Depth","Visualization"]},{"location":"applications/vpi_stereo/#input-video","title":"Input Video","text":"<p>Requires a V4L2 stereo camera, or recorded stereo video, and matching calibration data. By default, the application will share the same source data as stereo_vision.</p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Camera","Depth","Visualization"]},{"location":"applications/vpi_stereo/#requirements","title":"Requirements","text":"<p>This demo requires VPI version 3.2 or greater. The included Dockerfile will install VPI and its dependencies for either an amd64 target (with discrete NVIDIA GPU), or arm64 target (NVIDIA IGX, AGX, or NX).</p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Camera","Depth","Visualization"]},{"location":"applications/vpi_stereo/#build-and-run-instructions","title":"Build and Run Instructions","text":"<p>Using default video source (same as stereo_vision application): <pre><code>./holohub run vpi_stereo\n</code></pre> Using a v4l2 video source (live camera or loopback device): <pre><code>./holohub run vpi_stereo --run-args=\"--source v4l2\"\n</code></pre></p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Camera","Depth","Visualization"]},{"location":"applications/webrtc_holoviz_server/","title":"WebRTC Holoviz Server","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p></p> <p>This app generates video frames with user specified content using Holoviz and sends it to a browser using WebRTC. The goal is to show how to remote control operators and view the output of a Holoscan pipeline.</p> <p>The app starts a web server, the pipeline starts when a browser is connected to the web server and the <code>Start</code> button is pressed. The pipeline stops when the <code>Stop</code> button is pressed.</p> <p>The web page has user inputs for specifying text and for the speed the text moves across the screen.</p> <pre><code>flowchart LR\n    subgraph Server\n        GeometryGenerationOp --&gt; HolovizOp\n        HolovizOp --&gt; FormatConverterOp\n        FormatConverterOp --&gt; WebRTCServerOp\n        WebServer\n    end\n    subgraph Client\n        WebServer &lt;--&gt; Browser\n        WebRTCServerOp &lt;--&gt; Browser\n    end\n</code></pre> <p>NOTE: When using VPN there might be a delay of several seconds between pressing the <code>Start</code> button and the first video frames are display. The reason for this is that the STUN server <code>stun.l.google.com:19302</code> used by default might not be available when VPN is active and the missing support for Trickle ICE in the used aiortc library. Trickle ICE is an optimization to speed up connection establishment. Normally, possible connections paths are tested one after another. If connections time out this is blocking the whole process. Trickle ICE checks each possible connection path in parallel so the connection timing out won't block the process.</p>","tags":["Computer Vision and Perception","Networking and Distributed Computing","WebRTC","Visualization","Holoviz"]},{"location":"applications/webrtc_holoviz_server/#prerequisites","title":"Prerequisites","text":"<p>The app is using AIOHTTP for the web server and AIORTC for WebRTC. Install both using pip.</p> <pre><code>pip install aiohttp aiortc\n</code></pre>","tags":["Computer Vision and Perception","Networking and Distributed Computing","WebRTC","Visualization","Holoviz"]},{"location":"applications/webrtc_holoviz_server/#run-instructions","title":"Run Instructions","text":"<p>Run the command:</p> <pre><code>./holohub run webrtc_holoviz_server --local --no-local-build\n</code></pre> <p>On the same machine open a browser and connect to <code>127.0.0.1:8080</code>. You can also connect from a different machine by connecting to the IP address the app is running on.</p> <p>Press the <code>Start</code> button. Video frames are displayed. To stop, press the <code>Stop</code> button. Pressing <code>Start</code> again will continue the video.</p> <p>Change the text input and the speed slider to control the generated video frame content.</p>","tags":["Computer Vision and Perception","Networking and Distributed Computing","WebRTC","Visualization","Holoviz"]},{"location":"applications/webrtc_holoviz_server/#command-line-arguments","title":"Command Line Arguments","text":"<pre><code>usage: webrtc_server.py [-h] [--cert-file CERT_FILE] [--key-file KEY_FILE] [--host HOST] [--port PORT] [--verbose VERBOSE] [--ice-server ICE_SERVER]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --cert-file CERT_FILE\n                        SSL certificate file (for HTTPS)\n  --key-file KEY_FILE   SSL key file (for HTTPS)\n  --host HOST           Host for HTTP server (default: 0.0.0.0)\n  --port PORT           Port for HTTP server (default: 8080)\n  --verbose, -v\n  --ice-server ICE_SERVER\n                        ICE server config in the form of `turn:&lt;ip&gt;:&lt;port&gt;[&lt;username&gt;:&lt;password&gt;]` or `stun:&lt;ip&gt;:&lt;port&gt;`. This option can be specified multiple times to add multiple ICE servers.\n</code></pre>","tags":["Computer Vision and Perception","Networking and Distributed Computing","WebRTC","Visualization","Holoviz"]},{"location":"applications/webrtc_holoviz_server/#running-with-turn-server","title":"Running With TURN server","text":"<p>A TURN server may be needed if you're running in a containerized environment without host networking (e.g. Kubernetes or Docker). Here are some basic steps to run this example with a TURN server.</p> <p>Run the TURN server in the same machine that you're running the app on.</p> <p>Note: It is strongly recommended to run the TURN server with docker network=host for best performance</p> <pre><code># This is the external IP address of the machine running the TURN server\nexport TURN_SERVER_EXTERNAL_IP=\"&lt;ip&gt;\"\n\n# Command below use admin:admin as the username and password as an example\ndocker run -d --rm --network=host instrumentisto/coturn \\\n    -n --log-file=stdout \\\n    --external-ip=$TURN_SERVER_EXTERNAL_IP \\\n    --listening-ip=$TURN_SERVER_EXTERNAL_IP \\\n    --lt-cred-mech --fingerprint \\\n    --user=admin:admin \\\n    --no-multicast-peers \\\n    --verbose \\\n    --realm=default.realm.org\n</code></pre> <p>Then you can pass in the TURN server config into the app</p> <pre><code>python webrtc_server.py --ice-server \"turn:&lt;ip&gt;:3478[admin:admin]\"\n</code></pre> <p>This will enable you to access the webRTC browser application from different machines.</p>","tags":["Computer Vision and Perception","Networking and Distributed Computing","WebRTC","Visualization","Holoviz"]},{"location":"applications/webrtc_video_client/","title":"WebRTC Video Client","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p></p> <p>This app receives video frames from a web cam connected to a browser and display them on the screen.</p> <p>The app starts a web server, the pipeline starts when a browser is connected to the web server and the <code>Start</code> button is pressed. The pipeline stops when the <code>Stop</code> button is pressed.</p> <p>The video resolution and video codec can be selected in browser.</p> <pre><code>flowchart LR\n    subgraph Server\n        WebRTCClientOp --&gt; HolovizOp\n        WebServer\n    end\n    subgraph Client\n        Webcam --&gt; Browser\n        Browser &lt;--&gt; WebRTCClientOp\n        Browser &lt;--&gt; WebServer\n    end\n</code></pre> <p>NOTE: When using VPN there might be a delay of several seconds between pressing the <code>Start</code> button and the first video frames are display. The reason for this is that the STUN server <code>stun.l.google.com:19302</code> used by default might not be available when VPN is active and the missing support for Trickle ICE in the used aiortc library. Trickle ICE is an optimization to speed up connection establishment. Normally, possible connections paths are tested one after another. If connections time out this is blocking the whole process. Trickle ICE checks each possible connection path in parallel so the connection timing out won't block the process.</p>","tags":["Networking and Distributed Computing","Video","SDP Exchange","WebRTC","Camera","Visualization"]},{"location":"applications/webrtc_video_client/#prerequisites","title":"Prerequisites","text":"<p>The app is using AIOHTTP for the web server and AIORTC for WebRTC. Install both using pip.</p> <pre><code>pip install aiohttp aiortc\n</code></pre>","tags":["Networking and Distributed Computing","Video","SDP Exchange","WebRTC","Camera","Visualization"]},{"location":"applications/webrtc_video_client/#run-instructions","title":"Run Instructions","text":"<p>Run the command:</p> <pre><code>./holohub run webrtc_video_client --local --no-local-build\n</code></pre> <p>On the same machine open a browser and connect to <code>127.0.0.1:8080</code>.</p> <p>Select the video resolution and codec or keep the defaults.</p> <p>Press the <code>Start</code> button. Video frames are displayed. To stop, press the <code>Stop</code> button. Pressing <code>Start</code> again will continue the video.</p> <p>You can also connect from a different machine by connecting to the IP address the app is running on. Chrome disables features such as getUserMedia when it comes from an unsecured origin. <code>http://localhost</code> is considered as a secure origin by default, however if you use an origin that does not have an SSL/TLS certificate then Chrome will consider the origin as unsecured and disable getUserMedia.</p> <p>Solutions</p> <ul> <li>Create an self-signed SSL/TLS certificate with <code>openssl req -new -newkey rsa:4096 -x509 -sha256 -days 365 -nodes -out MyCertificate.crt -keyout MyKey.key</code>. Pass the generated files to the <code>webrtc_client</code> using the <code>--cert-file</code> and <code>--key-file</code> arguments. Connect the browser to <code>https://{YOUR HOST IP}:8080</code>.</li> <li>Go to chrome://flags, search for the flag <code>unsafely-treat-insecure-origin-as-secure</code>, enter the origin you want to treat as secure such as <code>http://{YOUR HOST IP}:8080</code>, enable the feature and relaunch the browser.</li> </ul>","tags":["Networking and Distributed Computing","Video","SDP Exchange","WebRTC","Camera","Visualization"]},{"location":"applications/webrtc_video_client/#command-line-arguments","title":"Command Line Arguments","text":"<pre><code>usage: webrtc_client.py [-h] [--cert-file CERT_FILE] [--key-file KEY_FILE] [--host HOST] [--port PORT] [--verbose VERBOSE]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --cert-file CERT_FILE\n                        SSL certificate file (for HTTPS)\n  --key-file KEY_FILE   SSL key file (for HTTPS)\n  --host HOST           Host for HTTP server (default: 0.0.0.0)\n  --port PORT           Port for HTTP server (default: 8080)\n  --verbose, -v\n</code></pre>","tags":["Networking and Distributed Computing","Video","SDP Exchange","WebRTC","Camera","Visualization"]},{"location":"applications/webrtc_video_server/","title":"WebRTC Video Server","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p></p> <p>This app reads video frames from a file and sends it to a browser using WebRTC.</p> <p>The app starts a web server, the pipeline starts when a browser is connected to the web server and the <code>Start</code> button is pressed. The pipeline stops when the <code>Stop</code> button is pressed.</p> <pre><code>flowchart LR\n    subgraph Server\n        A[(VideoFile)] --&gt; VideoStreamReplayerOp\n        VideoStreamReplayerOp --&gt; FormatConverterOp\n        FormatConverterOp --&gt; WebRTCServerOp\n        WebServer\n    end\n    subgraph Client\n        WebServer &lt;--&gt; Browser\n        WebRTCServerOp &lt;--&gt; Browser\n    end\n</code></pre> <p>NOTE: When using VPN there might be a delay of several seconds between pressing the <code>Start</code> button and the first video frames are display. The reason for this is that the STUN server <code>stun.l.google.com:19302</code> used by default might not be available when VPN is active and the missing support for Trickle ICE in the used aiortc library. Trickle ICE is an optimization to speed up connection establishment. Normally, possible connections paths are tested one after another. If connections time out this is blocking the whole process. Trickle ICE checks each possible connection path in parallel so the connection timing out won't block the process.</p>","tags":["Networking and Distributed Computing","Video","WebRTC","Communications"]},{"location":"applications/webrtc_video_server/#prerequisites","title":"Prerequisites","text":"<p>The app is using AIOHTTP for the web server and AIORTC for WebRTC. Install both using pip.</p> <pre><code>pip install aiohttp aiortc\n</code></pre>","tags":["Networking and Distributed Computing","Video","WebRTC","Communications"]},{"location":"applications/webrtc_video_server/#run-instructions","title":"Run Instructions","text":"<p>Run the command:</p> <pre><code>./holohub run webrtc_video_server --local --no-local-build\n</code></pre> <p>On the same machine open a browser and connect to <code>127.0.0.1:8080</code>. You can also connect from a different machine by connecting to the IP address the app is running on.</p> <p>Press the <code>Start</code> button. Video frames are displayed. To stop, press the <code>Stop</code> button. Pressing <code>Start</code> again will continue the video.</p>","tags":["Networking and Distributed Computing","Video","WebRTC","Communications"]},{"location":"applications/webrtc_video_server/#command-line-arguments","title":"Command Line Arguments","text":"<pre><code>usage: webrtc_server.py [-h] [--cert-file CERT_FILE] [--key-file KEY_FILE] [--host HOST] [--port PORT] [--verbose] [--ice-server ICE_SERVER]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --cert-file CERT_FILE\n                        SSL certificate file (for HTTPS)\n  --key-file KEY_FILE   SSL key file (for HTTPS)\n  --host HOST           Host for HTTP server (default: 0.0.0.0)\n  --port PORT           Port for HTTP server (default: 8080)\n  --verbose, -v\n  --ice-server ICE_SERVER\n                        ICE server config in the form of `turn:&lt;ip&gt;:&lt;port&gt;[&lt;username&gt;:&lt;password&gt;]` or `stun:&lt;ip&gt;:&lt;port&gt;`. This option can be specified multiple times to add multiple ICE servers.\n</code></pre>","tags":["Networking and Distributed Computing","Video","WebRTC","Communications"]},{"location":"applications/webrtc_video_server/#running-with-turn-server","title":"Running With TURN server","text":"<p>A TURN server may be needed if you're running in a containerized environment without host networking (e.g. Kubernetes or Docker). Here are some basic steps to run this example with a TURN server.</p> <p>Run the TURN server in the same machine that you're running the app on.</p> <p>Note: It is strongly recommended to run the TURN server with docker network=host for best performance</p> <pre><code># This is the external IP address of the machine running the TURN server\nexport TURN_SERVER_EXTERNAL_IP=\"&lt;ip&gt;\"\n\n# Command below use admin:admin as the username and password as an example\ndocker run -d --rm --network=host instrumentisto/coturn \\\n    -n --log-file=stdout \\\n    --external-ip=$TURN_SERVER_EXTERNAL_IP \\\n    --listening-ip=$TURN_SERVER_EXTERNAL_IP \\\n    --lt-cred-mech --fingerprint \\\n    --user=admin:admin \\\n    --no-multicast-peers \\\n    --verbose \\\n    --realm=default.realm.org\n</code></pre> <p>Then you can pass in the TURN server config into the app</p> <pre><code>python webrtc_server.py --ice-server \"turn:&lt;ip&gt;:3478[admin:admin]\"\n</code></pre> <p>This will enable you to access the webRTC browser application from different machines.</p>","tags":["Networking and Distributed Computing","Video","WebRTC","Communications"]},{"location":"applications/xr_holoviz/","title":"XR + Holoviz","text":"<p> Authors: Connor Smith (NVIDIA), Rafael Wiltz (NVIDIA), Mimi Liao (NVIDIA) Supported platforms: x86_64 Language: C++ Last modified: July 1, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 3.1.0 Tested Holoscan SDK versions: 3.1.0 Contribution metric: Level 4 - Experimental</p> <p>This application demonstrates the integration of Holoscan-XR with Holoviz for extended reality visualization.</p>","tags":["Extended Reality","Visualization","Rendering","Stereo Vision"]},{"location":"applications/xr_holoviz/#quick-start-guide","title":"Quick Start Guide","text":"","tags":["Extended Reality","Visualization","Rendering","Stereo Vision"]},{"location":"applications/xr_holoviz/#1-build-the-docker-image","title":"1. Build the Docker Image","text":"<p>Run the following command in the top-level HoloHub directory: <pre><code>./holohub build-container xr_holoviz\n</code></pre></p>","tags":["Extended Reality","Visualization","Rendering","Stereo Vision"]},{"location":"applications/xr_holoviz/#2-run-the-application","title":"2. Run the application","text":"","tags":["Extended Reality","Visualization","Rendering","Stereo Vision"]},{"location":"applications/xr_holoviz/#terminal-1-launch-container-and-start-monado-service","title":"Terminal 1: Launch Container and Start Monado Service","text":"<p><pre><code># Launch the container and start the Monado service\n./holohub run-container --img holohub:xr_holoviz -- monado-service\n</code></pre> Keep this terminal open and running.</p>","tags":["Extended Reality","Visualization","Rendering","Stereo Vision"]},{"location":"applications/xr_holoviz/#terminal-2-build-and-run-the-application","title":"Terminal 2: Build and Run the Application","text":"<pre><code># Enter the same container (replace &lt;container_id&gt; with actual ID from 'docker ps')\ndocker exec -it &lt;container_id&gt; bash\n\n# Build and run the application\n./holohub run xr_holoviz\n</code></pre>","tags":["Extended Reality","Visualization","Rendering","Stereo Vision"]},{"location":"applications/xr_holoviz/#set-up-width-and-height-correctly","title":"Set up width and height correctly","text":"<p>The width and height of the HolovizOp should be set to the width and height of the XR display, which can only be obtained during runtime. To set the width and height correctly, we need to:</p> <ol> <li>Run the application</li> <li>Find the log showing  <pre><code>XrCompositionLayerManager initialized width: XXX height: YYY\n</code></pre></li> <li>Copy the width and height</li> <li>Set the width and height of the HolovizOp in <code>config.yaml</code></li> </ol>","tags":["Extended Reality","Visualization","Rendering","Stereo Vision"]},{"location":"applications/yolo_model_deployment/","title":"Yolo Object Detection","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 2 - Trusted</p> <p>This project is aiming to provide basic guidance to deploy Yolo-based model to Holoscan SDK as \"Bring Your Own Model\"</p>","tags":["Computer Vision and Perception","Video","Tensor Optimization","YOLO Detection","Visualization","Camera"]},{"location":"applications/yolo_model_deployment/#model","title":"Model","text":"<ul> <li>Yolo v8 model: https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt</li> <li>Yolo v8 export repository: https://github.com/triple-Mu/YOLOv8-TensorRT</li> </ul> <p>In this application example, we use the YOLOv8s model, which is converted to ONNX format using the repository mentioned above. Note that if you provide your own ONNX model, ensure it includes the EfficientNMS_TRT layer. You can verify it using Netron. Additionally, we employ the <code>graph_surgeon.py</code> script to modify the input shape. For more details on this script, refer to graph_surgeonpy.</p> <p>The detailed process is documented in the <code>CMakeLists.txt</code> file.</p>","tags":["Computer Vision and Perception","Video","Tensor Optimization","YOLO Detection","Visualization","Camera"]},{"location":"applications/yolo_model_deployment/#input-source","title":"Input Source","text":"<p>This app currently supports two input options:</p> <ol> <li>v4l2 compatible input device</li> <li>Pre-recorded video</li> </ol>","tags":["Computer Vision and Perception","Video","Tensor Optimization","YOLO Detection","Visualization","Camera"]},{"location":"applications/yolo_model_deployment/#data","title":"Data","text":"<p>This application downloads a pre-recorded video from Pexels when the application is built.  Please review the license terms from Pexels.</p>","tags":["Computer Vision and Perception","Video","Tensor Optimization","YOLO Detection","Visualization","Camera"]},{"location":"applications/yolo_model_deployment/#run","title":"Run","text":"<p>Build and launch container. Note that this will use a v4l2 input source as default.</p> <pre><code>./holohub run yolo_model_deployment\n</code></pre>","tags":["Computer Vision and Perception","Video","Tensor Optimization","YOLO Detection","Visualization","Camera"]},{"location":"applications/yolo_model_deployment/#video-replayer-support","title":"Video Replayer Support","text":"<p>If you don't have a v4l2 compatible device plugged in, you can also run this application on a pre-recorded video. To launch the application using the Video Stream Replayer as the input source, run:</p> <pre><code>./holohub run yolo_model_deployment --run-args=\"--source replayer\"\n</code></pre>","tags":["Computer Vision and Perception","Video","Tensor Optimization","YOLO Detection","Visualization","Camera"]},{"location":"applications/yolo_model_deployment/#configuration","title":"Configuration","text":"<p>For application configuration, please refer to the <code>yolo_detection.yaml</code>.</p>","tags":["Computer Vision and Perception","Video","Tensor Optimization","YOLO Detection","Visualization","Camera"]},{"location":"benchmarks/","title":"Benchmarks","text":"<p>The HoloHub benchmark resources are a critical resource for developers aiming to optimize and validate the performance of their AI sensor processing applications built with the Holoscan SDK. </p> <p>Holohub provides a collection of benchmarking tools and reference implementations designed to measure and compare the efficiency, speed, and scalability of various Holoscan workflows. By offering detailed performance metrics and best practices, these benchmarks help developers identify bottlenecks and optimize their applications for high performance and low latency. Whether you are focusing on real-time data processing, model inference, or end-to-end workflow performance, the benchmarks on this page provide valuable insights and guidelines to ensure your applications meet the highest standards.</p>"},{"location":"benchmarks/exclusive_display/","title":"Exclusive Display Benchmark","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: aarch64 Last modified: May 13, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This document investigates the performance of the exclusive display feature in <code>holoviz</code> operator of Holoscan SDK.</p>","tags":["Benchmarking","Visualization"]},{"location":"benchmarks/exclusive_display/#introduction","title":"Introduction","text":"<p>By default, Holoscan SDK application uses the <code>holoviz</code> operator for display, and it uses a windowing system like X11. X11 is a compositor which combines the display requests from different applications and renders them on the screen. In <code>exclusive_display</code> mode, <code>holoviz</code> turns off the default display manager and directly renders the output in full-screen mode. This mode is obviously more performant as a single application exclusively uses the monitor to display its output.</p> <p>In this document, we provide the performance measurements of the <code>exclusive_display</code> mode and compare it with the default mode that uses <code>X11</code>. We execute the endoscopy tool tracking in these two display modes and measure its maximum and average end-to-end latency. In addition, we also run a number of \"headless\" applications simultaneously. These headless applications are executing both AI workloads and graphics processing but do not utilize the screen to display any output. They are representative of background workloads. Usually, these background workloads run alongside a primary display application which, in this case, is the endoscopy tool tracking application using display in either <code>exclusive</code> or <code>default</code> mode.</p>","tags":["Benchmarking","Visualization"]},{"location":"benchmarks/exclusive_display/#platform","title":"Platform","text":"<p>The experiments are conducted with Holoscan v2.1 container on IGX Orin with RTX A6000 GPU flashed with IGX SW 1.0.</p>","tags":["Benchmarking","Visualization"]},{"location":"benchmarks/exclusive_display/#results","title":"Results","text":"<p>For the experiment, we use the endoscopy tool tracking application which is using the display monitor for outputs in two modes, as said above. This application is executed with <code>realtime: false</code> for the video stream replayer source, so that the source feeds the frames as fast as possible, without an external frame-rate limitation.</p> <p>For the headless applications, we run the same endoscopy tool application in different process instances but in <code>headless: true</code> mode.</p> <p>In the graphs below, Y-axis shows the end-to-end latency of the endoscopy tool tracking with display. In the X-axis, we vary the number of headless applications from 0 to 11. <code>0</code> means only the endoscopy application with display is running. We do not show any numbers when the latency is more than 200ms.</p>","tags":["Benchmarking","Visualization"]},{"location":"benchmarks/exclusive_display/#maximum-end-to-end-latency","title":"Maximum End-to-end Latency","text":"<p>The maximum end-to-end latency results are given below:</p> <p></p> <p>In the above graph, the maximum end-to-end latency for the default mode increases from 15 ms to 23 ms when the number of background headless applications rises from 0 to 3. For more than 3 background headless applications, the maximum end-to-end latency in default mode is more than 200 ms.</p> <p>The exclusive display mode performs much better than the default mode because of no overhead of the compositor. The maximum end-to-end latency is 20 to 30% lower in presence of up to 3 headless applications. The benefits are more pronounced when the number of background headless applications is more than 3.</p> <p>Despite better performance with exclusive display, the maximum end-to-end latency increases to 51 ms when the number of background headless applications is 11. Therefore, exclusive display mode alone cannot guarantee an upper bound on the latency if the number of applications using the GPU increases.</p>","tags":["Benchmarking","Visualization"]},{"location":"benchmarks/exclusive_display/#average-end-to-end-latency","title":"Average End-to-end Latency","text":"<p>The average end-to-end latency results are given below:</p> <p></p> <p>In the above graph, the average end-to-end latency for the default mode increases from 8 ms to 136 ms when the number of headless applications increases from 0 to 9. For more than 9 headless applications, the average end-to-end latency in default mode is more than 200 ms.</p> <p>The exclusive display mode performs much better than the default mode in average latency as well. Average end-to-end latency is up to 80% lower in exclusive display mode compared to the default mode, for up to 9 simultaneous headless applications. The average latency increases from 8 ms to 29 ms in exclusive mode when the number of headless applications increases from 0 to 11.</p>","tags":["Benchmarking","Visualization"]},{"location":"benchmarks/exclusive_display/#conclusions","title":"Conclusions","text":"<ul> <li>The exclusive display mode provides better average latency and deterministic performance   (maximum latency) than the default mode.</li> <li>Headless applications using the GPU which are running in the background, impact the performance both in default and exclusive display   modes. However, exclusive display mode is more resilient than the default mode to the background   applications using the GPU.</li> <li>Even in the exclusive display mode, the maximum latency, capturing the performance predictability,   increases 4-5x while the background headless applications increase from 0 to 11. Therefore, the exclusive mode does not provide a guarantee on the upper bound of the latency in presence of other GPU workloads.</li> </ul>","tags":["Benchmarking","Visualization"]},{"location":"benchmarks/holoscan_flow_benchmarking/","title":"Holoscan Flow Benchmarking for HoloHub","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Last modified: May 13, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 3 - Developmental</p> <p>Holoscan Flow Benchmarking is a comprehensive performance evaluation tool designed to measure and analyze the execution characteristics of HoloHub and Holoscan applications. It provides detailed insights into operator execution times, data transfer latencies, and overall application performance.</p> <p>For detailed information, refer to:</p> <ul> <li>Holoscan Flow Benchmarking Tutorial (up-to-date)</li> <li>Holoscan Flow Benchmarking Whitepaper</li> </ul> <p>Key Features:</p> <ul> <li>Support for all Holoscan applications (Python support since v1.0)</li> <li>Real-time performance monitoring</li> <li>Detailed latency analysis</li> <li>Visual performance graphs</li> </ul>","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Environment Setup</li> <li>Holohub Docker Container (recommended)</li> <li>Bare-metal Installation</li> <li>Step-by-step Guide to Holoscan Flow Benchmarking</li> <li>1. Build Applications</li> <li>2. Run Benchmarks</li> <li>3. Analyze Results</li> <li>Generate Application Graph with Latency Numbers</li> <li>Monitor Application Performance in Real-time</li> </ul>","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#environment-setup","title":"Environment Setup","text":"","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#holohub-docker-container-recommended","title":"Holohub Docker Container (recommended)","text":"<p>All dependencies are automatically managed by the Holohub Docker container. You can simply run:</p> <pre><code>./holohub run-container [&lt;application_name&gt;]\n</code></pre>","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#bare-metal-installation","title":"Bare-metal Installation","text":"<p>If not using the Holohub Docker container, apart from the holoscan and application's specific dependencies, additional Python packages should be installed:</p> <pre><code>pip install -r benchmarks/holoscan_flow_benchmarking/requirements.txt\n</code></pre> <p>These python dependencies include:</p> <ul> <li>numpy: Data processing</li> <li>matplotlib: Graph generation</li> <li>nvitop: GPU monitoring</li> <li>argparse: CLI handling</li> <li>pydot: Graph creation</li> <li>xdot: Graph visualization</li> </ul> <p>Note: <code>xdot</code> has additional system dependencies. See xdot requirements.</p>","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#step-by-step-guide-to-holoscan-flow-benchmarking","title":"Step-by-step Guide to Holoscan Flow Benchmarking","text":"","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#1-build-applications","title":"1. Build Applications","text":"","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#automatic-build-with-benchmarking","title":"Automatic Build with Benchmarking","text":"<pre><code>./holohub build &lt;application_name&gt; [options] --benchmark\n</code></pre> <p>This command:</p> <ul> <li>Patches the application source</li> <li>Builds with benchmarking enabled</li> <li>Automatically restores source files after build</li> </ul> <p>For example: <pre><code>./holohub build endoscopy_tool_tracking --benchmark --language cpp\n</code></pre></p>","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#manual-patching-only-if-need-to","title":"Manual Patching [only if need to]","text":"<pre><code># Apply patches\n./benchmarks/holoscan_flow_benchmarking/patch_application.sh &lt;app_directory&gt;\n\n# Example: Patch endoscopy tool tracking\n./benchmarks/holoscan_flow_benchmarking/patch_application.sh applications/endoscopy_tool_tracking\n\n# Restore original files when done\n./benchmarks/holoscan_flow_benchmarking/restore_application.sh &lt;app_directory&gt;\n</code></pre> <p>Note: Source files are backed up as <code>*.bak</code> during patching.</p>","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#important-notes","title":"Important notes","text":"<ul> <li>Verify the application runs correctly after building and before proceeding with performance evaluation.   For example, run the app <code>./holohub run endoscopy_tool_tracking --language python</code> (append <code>--local</code> if you are using the bare-metal installation)</li> <li>For applications using TensorRT, run once to generate engine files (e.g., for the endoscopy tool tracking application).</li> <li>See patch_application.sh and restore_application.sh for more details about the patching process.</li> </ul>","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#2-run-benchmarks","title":"2. Run Benchmarks","text":"<pre><code>python benchmarks/holoscan_flow_benchmarking/benchmark.py -a &lt;app_name&gt; [options]\n</code></pre> <p>Key Options:</p> <ul> <li><code>-a, --app</code>: Application name</li> <li><code>-r, --runs</code>: Number of runs</li> <li><code>-i, --instances</code>: Instances per run</li> <li><code>-m, --max-frames</code>: Number of frames to process</li> <li><code>--sched</code>: Scheduler type</li> <li><code>-d, --directory</code>: Output directory</li> <li><code>--run-command</code>: Custom run command (if needed)</li> </ul> <p>For a complete list of arguments, run:</p> <pre><code>python benchmarks/holoscan_flow_benchmarking/benchmark.py -h\n</code></pre> <p>Example:</p> <pre><code># Run endoscopy tool tracking benchmark\npython benchmarks/holoscan_flow_benchmarking/benchmark.py \\\n    -a endoscopy_tool_tracking \\\n    -r 3 -i 3 -m 200 \\\n    --sched greedy \\\n    -d myoutputs\n</code></pre> <p>Output Files:</p> <ul> <li>Data flow logs: <code>logger_&lt;scheduler&gt;_&lt;run&gt;_&lt;instance&gt;.log</code></li> <li>GPU utilization: <code>gpu_utilization_&lt;scheduler&gt;_&lt;run&gt;.csv</code></li> </ul>","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#3-analyze-results","title":"3. Analyze Results","text":"","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#31-basic-analysis","title":"3.1 Basic Analysis","text":"<pre><code>python benchmarks/holoscan_flow_benchmarking/analyze.py \\\n    -g myoutputs/logger_greedy_* MyCustomGroup \\\n    -m -a  # Show max and average latencies\n</code></pre>","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#32-generate-cdf-plot","title":"3.2 Generate CDF Plot","text":"<pre><code>python benchmarks/holoscan_flow_benchmarking/analyze.py \\\n    --draw-cdf https://github.com/nvidia-holoscan/holohub/blob/main/benchmarks/holoscan_flow_benchmarking/single_path_cdf.png?raw=true \\\n    -g myoutputs/logger_greedy_* MyCustomGroup \\\n    --no-display-graphs\n</code></pre>","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#33-historical-analysis","title":"3.3 Historical Analysis","text":"<pre><code>python bar_plot_avg_datewise.py \\\n    avg_values_2023-10-{19,20,21}.csv \\\n    stddev_values_2023-10-{19,20,21}.csv\n</code></pre>","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#generate-application-graph-with-latency-numbers","title":"Generate Application Graph with Latency Numbers","text":"<p>The app_perf_graph.py script can be used to generate a graph of a Holoscan application with latency data from benchmarking embedded into the graph. The graph looks like the figure below, where graph nodes are operators along with their average and maximum execution times, and edges represent connection between operators along with the average and maximum data transfer latencies.</p> <p></p>","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#monitor-application-performance-in-real-time","title":"Monitor application performance in real-time","text":"<pre><code># Terminal 1: Run benchmark\npython3 benchmarks/holoscan_flow_benchmarking/benchmark.py \\\n    -a endoscopy_tool_tracking \\\n    -i 1 -r 3 -m 1000 \\\n    --sched=greedy \\\n    -d endoscopy_results\n\n# Terminal 2: Generate live graph\npython3 benchmarks/holoscan_flow_benchmarking/app_perf_graph.py \\\n    -o live_app_graph.dot \\\n    -l endoscopy_results\n\n# Terminal 3: View live graph\nxdot live_app_graph.dot\n</code></pre>","tags":["Benchmarking"]},{"location":"benchmarks/model_benchmarking/","title":"Benchmark Model","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.2 Tested Holoscan SDK versions: 1.0.2 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates an easy, quick and straightforward way to benchmark the scalaibility of inferencing with a model against a single video data stream in a typical Holoscan application. The video stream is played via a V4L2 loopback device. Then, the stream is preprocessed and fed to the model for inferencing. Then, the results are visualized after postprocessing.</p>","tags":["Benchmarking"]},{"location":"benchmarks/model_benchmarking/#usage","title":"Usage","text":"<p>As this application is, by default, set to use the ultrasound segmentation example, you can build and run the ultrasound segmentation example first, and then try running this benchmarking application.</p> <p>Build and run the ultrasound segmentation application: <pre><code>./holohub run ultrasound_segmentation --language=cpp --local\n</code></pre></p> <p>Now, this benchmarking application can be built and run. However, before doing so, the v4l2loopback must be run first. Check out the notes and prerequisites here to play a video via a V4L2 loopback device. Assuming, everything is set up correctly, the ultrasound segmentation example video could be run with the following command:</p> <p>Note: we are playing the video to <code>/dev/video6</code> after running <code>sudo modprobe v4l2loopback video_nr=6 max_buffers=4</code> <pre><code>$ ffmpeg -stream_loop -1 -re -i ./data/ultrasound_segmentation/ultrasound_256x256.avi -pix_fmt yuyv422 -f v4l2 /dev/video6\nffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/\n  ...\n  libavutil      56. 31.100 / 56. 31.100\n  libavcodec     58. 54.100 / 58. 54.100\n  ...\n[mov,mp4,m4a,3gp,3g2,mj2 @ 0x560a570b0740] st: 0 edit list: 1 Missing key frame while searching for timestamp: 0\n...\nInput #0, mov,mp4,m4a,3gp,3g2,mj2, from './data/ultrasound_segmentation/ultrasound_256x256.avi':\n...\n</code></pre></p> <p>Now, the benchmarking application can be built and run: <pre><code>./holohub run model_benchmarking --language=&lt;cpp/python&gt; --local\n</code></pre></p> <p>To use a different video, the video can be played via the above <code>ffmpeg</code> command.</p> <p>To use a different model, you can specify the data path in the <code>./holohub run model_benchmarking --language=&lt;cpp/python&gt; --local --no-local-build</code> command with the <code>-d</code> option, and the model name, residing in the data path directory, with the <code>-m</code> option.</p> <pre><code>./holohub run model_benchmarking --language=&lt;cpp/python&gt; --local --no-local-build --run-args=\"-d &lt;data_path&gt; -m &lt;model_name&gt;\"\n</code></pre> <p>To check the full list of options, run: <pre><code>./holohub run model_benchmarking --language=&lt;cpp/python&gt; --local --no-local-build --run-args=\"-h\"\n</code></pre></p>","tags":["Benchmarking"]},{"location":"benchmarks/model_benchmarking/#capabilities","title":"Capabilities","text":"<p>This benchmarking application can be used to measure performance of parallel inferences for the same model on a single video stream. The <code>-l</code> option can be used to specify the number of parallel inferences to run. Then, the same model will be loaded to the GPU multiple times defined by the <code>-l</code> parameter.</p> <p>The schematic diagram of this benchmarking application is in Figure 1. The visualization and (visualization + postprocessing) steps are marked as grey, as they can optionally be turned off with, respectively, <code>-p</code> and <code>-i</code> options. The figure shows a single video data stream is used in the application. Multiple ML/AI models are ingested by the Holoscan Inference operator to perform inference on a single data stream. The same ML model is replicated to be loaded multiple times to the GPU in this application.</p> <p></p> <p>Figure 1. The schematic diagram of the benchmarking application</p>","tags":["Benchmarking"]},{"location":"benchmarks/release_benchmarking/","title":"Release Benchmarking Guide","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Last modified: July 1, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 2.3.0 Tested Holoscan SDK versions: 2.3.0 Contribution metric: Level 3 - Developmental</p> <p>This tutorial provides a reproducible workflow for developers to accurately measure the latency of curated HoloHub applications across various SDK releases and different deployment scenarios, from single-application to multi-model use cases.</p> <p>Developers can use the Holoscan Flow Benchmarking tools referenced within this guide to systematically analyze performance bottlenecks, optimize execution times, and fine-tune their own applications for real-time, low-latency processing.</p>","tags":["Benchmarking","Performance"]},{"location":"benchmarks/release_benchmarking/#contents","title":"Contents","text":"<ul> <li>Background</li> <li>Previous Holoscan Release Benchmark Reports</li> <li>Running the Tutorial</li> <li>Running Benchmarks</li> <li>Summarizing Data</li> <li>Presenting Data</li> <li>Troubleshooting</li> <li>Developer References</li> </ul>","tags":["Benchmarking","Performance"]},{"location":"benchmarks/release_benchmarking/#background","title":"Background","text":"<p>Holoscan SDK emphasizes low end-to-end latency in application pipelines. In addition to other benchmarks, we can use HoloHub applications to evaluate Holoscan SDK performance over releases.</p> <p>In this tutorial we provide a reproducible workflow to evaluate end-to-end latency performance on the Endoscopy Tool Tracking and Multi-AI Ultrasound HoloHub projects. These projects are generally maintained by the NVIDIA Holoscan team and demonstrate baseline Holoscan SDK inference pipelines with video replay and Holoviz rendering output.</p> <p>Benchmark scenarios include: - Running multiple Holoscan SDK pipelines concurrently on a single machine - Running video replay input at real-time speeds or as fast as possible - Running Holoviz output with either visual rendering or in headless mode</p> <p>We plan to release HoloHub benchmarks in the release subfolder following Holoscan SDK general releases. You can follow the tutorial below to similarly evaluate performance on your own machine.</p> <p>Refer to related documents for more information: - the results report template file provides additional information on definitions and background - versioned releases are available for review in the release subfolder</p>","tags":["Benchmarking","Performance"]},{"location":"benchmarks/release_benchmarking/#previous-holoscan-release-benchmark-reports","title":"Previous Holoscan Release Benchmark Reports","text":"<ul> <li>Holoscan SDK v2.3.0</li> <li>Holoscan SDK v2.6.0</li> <li>Holoscan SDK v3.0.0</li> </ul>","tags":["Benchmarking","Performance"]},{"location":"benchmarks/release_benchmarking/#running-benchmarks-getting-started","title":"Running Benchmarks: Getting Started","text":"<p>Data collection can be run in the HoloHub base container for both the Endoscopy Tool Tracking and the Multi-AI Ultrasound applications. We've provided a custom Dockerfile with tools to process collected data into a benchmark report.</p> <pre><code># Build and launch the container\n./holohub run-container \\\n    --img holohub:release_benchmarking \\\n    --docker-file benchmarks/release_benchmarking/Dockerfile \\\n    --base-img nvcr.io/nvidia/clara-holoscan/holoscan:&lt;holoscan-sdk-version&gt;-$(./dev_container get_host_gpu)\n\n# Inside the container, build the applications in benchmarking mode\n./holohub build endoscopy_tool_tracking --benchmark --language=cpp\n./holohub build multiai_ultrasound --benchmark --language=cpp\n\n./holohub build release_benchmarking\n</code></pre> <p>Run the benchmarking script with no arguments to collect performance logs in the <code>./output</code> directory. <pre><code>./holohub run release_benchmarking --no-local-build\n</code></pre></p>","tags":["Benchmarking","Performance"]},{"location":"benchmarks/release_benchmarking/#summarizing-data","title":"Summarizing Data","text":"<p>After running benchmarks, inside the dev environment, use <code>./holohub run</code> to process data statistics and create bar plot PNGs: <pre><code>./holohub run-container --img holohub:release_benchmarking --no-docker-build\n./holohub run release_benchmarking --no-local-build --run-args \"--process benchmarks/release_benchmarking\"\n</code></pre></p> <p>Alternatively, collect results across platforms. On each machine: 1. Run benchmarks: <pre><code>./holohub run release_benchmarking --no-local-build\n</code></pre> 2. Add platform configuration information: <pre><code>./holohub run release_benchmarking --no-local-build --run-args \"--print\" &gt; benchmarks/release_benchmarking/output/platform.txt\n</code></pre> 3. Transfer output contents from each platform to a single machine: <pre><code># Compress information for transfer\npushd benchmarks/release_benchmarking\ntar cvf benchmarks-&lt;platform-name&gt;.tar.gz output/*\n\n# Migrate the results archive with your transfer tool of choice, such as SCP\n\n# Extract results to a subfolder on the target machine\nmkdir -p output/&lt;release&gt;/&lt;platform-name&gt;/\npushd output/&lt;release&gt;/&lt;platform-name&gt;\ntar xvf benchmarks-&lt;platform-name&gt;\n</code></pre> 4. Use multiple <code>--process</code> flags to generate a batch of bar plots for multiple platform results: <pre><code>./holohub run release_benchmarking --no-local-build --run-args \"\\\n    --process benchmarks/release_benchmarking/2.4/x86_64 \\\n    --process benchmarks/release_benchmarking/2.4/IGX_iGPU \\\n    --process benchmarks/release/benchmarking/2.4/IGX_dGPU\"\n</code></pre></p>","tags":["Benchmarking","Performance"]},{"location":"benchmarks/release_benchmarking/#presenting-data","title":"Presenting Data","text":"<p>You can use the template markdown file in the <code>template</code> folder to generate a markdown or PDF report with benchmark data with <code>pandoc</code> and <code>Jinja2</code>.</p> <ol> <li>Copy and edit <code>template/release.json</code> with information about the benchmarking configuration, including the release version, platform configurations, and local paths to processed data. Run <code>./holohub run</code> to print JSON-formatted platform details to the console about the current system: <pre><code>./holohub run-container --img holohub:release_benchmarking --no-docker-build\n./holohub run release_benchmarking --no-local-build --run-args=\"--print\"\n</code></pre></li> <li>Render the document with the Jinja CLI tool: <pre><code>pushd benchmarks/release_benchmarking\njinja2 template/results.md.tmpl template/&lt;release-version&gt;.json --format=json &gt; output/&lt;release-version&gt;.md\n</code></pre></li> </ol>","tags":["Benchmarking","Performance"]},{"location":"benchmarks/release_benchmarking/#optional-generating-a-pdf-report-document","title":"(Optional) Generating a PDF report document","text":"<p>You can convert the report to PDF format as an easy way to share your report as a single file with embedded plots.</p> <ol> <li>In your copy of <code>template/release.json</code>, update the <code>\"format\"</code> string to <code>\"pdf\"</code>.</li> <li>Follow the instructions above to generate your markdown report with Jinja2.</li> <li>Use <code>pandoc</code> to convert the markdown file to PDF: <pre><code>pushd output\npandoc &lt;release-version&gt;.md -o &lt;release-version&gt;.pdf --toc\n</code></pre></li> </ol>","tags":["Benchmarking","Performance"]},{"location":"benchmarks/release_benchmarking/#optional-submitting-results-to-holohub","title":"(Optional) Submitting Results to HoloHub","text":"<p>The Holoscan SDK team may submit release benchmarking reports to HoloHub git history for general visibility. We use Markdown formatting to make plot diagrams accessible for direct download.</p> <ol> <li>Move <code>&lt;release-version&gt;.md</code> and accompanying plots to a new <code>release/&lt;version&gt;</code> folder.</li> <li>Update image paths in <code>&lt;release-version.md&gt;</code> and verify locally with a markdown renderer such as VS Code.</li> <li>Commit changes, push to GitHub, and open a Pull Request.</li> </ol>","tags":["Benchmarking","Performance"]},{"location":"benchmarks/release_benchmarking/#cleanup","title":"Cleanup","text":"<p>Benchmarking changes to application YAML files can be discarded after benchmarks complete. <pre><code>git checkout applications/*.yaml\n</code></pre></p>","tags":["Benchmarking","Performance"]},{"location":"benchmarks/release_benchmarking/#troubleshooting","title":"Troubleshooting","text":"<p>Why am I seeing high end-to-end latency spikes as outliers in my data?</p> <p>Latency spikes may occur in display-driven benchmarking if the display goes to sleep. Please configure your display settings to prevent the display from going to sleep before running benchmarks.</p> <p>We have also infrequently observed latency spikes in cases where display drivers and CUDA Toolkit versions are not matched, and due to suboptimal GPU task preemption policies. We are still investigating these issues.</p> <p>Benchmark applications are failing silently without writing log files.</p> <p>Silent failures may indicate an issue with the underlying applications undergoing benchmarking. Try running the applications directly and verify execution is as expected: - <code>./holohub run endoscopy_tool_tracking --language=cpp</code> - <code>./holohub run multiai_ultrasound --language=cpp</code></p> <p>In some cases you may need to clear your HoloHub build or data folders to address errors: - <code>./holohub clear-cache</code> - <code>rm -rf ./data</code></p>","tags":["Benchmarking","Performance"]},{"location":"benchmarks/release_benchmarking/#developer-references","title":"Developer References","text":"<p>While this tutorial is tailored to curated configurations of the Endoscopy Tool Tracking and Multi-AI Ultrasound HoloHub applications, developers utilize underlying Holoscan data frame flow tracking tools to similarly measure and analyze performance in custom Holoscan applications.</p> <ul> <li>Refer to the Holoscan Flow Benchmarking project for general Holoscan performance profiling tools for both C++ and Python applications.</li> <li>Refer to the Holoscan Flow Benchmarking whitepaper and tutorial for a comprehensive overview of pipeline profiling tools.</li> <li>Refer to <code>run_benchmarks.sh</code> for additional examples demonstrating performance data collection and reporting with Holoscan Flow Tracking scripts.</li> </ul>","tags":["Benchmarking","Performance"]},{"location":"operators/","title":"Operators","text":"<p>Operators are fundamental components that extend the functionality of the Holoscan SDK, enabling developers to build custom AI sensor processing workflows.  These operators are designed to handle specific tasks such as data ingestion, preprocessing, model inference, and postprocessing, and can be seamlessly integrated into Holoscan applications.  The repository provides a collection of pre-built operators that serve as reference implementations, demonstrating best practices for efficient and scalable AI processing.</p>"},{"location":"operators/advanced_network/","title":"Advanced Network library","text":"<p> Authors: Cliff Burdick (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: May 29, 2025 Latest version: 1.5 Minimum Holoscan SDK version: 2.6.0 Tested Holoscan SDK versions: 2.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>[!NOTE] The Advanced Network library previously included standard operators for transmitting and receiving packets to/from the NIC, also referred to as the Advanced Network Operator (ANO). These operators were removed to lower overhead, as aggregation/disaggregation of the packets still needed to be done in separate operators. We plan to provide more full-fledged generic operators in the future. In the meantime, you can continue to use this library to develop Holoscan operators adapted to your use case, now including the direct packet transaction with the NIC. Referred to the Benchmarking sample application for an example.</p> <p>[!WARNING] The library is undergoing large improvements as we aim to better support it as an NVIDIA product. API breakages might be more frequent until we reach version 1.0.</p> <p>[!TIP] Review the High Performance Networking tutorial for guided instructions to configure your system and test the Advanced Network library.</p> <p>The Advanced Network library provides a way for users to achieve the highest throughput and lowest latency for transmitting and receiving Ethernet frames out of and into Holoscan operators. Direct access to the NIC hardware is available in userspace, thus bypassing the kernel's networking stack entirely.</p>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#requirements","title":"Requirements","text":"<ul> <li>Linux</li> <li>An NVIDIA NIC with a ConnectX-6 or later chip</li> <li>System tuning as described here</li> <li>DPDK 22.11</li> <li>MOFED 5.8-1.0.1.1 or later</li> <li>DOCA 2.7 or later</li> </ul>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#features","title":"Features","text":"<ul> <li>High Throughput: Hundreds of gigabits per second is possible with the proper hardware</li> <li>Low Latency: With direct access to the NIC's ring buffers, most latency incurred is only PCIe latency</li> <li>Since the kernel's networking stack is bypassed, the user is responsible for defining the protocols used         over the network. In most cases Ethernet, IP, and UDP are ideal for this type of processing because of their         simplicity, but any type of protocol can be implemented or used. The advanced network library         gives the option to use several primitives to remove the need for filling out these headers for basic packet types,         but raw headers can also be constructed.</li> <li>GPUDirect: Optionally send data directly from the NIC to GPU, or directly from the GPU to NIC. GPUDirect has two modes:</li> <li>Header-data split: Split the header portion of the packet to the CPU and the rest (payload) to the GPU. The split point is     configurable by the user. This option should be the preferred method in most cases since it's easy to use and still     gives near peak performance.</li> <li>Batched GPU: Receive batches of whole packets directly into the GPU memory. This option requires the GPU kernel to inspect     and determine how to handle packets. While performance may increase slightly over header-data split, this method     requires more effort and should only be used for advanced users.</li> <li>GPUComms: Optionally control the send or receive communications from the GPU through the GPUDirect Async Kernel-Initiated network technology (enabled with the DOCA GPUNetIO backend only).</li> <li>Flow Configuration: Configure the NIC's hardware flow engine for configurable patterns. Currently only UDP source     and destination are supported.</li> </ul>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#limitations","title":"Limitations","text":"<p>The limitations below will be removed in a future release.</p> <ul> <li>Only UDP fill mode is supported</li> </ul>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#managers","title":"Managers","text":"<p>Internally the advanced network library is implemented by different backends, each offering different features.</p> <p>It is specified with the <code>manager</code> parameter, passed to the <code>advanced_network::adv_net_init</code> function before starting an application, along with all of the NIC parameters. This step allocates all packet buffers, initializes the queues on the NIC, and starts the appropriate number of internal threads to take packets off or put packets onto the NIC as fast as possible, using the backend-specific implementation.</p> <p>Developers can then use the rest of the Advanced Network library API to send and receive packets, and do any additional processing needed (e.g. aggregate, reorder, etc.), as described in the API Structures section.</p> <p>[!NOTE] To achieve zero copy throughout the whole pipeline only pointers are passed between each entity above. When the user receives the packets from the network library it's using the same buffers that the NIC wrote to either CPU or GPU memory. This architecture also implies that the user must explicitly decide when to free any buffers it's owning. Failure to free buffers will result in errors in the advanced network library not being able to allocate buffers.</p>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#dpdk","title":"DPDK","text":"<p>DPDK is an open-source userspace packet processing library supported across platforms and vendors.</p> <p>It is the default manager, and can be set with the values <code>dpdk</code> or <code>default</code>.</p>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#doca-gpunetio","title":"DOCA GPUNetIO","text":"<p>NVIDIA DOCA brings together a wide range of powerful APIs, libraries, and frameworks for programming and accelerating modern data center infrastructures\u200b. DOCA GPUNetIO is one of the libraries included in the DOCA SDK. It enables the GPU to control, from a CUDA kernel, network communications directly interacting with the network card and completely removing the CPU from the critical data path.</p> <p>If the application wants to enable GPU communications, it must chose <code>gpunetio</code> as backend. The behavior of the GPUNetIO backend is similar to the DPDK one except that the receive and send are executed by CUDA kernels. Specifically:</p> <ul> <li>Receive: a persistent CUDA kernel is running on a dedicated stream and keeps receiving packets, providing packets' info to the application level. Due to the nature of the operator, the CUDA receiver kernel now is responsible only to receive packets but in a real-world application, it can be extended to receive and process in real-time network packets (DPI, filtering, decrypting, byte modification, etc..) before forwarding packets to the application.</li> <li>Send: every time the application wants to send packets it launches one or more CUDA kernels to prepare data and create Ethernet packets and then (without the need of synchronizing) forward the send request to the operator. The operator then launches another CUDA kernel that in turn sends the packets (still no need to synchronize with the CPU). The whole pipeline is executed on the GPU. Due to the nature of the operator, the packets' creation and packets' send must be split in two CUDA kernels but in a real-word application, they can be merged into a single CUDA kernel responsible for both packet processing and packet sending.</li> </ul> <p>Please refer to the DOCA GPUNetIO programming guide to correctly configure your system before using this transport layer.</p> <p>The GPUNetIO manager does not support the <code>split-boundary</code> option.</p> <p>To build and run the Dockerfile with DOCA support, please follow the steps below:</p> <pre><code># To build Docker image\n./dev_container build --docker_file operators/advanced_network/Dockerfile --img holohub-doca:doca-28-ubuntu2204 --no-cache\n\n# Launch DOCA container\n./operators/advanced_network/run_doca.sh\n\n# To build operator + app from main dir\n./run build adv_networking_bench --configure-args \"-DANO_MGR=gpunetio\"\n\n# Run app\n./build/adv_networking_bench/applications/adv_networking_bench/cpp/adv_networking_bench adv_networking_bench_gpunetio_tx_rx.yaml\n</code></pre>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#rivermax","title":"RIVERMAX","text":"<p>NVIDIA Rivermax SDK Optimized networking SDK for media and data streaming applications. NVIDIA\u00ae Rivermax\u00ae offers a unique IP-based solution for any media and data streaming use case. Rivermax together with NVIDIA GPU accelerated computing technologies unlocks innovation for a wide range of applications in Media and Entertainment (M&amp;E), Broadcast, Healthcare, Smart Cities and more. Rivermax leverages NVIDIA ConnectX\u00ae and BlueField\u00ae DPU hardware-streaming acceleration technology that enables direct data transfers to and from the GPU, delivering best-in-class throughput and latency with minimal CPU utilization for streaming workloads. Rivermax is the only fully-virtualized streaming solution that complies with the stringent timing and traffic flow requirements of the SMPTE ST 2110-21 specification. Rivermax enables the future of cloud-based software-defined broadcasting. Product release highlights, documentation, platform support, installation and usage guides can be found in the Rivermax SDK Page. Frequently asked questions, customers product highlights, Video link and more are available on the Rivermax Product Page.</p> <p>To build and run the Dockerfile with <code>Rivermax</code> support, follow these steps:</p> <ul> <li>Visit the Rivermax SDK Page to download the Rivermax Release SDK.</li> <li>Obtain a Rivermax developer license from the same page. This is necessary for using the SDK.</li> <li>Copy the downloaded SDK tar file (e.g., <code>rivermax_ubuntu2204_1.60.1.tar.gz</code>) into your current working directory.</li> <li>You can adjust the path using the <code>RIVERMAX_SDK_ZIP_PATH</code> build argument if needed.</li> <li>Modify the version using the <code>RIVERMAX_VERSION</code> build argument if you're using a different SDK version.</li> <li>Place the obtained Rivermax developer license file (<code>rivermax.lic</code>) into the <code>/opt/mellanox/rivermax/</code> directory. You can change this path in the run_rivermax.sh script if necessary</li> <li>Build the Docker image:</li> </ul> <pre><code>./dev_container build --docker_file operators/advanced_network/Dockerfile --img holohub:rivermax --build-args \"--target rivermax\"\n</code></pre> <ul> <li>Launch Rivermax container</li> </ul> <pre><code># Launch Rivermax container\n./operators/advanced_network/run_rivermax.sh\n\n# To build operator + app from main dir\n./run build adv_networking_bench --configure-args \"-DANO_MGR=rivermax\"\n\n# Run app\n./build/adv_networking_bench/applications/adv_networking_bench/cpp/adv_networking_bench  adv_networking_bench_rmax_rx.yaml\n</code></pre>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#configuration-parameters","title":"Configuration Parameters","text":"","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#common-configuration","title":"Common Configuration","text":"<p>These common configurations are used by both TX and RX:</p> <ul> <li><code>version</code>: Version of the config. Only 1 is valid currently.</li> <li>type: <code>integer</code></li> <li><code>master_core</code>: Master core used to fork and join network threads. This core is not used for packet processing and can be bound to a non-isolated core. Should differ from isolated cores in queues below.</li> <li>type: <code>integer</code></li> <li><code>manager</code>: Backend networking library. default: <code>dpdk</code>. Other: <code>doca</code> (GPUNet IO), <code>rivermax</code></li> <li>type: <code>string</code></li> <li><code>log_level</code>: Backend log level. default: <code>warn</code>. Other: <code>trace</code> , <code>debug</code>, <code>info</code>, <code>error</code>, <code>critical</code>, <code>off</code></li> <li>type: <code>string</code></li> <li><code>tx_meta_buffers</code>: Metadata buffers for transmit. One buffer is used for each burst of packets (default: 4096)</li> <li>type: <code>integer</code></li> <li><code>rx_meta_buffers</code>: Metadata buffers for receive. One buffer is used for each burst of packets (default: 4096)</li> <li>type: <code>integer</code></li> </ul>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#memory-regions","title":"Memory regions","text":"<p><code>memory_regions:</code> List of regions where buffers are stored.</p> <ul> <li><code>name</code>: Memory Region name</li> <li>type: <code>string</code></li> <li><code>kind</code>: Location. Best options are <code>device</code> (GPU), or <code>huge</code> (pages - CPU). Not recommended: <code>host</code> (CPU), <code>host_pinned</code> (CPU).</li> <li>type: <code>string</code></li> <li><code>affinity</code>: GPU ID for GPU memory, NUMA Node ID for CPU memory</li> <li>type: <code>integer</code></li> <li><code>access</code>: Permissions to the rdma memory region ( <code>local</code> or <code>rmda_read</code> or <code>rdma_write</code>)</li> <li>type: <code>string</code></li> <li><code>num_bufs</code>: Higher value means more time to process, but less space on GPU BAR1. Too low means risk of dropped packets from NIC having nowhere to write (Rx) or higher latency from buffering (Tx). Good rule of \ud83d\udc4d : 3x batch_size</li> <li>type: <code>integer</code></li> <li><code>buf_size</code>: Size of buffer, equal to packet size or less if breaking down packets (ex: header data split)</li> <li>type: <code>integer</code></li> </ul>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#interfaces","title":"Interfaces","text":"<ul> <li><code>interfaces</code>:  List and configure ethernet interfaces     full path: <code>cfg\\interfaces\\</code><ul> <li><code>name</code>: Name of the interfaca</li> <li>type: <code>string</code></li> <li><code>address</code>: PCIe BDF address (lspci) or linux link name (ip link)</li> <li>type: <code>string</code></li> <li><code>rx|tx</code> category of queues below full path: <code>cfg\\interfaces\\[rx|tx]</code></li> </ul> </li> </ul>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#receive-configuration-rx","title":"Receive Configuration (rx)","text":"<ul> <li><code>queues</code>: List of queues on NIC     type: <code>list</code>     full path: <code>cfg\\interfaces\\rx\\queues</code><ul> <li><code>name</code>: Name of queue<ul> <li>type: <code>string</code></li> </ul> </li> <li><code>id</code>: Integer ID used for flow connection or lookup in operator compute method<ul> <li>type: <code>integer</code></li> </ul> </li> <li><code>cpu_core</code>: CPU core ID. Should be isolated when CPU polls the NIC for best performance.. Not in use for Doca GPUNetIO     Rivermax manager can accept coma separated list of CPU IDs<ul> <li>type: <code>string</code></li> </ul> </li> <li><code>batch_size</code>: Number of packets in a batch passed from the NIC to the downstream operator. A larger number increases throughput but reduces end-to-end latency, as it takes longer to populate a single buffer. A smaller number reduces end-to-end latency but can also reduce throughput.<ul> <li>type: <code>integer</code></li> </ul> </li> <li><code>memory_regions</code>: List of memory regions where buffers are stored. memory regions names are configured in the Memory Regions section     type: <code>list</code></li> <li><code>timeout_us</code>: Timeout value that a batch will be sent on even if not enough packets to fill a batch were received<ul> <li>type: <code>integer</code></li> </ul> </li> </ul> </li> </ul>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#transmit-configuration-tx","title":"Transmit Configuration (tx)","text":"<ul> <li> <p><code>queues</code>: List of queues on NIC     type: <code>list</code>     full path: <code>cfg\\interfaces\\rx\\queues</code></p> <ul> <li><code>name</code>: Name of queue<ul> <li>type: <code>string</code></li> </ul> </li> <li><code>id</code>: Integer ID used for flow connection or lookup in operator compute method<ul> <li>type: <code>integer</code></li> </ul> </li> <li><code>cpu_core</code>: CPU core ID. Should be isolated when CPU polls the NIC for best performance.. Not in use for Doca GPUNetIO     Rivermax manager can accept coma separated list of CPU IDs<ul> <li>type: <code>string</code></li> </ul> </li> <li><code>batch_size</code>: Number of packets in a batch passed from the NIC to the downstream operator. A larger number increases throughput but reduces end-to-end latency, as it takes longer to populate a single buffer. A smaller number reduces end-to-end latency but can also reduce throughput.<ul> <li>type: <code>integer</code></li> </ul> </li> <li><code>memory_regions</code>: List of memory regions where buffers are stored. memory regions names are configured in the Memory Regions section     type: <code>list</code></li> <li><code>accurate_send</code>: Accurate TX sending enabled for sending packets at a specific PTP timestamp<ul> <li>type: <code>boolean</code></li> </ul> </li> </ul> </li> <li> <p><code>flows</code>: List of flows - rules to apply to packets, mostly to divert to the right queue. (Not in use for Rivermax manager)   type: <code>list</code>   full path: <code>cfg\\interfaces\\[rx|tx]\\flows</code></p> <ul> <li><code>name</code>: Name of the flow</li> <li>type: <code>string</code></li> <li><code>id</code>: ID of the flow</li> <li>type: <code>integer</code></li> <li><code>action</code>: Action section of flow (what happens. Currently only supports steering to a given queue)</li> <li>type: <code>sequence</code><ul> <li><code>type</code>: Type of action. Only <code>queue</code> is supported currently.</li> <li>type: <code>string</code></li> <li><code>id</code>: ID of queue to steer to<ul> <li>type: <code>integer</code></li> </ul> </li> </ul> </li> <li><code>match</code>: Match section of flow</li> <li>type: <code>sequence</code><ul> <li><code>udp_src</code>: UDP source port or a range of ports (eg 1000-1010)</li> <li>type: <code>integer</code></li> <li><code>udp_dst</code>: UDP destination port or a range of ports (eg 1000-1010)</li> <li>type: <code>integer</code></li> <li><code>ipv4_len</code>: IPv4 payload length</li> <li>type: <code>integer</code></li> </ul> </li> </ul> </li> </ul>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#extended-receive-configuration-for-rivermax-manager","title":"Extended Receive Configuration for Rivermax manager","text":"<ul> <li> <p><code>rmax_rx_settings</code>: Extended RX settings for Rivermax Manager. Rivermax Manager supports receiving the same stream from multiple redundant paths (IPO - Inline Packet Ordering).     Each path is a combination of a source IP address, a destination IP address, a destination port, and a local IP address of the receiver device.   type: <code>list</code>   full path: <code>cfg\\interfaces\\rx\\queues\\rmax_rx_settings</code></p> <ul> <li><code>memory_registration</code>: Flag, when enabled, reduces the number of memory keys in use by registering all the memory in a single pass on the application side.     Can be used only together with HDS enabled<ul> <li>type: <code>boolean</code></li> <li>default:<code>false</code></li> </ul> </li> <li><code>max_path_diff_us</code>: Sets the maximum number of microseconds that receiver waits for the same packet to arrive from a different stream (if IPO is enabled)<ul> <li>type: <code>integer</code></li> <li>default:<code>0</code></li> </ul> </li> <li><code>ext_seq_num</code>: The RTP sequence number is used by the hardware to determine the location of arriving packets in the receive buffer.     The application supports two sequence number parsing modes: 16-bit RTP sequence number (default) and 32-bit extended sequence number,     consisting of 16 low order RTP sequence number bits and 16 high order bits from the start of RTP payload. When set to <code>true</code> 32-bit ext. sequence number will be used<ul> <li>type: <code>boolean</code></li> <li>default:<code>true</code></li> </ul> </li> <li><code>sleep_between_operations_us</code>: Specifies the duration, in microseconds, that the receiver will pause or sleep between two consecutive receive (RX) operations.<ul> <li>type: <code>integer</code></li> <li>default:<code>0</code></li> </ul> </li> <li><code>local_ip_addresses</code>: List of Local NIC IP Addresses (one address per receiving path)<ul> <li>type: <code>sequence</code></li> </ul> </li> <li><code>source_ip_addresses</code>: List of Sender IP Addresses (one address per receiving path)<ul> <li>type: <code>sequence</code></li> </ul> </li> <li><code>destination_ip_addresses</code>: List of Destination IP Addresses (one address per receiving path), can be multicast<ul> <li>type: <code>sequence</code></li> </ul> </li> <li><code>destination_ports</code>: List of Destination IP ports (one port per receiving path)<ul> <li>type: <code>sequence</code></li> </ul> </li> <li><code>rx_stats_period_report_ms</code>: Specifies the duration, in milliseconds, that the receiver will display statistics in the log. Set <code>0</code> to disable statistics logging feature<ul> <li>type: <code>integer</code></li> <li>default:<code>0</code></li> </ul> </li> <li><code>send_packet_ext_info</code>: Enables the transmission of extended metadata for each received packet<ul> <li>type: <code>boolean</code></li> <li>default:<code>true</code></li> </ul> </li> </ul> </li> <li> <p>Example of the Rivermax queue configuration for redundant stream using HDS and GPU   This example demonstrates receiving a redundant stream sent from a sender with source addresses 192.168.100.4 and 192.168.100.3.   The stream is received via NIC which have local IP (same) 192.168.100.5 (listed twice, once per stream).   The multicast addresses and UDP ports on which the stream is being received are 224.1.1.1:5001 and 224.1.1.2:5001  The incoming packets are of size 1152 bytes. The initial 20 bytes are stripped from the payload as an  application header and placed in buffers allocated in RAM.  The remaining 1132 bytes are placed in dedicated payload buffers.  In this case, the payload buffers are allocated in GPU 0 memory. <pre><code>    memory_regions:\n    - name: \"Data_RX_CPU\"\n      kind: \"huge\"\n      affinity: 0\n      access:\n        - local\n      num_bufs: 43200\n      buf_size: 20\n    - name: \"Data_RX_GPU\"\n      kind: \"device\"\n      affinity: 0\n      access:\n        - local\n      num_bufs: 43200\n      buf_size: 1132\n    interfaces:\n    - address: 0005:03:00.0\n      name: data1\n      rx:\n        queues:\n        - name: Data1\n          id: 1\n          cpu_core: '11'\n          batch_size: 4320\n          rmax_rx_settings:\n            memory_registration: true\n            max_path_diff_us: 100\n            ext_seq_num: true\n            sleep_between_operations_us: 100\n            memory_regions:\n            - \"Data_RX_CPU\"\n            - \"Data_RX_GPU\"\n            local_ip_addresses:\n            - 192.168.100.5\n            - 192.168.100.5\n            source_ip_addresses:\n            - 192.168.100.4\n            - 192.168.100.4\n            destination_ip_addresses:\n            - 224.1.1.1\n            - 224.1.1.2\n            destination_ports:\n            - 50001\n            - 50001\n            rx_stats_period_report_ms: 3000\n            send_packet_ext_info: true\n</code></pre></p> </li> </ul>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#transmit-configuration-tx_1","title":"Transmit Configuration (tx)","text":"<p>(Current version of Rivermax manager doesn't support TX)</p> <ul> <li><code>queues</code>: List of queues on NIC     type: <code>list</code>     full path: <code>cfg\\interfaces\\tx\\queues</code><ul> <li><code>name</code>: Name of queue<ul> <li>type: <code>string</code></li> </ul> </li> <li><code>id</code>: Integer ID used for flow connection or lookup in operator compute method<ul> <li>type: <code>integer</code></li> </ul> </li> <li><code>cpu_core</code>: CPU core ID. Should be isolated when CPU polls the NIC for best performance.. Not in use for Doca GPUNetIO     Rivermax manager can accept coma separated list of CPU IDs<ul> <li>type: <code>string</code></li> </ul> </li> <li><code>batch_size</code>: Number of packets in a batch that the NIC needs to receive from the upstream operator before sending them over the network. A larger number increases throughput but reduces end-to-end latency. A smaller number reduces end-to-end latency but can also reduce throughput.<ul> <li>type: <code>integer</code></li> </ul> </li> <li><code>memory_regions</code>: List of memory regions where buffers are stored. memory regions names are configured in the Memory Regions section     type: <code>list</code></li> </ul> </li> </ul>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#api-structures","title":"API Structures","text":"<p>The Advanced Network library uses a common structure named <code>BurstParams</code> to pass data to/from other operators. <code>BurstParams</code> provides pointers to packet memory locations (e.g., CPU or GPU) and contains metadata needed by any operator to track allocations. Interacting with <code>BurstParams</code> should only be done with the helper functions described below.</p>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#example-api-usage","title":"Example API Usage","text":"<p>For an entire list of API functions, please see the <code>advanced_network/common.h</code> header file.</p>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#receive","title":"Receive","text":"<p>The section below describes a workflow using GPUDirect to receive packets using header-data split. The job of the user's operator(s) is to process and free the buffers as quickly as possible. This might be copying to interim buffers or freeing before the entire pipeline is done processing. This allows the networking piece to use relatively few buffers while still achieving very high rates.</p> <p>The first step in receiving from the NIC is to receive a <code>BurstParams</code> structure when a batch is complete:</p> <pre><code>BurstParams *burst;\nint port_id_ = 0;\nint queue_id_ = 0;\nauto status = get_rx_burst(&amp;burst, port_id_, queue_id_);\n</code></pre> <p>The packets arrive in scattered packet buffers. Depending on the application, you may need to iterate through the packets to aggregate them into a single buffer. Alternatively the operator handling the packet data can operate on a list of packet pointers rather than a contiguous buffer. Below is an example of aggregating separate GPU packet buffers into a single GPU buffer:</p> <pre><code>  for (int p = 0; p &lt; get_num_packets(burst); p++) {\n    h_dev_ptrs_[aggr_pkts_recv_ + p]   = get_cpu_packet_ptr(burst, p);\n    ttl_bytes_in_cur_batch_           += get_gpu_packet_length(burst, p) + sizeof(UDPPkt);\n  }\n\n  simple_packet_reorder(buffer, h_dev_ptrs, packet_len, burst-&gt;hdr.num_pkts);\n</code></pre> <p>For this example we are tossing the header portion (CPU), so we don't need to examine the packets. Since we launched a reorder kernel to aggregate the packets in GPU memory, we are also done with the GPU pointers. All buffers may be freed for the NIC to reuse at this point:</p> <pre><code>free_all_burst_packets_and_burst(burst_bufs_[b]);\n</code></pre>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#transmit","title":"Transmit","text":"<p>Transmitting packets works similar to the receive side, except the user is tasked with filling out the packets as much as it needs to. As mentioned above, helper functions are available to fill in most boilerplate header information if that doesn't change often.</p> <p>Before sending packets, the user's transmit operator must request a buffer from the NIC:</p> <pre><code>auto burst = create_tx_burst_params();\nset_header(burst, port_id, queue_id, batch_size, num_segments);\nif ((ret = get_tx_packet_burst(burst)) != Status::SUCCESS) {\n  HOLOSCAN_LOG_ERROR(\"Error returned from get_tx_packet_burst: {}\", static_cast&lt;int&gt;(ret));\n  return;\n}\n</code></pre> <p>The code above creates a shared <code>BurstParams</code>, and uses <code>get_tx_packet_burst</code> to populate the burst buffers with valid packet buffers. On success, the buffers inside the burst structure will be allocated and are ready to be filled in. Each packet must be filled in by the user. In this example we loop through each packet and populate a buffer:</p> <pre><code>for (int num_pkt = 0; num_pkt &lt; get_num_packets(burst); num_pkt++) {\n  void *payload_src = data_buf + num_pkt * payload_size;\n  if (set_udp_payload(burst, num_pkt, payload_src, payload_size) != Status::SUCCESS) {\n    HOLOSCAN_LOG_ERROR(\"Failed to create packet {}\", num_pkt);\n  }\n}\n</code></pre> <p>The code iterates over the number of packets in the burst (defined above by the user) and passes a pointer to the payload and the packet size to <code>set_udp_payload</code>. In this example our configuration is using <code>fill_mode</code> \"udp\" on the transmitter, so <code>set_udp_payload</code> will populate the Ethernet, IP, and UDP headers. The payload pointer passed by the user is also copied into the buffer. Alternatively a user could use the packet buffers directly as output from a previous stage to avoid this extra copy.</p> <p>With the <code>BurstParams</code> populated, the burst can be sent off to the NIC:</p> <pre><code>send_tx_burst(burst);\n</code></pre>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/aja_source/","title":"AJA Source Operator","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The AJA Source operator provides functionality to capture high-quality video streams from AJA capture cards and devices. It offers comprehensive support for both SDI (Serial Digital Interface) and HDMI (High-Definition Multimedia Interface) input sources, allowing for professional video capture in various formats and resolutions. The operator is designed to work seamlessly with AJA's hardware capabilities, including features like frame synchronization and format detection. Additionally, it provides an optional overlay channel capability that enables real-time mixing and compositing of multiple video streams, making it suitable for applications requiring picture-in-picture, graphics overlay, or other video mixing scenarios.</p>","tags":["Camera","AJA"]},{"location":"operators/aja_source/#requirements","title":"Requirements","text":"<ul> <li>AJA capture card (e.g., KONA HDMI)</li> <li>CUDA-capable GPU</li> <li>Holoscan SDK 1.0.3 or later</li> </ul>","tags":["Camera","AJA"]},{"location":"operators/aja_source/#parameters","title":"Parameters","text":"<p>The following parameters can be configured for this operator:</p> Parameter Type Description Default <code>device</code> string Device specifier (e.g., \"0\" for device 0) \"0\" <code>channel</code> NTV2Channel Camera channel to use for input NTV2_CHANNEL1 <code>width</code> uint32_t Width of the video stream 1920 <code>height</code> uint32_t Height of the video stream 1080 <code>framerate</code> uint32_t Frame rate of the video stream 60 <code>interlaced</code> bool Whether the video is interlaced false <code>rdma</code> bool Enable RDMA for video input false <code>enable_overlay</code> bool Enable overlay channel false <code>overlay_channel</code> NTV2Channel Camera channel to use for overlay NTV2_CHANNEL2 <code>overlay_rdma</code> bool Enable RDMA for overlay false","tags":["Camera","AJA"]},{"location":"operators/aja_source/#supported-video-formats","title":"Supported Video Formats","text":"<p>The operator supports various video formats based on resolution, frame rate, and scan type:</p> <ul> <li>720p (1280x720) at 50/59.94/60 fps</li> <li>1080i (1920x1080) at 50/59.94/60 fps</li> <li>1080p (1920x1080) at 23.98/24/25/29.97/30/50/59.94/60 fps</li> <li>UHD (3840x2160) at 23.98/24/25/29.97/30/50/59.94/60 fps</li> <li>4K (4096x2160) at 23.98/24/25/29.97/30/50/59.94/60 fps</li> </ul>","tags":["Camera","AJA"]},{"location":"operators/aja_source/#input-ports","title":"Input Ports","text":"<ul> <li>overlay_buffer_input (optional): Video buffer for overlay mixing when <code>enable_overlay</code> is true</li> </ul>","tags":["Camera","AJA"]},{"location":"operators/aja_source/#output-ports","title":"Output Ports","text":"<ul> <li>video_buffer_output: Video buffer containing the captured frame</li> <li>overlay_buffer_output (optional): Empty video buffer for overlay when <code>enable_overlay</code> is true</li> </ul>","tags":["Camera","AJA"]},{"location":"operators/apriltag_detector/","title":"apriltag_detector","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.2.0 Tested Holoscan SDK versions: 2.2.0 Contribution metric: Level 1 - Highly Reliable</p> <p>No documentation found.</p>","tags":["Camera"]},{"location":"operators/basic_network/","title":"Basic networking operator","text":"<p> Authors: Cliff Burdick (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>basic_network_operator</code> operator provides a way to send and receive data over Linux sockets. The destination can be on the same machine or over a network. The basic network operator contains separate operators for transmit and receive. Users may choose one or the other, or use both in applications  requiring bidirectional traffic.</p> <p>For TCP sockets the basic network operator only supports a single stream currently. Future versions may expand this to launch multiple threads to listen on different streams.</p> <p>The basic networking operators use class names: <code>BasicNetworkOpTx</code> and <code>BasicNetworkOpRx</code></p>","tags":["Networking and Distributed Computing","UDP","IP","TCP"]},{"location":"operators/basic_network/#nvidiaholoscanbasic_network_operator","title":"<code>nvidia::holoscan::basic_network_operator</code>","text":"<p>Basic networking operator</p>","tags":["Networking and Distributed Computing","UDP","IP","TCP"]},{"location":"operators/basic_network/#receiver-configuration-parameters","title":"Receiver Configuration Parameters","text":"<ul> <li><code>batch_size</code>: Bytes in batch</li> <li>type: <code>integer</code></li> <li><code>max_payload_size</code>: Maximum payload size for a single packet</li> <li>type: <code>integer</code></li> <li><code>udp_dst_port</code>: UDP destination port for packets</li> <li>type: <code>integer</code></li> <li><code>l4_proto</code>: Layer 4 protocol</li> <li>type: <code>string</code> (<code>udp</code>/<code>tcp</code>)</li> <li><code>ip_addr</code>: Destination IP address</li> <li>type: <code>string</code> </li> </ul>","tags":["Networking and Distributed Computing","UDP","IP","TCP"]},{"location":"operators/basic_network/#transmitter-configuration-parameters","title":"Transmitter Configuration Parameters","text":"<ul> <li><code>max_payload_size</code>: Maximum payload size for a single packet</li> <li>type: <code>integer</code></li> <li><code>udp_dst_port</code>: UDP destination port for packets</li> <li>type: <code>integer</code></li> <li><code>l4_proto</code>: Layer 4 protocol</li> <li>type: <code>string</code> (<code>udp</code>/<code>tcp</code>)</li> <li><code>ip_addr</code>: Destination IP address</li> <li>type: <code>string</code> </li> <li><code>min_ipg_ns</code>: Minimum inter-packet gap in nanoseconds</li> <li>type: <code>integer</code> </li> </ul>","tags":["Networking and Distributed Computing","UDP","IP","TCP"]},{"location":"operators/basic_network/#transmitter-and-receiver-operator-parameters","title":"Transmitter and Receiver Operator Parameters","text":"<p>The transmitter and receiver operator both use the <code>NetworkOpBurstParams</code> structure as input and output to their ports, respectively. <code>NetworkOpBurstParams</code> contains the following fields:</p> <ul> <li><code>data</code>: Pointer to batch of packet data</li> <li>type: <code>uint8_t *</code></li> <li><code>len</code>: Length of total buffer in bytes</li> <li>type: <code>integer</code></li> <li><code>num_pkts</code>: Number of packets in batch</li> <li>type: <code>integer</code></li> </ul> <p>To receive messages from the Receive operator use the output port <code>burst_out</code>. To send messages to the Transmit operator use the input port <code>burst_in</code>.</p>","tags":["Networking and Distributed Computing","UDP","IP","TCP"]},{"location":"operators/cvcuda_holoscan_interop/","title":"CVCUDA Holoscan Interoperability Operators","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This directory contains two operators to enable interoperability between the CVCUDA and Holoscan tensors: <code>holoscan::ops::CvCudaToHoloscan</code> and <code>holoscan::ops::HoloscanToCvCuda</code>.</p>","tags":["CV CUDA","Computer Vision and Perception"]},{"location":"operators/cvcuda_holoscan_interop/#holoscanopscvcudatoholoscan","title":"<code>holoscan::ops::CvCudaToHoloscan</code>","text":"<p>Operator class to convert a <code>nvcv::Tensor</code> to a <code>holoscan::Tensor</code>.</p>","tags":["CV CUDA","Computer Vision and Perception"]},{"location":"operators/cvcuda_holoscan_interop/#inputs","title":"Inputs","text":"<ul> <li><code>input</code>: a CV-CUDA tensor</li> <li>type: <code>nvcv::Tensor</code></li> </ul>","tags":["CV CUDA","Computer Vision and Perception"]},{"location":"operators/cvcuda_holoscan_interop/#outputs","title":"Outputs","text":"<ul> <li><code>output</code>: a Holoscan tensor as <code>holoscan::Tensor</code> in <code>holoscan::TensorMap</code></li> <li>type: <code>holoscan::TensorMap</code></li> </ul>","tags":["CV CUDA","Computer Vision and Perception"]},{"location":"operators/cvcuda_holoscan_interop/#holoscanopsholoscantocvcuda","title":"<code>holoscan::ops::HoloscanToCvCuda</code>","text":"","tags":["CV CUDA","Computer Vision and Perception"]},{"location":"operators/cvcuda_holoscan_interop/#inputs_1","title":"Inputs","text":"<ul> <li><code>input</code>: a <code>gxf::Entity</code> containing a Holoscan tensor as <code>holoscan::Tensor</code></li> <li>type: <code>gxf::Entity</code></li> </ul>","tags":["CV CUDA","Computer Vision and Perception"]},{"location":"operators/cvcuda_holoscan_interop/#outputs_1","title":"Outputs","text":"<ul> <li><code>output</code>: a CV-CUDA tensor</li> <li>type: <code>nvcv::Tensor</code></li> </ul>","tags":["CV CUDA","Computer Vision and Perception"]},{"location":"operators/dds/base/","title":"DDS Base Operator","text":"<p> Authors: Ian Stewart (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>The DDS Base Operator provides a base class which can be inherited by any operator class which requires access to a DDS domain.</p> <p>This operator requires an installation of RTI Connext to provide access to the DDS domain, as specified by the OMG Data-Distribution Service</p>","tags":["DDS","RTI Connext"]},{"location":"operators/dds/base/#holoscanopsddsoperatorbase","title":"<code>holoscan::ops::DDSOperatorBase</code>","text":"<p>Base class which provides the parameters and members required to access a DDS domain.</p> <p>For more documentation about how these parameters (and other similar inheriting-class parameters) are used, see the RTI Connext Documentation.</p>","tags":["DDS","RTI Connext"]},{"location":"operators/dds/base/#parameters","title":"Parameters","text":"<ul> <li><code>qos_provider</code>: URI for the DDS QoS Provider</li> <li>type: <code>std::string</code></li> <li><code>participant_qos</code>: Name of the QoS profile to use for the DDS DomainParticipant</li> <li>type: <code>std::string</code></li> <li><code>domain_id</code>: The ID of the DDS domain to use</li> <li>type: <code>uint32_t</code></li> </ul>","tags":["DDS","RTI Connext"]},{"location":"operators/dds/dds_shapes_subscriber/","title":"DDS Shape Subscriber Operator","text":"<p> Authors: Ian Stewart (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>The DDS Shape Subscriber Operator subscribes to and reads from the <code>Square</code>, <code>Circle</code>, and <code>Triangle</code> shape topics as used by the RTI Shapes Demo. It will then translate the received shape data to an internal <code>Shape</code> datatype for output to downstream operators.</p> <p>This operator requires an installation of RTI Connext to provide access to the DDS domain, as specified by the OMG Data-Distribution Service</p>","tags":["DDS","RTI Connext"]},{"location":"operators/dds/dds_shapes_subscriber/#holoscanopsddsshapessubscriberop","title":"<code>holoscan::ops::DDSShapesSubscriberOp</code>","text":"<p>Operator class for the DDS Shapes Subscriber.</p> <p>This operator also inherits the parameters from DDSOperatorBase.</p>","tags":["DDS","RTI Connext"]},{"location":"operators/dds/dds_shapes_subscriber/#parameters","title":"Parameters","text":"<ul> <li><code>reader_qos</code>: The name of the QoS profile to use for the DDS DataReader</li> <li>type: <code>std::string</code></li> </ul>","tags":["DDS","RTI Connext"]},{"location":"operators/dds/dds_shapes_subscriber/#outputs","title":"Outputs","text":"<ul> <li><code>output</code>: Output shapes, translated from those read from DDS</li> <li>type: <code>holoscan::ops::DDSShapesSubscriberOp::Shape</code></li> </ul>","tags":["DDS","RTI Connext"]},{"location":"operators/dds/video/","title":"DDS Video Operators","text":"<p> Authors: Ian Stewart (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>The DDS Video Operators allow applications to read or write video buffers to a DDS databus, enabling communication with other applications via the VideoFrame DDS topic.</p> <p>This operator requires an installation of RTI Connext to provide access to the DDS domain, as specified by the OMG Data-Distribution Service</p>","tags":["DDS","RTI Connext","Video"]},{"location":"operators/dds/video/#holoscanopsddsvideopublisherop","title":"<code>holoscan::ops::DDSVideoPublisherOp</code>","text":"<p>Operator class for the DDS video publisher. This operator accepts <code>VideoBuffer</code> objects as input and publishes each buffer to DDS as a VideoFrame.</p> <p>This operator also inherits the parameters from DDSOperatorBase.</p>","tags":["DDS","RTI Connext","Video"]},{"location":"operators/dds/video/#parameters","title":"Parameters","text":"<ul> <li><code>writer_qos</code>: The name of the QoS profile to use for the DDS DataWriter</li> <li>type: <code>std::string</code></li> <li><code>stream_id</code>: The ID to use for the video stream</li> <li>type: <code>uint32_t</code></li> </ul>","tags":["DDS","RTI Connext","Video"]},{"location":"operators/dds/video/#inputs","title":"Inputs","text":"<ul> <li><code>input</code>: Input video buffer</li> <li>type: <code>nvidia::gxf::VideoBuffer</code></li> </ul>","tags":["DDS","RTI Connext","Video"]},{"location":"operators/dds/video/#holoscanopsddsvideosubscriberop","title":"<code>holoscan::ops::DDSVideoSubscriberOp</code>","text":"<p>Operator class for the DDS video subscriber. This operator reads from the VideoFrame DDS topic and outputs each received frame as <code>VideoBuffer</code> objects.</p> <p>This operator also inherits the parameters from DDSOperatorBase.</p>","tags":["DDS","RTI Connext","Video"]},{"location":"operators/dds/video/#parameters_1","title":"Parameters","text":"<ul> <li><code>reader_qos</code>: The name of the QoS profile to use for the DDS DataReader</li> <li>type: <code>std::string</code></li> <li><code>stream_id</code>: The ID of the video stream to filter for</li> <li>type: <code>uint32_t</code></li> <li><code>allocator</code>: Allocator used to allocate the output data</li> <li>type: <code>std::shared_ptr&lt;Allocator&gt;</code></li> </ul>","tags":["DDS","RTI Connext","Video"]},{"location":"operators/dds/video/#outputs","title":"Outputs","text":"<ul> <li><code>output</code>: Output video buffer</li> <li>type: <code>nvidia::gxf::VideoBuffer</code></li> </ul>","tags":["DDS","RTI Connext","Video"]},{"location":"operators/deidentification/pixelator/","title":"Pixelator Operator","text":"<p> Authors: NVIDIA Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 3.0.0 Tested Holoscan SDK versions: 3.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p>A Pixelation-based Deidentification Operator</p>","tags":["Deidentification","Image Processing","Anonymization"]},{"location":"operators/deidentification/pixelator/#overview","title":"Overview","text":"<p>In medical and sensitive imaging workflows, pixelation is a common method for deidentification. The <code>PixelatorOp</code> is a Holoscan operator that performs pixelation-based deidentification on input images, suitable for applications such as surgical video anonymization where the camera may get out of the body and capture sensitive or protected information.</p>","tags":["Deidentification","Image Processing","Anonymization"]},{"location":"operators/deidentification/pixelator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK</li> <li>cupy</li> </ul>","tags":["Deidentification","Image Processing","Anonymization"]},{"location":"operators/deidentification/pixelator/#example-usage","title":"Example Usage","text":"<pre><code>from holohub.operators.deidentification.pixelator import PixelatorOp\nop = PixelatorOp(block_size_h=16, block_size_w=16)\n</code></pre>","tags":["Deidentification","Image Processing","Anonymization"]},{"location":"operators/deidentification/pixelator/#parameters","title":"Parameters","text":"<ul> <li><code>tensor_name</code>: The name of the tensor to be pixelated.</li> <li><code>block_size_h</code>: Height of the pixelation block.</li> <li><code>block_size_w</code>: Width of the pixelation block.</li> </ul>","tags":["Deidentification","Image Processing","Anonymization"]},{"location":"operators/deltacast_videomaster/","title":"DELTACAST VideoMaster Operators","text":"<p> Authors: Laurent Radoux (Deltacast) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0, 2.9.0, 3.0.0 Contribution metric: Level 2 - Trusted</p> <p>The DELTACAST VideoMaster operator provides functionality to capture and stream high-quality video streams from DELTACAST cards. It supports both SDI and HDMI input and output sources, enabling professional video capture in various formats and resolutions. DELTACAST VideoMaster operators are designed to work seamlessly with DELTACAST's hardware capabilities.</p> <p>This library contains two operators: - videomaster_source: Captures a signal from the DELTACAST capture card. - videomaster_transmitter: Streams a signal through the DELTACAST capture card.</p> <p>These operators wrap the GXF extension to provide support for the VideoMaster SDK.</p>","tags":["Camera","Deltacast"]},{"location":"operators/deltacast_videomaster/#requirements","title":"Requirements","text":"<ul> <li>VideoMaster SDK: Operators require the VideoMaster SDK from Deltacast.</li> <li>DELTACAST Hardware: Compatible DELTACAST capture cards.</li> <li>VideoMaster driver: To detect and use DELTACAST capture cards.</li> </ul>","tags":["Camera","Deltacast"]},{"location":"operators/deltacast_videomaster/#parameters","title":"Parameters","text":"","tags":["Camera","Deltacast"]},{"location":"operators/deltacast_videomaster/#videomaster_source","title":"videomaster_source","text":"<p>The following parameters can be configured for this operator:</p> Parameter Type Description Default <code>board</code> uint32_t Index of the DELTACAST.TV board to use as source 0 <code>rdma</code> bool Enable RDMA for video input (DELTACAST driver must be compiled with RDMA enabled to use this option) false <code>input</code> uint32_t Index of the RX channel to use on the selected board 0","tags":["Camera","Deltacast"]},{"location":"operators/deltacast_videomaster/#videomaster_transmitter","title":"videomaster_transmitter","text":"<p>The following parameters can be configured for this operator:</p> Parameter Type Description Default <code>board</code> uint32_t Index of the DELTACAST.TV board to use as source 0 <code>rdma</code> bool Enable RDMA for video input (DELTACAST driver must be compiled with RDMA enabled to use this option) false <code>output</code> uint32_t Index of the TX channel to use on the selected board 0 <code>width</code> uint32_t The width of the output stream 1920 <code>height</code> uint32_t The height of the output stream 1080 <code>progressive</code> bool interleaved or progressive true <code>framerate</code> uint32_t The framerate of the output stream 60 <code>enable_overlay</code> bool Is overlay is add by card or not false","tags":["Camera","Deltacast"]},{"location":"operators/deltacast_videomaster/#building-the-operator","title":"Building the operator","text":"<p>As part of Holohub, running CMake on Holohub and point to Holoscan SDK install tree.</p> <p>The path to the VideoMaster SDK is also mandatory and can be given through the VideoMaster_SDK_DIR parameter.</p>","tags":["Camera","Deltacast"]},{"location":"operators/deltacast_videomaster/#tests","title":"Tests","text":"<p>All tests performed with the DELTACAST VideoMaster SDK <code>6.30</code>.</p> Application Device Configuration Holoscan SDK 2.9 Holoscan SDK 3.0 Holoscan SDK 3.1 deltacast_transmitter DELTA-12G-elp-key 11 TX0 (SDI) / ~~RDMA~~ PASSED PASSED PASSED deltacast_transmitter DELTA-12G-elp-key 11 TX0 (SDI) / RDMA PASSED PASSED PASSED deltacast_transmitter DELTA-12G11-hmi11-e-key TX0 (SDI) / ~~RDMA~~ PASSED PASSED PASSED deltacast_transmitter DELTA-12G11-hmi11-e-key TX0 (SDI) / RDMA PASSED PASSED PASSED deltacast_transmitter DELTA-12G11-hmi11-e-key TX1 (HDMI) / ~~RDMA~~ PASSED PASSED PASSED deltacast_transmitter DELTA-12G11-hmi11-e-key TX1 (HDMI) / RDMA PASSED PASSED PASSED endoscopy_tool_tracking DELTA-12G-elp-key 11 RX0 (SDI) / ~~overlay~~ / ~~RDMA~~ PASSED PASSED PASSED endoscopy_tool_tracking DELTA-12G-elp-key 11 RX0 (SDI) / ~~overlay~~ / RDMA PASSED PASSED PASSED endoscopy_tool_tracking DELTA-12G-elp-key 11 RX0/TX0 (SDI) / overlay / ~~RDMA~~ PASSED PASSED PASSED endoscopy_tool_tracking DELTA-12G-elp-key 11 RX0/TX0 (SDI) / overlay / RDMA PASSED PASSED PASSED endoscopy_tool_tracking DELTA-12G11-hmi11-e-key RX0 (SDI) / ~~overlay~~ / ~~RDMA~~ PASSED PASSED PASSED endoscopy_tool_tracking DELTA-12G11-hmi11-e-key RX0 (SDI) / ~~overlay~~ / RDMA PASSED PASSED PASSED endoscopy_tool_tracking DELTA-12G11-hmi11-e-key RX0/TX0 (SDI) / overlay / ~~RDMA~~ PASSED PASSED PASSED endoscopy_tool_tracking DELTA-12G11-hmi11-e-key RX0/TX0 (SDI) / overlay / RDMA PASSED PASSED PASSED endoscopy_tool_tracking DELTA-12G11-hmi11-e-key RX1 (HDMI) / ~~overlay~~ / ~~RDMA~~ PASSED PASSED PASSED endoscopy_tool_tracking DELTA-12G11-hmi11-e-key RX1 (HDMI) / ~~overlay~~ / RDMA PASSED PASSED PASSED endoscopy_tool_tracking DELTA-12G11-hmi11-e-key RX1/TX1 (HDMI) / overlay / ~~RDMA~~ PASSED PASSED PASSED endoscopy_tool_tracking DELTA-12G11-hmi11-e-key RX1/TX1 (HDMI) / overlay / RDMA PASSED PASSED PASSED","tags":["Camera","Deltacast"]},{"location":"operators/ehr_query_llm/","title":"EHR Query LLM Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: June 28, 2025 Latest version: 1.1 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.5.0, 3.3.0 Contribution metric: Level 2 - Trusted</p>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/#overview","title":"Overview","text":"<p>The EHR Query LLM Operators are a Holoscan operator that provides a robust interface for querying and processing Electronic Health Records (EHR) using the FHIR (Fast Healthcare Interoperability Resources) standard. It enables seamless integration with FHIR services, supports OAuth2 authentication, and provides standardized medical record processing capabilities.</p>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/#features","title":"Features","text":"<ul> <li>FHIR service querying with support for patient search</li> <li>OAuth2 authentication support</li> <li>Configurable FHIR endpoint</li> <li>Support for various FHIR resource types including:</li> <li>Patient</li> <li>Observation</li> <li>Condition</li> <li>DiagnosticReport</li> <li>ImagingStudy</li> <li>DocumentReference</li> <li>And more</li> <li>ZeroMQ-based message handling for distributed systems</li> <li>Standardized medical record sanitization and processing</li> <li>Comprehensive error handling and logging</li> </ul>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/#components","title":"Components","text":"","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/#fhir-client-operator","title":"FHIR Client Operator","text":"<ul> <li>Handles FHIR service queries</li> <li>Supports patient search and resource retrieval</li> <li>Configurable authentication via OAuth2</li> <li>Processes FHIR responses into standardized format</li> </ul>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/#fhir-resource-sanitizer-operator","title":"FHIR Resource Sanitizer Operator","text":"<ul> <li>Sanitizes and standardizes FHIR resources</li> <li>Transforms raw FHIR data into AI-friendly format</li> <li>Maintains essential medical information</li> <li>Supports multiple resource types</li> </ul>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/#zeromq-message-handling","title":"ZeroMQ Message Handling","text":"<ul> <li>Publisher/Subscriber pattern for distributed communication</li> <li>Configurable topics and endpoints</li> <li>Support for both blocking and non-blocking operations</li> </ul>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/#dependencies","title":"Dependencies","text":"<ul> <li>holoscan &gt;= 2.5.0</li> <li>fhir.resources &gt;= 7.0.0</li> <li>pyzmq &gt;= 25.1.0</li> <li>requests &gt;= 2.31.0</li> <li>pydantic &gt;= 2.0.0</li> </ul>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/fhir_client_op/","title":"FHIR Client Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: June 28, 2025 Latest version: 1.1 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.5.0, 3.3.0 Contribution metric: Level 2 - Trusted</p> <p>A Holoscan operator that enables seamless interaction with FHIR (Fast Healthcare Interoperability Resources) services for querying and retrieving patient medical records.</p>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/fhir_client_op/#overview","title":"Overview","text":"<p>The FHIR Client Operator provides a standardized interface for healthcare data exchange through FHIR services. It enables applications to securely query patient medical records while handling authentication, resource management, and error handling. The operator is designed to work with any FHIR-compliant server and supports various FHIR resource types.</p> <p>Key features:</p> <ul> <li>FHIR service querying with patient search capabilities</li> <li>OAuth2 authentication support</li> <li>Configurable FHIR endpoint</li> <li>Support for various FHIR resource types</li> <li>Comprehensive error handling and logging</li> </ul>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/fhir_client_op/#requirements","title":"Requirements","text":"<ul> <li>holoscan</li> <li>requests</li> <li>fhir.resources</li> </ul>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/fhir_client_op/#example-usage","title":"Example Usage","text":"<p>Please check fhir_client.py in Generative AI Application on Holoscan integrating with FHIR Services.</p>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/fhir_client_op/#name-inputoutput","title":"Name Input/Output","text":"<ul> <li>Input: <code>request</code> - JSON representation of the FHIRQuery object containing search parameters</li> <li>Output: <code>out</code> - FHIRQueryResponse object containing the original request ID and matching patient records</li> </ul>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/fhir_client_op/#parameters","title":"Parameters","text":"<ul> <li><code>fhir_endpoint</code> (str): FHIR service endpoint URL (default: \"http://localhost:8080/\")</li> <li><code>token_provider</code> (TokenProvider): Optional OAuth2 token provider for authentication</li> <li><code>verify_cert</code> (bool): Whether to verify server certificates (default: True)</li> </ul>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/fhir_resource_sanitizer_op/","title":"FHIR Resource Sanitizer Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: June 28, 2025 Latest version: 1.1 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.5.0, 3.3.0 Contribution metric: Level 2 - Trusted</p> <p>A Holoscan operator that processes and sanitizes FHIR medical records into a standardized, AI-friendly format while maintaining essential medical information.</p>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/fhir_resource_sanitizer_op/#overview","title":"Overview","text":"<p>The FHIR Resource Sanitizer Operator is designed to transform raw FHIR (Fast Healthcare Interoperability Resources) medical records into a more standardized format suitable for AI processing. It handles various FHIR resource types including Patient, Observation, Condition, DiagnosticReport, ImagingStudy, and more, while implementing robust error handling and logging mechanisms.</p>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/fhir_resource_sanitizer_op/#requirements","title":"Requirements","text":"<ul> <li>holoscan</li> <li>fhir.resources</li> <li>pydantic</li> </ul>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/fhir_resource_sanitizer_op/#example-usage","title":"Example Usage","text":"<p>Please check fhir_client.py in Generative AI Application on Holoscan integrating with FHIR Services.</p>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/fhir_resource_sanitizer_op/#name-inputoutput","title":"Name Input/Output","text":"<ul> <li> <p>Input:  <code>records</code>: A FHIRQueryResponse object containing patient medical records</p> </li> <li> <p>Output: <code>out</code>: A sanitized FHIRQueryResponse object with standardized medical records</p> </li> </ul>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/fhir_resource_sanitizer_op/#parameters","title":"Parameters","text":"<ul> <li><code>fhir_endpoint</code> (str): FHIR service endpoint URL (default: \"http://localhost:8080/\")</li> </ul>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/zero_mq_publisher_op/","title":"ZeroMQ Publisher Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: June 28, 2025 Latest version: 1.1 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.5.0, 3.3.0 Contribution metric: Level 2 - Trusted</p> <p>A Holoscan operator that publishes messages to a ZeroMQ message queue using the PUB/SUB pattern.</p>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/zero_mq_publisher_op/#overview","title":"Overview","text":"<p>The ZeroMQ Publisher Operator provides a standardized interface for publishing messages to a ZeroMQ message queue. It enables applications to send messages to subscribers while handling connection management and error handling.</p>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/zero_mq_publisher_op/#requirements","title":"Requirements","text":"<ul> <li>holoscan</li> <li>pyzmq</li> </ul>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/zero_mq_publisher_op/#example-usage","title":"Example Usage","text":"<p>Please check fhir_client.py in Generative AI Application on Holoscan integrating with FHIR Services.</p>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/zero_mq_publisher_op/#name-inputoutput","title":"Name Input/Output","text":"<ul> <li>Input: <code>message</code>: Message to be published to ZeroMQ</li> <li>Output: None</li> </ul>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/zero_mq_publisher_op/#parameters","title":"Parameters","text":"<ul> <li><code>topic</code> (str): Topic name for message filtering</li> <li><code>queue_endpoint</code> (str): ZeroMQ endpoint URL (e.g., \"tcp://*:5556\") </li> </ul>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/zero_mq_subscriber_op/","title":"ZeroMQ Subscriber Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: June 28, 2025 Latest version: 1.1 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.5.0, 3.3.0 Contribution metric: Level 2 - Trusted</p> <p>A Holoscan operator that subscribes to messages from a ZeroMQ message queue using the PUB/SUB pattern.</p>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/zero_mq_subscriber_op/#overview","title":"Overview","text":"<p>The ZeroMQ Subscriber Operator provides a standardized interface for receiving messages from a ZeroMQ message queue. It enables applications to receive messages from publishers while handling connection management and error handling.</p>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/zero_mq_subscriber_op/#requirements","title":"Requirements","text":"<ul> <li>holoscan</li> <li>pyzmq</li> </ul>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/zero_mq_subscriber_op/#example-usage","title":"Example Usage","text":"<p>Please check fhir_client.py in Generative AI Application on Holoscan integrating with FHIR Services.</p>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/zero_mq_subscriber_op/#name-inputoutput","title":"Name Input/Output","text":"<ul> <li>Input: None</li> <li>Output: <code>request</code>: Message received from ZeroMQ</li> </ul>","tags":["LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/zero_mq_subscriber_op/#parameters","title":"Parameters","text":"<ul> <li><code>topic</code> (str): Topic name for message filtering</li> <li><code>queue_endpoint</code> (str): ZeroMQ endpoint URL (e.g., \"tcp://localhost:5556\")</li> <li><code>blocking</code> (bool): Whether to use blocking receive (default: False) </li> </ul>","tags":["LLM","Healthcare Interop"]},{"location":"operators/emergent_source/","title":"emergent_source","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>No documentation found.</p>","tags":["Camera"]},{"location":"operators/fft/","title":"fft (latest)","text":"","tags":["Signal Processing"]},{"location":"operators/fft/#fft-operator","title":"FFT Operator","text":"<p> Authors: John Moon john.moon@vts-i.com (Valley Tech Systems, Inc.), Eric Ferrara eric.ferrara@vts-i.com (Valley Tech Systems, Inc.), Matthew Luckenbihl matthew.luckenbihl@vts-i.com (Valley Tech Systems, Inc.) Supported platforms: x86_64 Language: C++ Last modified: May 22, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.5.0 Tested Holoscan SDK versions: 2.5.0, 2.6.0, 2.7.0, 2.8.0, 2.9.0, 3.0.0, 3.1.0 Contribution metric: Level 3 - Developmental</p>","tags":["Signal Processing"]},{"location":"operators/fft/#overview","title":"Overview","text":"<p>A thin wrapper over the MatX <code>fft()</code> executor.</p>","tags":["Signal Processing"]},{"location":"operators/fft/#description","title":"Description","text":"<p>The FFT operator takes in a tensor of complex float data, performs an FFT, and emits the resultant tensor.</p>","tags":["Signal Processing"]},{"location":"operators/fft/#requirements","title":"Requirements","text":"<ul> <li>MatX (dependency - assumed to be installed on system)</li> </ul>","tags":["Signal Processing"]},{"location":"operators/fft/#example-usage","title":"Example Usage","text":"<p>For an example of how to use this operator, see the <code>psd_pipeline</code> application.</p>","tags":["Signal Processing"]},{"location":"operators/fft/#multiple-channels","title":"Multiple Channels","text":"<p>The zero-indexed <code>channel_number</code> key will be looked up in <code>metadata()</code> on each <code>compute()</code> run. If no value is found, the default channel number is <code>0</code>.</p>","tags":["Signal Processing"]},{"location":"operators/fft/#configuration","title":"Configuration","text":"<p>The FFT operator takes in a few parameters:</p> <pre><code>fft:\n  burst_size: 1280\n  num_bursts: 625\n  num_channels: 1\n  spectrum_type: 1\n  averaging_type: 1\n  window_time: 0\n  window_type: 0\n  transform_points: 1280\n  window_points: 1280\n  resolution: 6250\n  span: 8000000\n  weighting_factor: 0\n  f1_index: -640\n  f2_index: 639\n  window_time_delta: 0\n</code></pre> <p>The only parameters that actually impacts FFT computation at this point are the <code>burst_size</code> and <code>num_bursts</code> params. The rest of the parameters are simply passed along in the metadata.</p> <ul> <li><code>burst_size</code>: Number of samples to process in each burst</li> <li><code>num_bursts</code>: Number of bursts to process at once</li> <li><code>num_channels</code>: Number of channels for which to allocate memory</li> <li><code>spectrum_type</code>: VITA 49.2 spectrum type to pass along in metadata</li> <li><code>spectrum_type</code>: VITA 49.2 spectrum type to pass along in metadata</li> <li><code>averaging_type</code>: VITA 49.2 averaging type to pass along in metadata</li> <li><code>window_time</code>: VITA 49.2 window time to pass along in metadata</li> <li><code>window_type</code>: VITA 49.2 window type to pass along in metadata</li> <li><code>transform_points</code>: Number of FFT points to take and VITA 49.2 transform points to pass along in metadata</li> <li><code>window_points</code>: VITA 49.2 window points to pass along in metadata</li> <li><code>resolution</code>: VITA 49.2 resolution to pass along in metadata</li> <li><code>span</code>: VITA 49.2 span to pass along in metadata</li> <li><code>weighting_factor</code>: VITA 49.2 weighting factor to pass along in metadata</li> <li><code>f1_index</code>: VITA 49.2 F1 index to pass along in metadata</li> <li><code>f2_index</code>: VITA 49.2 F2 index to pass along in metadata</li> <li><code>window_time_delta</code>: VITA 49.2 window time delta to pass along in metadata</li> </ul>","tags":["Signal Processing"]},{"location":"operators/grpc_operators/","title":"Holohub gRPC Plugins for Holoscan SDK","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.6.0 Tested Holoscan SDK versions: 2.6.0 Contribution metric: Level 1 - Highly Reliable</p>","tags":["gRPC","Visualization"]},{"location":"operators/grpc_operators/#overview","title":"Overview","text":"<p>This directory contains the Holohub gRPC plugins for Holoscan SDK, including:</p> <ul> <li><code>client</code>: gRPC client and Holoscan Operators for sending, receiving, and streaming data to a gRPC server.</li> <li><code>server</code>: gRPC server and Holoscan Operators for handling requests from a gRPC client and transmitting data back to the client.</li> <li><code>protos</code>: Protocol buffers definitions of Holoscan SDK.</li> <li><code>common</code>: Tensor &lt;-&gt; protobuf converters and Holoscan Resources to handle incoming and outgoing data.</li> </ul> <p>Please refer to the gRPC h.264 Endoscopy Tool Tracking application for additional details.</p>","tags":["gRPC","Visualization"]},{"location":"operators/high_rate_psd/","title":"high_rate_psd (latest)","text":"","tags":["Signal Processing"]},{"location":"operators/high_rate_psd/#high-rate-psd-operator","title":"High Rate PSD Operator","text":"<p> Authors: John Moon john.moon@vts-i.com (Valley Tech Systems, Inc.), Eric Ferrara eric.ferrara@vts-i.com (Valley Tech Systems, Inc.), Matthew Luckenbihl matthew.luckenbihl@vts-i.com (Valley Tech Systems, Inc.) Supported platforms: x86_64 Language: C++ Last modified: May 22, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.5.0 Tested Holoscan SDK versions: 2.5.0, 2.6.0, 2.7.0, 2.8.0, 2.9.0, 3.0.0, 3.1.0 Contribution metric: Level 3 - Developmental</p>","tags":["Signal Processing"]},{"location":"operators/high_rate_psd/#overview","title":"Overview","text":"<p>A thin wrapper over the MatX <code>abs2()</code> executor.</p>","tags":["Signal Processing"]},{"location":"operators/high_rate_psd/#description","title":"Description","text":"<p>The high rate PSD operator... - takes in a tensor of complex float data, - performs a squared absolute value operation on the tensor: real(t)^2 + imag(t)^2, - divides by the number of input elements - emits the resultant tensor</p>","tags":["Signal Processing"]},{"location":"operators/high_rate_psd/#requirements","title":"Requirements","text":"<ul> <li>MatX (dependency - assumed to be installed on system)</li> </ul>","tags":["Signal Processing"]},{"location":"operators/high_rate_psd/#example-usage","title":"Example Usage","text":"<p>For an example of how to use this operator, see the <code>psd_pipeline</code> application.</p>","tags":["Signal Processing"]},{"location":"operators/high_rate_psd/#multiple-channels","title":"Multiple Channels","text":"<p>The zero-indexed <code>channel_number</code> key will be looked up in <code>metadata()</code> on each <code>compute()</code> run. If no value is found, the default channel number is <code>0</code>.</p>","tags":["Signal Processing"]},{"location":"operators/high_rate_psd/#configuration","title":"Configuration","text":"<p>The operator only takes the following parameters:</p> <pre><code>high_rate_psd:\n  burst_size: 1280\n  num_bursts: 625\n  num_channels: 1\n</code></pre> <ul> <li><code>burst_size</code>: Number of samples to process in each burst</li> <li><code>num_bursts</code>: Number of bursts to process at once</li> <li><code>num_channels</code>: Number of channels for which to allocate memory</li> </ul>","tags":["Signal Processing"]},{"location":"operators/low_rate_psd/","title":"Low Rate PSD Operator","text":"<p> Authors: John Moon john.moon@vts-i.com (Valley Tech Systems, Inc.), Eric Ferrara eric.ferrara@vts-i.com (Valley Tech Systems, Inc.), Matthew Luckenbihl matthew.luckenbihl@vts-i.com (Valley Tech Systems, Inc.) Supported platforms: x86_64 Language: C++ Last modified: May 22, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.5.0 Tested Holoscan SDK versions: 2.5.0, 2.6.0, 2.7.0, 2.8.0, 2.9.0, 3.0.0, 3.1.0 Contribution metric: Level 3 - Developmental</p> <p>A Power Spectral Density (PSD) accumulator and averager operator.</p>","tags":["Signal Processing"]},{"location":"operators/low_rate_psd/#overview","title":"Overview","text":"<p>The Low Rate PSD Operator is a utility for computing and averaging the Power Spectral Density (PSD) of input signals. PSD is a fundamental tool in signal processing for analyzing the power distribution of a signal across frequency components. This operator is designed to efficiently accumulate and average PSDs over multiple bursts of input data, making it suitable for applications such as spectrum monitoring, signal diagnostics, and real-time analysis in embedded or high-throughput environments.</p> <p>The Low Rate PSD Operator performs the following steps:</p> <ol> <li>Input Accumulation: Receives <code>num_averages</code> tensors containing float data representing signal samples or pre-computed PSDs.</li> <li>Averaging: Computes the average of all accumulated tensors to smooth out noise and fluctuations.</li> <li>Logarithmic Scaling: Applies a <code>10 * log10()</code> operation to convert the averaged power values to decibel (dB) scale, which is standard for PSD representation.</li> <li>Clamping: Restricts the data to the range of 8-bit signed integers to ensure compatibility and efficient storage.</li> <li>Casting: Converts the clamped values to signed 8-bit integers.</li> <li>Emission: Outputs the final tensor for downstream processing or analysis.</li> </ol>","tags":["Signal Processing"]},{"location":"operators/low_rate_psd/#requirements","title":"Requirements","text":"<ul> <li>MatX: Required for tensor operations (assumed to be installed on your system).</li> </ul>","tags":["Signal Processing"]},{"location":"operators/low_rate_psd/#example-usage","title":"Example Usage","text":"<p>For a practical example, see the <code>psd_pipeline</code> application.</p>","tags":["Signal Processing"]},{"location":"operators/low_rate_psd/#basic-workflow","title":"Basic Workflow","text":"<ol> <li>Configure the operator parameters (see below).</li> <li>Feed input tensors (float32 arrays) to the operator.</li> <li>Collect the output signed 8-bit integer tensor representing the averaged PSD in dB.</li> </ol>","tags":["Signal Processing"]},{"location":"operators/low_rate_psd/#multiple-channels","title":"Multiple Channels","text":"<p>The operator supports processing multiple channels in parallel. The zero-indexed <code>channel_number</code> key is retrieved from <code>metadata()</code> on each <code>compute()</code> invocation. If <code>channel_number</code> is not provided, the default is <code>0</code> (single channel).</p>","tags":["Signal Processing"]},{"location":"operators/low_rate_psd/#configuration","title":"Configuration","text":"<p>Configure the operator in your application (e.g., YAML config):</p> <pre><code>low_rate_psd:\n  burst_size: 1280         # Number of samples processed per compute() call\n  num_averages: 625        # Number of PSDs to accumulate before averaging\n  num_channels: 1          # Number of signal channels to process\n</code></pre>","tags":["Signal Processing"]},{"location":"operators/low_rate_psd/#inputoutput","title":"Input/Output","text":"<ul> <li>Input: Tensors of type <code>float32</code>, shape determined by <code>burst_size</code> and <code>num_channels</code>.</li> <li>Output: Tensor of type <code>int8</code>, representing the averaged PSD in dB scale, clamped to [-128, 127].</li> </ul>","tags":["Signal Processing"]},{"location":"operators/low_rate_psd/#notes","title":"Notes","text":"<ul> <li>Ensure the input data is properly normalized and formatted as expected by the operator.</li> <li>The operator is optimized for performance and memory efficiency, leveraging MatX for tensor operations.</li> <li>For advanced use cases, refer to the Holoscan SDK documentation and the example pipeline linked above.</li> </ul>","tags":["Signal Processing"]},{"location":"operators/lstm_tensor_rt_inference/","title":"Custom LSTM Inference","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>lstm_tensor_rt_inference</code> extension provides LSTM (Long-Short Term Memory) stateful inference module using TensorRT.</p>","tags":["LSTM","TensorRT"]},{"location":"operators/lstm_tensor_rt_inference/#nvidiaholoscanlstm_tensor_rt_inferencetensorrtinference","title":"<code>nvidia::holoscan::lstm_tensor_rt_inference::TensorRtInference</code>","text":"<p>Codelet, taking input tensors and feeding them into TensorRT for LSTM inference.</p> <p>This implementation is based on <code>nvidia::gxf::TensorRtInference</code>. <code>input_state_tensor_names</code> and <code>output_state_tensor_names</code> parameters are added to specify tensor names for states in LSTM model.</p>","tags":["LSTM","TensorRT"]},{"location":"operators/lstm_tensor_rt_inference/#parameters","title":"Parameters","text":"<ul> <li><code>model_file_path</code>: Path to ONNX model to be loaded</li> <li>type: <code>std::string</code></li> <li><code>engine_cache_dir</code>: Path to a directory containing cached generated engines to be serialized and loaded from</li> <li>type: <code>std::string</code></li> <li><code>plugins_lib_namespace</code>: Namespace used to register all the plugins in this library (default: <code>\"\"</code>)</li> <li>type: <code>std::string</code></li> <li><code>force_engine_update</code>: Always update engine regard less of existing engine file. Such conversion may take minutes (default: <code>false</code>)</li> <li>type: <code>bool</code></li> <li><code>input_tensor_names</code>: Names of input tensors in the order to be fed into the model</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>input_state_tensor_names</code>: Names of input state tensors that are used internally by TensorRT</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>input_binding_names</code>: Names of input bindings as in the model in the same order of what is provided in input_tensor_names</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>output_tensor_names</code>: Names of output tensors in the order to be retrieved from the model</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>input_state_tensor_names</code>: Names of output state tensors that are used internally by TensorRT</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>output_binding_names</code>: Names of output bindings in the model in the same order of of what is provided in output_tensor_names</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>pool</code>: Allocator instance for output tensors</li> <li>type: <code>gxf::Handle&lt;gxf::Allocator&gt;</code></li> <li><code>cuda_stream_pool</code>: Instance of gxf::CudaStreamPool to allocate CUDA stream</li> <li>type: <code>gxf::Handle&lt;gxf::CudaStreamPool&gt;</code></li> <li><code>max_workspace_size</code>: Size of working space in bytes (default: <code>67108864l</code> (64MB))</li> <li>type: <code>int64_t</code></li> <li><code>dla_core</code>: DLA Core to use. Fallback to GPU is always enabled. Default to use GPU only (<code>optional</code>)</li> <li>type: <code>int32_t</code></li> <li><code>max_batch_size</code>: Maximum possible batch size in case the first dimension is dynamic and used as batch size (default: <code>1</code>)</li> <li>type: <code>int32_t</code></li> <li><code>enable_fp16_</code>: Enable inference with FP16 and FP32 fallback (default: <code>false</code>)</li> <li>type: <code>bool</code></li> <li><code>verbose</code>: Enable verbose logging on console (default: <code>false</code>)</li> <li>type: <code>bool</code></li> <li><code>relaxed_dimension_check</code>: Ignore dimensions of 1 for input tensor dimension check (default: <code>true</code>)</li> <li>type: <code>bool</code></li> <li><code>rx</code>: List of receivers to take input tensors</li> <li>type: <code>std::vector&lt;gxf::Handle&lt;gxf::Receiver&gt;&gt;</code></li> <li><code>tx</code>: Transmitter to publish output tensors</li> <li>type: <code>gxf::Handle&lt;gxf::Transmitter&gt;</code></li> </ul>","tags":["LSTM","TensorRT"]},{"location":"operators/medical_imaging/","title":"Medical Imaging Operators","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: June 2, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>Medical image processing and inference operators.</p>","tags":["Medical Imaging","Healthcare Interop","AI","MONAI","STL"]},{"location":"operators/medical_imaging/#overview","title":"Overview","text":"<p>This set of operators accelerate the development of medical imaging AI inference application with DICOM imaging network integration by providing the following,</p> <ul> <li>application classes to automate the inference with MONAI Bundle as well as normal TorchScript models</li> <li>classes to load supported AI model from files to detected devices, GPU or CPU</li> <li>classes to parse runtime options and well-known environment variables</li> <li>DICOM study parsing and selection classes, as well as DICOM instance to volume image conversion</li> <li>DICOM instance writers to encapsulate AI inference results in these DICOM OID,</li> <li>DICOM Segmentation</li> <li>DICOM Basic Text Structured Report</li> <li>DICOM Encapsulated PDF</li> <li>Surface mesh generation and storage in STL format</li> <li>Visualization with Clara-Viz integration, as needed</li> </ul>","tags":["Medical Imaging","Healthcare Interop","AI","MONAI","STL"]},{"location":"operators/medical_imaging/#requirements","title":"Requirements","text":"<p>This set of operators depends on Holoscan SDK Python package, as well as directly on the following,</p> <ul> <li>highdicom</li> <li>monai</li> <li>nibabel</li> <li>numpy</li> <li>numpy-stl</li> <li>Pillow</li> <li>pydicom</li> <li>PyPDF2</li> <li>scikit-image</li> <li>SimpleITK</li> <li>torch</li> <li>trimesh</li> <li>typeguard</li> </ul>","tags":["Medical Imaging","Healthcare Interop","AI","MONAI","STL"]},{"location":"operators/medical_imaging/#notices","title":"Notices","text":"<p>Many of this set of operators are <code>Derivative Works</code> of MONAI Deploy App SDK under its Apache-2.0 license, and Nvidia employees have been the main contributors to MONAI Deploy App SDK.</p> <p>The dependency packages' licences can be viewed at their respective links as shown in the above section.</p>","tags":["Medical Imaging","Healthcare Interop","AI","MONAI","STL"]},{"location":"operators/medical_imaging/clara_viz_operator/","title":"Clara Viz Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: June 2, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This operator integrates Clara Viz visualization into medical imaging pipelines.</p>","tags":["Medical Imaging","Visualization","Clara Viz"]},{"location":"operators/medical_imaging/clara_viz_operator/#overview","title":"Overview","text":"<p>The <code>ClaraVizOperator</code> enables advanced visualization of medical imaging data using Clara Viz, supporting GPU-accelerated rendering and interaction.</p>","tags":["Medical Imaging","Visualization","Clara Viz"]},{"location":"operators/medical_imaging/clara_viz_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package</li> <li>clara-viz</li> <li>IPython</li> <li>ipywidgets</li> </ul>","tags":["Medical Imaging","Visualization","Clara Viz"]},{"location":"operators/medical_imaging/clara_viz_operator/#example-usage","title":"Example Usage","text":"<pre><code>from holoscan.core import Fragment\nfrom operators.medical_imaging.clara_viz_operator import ClaraVizOperator\n\nfragment = Fragment()\nviz_op = ClaraVizOperator(\n    fragment,\n    name=\"clara_viz\",  # Optional operator name\n    input_name_image=\"image\",  # Name of the input port for the image\n    input_name_seg_image=\"seg_image\"  # Name of the input port for the segmentation image\n)\n</code></pre>","tags":["Medical Imaging","Visualization","Clara Viz"]},{"location":"operators/medical_imaging/dicom_data_loader_operator/","title":"DICOM Data Loader Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: June 2, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This operator loads DICOM studies into memory from a folder of DICOM instance files.</p>","tags":["Medical Imaging","DICOM","Loader"]},{"location":"operators/medical_imaging/dicom_data_loader_operator/#overview","title":"Overview","text":"<p>The <code>DICOMDataLoaderOperator</code> loads DICOM studies from a specified folder, making them available as a list of <code>DICOMStudy</code> objects for downstream processing in Holoscan medical imaging pipelines.</p>","tags":["Medical Imaging","DICOM","Loader"]},{"location":"operators/medical_imaging/dicom_data_loader_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package</li> <li>pydicom</li> </ul>","tags":["Medical Imaging","DICOM","Loader"]},{"location":"operators/medical_imaging/dicom_data_loader_operator/#example-usage","title":"Example Usage","text":"<pre><code>from pathlib import Path\nfrom holoscan.core import Fragment\nfrom operators.medical_imaging.dicom_data_loader_operator import DICOMDataLoaderOperator\n\nfragment = Fragment()\ndicom_loader = DICOMDataLoaderOperator(\n    fragment,\n    name=\"dicom_loader\",  # Optional operator name\n    input_folder=Path(\"input\"),  # Path to folder containing DICOM files\n    output_name=\"dicom_study_list\",  # Name of the output port\n    must_load=True  # Whether to raise an error if no DICOM files are found\n)\n</code></pre>","tags":["Medical Imaging","DICOM","Loader"]},{"location":"operators/medical_imaging/dicom_encapsulated_pdf_writer_operator/","title":"DICOM Encapsulated PDF Writer Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: June 2, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This operator writes encapsulated PDF documents into DICOM format for medical imaging workflows.</p>","tags":["Medical Imaging","DICOM","PDF","Writer"]},{"location":"operators/medical_imaging/dicom_encapsulated_pdf_writer_operator/#overview","title":"Overview","text":"<p>The <code>DICOMEncapsulatedPDFWriterOperator</code> converts PDF files into DICOM-compliant encapsulated PDF objects for storage and interoperability in medical imaging pipelines.</p>","tags":["Medical Imaging","DICOM","PDF","Writer"]},{"location":"operators/medical_imaging/dicom_encapsulated_pdf_writer_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package</li> <li>pydicom</li> <li>PyPDF2</li> </ul>","tags":["Medical Imaging","DICOM","PDF","Writer"]},{"location":"operators/medical_imaging/dicom_encapsulated_pdf_writer_operator/#example-usage","title":"Example Usage","text":"<pre><code>from pathlib import Path\nfrom holoscan.core import Fragment\nfrom operators.medical_imaging.dicom_encapsulated_pdf_writer_operator import DICOMEncapsulatedPDFWriterOperator\nfrom operators.medical_imaging.utils.dicom_utils import ModelInfo, EquipmentInfo\n\nfragment = Fragment()\npdf_writer_op = DICOMEncapsulatedPDFWriterOperator(\n    fragment,\n    name=\"pdf_writer\",  # Optional operator name\n    output_folder=Path(\"output\"),  # Path to save the generated DICOM file(s)\n    model_info=ModelInfo(\n        creator=\"ExampleCreator\",\n        name=\"ExampleModel\",\n        version=\"1.0.0\",\n        uid=\"1.2.3.4.5.6.7.8.9\"\n    ),\n    equipment_info=EquipmentInfo(\n        manufacturer=\"ExampleManufacturer\",\n        manufacturer_model=\"ExampleModel\",\n        series_number=\"0000\",\n        software_version_number=\"1.0.0\"\n    ),\n    copy_tags=True,  # Set to True to copy tags from a DICOMSeries\n    custom_tags={\"PatientName\": \"DOE^JOHN\"}  # Optional: custom DICOM tags as a dict\n)\n</code></pre>","tags":["Medical Imaging","DICOM","PDF","Writer"]},{"location":"operators/medical_imaging/dicom_seg_writer_operator/","title":"DICOM Segmentation Writer Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: June 2, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This operator writes segmentation results into DICOM Segmentation objects for medical imaging workflows.</p>","tags":["Medical Imaging","DICOM","Segmentation","Writer"]},{"location":"operators/medical_imaging/dicom_seg_writer_operator/#overview","title":"Overview","text":"<p>The <code>DICOMSegmentationWriterOperator</code> takes segmentation data and encodes it into DICOM-compliant segmentation objects, enabling interoperability and storage in clinical systems.</p>","tags":["Medical Imaging","DICOM","Segmentation","Writer"]},{"location":"operators/medical_imaging/dicom_seg_writer_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package</li> <li>pydicom</li> <li>highdicom</li> <li>SimpleITK (for image I/O, if using NIfTI or MHD files)</li> <li>numpy</li> </ul>","tags":["Medical Imaging","DICOM","Segmentation","Writer"]},{"location":"operators/medical_imaging/dicom_seg_writer_operator/#example-usage","title":"Example Usage","text":"<pre><code>from pathlib import Path\nfrom holoscan.core import Fragment\nfrom operators.medical_imaging.dicom_seg_writer_operator import DICOMSegmentationWriterOperator, SegmentDescription\nfrom highdicom import codes\n\nfragment = Fragment()\nseg_writer_op = DICOMSegmentationWriterOperator(\n    fragment,\n    name=\"seg_writer\",  # Optional operator name\n    segment_descriptions=[\n        SegmentDescription(\n            segment_label=\"Liver\",\n            segmented_property_category=codes.DCM.Organ,\n            segmented_property_type=codes.DCM.Liver,\n            algorithm_name=\"ExampleAlgorithm\",\n            algorithm_version=\"1.0.0\"\n        )\n    ],\n    output_folder=Path(\"output\"),  # Path to save the generated DICOM file(s)\n    custom_tags={\"PatientName\": \"DOE^JOHN\"},  # Optional: custom DICOM tags as a dict\n    omit_empty_frames=True  # Whether to omit frames with no segmentation\n)\n</code></pre>","tags":["Medical Imaging","DICOM","Segmentation","Writer"]},{"location":"operators/medical_imaging/dicom_series_selector_operator/","title":"DICOM Series Selector Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: June 2, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This operator selects specific DICOM series from a set of studies for further processing in medical imaging workflows.</p>","tags":["Medical Imaging","DICOM","Selector"]},{"location":"operators/medical_imaging/dicom_series_selector_operator/#overview","title":"Overview","text":"<p>The <code>DICOMSeriesSelectorOperator</code> enables filtering and selection of relevant DICOM series, streamlining downstream analysis and processing in Holoscan pipelines.</p>","tags":["Medical Imaging","DICOM","Selector"]},{"location":"operators/medical_imaging/dicom_series_selector_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package</li> <li>pydicom</li> </ul>","tags":["Medical Imaging","DICOM","Selector"]},{"location":"operators/medical_imaging/dicom_series_selector_operator/#example-usage","title":"Example Usage","text":"<pre><code>from holoscan.core import Fragment\nfrom operators.medical_imaging.dicom_series_selector_operator import DICOMSeriesSelectorOperator\n\nfragment = Fragment()\nselector_op = DICOMSeriesSelectorOperator(\n    fragment,\n    name=\"series_selector\",  # Optional operator name\n    rules=\"\"\"\n    {\n        \"Modality\": \"CT\",\n        \"SeriesDescription\": \"Axial\"\n    }\n    \"\"\",  # JSON string defining selection rules\n    all_matched=False  # Whether all rules must match (AND) or any rule can match (OR)\n)\n</code></pre>","tags":["Medical Imaging","DICOM","Selector"]},{"location":"operators/medical_imaging/dicom_series_to_volume_operator/","title":"DICOM Series to Volume Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: June 2, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This operator converts a DICOM series into a volumetric image for downstream analysis in medical imaging workflows.</p>","tags":["Medical Imaging","DICOM","Volume"]},{"location":"operators/medical_imaging/dicom_series_to_volume_operator/#overview","title":"Overview","text":"<p>The <code>DICOMSeriesToVolumeOperator</code> reads a DICOM series and constructs a volume image suitable for 3D processing and visualization in Holoscan pipelines.</p>","tags":["Medical Imaging","DICOM","Volume"]},{"location":"operators/medical_imaging/dicom_series_to_volume_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package</li> <li>pydicom</li> <li>numpy</li> </ul>","tags":["Medical Imaging","DICOM","Volume"]},{"location":"operators/medical_imaging/dicom_series_to_volume_operator/#example-usage","title":"Example Usage","text":"<pre><code>from holoscan.core import Fragment\nfrom operators.medical_imaging.dicom_series_to_volume_operator import DICOMSeriesToVolumeOperator\n\nfragment = Fragment()\nvol_op = DICOMSeriesToVolumeOperator(\n    fragment,\n    name=\"series_to_volume\"  # Optional operator name\n)\n</code></pre>","tags":["Medical Imaging","DICOM","Volume"]},{"location":"operators/medical_imaging/dicom_text_sr_writer_operator/","title":"DICOM Text SR Writer Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: June 2, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This operator writes DICOM Structured Report (SR) objects containing text results for medical imaging workflows.</p>","tags":["Medical Imaging","DICOM","SR","Text"]},{"location":"operators/medical_imaging/dicom_text_sr_writer_operator/#overview","title":"Overview","text":"<p>The <code>DICOMTextSRWriterOperator</code> encodes textual results into DICOM-compliant Structured Report objects, enabling standardized storage and interoperability in clinical systems.</p>","tags":["Medical Imaging","DICOM","SR","Text"]},{"location":"operators/medical_imaging/dicom_text_sr_writer_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package</li> <li>pydicom</li> <li>highdicom</li> </ul>","tags":["Medical Imaging","DICOM","SR","Text"]},{"location":"operators/medical_imaging/dicom_text_sr_writer_operator/#example-usage","title":"Example Usage","text":"<pre><code>from pathlib import Path\nfrom holoscan.core import Fragment\nfrom operators.medical_imaging.dicom_text_sr_writer_operator import DICOMTextSRWriterOperator\nfrom operators.medical_imaging.utils.dicom_utils import ModelInfo, EquipmentInfo\n\nfragment = Fragment()\nsr_writer_op = DICOMTextSRWriterOperator(\n    fragment,\n    name=\"sr_writer\",  # Optional operator name\n    output_folder=Path(\"output\"),  # Path to save the generated DICOM file(s)\n    model_info=ModelInfo(\n        creator=\"ExampleCreator\",\n        name=\"ExampleModel\",\n        version=\"1.0.0\",\n        uid=\"1.2.3.4.5.6.7.8.9\"\n    ),\n    equipment_info=EquipmentInfo(\n        manufacturer=\"ExampleManufacturer\",\n        manufacturer_model=\"ExampleModel\",\n        series_number=\"0000\",\n        software_version_number=\"1.0.0\"\n    ),\n    copy_tags=True,  # Set to True to copy tags from a DICOMSeries\n    custom_tags={\"PatientName\": \"DOE^JOHN\"}  # Optional: custom DICOM tags as a dict\n)\n</code></pre>","tags":["Medical Imaging","DICOM","SR","Text"]},{"location":"operators/medical_imaging/inference_operator/","title":"Inference Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: June 2, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This operator provides a base class for running inference in medical imaging pipelines.</p>","tags":["Medical Imaging","Inference","Base"]},{"location":"operators/medical_imaging/inference_operator/#overview","title":"Overview","text":"<p>The <code>InferenceOperator</code> serves as a foundation for building specialized inference operators, handling model loading, execution, and result management.</p>","tags":["Medical Imaging","Inference","Base"]},{"location":"operators/medical_imaging/inference_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package</li> <li>torch (optional, for deep learning models)</li> </ul>","tags":["Medical Imaging","Inference","Base"]},{"location":"operators/medical_imaging/inference_operator/#example-usage","title":"Example Usage","text":"<pre><code>from holoscan.core import Fragment\nfrom operators.medical_imaging.inference_operator import InferenceOperator\n\nclass MyInferenceOperator(InferenceOperator):\n    def __init__(self, fragment, *args, **kwargs):\n        super().__init__(fragment, *args, **kwargs)\n\n    def pre_process(self, data, *args, **kwargs):\n        # Implement preprocessing logic\n        return data\n\n    def predict(self, data, *args, **kwargs):\n        # Implement inference logic\n        return data\n\n    def post_process(self, data, *args, **kwargs):\n        # Implement postprocessing logic\n        return data\n\nfragment = Fragment()\ninference_op = MyInferenceOperator(\n    fragment,\n    name=\"my_inference\"  # Optional operator name\n)\n</code></pre>","tags":["Medical Imaging","Inference","Base"]},{"location":"operators/medical_imaging/monai_bundle_inference_operator/","title":"MONAI Bundle Inference Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: June 2, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This operator performs inference using MONAI Bundles for medical imaging tasks.</p>","tags":["Medical Imaging","MONAI","Bundle","Inference"]},{"location":"operators/medical_imaging/monai_bundle_inference_operator/#overview","title":"Overview","text":"<p>The <code>MonaiBundleInferenceOperator</code> loads a MONAI Bundle model and applies it to input medical images for inference, supporting flexible deployment in Holoscan pipelines.</p>","tags":["Medical Imaging","MONAI","Bundle","Inference"]},{"location":"operators/medical_imaging/monai_bundle_inference_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package</li> <li>MONAI</li> <li>torch</li> </ul>","tags":["Medical Imaging","MONAI","Bundle","Inference"]},{"location":"operators/medical_imaging/monai_bundle_inference_operator/#example-usage","title":"Example Usage","text":"<pre><code>from pathlib import Path\nfrom holoscan.core import Fragment\nfrom operators.medical_imaging.monai_bundle_inference_operator import MonaiBundleInferenceOperator\nfrom operators.medical_imaging.core import AppContext, IOMapping, IOType, Image\n\nfragment = Fragment()\napp_context = AppContext({})  # Initialize with empty args dict\n\nbundle_op = MonaiBundleInferenceOperator(\n    fragment,\n    name=\"monai_bundle\",  # Optional operator name\n    app_context=app_context,\n    input_mapping=[\n        IOMapping(\n            label=\"image\",\n            data_type=Image,\n            storage_type=IOType.IN_MEMORY\n        )\n    ],\n    output_mapping=[\n        IOMapping(\n            label=\"pred\",\n            data_type=Image,\n            storage_type=IOType.IN_MEMORY\n        )\n    ],\n    model_name=\"model\",  # Name of the model in the bundle\n    bundle_path=Path(\"model/model.ts\")  # Path to the MONAI bundle\n)\n</code></pre>","tags":["Medical Imaging","MONAI","Bundle","Inference"]},{"location":"operators/medical_imaging/monai_seg_inference_operator/","title":"MONAI Segmentation Inference Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: June 2, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This segmentation operator uses MONAI transforms and Sliding Window Inference to segment medical images.</p>","tags":["Medical Imaging","MONAI","Segmentation","Inference"]},{"location":"operators/medical_imaging/monai_seg_inference_operator/#overview","title":"Overview","text":"<p>The <code>MonaiSegInferenceOperator</code> performs pre-transforms on input images, runs segmentation inference using a specified model, and applies post-transforms. The segmentation result is returned as an in-memory image object and can optionally be saved to disk.</p>","tags":["Medical Imaging","MONAI","Segmentation","Inference"]},{"location":"operators/medical_imaging/monai_seg_inference_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package</li> <li>MONAI</li> <li>torch</li> </ul>","tags":["Medical Imaging","MONAI","Segmentation","Inference"]},{"location":"operators/medical_imaging/monai_seg_inference_operator/#example-usage","title":"Example Usage","text":"<pre><code>from pathlib import Path\nimport torch\nfrom monai.transforms import Compose, LoadImage, ScaleIntensity, EnsureChannelFirst\nfrom holoscan.core import Fragment\nfrom operators.medical_imaging.monai_segmentation_inference_operator import MonaiSegInferenceOperator\nfrom operators.medical_imaging.core import AppContext, IOMapping, IOType, Image\n\n# Initialize the fragment\nfragment = Fragment()\n\n# Create app context\napp_context = AppContext({})\n\n# Define transforms\npre_transforms = Compose([\n    LoadImage(image_only=True),\n    EnsureChannelFirst(),\n    ScaleIntensity(),\n])\n\npost_transforms = Compose([\n    # Add your post-processing transforms here\n])\n\n# Initialize the segmentation operator\nseg_op = MonaiSegInferenceOperator(\n    fragment,\n    roi_size=(96, 96, 96),  # Example ROI size for 3D images\n    pre_transforms=pre_transforms,\n    post_transforms=post_transforms,\n    app_context=app_context,\n    model_name=\"unet\",  # Example model name\n    overlap=0.25,\n    sw_batch_size=4,\n    model_path=Path(\"/path/to/your/model.pt\")  # Replace with your model path\n)\n</code></pre>","tags":["Medical Imaging","MONAI","Segmentation","Inference"]},{"location":"operators/medical_imaging/nii_data_loader_operator/","title":"NIfTI Data Loader Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: June 2, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This operator loads NIfTI (nii/nii.gz) medical images for use in Holoscan pipelines.</p>","tags":["Medical Imaging","NIfTI","Data Loading"]},{"location":"operators/medical_imaging/nii_data_loader_operator/#overview","title":"Overview","text":"<p>The <code>NIIDataLoaderOperator</code> reads NIfTI files and makes them available as in-memory images for downstream processing in medical imaging workflows.</p>","tags":["Medical Imaging","NIfTI","Data Loading"]},{"location":"operators/medical_imaging/nii_data_loader_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package (version &gt;= 1.0.3)</li> <li>SimpleITK</li> <li>numpy</li> </ul>","tags":["Medical Imaging","NIfTI","Data Loading"]},{"location":"operators/medical_imaging/nii_data_loader_operator/#example-usage","title":"Example Usage","text":"<pre><code>from pathlib import Path\nfrom holoscan.core import Fragment\nfrom operators.medical_imaging.nii_data_loader_operator import NiftiDataLoader\n\n# Create a fragment\nfragment = Fragment()\n\n# Initialize the NIfTI loader with a path to your NIfTI file\nnii_path = Path(\"path/to/your/image.nii\")  # or .nii.gz\nnii_loader = NiftiDataLoader(fragment, input_path=nii_path)\n\n# The operator can be used in a pipeline\n# The output port 'image' will contain the loaded image as a numpy array\n# You can connect it to other operators that expect image data\n</code></pre>","tags":["Medical Imaging","NIfTI","Data Loading"]},{"location":"operators/medical_imaging/nii_data_loader_operator/#inputoutput-ports","title":"Input/Output Ports","text":"<ul> <li>Input:</li> <li><code>image_path</code> (optional): Path to the NIfTI file. If not provided, uses the path specified during initialization.</li> <li>Output:</li> <li><code>image</code>: Numpy array containing the loaded image data.</li> </ul>","tags":["Medical Imaging","NIfTI","Data Loading"]},{"location":"operators/medical_imaging/nii_data_loader_operator/#notes","title":"Notes","text":"<ul> <li>The operator supports both .nii and .nii.gz file formats</li> <li>The output image is transposed to match the expected orientation (axes order: [2, 1, 0])</li> </ul>","tags":["Medical Imaging","NIfTI","Data Loading"]},{"location":"operators/medical_imaging/png_converter_operator/","title":"PNG Converter Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: June 2, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This operator converts medical images to PNG format for visualization or storage.</p>","tags":["Medical Imaging","PNG","Image Conversion"]},{"location":"operators/medical_imaging/png_converter_operator/#overview","title":"Overview","text":"<p>The <code>PNGConverterOperator</code> takes medical imaging data and outputs PNG images, facilitating integration with visualization tools and pipelines.</p>","tags":["Medical Imaging","PNG","Image Conversion"]},{"location":"operators/medical_imaging/png_converter_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package</li> <li>Pillow</li> </ul>","tags":["Medical Imaging","PNG","Image Conversion"]},{"location":"operators/medical_imaging/png_converter_operator/#example-usage","title":"Example Usage","text":"<pre><code>from pathlib import Path\nfrom holoscan.core import Fragment\nfrom operators.medical_imaging.png_converter_operator import PNGConverterOperator\nfrom operators.medical_imaging.core import Image\nimport numpy as np\n\n# Create a Fragment\nfragment = Fragment()\n\n# Create output directory\noutput_folder = Path(\"output_png\")\noutput_folder.mkdir(exist_ok=True)\n\n# Create the PNG converter operator\npng_op = PNGConverterOperator(\n    fragment,\n    output_folder=output_folder,\n    name=\"png_converter\"\n)\n\n# Example: Convert a 3D medical image to PNG slices\n# Assuming you have a 3D numpy array or Image object\n# For a 3D array of shape (slices, height, width)\nimage_data = np.random.randint(0, 255, (10, 512, 512), dtype=np.uint8)  # Example data\nmedical_image = Image(image_data)  # Create Image object\n\n# Convert and save the slices\npng_op.convert_and_save(medical_image, output_folder)\n</code></pre> <p>The operator will save individual PNG files for each slice in the specified output folder, named sequentially (0.png, 1.png, etc.).</p>","tags":["Medical Imaging","PNG","Image Conversion"]},{"location":"operators/medical_imaging/publisher_operator/","title":"Publisher Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: June 2, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This operator publishes medical imaging data to downstream consumers or external systems.</p>","tags":["Medical Imaging","Publisher"]},{"location":"operators/medical_imaging/publisher_operator/#overview","title":"Overview","text":"<p>The <code>PublisherOperator</code> enables flexible publishing of processed medical imaging data for visualization, storage, or further analysis in Holoscan pipelines.</p>","tags":["Medical Imaging","Publisher"]},{"location":"operators/medical_imaging/publisher_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package</li> </ul>","tags":["Medical Imaging","Publisher"]},{"location":"operators/medical_imaging/publisher_operator/#example-usage","title":"Example Usage","text":"<pre><code>from holoscan.core import Fragment\nfrom operators.medical_imaging.publisher_operator import PublisherOperator\nfrom pathlib import Path\n\n# Create a fragment\nfragment = Fragment()\n\n# Initialize the publisher operator with input and output folders\npub_op = PublisherOperator(\n    fragment,\n    input_folder=Path(\"path/to/input\"),  # Folder containing input and segment mask files\n    output_folder=Path(\"path/to/output\")  # Folder where published files will be saved\n)\n\n# Add the operator to the fragment\nfragment.add_operator(pub_op)\n</code></pre> <p>The operator expects:</p> <ul> <li>Input folder containing medical imaging files (nii, nii.gz, or mhd format)</li> <li>Output folder where the published files will be saved</li> <li>The operator will automatically find density and mask files in the input folder</li> <li>Published files will include the original images and configuration files for visualization</li> </ul>","tags":["Medical Imaging","Publisher"]},{"location":"operators/medical_imaging/stl_conversion_operator/","title":"STL Conversion Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: June 2, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This operator converts medical imaging data to STL format for 3D visualization and printing.</p>","tags":["Medical Imaging","STL","Conversion"]},{"location":"operators/medical_imaging/stl_conversion_operator/#overview","title":"Overview","text":"<p>The <code>STLConversionOperator</code> takes volumetric or surface data and outputs STL files, supporting workflows for 3D modeling and printing in medical imaging.</p>","tags":["Medical Imaging","STL","Conversion"]},{"location":"operators/medical_imaging/stl_conversion_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package</li> <li>numpy</li> <li>numpy-stl</li> </ul>","tags":["Medical Imaging","STL","Conversion"]},{"location":"operators/medical_imaging/stl_conversion_operator/#example-usage","title":"Example Usage","text":"<p>Here's a basic example of how to use the STLConversionOperator:</p> <pre><code>from holoscan.core import Fragment\nfrom operators.medical_imaging.stl_conversion_operator import STLConversionOperator\nfrom pathlib import Path\n\n# Create a fragment\nfragment = Fragment()\n\n# Initialize the STL conversion operator\nstl_operator = STLConversionOperator(\n    fragment,\n    output_file=\"output/surface_mesh.stl\",  # Path to save the STL file\n    is_smooth=True,  # Enable mesh smoothing\n    keep_largest_connected_component=True  # Keep only the largest connected component\n)\n\n# Setup the operator\nstl_operator.setup()\n\n# Example: Convert an image to STL\n# Assuming 'image' is an Image object with volumetric data\nstl_bytes = stl_operator._convert(image, Path(\"output/surface_mesh.stl\"))\n</code></pre> <p>For a complete workflow example that includes loading DICOM data and converting it to STL, please refer to the tutorial on Processing DICOM to USD with MONAI Deploy and Holoscan.</p>","tags":["Medical Imaging","STL","Conversion"]},{"location":"operators/medical_imaging/stl_conversion_operator/#parameters","title":"Parameters","text":"<p>The STLConversionOperator accepts the following parameters:</p> <ul> <li><code>output_file</code> (Path or str): Path where the STL file will be saved</li> <li><code>class_id</code> (array, optional): Class label IDs to include in the conversion</li> <li><code>is_smooth</code> (bool, optional): Whether to apply mesh smoothing (default: True)</li> <li><code>keep_largest_connected_component</code> (bool, optional): Whether to keep only the largest connected component (default: True)</li> </ul>","tags":["Medical Imaging","STL","Conversion"]},{"location":"operators/mesh_to_usd/","title":"mesh_to_usd","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 2 - Trusted</p> <p>No documentation found.</p>","tags":["OpenUSD","STL"]},{"location":"operators/npp_filter/","title":"NPP Filter","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 22, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>npp_filter</code> operator uses NPP to apply a filters to a Tensor or VideBuffer.</p>","tags":["NPP"]},{"location":"operators/npp_filter/#holoscanopsnppfilter","title":"<code>holoscan::ops::NppFilter</code>","text":"<p>Operator class to apply a filter of the NPP library to a Tensor or VideBuffer.</p>","tags":["NPP"]},{"location":"operators/npp_filter/#parameters","title":"Parameters","text":"<ul> <li><code>filter</code>: Name of the filter to apply (supported Gauss, SobelHoriz, SobelVert)</li> <li>type: <code>std::string</code></li> <li><code>mask_size</code>: Filter mask size (supported values 3, 5, 7, 9, 11, 13)</li> <li>type: <code>uint32_t</code></li> <li><code>allocator</code>: Allocator used to allocate the output data</li> <li>type: <code>std::shared_ptr&lt;Allocator&gt;</code></li> </ul>","tags":["NPP"]},{"location":"operators/npp_filter/#inputs","title":"Inputs","text":"<ul> <li><code>input</code>: Input frame data</li> <li>type: <code>nvidia::gxf::Tensor</code> or <code>nvidia::gxf::VideoBuffer</code></li> </ul>","tags":["NPP"]},{"location":"operators/npp_filter/#outputs","title":"Outputs","text":"<ul> <li><code>input</code>: Output frame data</li> <li>type: <code>nvidia::gxf::Tensor</code> or <code>nvidia::gxf::VideoBuffer</code></li> </ul>","tags":["NPP"]},{"location":"operators/openigtlink/","title":"OpenIGTLink operator","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 2 - Trusted</p> <p>The <code>openigtlink</code> operator provides a way to send and receive imaging data using the OpenIGTLink library. The <code>openigtlink</code> operator contains separate operators for transmit and receive. Users may choose one or the other, or use both in applications requiring bidirectional traffic.</p> <p>The <code>openigtlink</code> operators use class names: <code>OpenIGTLinkTxOp</code> and <code>OpenIGTLinkRxOp</code></p>","tags":["Streaming","3D Slicer"]},{"location":"operators/openigtlink/#nvidiaholoscanopenigtlink","title":"<code>nvidia::holoscan::openigtlink</code>","text":"<p>Operator class to send and transmit data using the OpenIGTLink protocol.</p>","tags":["Streaming","3D Slicer"]},{"location":"operators/openigtlink/#receiver-configuration-parameters","title":"Receiver Configuration Parameters","text":"<ul> <li><code>port</code>: Port number of server</li> <li>type: <code>integer</code></li> <li><code>out_tensor_name</code>: Name of output tensor</li> <li>type: <code>string</code></li> <li><code>flip_width_height</code>: Flip width and height (necessary for receiving from 3D Slicer)</li> <li>type: <code>bool</code></li> </ul>","tags":["Streaming","3D Slicer"]},{"location":"operators/openigtlink/#transmitter-configuration-parameters","title":"Transmitter Configuration Parameters","text":"<ul> <li><code>device_name</code>: OpenIGTLink device name</li> <li>type: <code>string</code></li> <li><code>input_names</code>: Names of input messages</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>host_name</code>: Host name</li> <li>type: <code>string</code></li> <li><code>port</code>: Port number of server</li> <li>type: <code>integer</code></li> </ul>","tags":["Streaming","3D Slicer"]},{"location":"operators/orsi/orsi_format_converter/","title":"orsi_format_converter","text":"<p> Authors: Jasper Hofman (Orsi Academy) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 3 - Developmental</p> <p>No documentation found.</p>","tags":["converter"]},{"location":"operators/orsi/orsi_segmentation_postprocessor/","title":"orsi_segmentation_postprocessor","text":"<p> Authors: Jasper Hofman (Orsi Academy) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 3 - Developmental</p> <p>No documentation found.</p>"},{"location":"operators/orsi/orsi_segmentation_preprocessor/","title":"orsi_segmentation_preprocessor","text":"<p> Authors: Jasper Hofman (Orsi Academy) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 3 - Developmental</p> <p>No documentation found.</p>"},{"location":"operators/orsi/orsi_visualizer/","title":"orsi_visualizer","text":"<p> Authors: Jasper Hofman (Orsi Academy) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 3 - Developmental</p> <p>No documentation found.</p>","tags":["Visualization"]},{"location":"operators/prohawk_video_processing/","title":"prohawk_video_processing","text":"<p> Authors: Tim Wooldridge (Prohawk Technology Group) Supported platforms: aarch64 Language: C++ Last modified: May 22, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 0.5.1 Tested Holoscan SDK versions: 0.5.1, 0.6.0 Contribution metric: Level 4 - Experimental</p> <p>No documentation found.</p>","tags":["Video"]},{"location":"operators/qt_video/","title":"Qt Video Operator","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 22, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>qt_video</code> operator is used to display a video in a QtQuick application.</p> <p>For more information on how to use this operator in an application see Qt video replayer example.</p>","tags":["Qt","Video","UI"]},{"location":"operators/qt_video/#holoscanopsqtvideoop","title":"<code>holoscan::ops::QtVideoOp</code>","text":"<p>Operator class.</p>","tags":["Qt","Video","UI"]},{"location":"operators/qt_video/#parameters","title":"Parameters","text":"<ul> <li><code>QtHoloscanVideo</code>: Instance of QtHoloscanVideo to be used<ul> <li>type: `QtHoloscanVideo</li> </ul> </li> </ul>","tags":["Qt","Video","UI"]},{"location":"operators/qt_video/#inputs","title":"Inputs","text":"<ul> <li><code>input</code>: Input frame data</li> <li>type: <code>nvidia::gxf::Tensor</code> or <code>nvidia::gxf::VideoBuffer</code></li> </ul>","tags":["Qt","Video","UI"]},{"location":"operators/realsense_camera/","title":"Intel RealSense Camera Operator","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 22, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 1 - Highly Reliable</p>"},{"location":"operators/realsense_camera/#overview","title":"Overview","text":"<p>Captures frames from an Intel RealSense camera.</p>"},{"location":"operators/tensor_to_video_buffer/","title":"GXF Tensor to VideoBuffer Converter","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>tensor_to_video_buffer</code> converts GXF Tensor to VideoBuffer.</p>","tags":["Tensor","Video"]},{"location":"operators/tensor_to_video_buffer/#holoscanopstensortovideobufferop","title":"<code>holoscan::ops::TensorToVideoBufferOp</code>","text":"<p>Operator class to convert GXF Tensor to VideoBuffer. This operator is required for data transfer  between Holoscan operators that output GXF Tensor and the other Holoscan Wrapper Operators that understand only VideoBuffer. It receives GXF Tensor as input and outputs GXF VideoBuffer created from it.</p>","tags":["Tensor","Video"]},{"location":"operators/tensor_to_video_buffer/#parameters","title":"Parameters","text":"<ul> <li><code>data_in</code>: Data in GXF Tensor format</li> <li>type: <code>holoscan::IOSpec*</code></li> <li><code>data_out</code>: Data in GXF VideoBuffer format</li> <li>type: <code>holoscan::IOSpec*</code></li> <li><code>in_tensor_name</code>: Name of the input tensor</li> <li>type: <code>std::string</code></li> <li><code>video_format</code>: The video format, supported values: \"yuv420\", \"rgb\"</li> <li>type: <code>std::string</code></li> </ul>","tags":["Tensor","Video"]},{"location":"operators/tool_tracking_postprocessor/","title":"Tool tracking postprocessor","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>tool_tracking_postprocessor</code> extension provides a codelet that converts inference output of <code>lstm_tensor_rt_inference</code> used in the endoscopy tool tracking pipeline to be consumed by the <code>holoviz</code> codelet.</p>","tags":["Visualization"]},{"location":"operators/tool_tracking_postprocessor/#nvidiaholoscantool_tracking_postprocessor","title":"<code>nvidia::holoscan::tool_tracking_postprocessor</code>","text":"<p>Tool tracking postprocessor codelet</p>","tags":["Visualization"]},{"location":"operators/tool_tracking_postprocessor/#parameters","title":"Parameters","text":"<ul> <li><code>in</code>: Input channel, type <code>gxf::Tensor</code></li> <li>type: <code>gxf::Handle&lt;gxf::Receiver&gt;</code></li> <li><code>out</code>: Output channel, type <code>gxf::Tensor</code></li> <li>type: <code>gxf::Handle&lt;gxf::Transmitter&gt;</code></li> <li><code>min_prob</code>: Minimum probability, (default: 0.5)</li> <li>type: <code>float</code></li> <li><code>overlay_img_colors</code>: Color of the image overlays, a list of RGB values with components between 0 and 1, (default: 12 qualitative classes color scheme from colorbrewer2)</li> <li>type: <code>std::vector&lt;std::vector&lt;float&gt;&gt;</code></li> <li><code>device_allocator</code>: Output Allocator</li> <li>type: <code>gxf::Handle&lt;gxf::Allocator&gt;</code></li> <li><code>cuda_stream_pool</code>: Instance of gxf::CudaStreamPool</li> <li>type: <code>gxf::Handle&lt;gxf::CudaStreamPool&gt;</code></li> </ul>","tags":["Visualization"]},{"location":"operators/unzip/","title":"Unzip Operator","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.1 Tested Holoscan SDK versions: 2.0.1 Contribution metric: Level 2 - Trusted</p> <p>The <code>unzip</code> operator decompresses a zip compressed file into its original contents.</p>"},{"location":"operators/unzip/#parameters","title":"Parameters","text":"<ul> <li><code>filter</code>: File filter for the decompressed files to be copied to the <code>output_path</code></li> <li>type: <code>str</code></li> <li><code>output_path</code>: The directory where the unzipped files will be stored.</li> <li>type: <code>str</code></li> </ul>"},{"location":"operators/velodyne_lidar/cpp/","title":"Velodyne Lidar Operator","text":"<p> Authors: Holoscan Team (NVIDIA), nvMap Team (NVIDIA), nvMap Embedded Team (NVIDIA), Tom Birdsong (NVIDIA), Julien Jomier (NVIDIA), Jiahao Yin (NVIDIA), Marlene Wan (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 4 - Experimental</p>","tags":["Lidar","Point Cloud"]},{"location":"operators/velodyne_lidar/cpp/#overview","title":"Overview","text":"<p>A Holoscan operator to convert packets from the Velodyne VLP-16 Lidar sensor to a point cloud tensor format.</p>","tags":["Lidar","Point Cloud"]},{"location":"operators/velodyne_lidar/cpp/#description","title":"Description","text":"<p>This operator receives packets from a Velodyne VLP-16 lidar and processes them into a point cloud of fixed size in Cartesian space.</p> <p>The operator performs the following steps: 1. Interpret a fixed-size UDP packet as a Velodyne VLP-16 lidar packet,    which contains 12 data blocks (azimuths) and 32 spherical data points per block. 2. Transform the spherical data points into Cartesian coordinates (x, y, z)    and add them to the output point cloud tensor, overwriting a previous cloud segment. 3. Output the point cloud tensor and update the tensor insertion pointer to prepare    for the next incoming packet.</p> <p>We recommend relying on HoloHub networking operators to receive Velodyne VLP-16 lidar packets over UDP/IP and forward them to this operator.</p>","tags":["Lidar","Point Cloud"]},{"location":"operators/velodyne_lidar/cpp/#requirements","title":"Requirements","text":"<p>Hardware requirements: - Holoscan supported platform (x64 or NVIDIA IGX devkit); - Velodyne VLP-16 Lidar sensor</p>","tags":["Lidar","Point Cloud"]},{"location":"operators/velodyne_lidar/cpp/#example-usage","title":"Example Usage","text":"<p>See the HoloHub Lidar Sample Application to get started.</p>","tags":["Lidar","Point Cloud"]},{"location":"operators/velodyne_lidar/cpp/#acknowledgements","title":"Acknowledgements","text":"<p>This operator was developed in part with support from the NVIDIA nvMap team and adapts portions of the NVIDIA DeepMap SDK.</p>","tags":["Lidar","Point Cloud"]},{"location":"operators/video_encoder/video_encoder_request/","title":"Video Encoder Request","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>video_encoder_request</code> handles the input for encoding YUV frames to H264 bit stream.</p>","tags":["Video","Encoder"]},{"location":"operators/video_encoder/video_encoder_request/#holoscanopsvideoencoderop","title":"<code>holoscan::ops::VideoEncoderOp</code>","text":"<p>Operator class to handle the input for encoding YUV frames to H264 bit stream.</p> <p>This implementation is based on <code>nvidia::gxf::VideoEncoderRequest</code>.</p>","tags":["Video","Encoder"]},{"location":"operators/video_encoder/video_encoder_request/#parameters","title":"Parameters","text":"<ul> <li><code>input_frame</code>: Receiver to get the input frame.</li> <li>type: <code>holoscan::IOSpec*</code></li> <li><code>videoencoder_context</code>: Encoder context Handle.</li> <li>type: <code>std::shared_ptr&lt;holoscan::ops::VideoEncoderContext&gt;</code></li> <li><code>inbuf_storage_type</code>: Input Buffer storage type, 0: kHost, 1: kDevice. Default: 1</li> <li>type: <code>uint32_t</code></li> <li><code>codec</code>: Video codec to use,  0: H264, only H264 supported. Default: 0.</li> <li>type: <code>int32_t</code></li> <li><code>input_height</code>: Input frame height.</li> <li>type: <code>uint32_t</code></li> <li><code>input_width</code>: Input image width.</li> <li>type: <code>uint32_t</code></li> <li><code>input_format</code>: Input color format, nv12,nv24,yuv420planar. Default: nv12.</li> <li>type: <code>nvidia::gxf::EncoderInputFormat</code></li> <li><code>profile</code>: Encode profile, 0: Baseline Profile, 1: Main, 2: High. Default: 2.</li> <li>type: <code>int32_t</code></li> <li><code>bitrate</code>: Bitrate of the encoded stream, in bits per second. Default: 20000000.</li> <li>type: <code>int32_t</code></li> <li><code>framerate</code>: Frame Rate, frames per second. Default: 30.</li> <li>type: <code>int32_t</code></li> <li><code>qp</code>: Encoder constant QP value. Default: 20.</li> <li>type: <code>uint32_t</code></li> <li><code>level</code>: Video H264 level. Maximum data rate and resolution, select from 0 to 14. Default: 14.</li> <li>type: <code>int32_t</code></li> <li><code>iframe_interval</code>: I Frame Interval, interval between two I frames. Default: 30.</li> <li>type: <code>int32_t</code></li> <li><code>rate_control_mode</code>: Rate control mode, 0: CQP[RC off], 1: CBR, 2: VBR. Default: 1.</li> <li>type: <code>int32_t</code></li> <li><code>config</code>: Preset of parameters, select from pframe_cqp, iframe_cqp, custom. Default: custom.</li> <li>type: <code>nvidia::gxf::EncoderConfig</code></li> </ul>","tags":["Video","Encoder"]},{"location":"operators/vita49_psd_packetizer/","title":"vita49_psd_packetizer (latest)","text":"","tags":["Signal Processing"]},{"location":"operators/vita49_psd_packetizer/#vita49-psd-packetizer","title":"VITA49 PSD Packetizer","text":"<p> Authors: John Moon john.moon@vts-i.com (Valley Tech Systems, Inc.), Eric Ferrara eric.ferrara@vts-i.com (Valley Tech Systems, Inc.), Matthew Luckenbihl matthew.luckenbihl@vts-i.com (Valley Tech Systems, Inc.) Supported platforms: x86_64 Language: C++ Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.5.0 Tested Holoscan SDK versions: 2.5.0, 2.6.0, 2.7.0, 2.8.0, 2.9.0, 3.0.0, 3.1.0 Contribution metric: Level 3 - Developmental</p>","tags":["Signal Processing"]},{"location":"operators/vita49_psd_packetizer/#overview","title":"Overview","text":"<p>Generate VITA 49.2 spectral data packets from incoming data.</p>","tags":["Signal Processing"]},{"location":"operators/vita49_psd_packetizer/#description","title":"Description","text":"<p>This operator will take in PSD data computed by upstream operators and format it into VITA 49.2 Spectral Data packets.</p> <p>After creating the VRT packets, it will send the packets to the configured UDP IP/port.</p>","tags":["Signal Processing"]},{"location":"operators/vita49_psd_packetizer/#requirements","title":"Requirements","text":"<ul> <li>MatX (dependency - assumed to be installed on system)</li> <li>Rust (language dependency)</li> <li>vita49 (Rust library dependency)</li> </ul> <p>Note: this operator depends on a Rust component. The <code>Dockerfile</code> provided in this directory will install Rust in the dev container from the official Ubuntu repos.</p>","tags":["Signal Processing"]},{"location":"operators/vita49_psd_packetizer/#multiple-channels","title":"Multiple Channels","text":"<p>If multiple channels are configured, the packetizer will use the base port in the configuration and add the channel index. So, with <code>base_dest_port: 4991</code>, channel <code>0</code> would send data to <code>4991</code>, but channel <code>1</code> would send data to <code>4992</code>.</p> <p>The zero-indexed <code>channel_number</code> key will be looked up in <code>metadata()</code> on each <code>compute()</code> run. If no value is found, the default channel number is <code>0</code>.</p>","tags":["Signal Processing"]},{"location":"operators/vita49_psd_packetizer/#example-usage","title":"Example Usage","text":"<p>For an example of how to use this operator, see the <code>psd_pipeline</code> application.</p>","tags":["Signal Processing"]},{"location":"operators/vita49_psd_packetizer/#configuration","title":"Configuration","text":"<p>The packetizer takes the following parameters:</p> <pre><code>vita49_psd_packetizer:\n  burst_size: 1280\n  num_channels: 1\n  dest_host: 127.0.0.1\n  base_dest_port: 4991\n  manufacturer_oui: 0xFF5646\n  device_code: 0x80\n</code></pre> <ul> <li><code>burst_size</code>: Number of samples to process in each burst</li> <li><code>num_channels</code>: Number of channels for which to allocate memory</li> <li><code>dest_host</code>: Destination host</li> <li><code>base_dest_port</code>: Base destination UDP port</li> <li><code>manufacturer_oui</code>: Manufacturer identifier to embed in the context packets</li> <li><code>device_code</code>: Device code to embed in the context packets</li> </ul>","tags":["Signal Processing"]},{"location":"operators/volume_loader/","title":"Volume Loader","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 22, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0, 1.0.3, 2.0.0, 2.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>volume_loader</code> operator reads 3D volumes from the specified input file.</p>","tags":["Volume"]},{"location":"operators/volume_loader/#supported-formats","title":"Supported Formats","text":"<p>The operator supports these medical volume file formats: * MHD (MetaImage)   * Detached-header format only (<code>.mhd</code> + <code>.raw</code>) * NIFTI * NRRD (Nearly Raw Raster Data)   * Attached-header format (<code>.nrrd</code>)   * Detached-header format (<code>.nhdr</code> + <code>.raw</code>)</p> <p>You must convert your data to one of these formats to load it with <code>VolumeLoaderOp</code>. Some third party open source tools for volume file format conversion include: - Command Line Tools   - the Insight Toolkit (ITK) (PyPI, Image IO Examples)   - SimpleITK (PyPI)   - Utah NRRD Utilities (unu) - GUI Applications   - 3D Slicer   - ImageJ</p>","tags":["Volume"]},{"location":"operators/volume_loader/#api","title":"API","text":"","tags":["Volume"]},{"location":"operators/volume_loader/#holoscanopsvolumeloaderop","title":"<code>holoscan::ops::VolumeLoaderOp</code>","text":"<p>Operator class to read a volume.</p>","tags":["Volume"]},{"location":"operators/volume_loader/#parameters","title":"Parameters","text":"<ul> <li><code>file_name</code>: Volume data file name</li> <li>type: <code>std::string</code></li> <li><code>allocator</code>: Allocator used to allocate the volume data</li> <li>type: <code>std::shared_ptr&lt;Allocator&gt;</code></li> </ul>","tags":["Volume"]},{"location":"operators/volume_loader/#outputs","title":"Outputs","text":"<ul> <li><code>volume</code>: Output volume data</li> <li>type: <code>nvidia::gxf::Tensor</code></li> <li><code>spacing</code>: Physical size of each volume element</li> <li>type: <code>std::array&lt;float, 3&gt;</code></li> <li><code>permute_axis</code>: Volume axis permutation of data space to world space, e.g. if x and y of a volume is swapped this is {1, 0, 2}</li> <li>type: <code>std::array&lt;uint32_t, 3&gt;</code></li> <li><code>flip_axes</code>: Volume axis flipping from data space to world space, e.g. if x is flipped this is {true, false, false}</li> <li>type: <code>std::array&lt;bool, 3&gt;</code></li> <li><code>extent</code>: Physical size of the the volume in world space</li> <li>type: <code>std::array&lt;float, 3&gt;</code></li> </ul>","tags":["Volume"]},{"location":"operators/volume_renderer/","title":"Volume Renderer","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 22, 2025 Latest version: 2.0.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3, 2.0.0, 2.1.0, 3.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>volume_renderer</code> operator renders a volume using ClaraViz (https://github.com/NVIDIA/clara-viz).</p>","tags":["Volume","Rendering"]},{"location":"operators/volume_renderer/#holoscanopsvolumerenderer","title":"<code>holoscan::ops::VolumeRenderer</code>","text":"<p>Operator class to render a volume.</p>","tags":["Volume","Rendering"]},{"location":"operators/volume_renderer/#parameters","title":"Parameters","text":"<ul> <li><code>config_file</code>: Config file path. The content of the file is passed to <code>clara::viz::JsonInterface::SetSettings()</code> at initialization time. See Configuration for details.</li> <li>type: <code>std::string</code></li> <li><code>allocator</code>: Allocator used to allocate render buffer outputs when no pre-allocated color or depth buffer is passed to <code>color_buffer_in</code> or <code>depth_buffer_in</code>. Allocator needs to be capable to allocate device memory.</li> <li>type: <code>std::shared_ptr&lt;Allocator&gt;</code></li> <li><code>alloc_width</code>: Width of the render buffer to allocate when no pre-allocated buffers are provided.</li> <li>type: <code>uint32_t</code></li> <li><code>alloc_height</code>: Height of the render buffer to allocate when no pre-allocated buffers are provided.</li> <li>type: <code>uint32_t</code></li> </ul>","tags":["Volume","Rendering"]},{"location":"operators/volume_renderer/#inputs","title":"Inputs","text":"<p>All inputs are optional.</p> <ul> <li><code>volume_pose</code>: Transform the volume.</li> <li>type: <code>nvidia::gxf::Pose3D</code></li> <li><code>crop_box</code>: Volume crop box. Each <code>nvidia::gxf::Vector2f</code> contains the min and max values in range <code>[0, 1]</code> of the x, y and z axes of the volume.</li> <li>type: <code>std::array&lt;nvidia::gxf::Vector2f, 3&gt;</code></li> <li><code>depth_range</code>: The distance to the near and far frustum planes.</li> <li>type: <code>nvidia::gxf::Vector2f</code></li> <li><code>left_camera_pose</code>: Camera pose for the left camera when rendering in stereo mode.</li> <li>type: <code>nvidia::gxf::Pose3D</code></li> <li><code>right_camera_pose</code>: Camera pose for the right camera when rendering in stereo mode.</li> <li>type: <code>nvidia::gxf::Pose3D</code></li> <li><code>left_camera_model</code>: Camera model for the left camera when rendering in stereo mode.</li> <li>type: <code>nvidia::gxf::CameraModel</code></li> <li><code>right_camera_model</code>: Camera model for the right camera when rendering in stereo mode.</li> <li>type: <code>nvidia::gxf::CameraModel</code></li> <li><code>camera_pose</code>: Camera pose when not rendering in stereo mode.</li> <li>type: <code>std::array&lt;float, 16&gt;</code> or <code>nvidia::gxf::Pose3D</code></li> <li><code>color_buffer_in</code>: Buffer to store the rendered color data to, format needs to be 8 bit per component RGBA and buffer needs to be in device memory.</li> <li>type: <code>nvidia::gxf::VideoBuffer</code></li> <li><code>depth_buffer_in</code>: Buffer to store the rendered depth data to, format needs to be 32 bit float single component buffer needs to be in device memory.</li> <li>type: <code>nvidia::gxf::VideoBuffer</code></li> <li><code>density_volume</code>: Density volume data. Needs to be a 3D single component array. Supported data types are signed|unsigned 8|16|32 bit integer and 32 bit floating point.</li> <li>type: <code>nvidia::gxf::Tensor</code></li> <li><code>density_spacing</code>: Physical size of each density volume element.</li> <li>type: <code>std::array&lt;float, 3&gt;</code></li> <li><code>density_permute_axis</code>: Density volume axis permutation of data space to world space, e.g. if x and y of a volume is swapped this is {1, 0, 2}.</li> <li>type: <code>std::array&lt;uint32_t, 3&gt;</code></li> <li><code>density_flip_axes</code>: Density volume axis flipping from data space to world space, e.g. if x is flipped this is {true, false, false}.</li> <li>type: <code>std::array&lt;bool, 3&gt;</code></li> <li><code>mask_volume</code>: Mask volume data. Needs to be a 3D single component array. Supported data types are signed|unsigned 8|16|32 bit integer and 32 bit floating point.</li> <li>type: <code>nvidia::gxf::Tensor</code></li> <li><code>mask_spacing</code>: Physical size of each mask volume element.</li> <li>type: <code>std::array&lt;float, 3&gt;</code></li> <li><code>mask_permute_axis</code>: Mask volume axis permutation of data space to world space, e.g. if x and y of a volume is swapped this is {1, 0, 2}.</li> <li>type: <code>std::array&lt;uint32_t, 3&gt;</code></li> <li><code>mask_flip_axes</code>: Mask volume axis flipping from data space to world space, e.g. if x is flipped this is {true, false, false}.</li> <li>type: <code>std::array&lt;bool, 3&gt;</code></li> </ul>","tags":["Volume","Rendering"]},{"location":"operators/volume_renderer/#outputs","title":"Outputs","text":"<ul> <li><code>color_buffer_out</code>: Buffer with rendered color data, format is 8 bit per component RGBA and buffer is in device memory.</li> <li>type: <code>nvidia::gxf::VideoBuffer</code></li> <li><code>depth_buffer_out</code>: Buffer with rendered depth data, format is be 32 bit float single component and buffer is in device memory.</li> <li>type: <code>nvidia::gxf::VideoBuffer</code></li> </ul>","tags":["Volume","Rendering"]},{"location":"operators/volume_renderer/#configuration","title":"Configuration","text":"<p>The renderer accepts a ClaraViz JSON configuration file at startup to control rendering settings, including - camera parameters; - transfer functions; - lighting; - and more.</p> <p>The ClaraViz JSON configuration file exists in addition to and independent of a Holoscan SDK <code>.yaml</code> configuration file that may be passed to an application.</p> <p>See the <code>volume_rendering_xr</code> application for a sample configuration file. Visit the ClaraViz <code>render_server.proto</code> gRPC specification for insight into configuration file field values.</p>","tags":["Volume","Rendering"]},{"location":"operators/volume_renderer/#transfer-functions","title":"Transfer Functions","text":"<p>Usually CT datasets are stored in Hounsfield scale. The renderer maps these values in Hounsfield scale to opacity in order to display the volume. These mappings are called transfer functions. Multiple transfer functions for different input value regions can be defined. Transfer functions also include material properties like diffuse, specular and emissive color. The range of input values the transfer function is applied to is in normalized input range <code>[0, 1]</code>.</p>","tags":["Volume","Rendering"]},{"location":"operators/volume_renderer/#segmentation-mask-volume","title":"Segmentation (Mask) Volume","text":"<p>Different organs often have very similar Hounsfield values, therefore additionally an segmentation volume is supported. The segmentation volume contains an integer index for each element of the volume. Transfer functions can be restricted on specific segmentation indices. The segmentation volume can, for example, be generated using TotalSegmentator.</p>","tags":["Volume","Rendering"]},{"location":"operators/volume_renderer/#creating-a-configuration-file","title":"Creating a Configuration File","text":"<p>Configuration files are typically specific to a given dataset or modality, and are tailored to a specific voxel intensity range. It may be necessary to create a new configuration file when working with a new dataset in order to produce a meaningful rendering.</p> <p>There are two options to create a configuration file for a new dataset: - Copy from an existing configuration file as a reference and modify parameters manually. An example configuration file is available in the <code>volume_rendering_xr</code> application config folder. - Use <code>VolumeRendererOp</code> to deduce settings for the input dataset. Follow these steps:   1. Use the HoloHub <code>volume_rendering</code> app or a similar application that will load an input dataset and pass it to <code>VolumeRendererOp</code>.   2. Configure application settings via a Holoscan SDK YAML file or command line settings to run with the following values:     - Set the <code>VolumeRendererOp</code> <code>config_file</code> parameter to an empty string to indicate no default config file is present;     - Set the <code>VolumeRendererOp</code> <code>write_config_file</code> parameter to the desired output JSON configuration filepath.   3. Run the application with the desired input volume. The operator will deduce settings and write out the JSON file to reuse on subsequent runs via the <code>config_file</code> parameter.</p>","tags":["Volume","Rendering"]},{"location":"operators/vtk_renderer/","title":"vtk_renderer operator","text":"<p> Authors: Kitware Team (Kitware Inc) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>vtk_renderer</code> extension takes the output of the source video player and the output of the <code>tool_tracking_postprocessor</code> operator and renders the video stream with an overlay annotation of the label using VTK.</p> <p>VTK can be a useful addition to holohub stack since VTK is a industry leading visualization toolkit. It is important to mention that this renderer operator needs to copy the input from device memory to host due to limitations of VTK. While this is a strong limitation for VTK we believe that VTK can still be a good addition and VTK is an evolving project. Perhaps in the future we could overcome this limitation.</p>","tags":["Visualization"]},{"location":"operators/vtk_renderer/#how-to-build-this-operator","title":"How to build this operator","text":"<p>Build the HoloHub container as described at the root README.md</p> <p>You need to create a docker image which includes VTK with the provided <code>vtk.Dockerfile</code>:</p> <pre><code>docker build -t vtk:latest -f vtk.Dockerfile .\n</code></pre> <p>Then, you can build the tool tracking application with the provided <code>Dockerfile</code>:</p> <pre><code>./dev_container launch --img vtk:latest\n</code></pre> <p>Inside the container you can build the holohub application with:</p> <pre><code>./run build &lt;application&gt; --with vtk_renderer\n</code></pre>","tags":["Visualization"]},{"location":"operators/vtk_renderer/#parameters","title":"Parameters","text":"<ul> <li><code>videostream</code>: Input channel for the videostream, type <code>gxf::Tensor</code></li> <li>type: <code>gxf::Handle&lt;gxf::Receiver&gt;</code></li> <li><code>annotations</code>: Input channel for the annotations, type <code>gxf::Tensor</code></li> <li>type: <code>gxf::Handle&lt;gxf::Receiver&gt;</code></li> <li><code>window_name</code>: Compositor window name.</li> <li>type: <code>std::string</code></li> <li><code>width</code>: width of the renderer window.</li> <li>type: <code>int</code></li> <li><code>height</code>: height of the renderer window.</li> <li>type: <code>int</code></li> <li><code>labels</code>: labels to be displayed on the rendered image.</li> <li>type: <code>std::vector&lt;std::string&gt;&gt;</code></li> </ul>","tags":["Visualization"]},{"location":"operators/webrtc_client/","title":"WebRTC Client Operator","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>webrtc_client</code> operator receives video frames through a WebRTC connection. The application using this operator needs to call the <code>offer</code> method of the operator when a new WebRTC connection is available.</p>","tags":["WebRTC","Video"]},{"location":"operators/webrtc_client/#methods","title":"Methods","text":"<ul> <li><code>async def offer(self, sdp, type) -&gt; (local_sdp, local_type)</code>   Start a connection between the local computer and the peer.</li> </ul> <p>Parameters   - sdp peer Session Description Protocol object   - type peer session type</p> <p>Return values   - sdp local Session Description Protocol object   - type local session type</p>","tags":["WebRTC","Video"]},{"location":"operators/webrtc_client/#outputs","title":"Outputs","text":"<ul> <li><code>output</code>: Tensor with 8 bit per component RGB data</li> <li>type: <code>Tensor</code></li> </ul>","tags":["WebRTC","Video"]},{"location":"operators/webrtc_server/","title":"WebRTC Server Operator","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>webrtc_server</code> operator sends video frames through a WebRTC connection. The application using this operator needs to call the <code>offer</code> method of the operator when a new WebRTC connection is available.</p>","tags":["WebRTC","Video"]},{"location":"operators/webrtc_server/#methods","title":"Methods","text":"<ul> <li><code>async def offer(self, sdp, type) -&gt; (local_sdp, local_type)</code>   Start a connection between the local computer and the peer.</li> </ul> <p>Parameters   - sdp peer Session Description Protocol object   - type peer session type</p> <p>Return values   - sdp local Session Description Protocol object   - type local session type</p>","tags":["WebRTC","Video"]},{"location":"operators/webrtc_server/#inputs","title":"Inputs","text":"<ul> <li><code>input</code>: Tensor or numpy array with 8 bit per component RGB data</li> <li>type: <code>Tensor</code></li> </ul>","tags":["WebRTC","Video"]},{"location":"operators/xr/","title":"Holoscan XR","text":"<p> Authors: Connor Smith (NVIDIA), Rafael Wiltz (NVIDIA), Mimi Liao (NVIDIA) Supported platforms: x86_64 Language: C++ Last modified: May 22, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 3.1.0 Tested Holoscan SDK versions: 3.1.0 Contribution metric: Level 4 - Experimental</p> <p>No documentation found.</p>","tags":["Extended Reality","Vulkan","Visualization"]},{"location":"operators/yuan_qcap/","title":"yuan_qcap","text":"<p> Authors: David Su (Yuan) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: May 22, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 2 - Trusted</p> <p>No documentation found.</p>","tags":["Camera"]},{"location":"tutorials/","title":"Tutorials","text":"<p>The HoloHub tutorials are an invaluable resource for developers looking to master the Holoscan SDK and build advanced AI sensor processing applications.  This page offers a collection of comprehensive tutorials that guide users through various aspects of the Holoscan platform, from basic setup to advanced workflow optimization.</p> <p>Each tutorial is designed to provide step-by-step instructions, code examples, and best practices, making it easier for developers to integrate Holoscan's powerful features into their projects. Whether you are new to Holoscan or an experienced developer seeking to enhance your skills, these tutorials offer practical insights and hands-on exercises to help you leverage the full potential of the Holoscan platform.</p>"},{"location":"tutorials/creating-multi-node-applications/","title":"Creating Multi Node Applications","text":"<p> Authors: Jin Li (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 2 - Trusted</p> <p>In this tutorial, we will walk through the process in two scenarios of creating a multi node application from existing applications. We will demonstrate the process with Python applications but it's similar for C++ applications as well.</p> <ol> <li> <p>When we would want to divide an application that was previously running on a single node into two fragments running on two nodes. This corresponds to use cases where we want to separate the compute and visualization workloads onto two different nodes, for example in the case of surgical robotics, the visualization node should be closest to the surgeon. For this purpose we choose the example of the <code>multiai_endoscopy</code> application.</p> </li> <li> <p>When we would want to connect and combine two previously independent applications into one application with two fragments. This corresponds to the use cases where we want to run time-critical task(s) on a node closest to the data stream, and non time-critical task(s) on another node that can have a bit more latency, for example saving the inbody video recording of the surgery to cloud can have a higher latency than the real-time visualization. For this purpose we choose the example of the <code>endoscopy_tool_tracking</code> application and the <code>endoscopy_out_of_body_detection</code> application.</p> </li> </ol> <p>The SDK documentation on Creating a Distributed Application contains the necessary core concepts and description for distributed applications, please familiarize yourself with the documentation before proceeding to this tutorial.</p> <p>Please also see two SDK examples ping_distributed and video_replayer_distributed on simple examples of creating distributed applications.</p> <p>In this tutorial, we will focus on modifying existing applications you have created into distributed applications.</p>","tags":["Distributed"]},{"location":"tutorials/creating-multi-node-applications/#scenario-1-divide-an-application-into-two-fragments","title":"Scenario 1 - Divide an application into two fragments","text":"<p>The <code>multiai_endoscopy</code> application has its app graph like below:</p> <p></p> <p>We will divide it into two fragments. The first fragment will include all operators excluding the visualizer and the second fragment will include the visualizer, as illustrated below:</p> <p></p>","tags":["Distributed"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-1-extra-imports","title":"Changes in scenario 1 - Extra Imports","text":"<p>To created a distributed application, we will need to import the Fragment object.  </p> <pre><code>from holoscan.core import Fragment\n</code></pre>","tags":["Distributed"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-1-changing-the-way-command-line-arguments-are-parsed","title":"Changes in scenario 1 - Changing the way command-line arguments are parsed","text":"<p>As seen in the documentation, it is recommended to parse user-defined arguments from the <code>argv ((C++/Python))</code> method/property of the application. To parse in user-defined command line arguments (such as <code>--data</code>, <code>--source</code>, <code>--labelfile</code> in this app), let's make sure to avoid arguments that are unique to the multi-fragment applications, such as  <code>--driver</code>, <code>--worker</code>, <code>--address</code>, <code>--worker-address</code>, <code>--fragments</code> (see the documentation for more details on using those arguments).  In the non-distributed application, we would have  <pre><code>if __name__ == \"__main__\":\n    parser = ArgumentParser(description=\"Multi-AI Detection Segmentation application.\")\n    parser.add_argument(...) # for the app config yaml file via --config \n    parser.add_argument(...) # for args needed in app init --source --data --labelfile\n    args = parser.parse_args()\n    # logic to define args (config_file, labelfile) needed to pass into application init and config\n    app = MultiAIDetectionSegmentation(source=args.source, data=args.data, labelfile=labelfile)\n    app.config(config_file)\n    app.run()\n</code></pre></p> <p>In the distributed application, we need to make the following changes mainly to <code>parser.parse_args</code>: <code>``python !5,6 if __name__ == \"__main__\":     parser = ArgumentParser(description=\"Multi-AI Detection Segmentation application.\")     parser.add_argument(...) # for the app config yaml file via --config      parser.add_argument(...) # for args needed in app init --source --data --labelfile     apps_argv = Application().argv # difference     args = parser.parse_args(apps_argv[1:]) # difference     # logic to define args (config_file, labelfile) needed to pass into application init and config     app = MultiAIDetectionSegmentation(source=args.source, data=args.data, labelfile=labelfile)     app.config(config_file)     app.run() <pre><code>### Changes in scenario 1 - Changing the application structure\nPreviously, we defined our non distributed applications with the `__init__()` and `compose()` methods. \n```python\nclass MultiAIDetectionSegmentation(Application):\n    def __init__(self, data, source=\"replayer\", labelfile=\"\"):\n        ...\n\n    def compose(self):\n        # define operators and add flow\n        ...\n</code></pre> Now we will define two fragments, and add and connect them in the application's</code>compose()` method: <pre><code>class Fragment1(Fragment):\n    # operators in fragment1 need the objects: sample_data_path, source, label_dict \n    def __init__(self, app, name, sample_data_path, source, label_dict):\n        super().__init__(app, name)\n        ...\n    def compose(self):\n        # define fragment1 operators\n        # add flow\n        ... \n\nclass Fragment2(Fragment):\n    # operators in fragment2 need the object: label_dict \n    def __init__(self, app, name, label_dict):\n        super().__init__(app, name)\n        ...\n    def compose(self):\n        # define the one operator in fragment2 (Holoviz)\n        # add operator\n        # no need to add_flow because there's only one operator in this fragment\n        ... \n\nclass MultiAIDetectionSegmentation(Application):\n    def __init__(self, data, source=\"replayer\", labelfile=\"\"):\n        super().__init__()\n        # set self.name\n        # get self.label_dict from labelfile,self.source, self.sample_data_path\n        ...\n\n    def compose(self):\n        # define the two fragments in this app: fragment1, fragment2\n        # pass in the objects needed to each fragment's operators when defining each fragment\n        # operators in fragment1 need the objects: sample_data_path, source, label_dict \n        fragment1 = Fragment1(self, name=\"fragment1\", source=self.source, sample_data_path=self.sample_data_path, label_dict=self.label_dict)\n        # operators in fragment2 need the object: label_dict \n        fragment2 = Fragment2(self, name=\"fragment2\", label_dict=self.label_dict)\n\n        # Connect the two fragments \n        # There are three connections between fragment1 and fragment2:\n        # (1) from the data source to Holoviz\n        source_output = self.source + \".video_buffer_output\" if self.source.lower() == \"aja\" else self.source + \".output\"\n        self.add_flow(fragment1, fragment2, {(source_output, \"holoviz.receivers\")})\n        # (2) from the detection postprocessor to Holoviz\n        self.add_flow(fragment1, fragment2, {(\"detection_postprocessor.out\" , \"holoviz.receivers\")})\n        # (3) from the segmentation postprocessor to Holoviz\n        self.add_flow(fragment1, fragment2, {(\"segmentation_postprocessor.out_tensor\" , \"holoviz.receivers\")})\n</code></pre></p>","tags":["Distributed"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-1-defining-objects-shared-among-fragments","title":"Changes in scenario 1 - Defining objects shared among fragments","text":"<p>When creating your fragments, first make a list of all the objects each fragment's operators will need. If there are objects that are needed across multiple fragments (such as <code>self.label_dict</code> in this case), before passing them into fragments in the application's <code>compose()</code> method, create such objects in the app's <code>__init()__</code> method ideally. In the non-distributed application, in the app's <code>compose()</code> method we define <code>label_dict</code> from <code>self.labelfile</code>, and continue using <code>label_dict</code> while composing the application. In the distributed application, we move the definition of <code>label_dict</code> from <code>self.labelfile</code> into the application's <code>__init()__</code> method, and refer to <code>self.label_dict</code> in the application's <code>compose()</code> method and each fragment's <code>__init__()</code>/<code>compose()</code> methods.</p> <p>Non distributed application: <pre><code>class MultiAIDetectionSegmentation(Application):\n    def compose(self):\n        # construct the labels dictionary if the commandline arg for labelfile isn't empty\n        label_dict = self.get_label_dict(self.labelfile)\n</code></pre></p> <p>Distributed application: <pre><code>class Fragment1(Fragment):\n    def __init__(self, app, name, sample_data_path, source, label_dict):\n        super().__init__(app, name)\n\n        self.source = source\n        self.label_dict = label_dict\n        self.sample_data_path = sample_data_path\n\n    def compose(self):\n        ...\n        # Use self.label_dict\n\nclass Fragment2(Fragment):\n    def __init__(self, app, name, label_dict):\n        super().__init__(app, name)\n\n        self.label_dict = label_dict\n\n    def compose(self):\n        ...\n        # Use self.label_dict\n\nclass MultiAIDetectionSegmentation(Application):\n    def __init__(self, data, source=\"replayer\", labelfile=\"\"):\n        ...\n        self.label_dict = self.get_label_dict(labelfile)\n    def compose(self):\n        fragment1 = Fragment1(self, name=\"fragment1\", source=self.source, sample_data_path=self.sample_data_path, label_dict=self.label_dict)\n        fragment2 = Fragment2(self, name=\"fragment2\", label_dict=self.label_dict)\n        ...\n</code></pre></p>","tags":["Distributed"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-1-adding-operators-to-app-graph","title":"Changes in scenario 1 - Adding Operators to App Graph","text":"<p>When composing a non-distributed application, operators are created in the <code>compose()</code> method, then added to the app graph one of two ways:    1. For applications with a single operator (rare), <code>add_operator()</code> should be called.    1. for applications with multiple operators, using <code>add_flow()</code> will take care of adding each operator to the app graph on top of connecting them. This applies to distributed applications as well: when composing multiple fragments, each of them are responsible for adding all their operators to the app graph in their <code>compose()</code> method. Calling <code>add_flow()</code> in the <code>compose()</code> method of the distributed application when connecting fragments together does not add the operators to the app graph. This is often relevant when breaking down a single fragment application in a multi fragments application for distributed use cases, as some fragments might end up owning a single operator, and the absence of <code>add_flow()</code> in that fragment should come with the addition of <code>add_operator()</code> instead.</p> <p>In the non distributed application: <pre><code>class MultiAIDetectionSegmentation(Application):\n    def compose(self):\n        # define operators\n        source = SourceClass(...)\n        detection_preprocessor = FormatConverterOp(...)\n        segmentation_preprocessor = FormatConverterOp(...)\n        multi_ai_inference = InferenceOp(...)\n        detection_postprocessor = DetectionPostprocessorOp(...)\n        segmentation_postprocessor = SegmentationPostprocessorOp(...)\n        holoviz = HolovizOp(...)\n\n        # add flow between operators\n        ...\n</code></pre></p> <p>In a distributed application: ```python #11-17 class Fragment1(Fragment):     def compose(self):         # define operators         source = SourceClass(...)         detection_preprocessor = FormatConverterOp(...)         segmentation_preprocessor = FormatConverterOp(...)         multi_ai_inference = InferenceOp(...)         detection_postprocessor = DetectionPostprocessorOp(...)         segmentation_postprocessor = SegmentationPostprocessorOp(...)</p> <pre><code>    # add flow between operators\n    ...\n</code></pre> <p>class Fragment2(Fragment):     def compose(self):         holoviz = HolovizOp(...)         self.add_operator(holoviz) <pre><code>### Changes in scenario 1 - Shared resources\nIn a non distributed application, there may be some shared resources defined in the application's `compose()` method, such as a `UnboundedAllocator` for various operators. When splitting the application into multiple fragments, remember to create those resources once for each fragment.\n\nNon distributed application:\n```python\nclass MultiAIDetectionSegmentation(Application):\n    def compose(self):\n        pool = UnboundedAllocator(self, name=\"pool\")\n</code></pre></p> <p>Distributed application: <pre><code>class Fragment1(Fragment):\n    def compose(self):\n        pool = UnboundedAllocator(self, name=\"pool\")\n        # ... operator definitions in fragment1 with parameter pool=pool,\n\nclass Fragment2(Fragment):\n    def compose(self):\n        pool = UnboundedAllocator(self, name=\"pool\")\n        # ... operator definitions in fragment2 with parameter pool=pool,\n</code></pre></p>","tags":["Distributed"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-1-running-the-application","title":"Changes in scenario 1 - Running the application","text":"<p>Previously for a non distributed application, the command to launch is <code>python3 multi_ai.py --data &lt;DATA_DIR&gt;</code>, now in the distributed application we will have the option to specify a few more things: <pre><code># To run fragment 1 on current node as driver and worker:\npython3 multi_ai.py --data /workspace/holohub/data/ --driver --worker --address &lt;node 1 IP address&gt;:&lt;port&gt; --fragments fragment1\n# To run fragment 2 on current node as worker:\npython3 multi_ai.py --data /workspace/holohub/data/  --worker --address &lt;node 1 IP address&gt;:&lt;port&gt; --fragments fragment2\n</code></pre></p> <p>For more details on the commandline arguments for multi fragment apps, see the documentation.</p> <p>To run the app we create in scenario 1, please see Running the Applications.</p>","tags":["Distributed"]},{"location":"tutorials/creating-multi-node-applications/#scenario-2-connect-two-applications-into-a-multi-node-application-with-two-fragments","title":"Scenario 2 - Connect two applications into a multi-node application with two fragments","text":"<p>In this scenario, we will combine the existing application endoscopy_tool_tracking and endoscopy_out_of_body_detection into a distributed application with 2 fragments.</p> <p>Since endoscopy_out_of_body_detection is implemented in C++, we will quickly implement the Python version of the app for our Fragment2.</p> <p>The app graph for <code>endoscopy_tool_tracking</code>:  The app graph for <code>endoscopy_out_of_body_detection</code>:  The distributed app graph we want to create: </p>","tags":["Distributed"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-2-extra-imports","title":"Changes in scenario 2  - Extra Imports","text":"<p>Similar to scenario 1, to created a distributed application, we will need to import the Fragment object. When combining two non distributed apps into a multi-fragment application, remember to import all prebuilt Holoscan operators needed in both fragments.</p> <pre><code>from holoscan.core import Fragment\nfrom holoscan.operators import (\n    AJASourceOp,\n    FormatConverterOp,\n    HolovizOp,\n    VideoStreamRecorderOp,\n    VideoStreamReplayerOp,\n    InferenceOp,\n    InferenceProcessorOp\n)\n</code></pre>","tags":["Distributed"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-2-changing-the-way-command-line-arguments-are-parsed","title":"Changes in scenario 2 - Changing the way command-line arguments are parsed","text":"<p>Similar to Changing the way command-line arguments are parse in scenario 1, instead of the following for the non distributed app: <pre><code>if __name__ == \"__main__\":\n    ...\n    args = parser.parse_args()\n    ...\n</code></pre> For the distributed app we will have: <pre><code>if __name__ == \"__main__\":\n    ...\n    apps_argv = Application().argv\n    args = parser.parse_args(apps_argv[1:])\n    ...\n</code></pre></p>","tags":["Distributed"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-2-modifying-non-distributed-applications-into-a-distributed-application","title":"Changes in scenario 2 - Modifying non-distributed application(s) into a distributed application","text":"<p>In the new distributed app, we will define the app graph in <code>endoscopy_tool_tracking</code> as the new app's fragment 1.  The non distributed app <code>endoscopy_tool_tracking</code> had its <code>__init__()</code> and <code>compose()</code> methods structured like following: <pre><code>class EndoscopyApp(Application):\n    def __init__(self, data, record_type=None, source=\"replayer\"):\n        super().__init__()\n        # set self.name\n        ...\n        # get parameters for app graph composition: \n        # self.record_type, self.source, self.sample_data_path \n        ...\n    def compose(self):\n        # Create operators including:\n        # source operator (video replayer, AJA, Yuan), \n        # format converters, recorder\n        # LSTMTensorRTInferenceOp, ToolTrackingPostprocessorOp\n        # HolovizOp\n        ...\n\n        # Add flow between operators\n        ...\n</code></pre></p> <p>We can define our fragment1 modified from <code>endoscopy_tool_tracking</code> app with the following structure.</p> <pre><code>class Fragment1(Fragment):\n    def __init__(self, app, name, sample_data_path, source, record_type):\n        super().__init__(app, name)\n\n        self.source = source\n        self.sample_data_path = sample_data_path\n        self.record_type = record_type\n    def compose(self):\n        # Create operators including:\n        # source operator (video replayer, AJA, Yuan), \n        # format converters, recorder\n        # LSTMTensorRTInferenceOp, ToolTrackingPostprocessorOp\n        # HolovizOp\n        ...\n\n        # Add flow between operators\n        ...\n</code></pre> <p>We will define the app graph in <code>endoscopy_out_of_body_detection</code> as the new app's fragment 2. <pre><code>class Fragment2(Fragment):\n    def __init__(self, app, name, source, model_path, record_type):\n        super().__init__(app, name)\n\n        self.source = source\n        self.record_type = record_type\n        self.model_path = model_path\n    def compose(self):\n\n        is_aja = self.source.lower() == \"aja\"\n\n        pool = UnboundedAllocator(self, name=\"fragment2_pool\")\n        in_dtype = \"rgba8888\" if is_aja else \"rgb888\"\n\n        out_of_body_preprocessor = FormatConverterOp(\n            self,\n            name=\"out_of_body_preprocessor\",\n            pool=pool,\n            in_dtype=in_dtype,\n            **self.kwargs(\"out_of_body_preprocessor\"),\n        )\n\n        model_path_map = {\"out_of_body\": os.path.join(self.model_path, \"out_of_body_detection.onnx\")}\n        for k, v in model_path_map.items():\n            if not os.path.exists(v):\n                raise RuntimeError(f\"Could not find model file: {v}\")\n        inference_kwargs = self.kwargs(\"out_of_body_inference\")\n        inference_kwargs[\"model_path_map\"] = model_path_map\n        out_of_body_inference = InferenceOp(\n            self,\n            name=\"out_of_body_inference\",\n            allocator=pool,\n            **inference_kwargs,\n        )\n        out_of_body_postprocessor = InferenceProcessorOp(\n            self,\n            name=\"out_of_body_postprocessor\",\n            allocator=pool,\n            disable_transmitter=True,\n            **self.kwargs(\"out_of_body_postprocessor\")\n        )\n\n        # add flow between operators\n        self.add_flow(out_of_body_preprocessor, out_of_body_inference, {(\"\", \"receivers\")})\n        self.add_flow(out_of_body_inference, out_of_body_postprocessor, {(\"transmitter\", \"receivers\")})\n</code></pre> We can then define our distributed application with the following structure. Notice how the objects <code>self.record_type</code>, <code>self.source</code>, and <code>self.sample_data_path</code> are passed into each fragment.</p> <pre><code>class EndoscopyDistributedApp(Application):\n    def __init__(self, data, record_type=None, source=\"replayer\"):\n        super().__init__()\n        # set self.name\n        ...\n        # get parameters for app graph composition: \n        # self.record_type, self.source, self.sample_data_path \n        ...\n    def compose(self):\n        is_aja = self.source.lower() == \"aja\"\n\n        fragment1 = Fragment1(self, \n                            name=\"fragment1\", \n                            source = self.source, \n                            sample_data_path=os.path.join(self.sample_data_path, \"endoscopy\"), \n                            record_type=self.record_type)\n        fragment2 = Fragment2(self, \n                            name=\"fragment2\", \n                            source = self.source, \n                            model_path=os.path.join(self.sample_data_path, \"endoscopy_out_of_body_detection\"),\n                            record_type = self.record_type)\n\n        self.add_flow(fragment1, fragment2, {(\"aja.video_buffer_output\" if is_aja else \"replayer.output\", \"out_of_body_preprocessor\")})\n</code></pre>","tags":["Distributed"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-2-combining-objects-needed-by-each-fragments-in-distributed-app-init-time","title":"Changes in scenario 2 - Combining objects needed by each fragments in distributed app init time","text":"<p>Note how the <code>__init__()</code> method in the distributed app structured above is now the place to get parameters for the app graph composition for both fragment1 and fragment 2. In this case, Fragment1 needs the following objects at <code>__init__()</code> time:  <pre><code>class Fragment1(Fragment):\n    def __init__(self, app, name, sample_data_path, source, record_type):\n        super().__init__(app, name)\n\n        self.source = source\n        self.sample_data_path = sample_data_path\n        self.record_type = record_type\n</code></pre> and Fragment 2 needs the following objects at <code>__init__()</code> time: <pre><code>class Fragment2(Fragment):\n    def __init__(self, app, name, source, model_path, record_type):\n        super().__init__(app, name)\n\n        self.source = source\n        self.record_type = record_type\n        self.model_path = model_path\n</code></pre> We need to make sure in the distributed app's <code>__init__()</code> method, we are creating the corresponding objects to pass in to each fragment's <code>__init__()</code> time.</p>","tags":["Distributed"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-2-configuration-file","title":"Changes in scenario 2 - Configuration File","text":"<p>If you had two <code>yaml</code> files for configuring each of the non distributed applications, now in the combined distributed application you will need to combine the content of both in a <code>yaml</code> file.</p>","tags":["Distributed"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-2-running-the-application","title":"Changes in scenario 2 - Running the application","text":"<p>In the newly created distributed application we will launch the application like below: <pre><code># To run fragment 1 on current node as driver and worker:\npython3 /workspace/holohub/applications/distributed_app/python/endoscopy_tool_tracking.py --source replayer --data /workspace/holohub/data --driver --worker --fragments fragment1 --address &lt;node 1 IP address&gt;:&lt;port&gt;\n\n# To run fragment 2 on current node as worker:\npython3 /workspace/holohub/applications/distributed_app/python/endoscopy_tool_tracking.py --data /workspace/holohub/data --source replayer --worker --fragments fragment2 --address &lt;node 1 IP address&gt;:&lt;port&gt;\n\n# To run on a single node:\npython3 /workspace/holohub/applications/distributed_app/python/endoscopy_tool_tracking.py --source replayer --data /workspace/holohub/data --driver --worker --fragments fragment1,fragment2 \n</code></pre></p> <p>For more details on the commandline arguments for multi fragment apps, see the documentation.</p> <p>To run the app we create in scenario 2, please see Running the Applications.</p>","tags":["Distributed"]},{"location":"tutorials/creating-multi-node-applications/#configuration","title":"Configuration","text":"<p>You can run a distributed application across any combination of hardware that are compatible with the Holoscan SDK. Please see SDK Installation Prerequisites for a list of compatible hardware. </p> <p>If you would like the distributed application fragments to communicate through high speed ConnectX NICs, enable the ConnectX NIC for GPU Direct RDMA, for example the ConnectX-7 NIC on the IGX Orin Developer Kit, follow these instructions. Enabling ConnectX NICs could bring significant speedup to your distributed app connection, for example, the RJ45 ports on IGX Orin Developer Kit supports 1GbE while the QSFP28 ports connected to ConnectX-7 support up to 100 GbE.</p> <p>If connecting together two IGX Orin Developer Kits with dGPU, follow the above instructions on each devkit to enable GPU Direct RDMA through ConnectX-7, and make the hardware connection through the QSFP28 ports on the back panel. A QSFP-QSFP cable should be included in your devkit box alongside power cables etc in the inner small box. Use either one of the two port on the devkit, and make sure to find out the logical name of the port as detailed in the instructions.</p> <p></p> <p>On each machine, make sure to specify an address for the network logical name that has the QSFP cable connected, and when running your distributed application, make sure to specify that address for the <code>--driver</code> machine.</p>","tags":["Distributed"]},{"location":"tutorials/creating-multi-node-applications/#running-the-applications","title":"Running the Applications","text":"<p>Follow Container Build instructions to build and launch the HoloHub dev container.</p>","tags":["Distributed"]},{"location":"tutorials/creating-multi-node-applications/#scenario-1-application","title":"Scenario 1 Application","text":"<p>Before we start to launch the application, let's first run the original application. In the dev container, make sure to build and launch the <code>multiai_endoscopy</code> app, this will convert the downloaded ONNX model file into a TensorRT engine at first run. </p> <p>Build and run instructions may change in the future, please refer to the original application. <pre><code># on the node that runs fragment 1, or a node that runs the entire app\n./run build multiai_endoscopy\n./run launch multiai_endoscopy python\n</code></pre> Now we're ready to launch the distributed application in scenario 1. <pre><code>cd scenario1/\n</code></pre> To launch the two fragments on separate nodes, launch fragment 1 on node 1 as the driver node with: <pre><code># with replayer as source:\npython3 multi_ai.py --source replayer --data /workspace/holohub/data/ --driver --worker --address &lt;node 1 IP address&gt;:&lt;port number&gt; --fragments fragment1\n# with AJA video capture card as source:\npython3 multi_ai.py --source aja  --data /workspace/holohub/data/ --driver --worker --address &lt;node 1 IP address&gt;:&lt;port number&gt; --fragments fragment1\n</code></pre> and launch fragment 2 with: <pre><code>python3 multi_ai.py --worker --address &lt;node 1 IP address&gt;:&lt;port number&gt; --fragments fragment2\n</code></pre> To launch the two fragments together on a single node, simply launch the application without specifying the additional parameters: <pre><code># with replayer as source:\npython3 multi_ai.py --source replayer --data /workspace/holohub/data/ \n# with AJA video capture card as source:\npython3 multi_ai.py --source aja  --data /workspace/holohub/data/ \n</code></pre></p>","tags":["Distributed"]},{"location":"tutorials/creating-multi-node-applications/#scenario-2-application","title":"Scenario 2 Application","text":"<p>Let's first run the original applications. In the dev container, make sure to build and launch the <code>endoscopy_tool_tracking</code> and <code>endoscopy_out_of_body_detection</code> apps, this will convert the downloaded ONNX models into TensorRT engines at first run. </p> <p>Build and run instructions may change in the future, please refer to the original applications. <pre><code># On the node that runs fragment 1 or a node that runs the entire app\n./run build endoscopy_tool_tracking\n./run launch endoscopy_tool_tracking python\n\n# On the node that runs fragment 2 or a node that runs the entire app\n./run build endoscopy_out_of_body_detection\ncd build &amp;&amp; applications/endoscopy_out_of_body_detection/endoscopy_out_of_body_detection --data ../data/endoscopy_out_of_body_detection\n</code></pre> Now we're ready to launch the distributed application in scenario 2.  <pre><code># don't forget to do this on both machines\n# configure your PYTHONPATH environment variable \nexport PYTHONPATH=/opt/nvidia/holoscan/lib/cmake/holoscan/../../../python/lib:/workspace/holohub/build/python/lib\n# run in the build directory of Holohub in order to load extensions\ncd /workspace/holohub/build\n</code></pre></p> <p>To launch the two fragments on separate nodes, launch fragment 1 on node 1 as the driver node with: <pre><code># with replayer as source:\npython3 /workspace/holohub/tutorials/creating-multi-node-applications/scenario2/endoscopy_distributed_app.py --source replayer --data /workspace/holohub/data --driver --worker --fragments fragment1 --address &lt;node 1 IP address&gt;:&lt;port number&gt;\n# with AJA video capture card as source:\npython3 /workspace/holohub/tutorials/creating-multi-node-applications/scenario2/endoscopy_distributed_app.py --source aja --driver --worker --fragments fragment1 --address &lt;node 1 IP address&gt;:&lt;port number&gt;\n</code></pre> and launch fragment 2 with: <pre><code># with replayer as source:\npython3 /workspace/holohub/tutorials/creating-multi-node-applications/scenario2/endoscopy_distributed_app.py  --worker --fragments fragment2 --address &lt;node 1 IP address&gt;:&lt;port number&gt;\n# with AJA video capture card as source:\npython3 /workspace/holohub/tutorials/creating-multi-node-applications/scenario2/endoscopy_distributed_app.py  --source aja --worker --fragments fragment2 --address &lt;node 1 IP address&gt;:&lt;port number&gt;\n</code></pre> To launch the two fragments together on a single node, simply launch the application without specifying the additional parameters: <pre><code># with replayer as source:\npython3 /workspace/holohub/tutorials/creating-multi-node-applications/scenario2/endoscopy_distributed_app.py --source replayer --data /workspace/holohub/data --driver --worker --fragments fragment1,fragment2\n# with AJA video capture card as source:\npython3 /workspace/holohub/tutorials/creating-multi-node-applications/scenario2/endoscopy_distributed_app.py --source aja --driver --worker --fragments fragment1,fragment2 \n</code></pre></p>","tags":["Distributed"]},{"location":"tutorials/cuda_mps/","title":"CUDA MPS Tutorial for Holoscan Applications","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Last modified: May 13, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>CUDA MPS is NVIDIA's Multi-Process Service for CUDA applications. It allows multiple CUDA applications to share a single GPU, which can be useful for running more than one Holoscan application on a single machine featuring one or more GPUs. This tutorial describes the steps to enable CUDA MPS and demonstrate few performance benefits of using it.</p>","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Steps to enable CUDA MPS</li> <li>Customization</li> <li>x86 System Performance</li> <li>IGX Orin<ol> <li>Model Benchmarking Application Setup</li> <li>Performance Benchmark Setup</li> <li>Performance Benefits on IGX Orin w/ Discrete GPU<ol> <li>Varying Number of Instances</li> <li>Varying Number of Parallel Inferences</li> </ol> </li> <li> IGX Orin w/ iGPU<ol> <li>MPS Setup on IGX-iGPU</li> <li>Performance Benefits on IGX Orin w/ Integrated GPU</li> </ol> </li> </ol> </li> </ol>","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#steps-to-enable-cuda-mps","title":"Steps to enable CUDA MPS","text":"<p>Before enabling CUDA MPS, please check whether your system supports CUDA MPS.</p> <p>CUDA MPS can be enabled by running the <code>nvidia-cuda-mps-control -d</code> command and stopped by running  <code>echo quit | nvidia-cuda-mps-control</code> command. More control commands are described  here. </p> <p>CUDA MPS does not require any changes to an existing Holoscan application; even an already compiled application binary works as it is. Therefore, a Holoscan application can work with CUDA MPS without any  changes to its source code or binary. However, a machine learning model like a TRT engine file may need to be recompiled  for the first time after enabling CUDA MPS.</p> <p>We have included a helper script in this tutorial <code>start_mps_daemon.sh</code> to enable  CUDA MPS with necessary environment variables.</p> <pre><code>./start_mps_daemon.sh\n</code></pre>","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#customization","title":"Customization","text":"<p>CUDA MPS provides many options to customize resource allocation for MPS clients. For example, it has an option to limit the maximum number of GPU threads that can  be used by every MPS client.  The <code>CUDA_MPS_ACTIVE_THREAD_PERCENTAGE</code> environment variable can be used to control this limit system-wide. This limit can also be configured by communicating the active thread percentage to the control daemon with <code>echo \"set_default_active_thread_percentage &lt;Thread Percentage&gt;\" | nvidia-cuda-mps-control</code>. Our <code>start_mps_daemon.sh</code> script takes this percentage as the first argument as well.</p> <pre><code>./start_mps_daemon.sh &lt;Active Thread Percentage&gt;\n</code></pre> <p>For different applications, one may want to set different limits on the number of GPU threads available to each of them. This can be done by setting the <code>CUDA_MPS_ACTIVE_THREAD_PERCENTAGE</code> environment variable separately for each application. It is elaborated in details here.</p> <p>There are other customizations available in CUDA MPS as well. Please refer to the CUDA MPS documentation to know more about them. Please note that concurrently running Holoscan applications may increase the GPU device memory footprint. Therefore, one needs to be careful about hitting the GPU memory size and potential delay due to page faults.</p> <p>CUDA MPS improves the performance for concurrently running Holoscan applications.  Since multiple applications can simultaneously execute more than one CUDA compute tasks with CUDA MPS, it can also improve the overall GPU utilization.</p>","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#performance-benefits-on-x86-system","title":"Performance Benefits on x86 System","text":"<p>Note: Endoscopy Tool Tracking does not work with CUDA MPS after holohub/Holoscan-SDK-v2.6.0 due to the unavailability of CUDA dynamic parallelism implemented in this PR in CUDA MPS. In case endoscopy tool tracking needs to be tested with CUDA MPS, please use the <code>holoscan-sdk-v2.6.0</code> tag or earlier.</p> <p>Suppose, we want to run the endoscopy tool tracking and ultrasound segmentation applications concurrently on an x86 workstation with RTX A6000 GPU. The below table shows the maximum end-to-end latency performance without and with CUDA MPS, where the active thread percentage is set to 40\\% for each application. It demonstrates 18% and 50% improvement in the maximum end-to-end latency for the endoscopy tool tracking and ultrasound segmentation applications, respectively.</p> Application Without MPS (ms) With MPS (ms) Endoscopy Tool Tracking 115.38 94.20 Ultrasound Segmentation 121.48 60.94 <p>In another set of experiments, we concurrently run multiple instances of the endoscopy tool tracking application in different processes. We set the active thread percentage to be 20\\% for each MPS client. The below graph shows the maximum end-to-end latency with and without CUDA MPS. The experiment demonstrates up to 36% improvement with CUDA MPS.</p> <p></p> <p>Such experiments can easily be conducted with Holoscan Flow Benchmarking to retrieve various end-to-end latency performance metrics.</p>","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#igx-orin","title":"IGX Orin","text":"<p>CUDA MPS is available on IGX Orin since CUDA 12.5. Please check you CUDA version and upgrade to CUDA 12.5+ to test CUDA MPS. We evaluate the benefits of MPS on IGX Orin with discrete and integrated GPUs. Please follow the steps outlined in Steps to enable CUDA MPS to start running the MPS server on IGX Orin. </p> <p>We use the model benchmarking application to demonstrate the benefits of CUDA MPS. In general, MPS improves performance by enabling multiple concurrent processes to share a CUDA context and scheduling resources. We show the benefits of using CUDA MPS along two dimensions: (a) increasing the workload per application instance (varying the number of parallel inferences for the same model) and (b) increasing the total number of instances. </p>","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#model-benchmarking-application-setup","title":"Model Benchmarking Application Setup","text":"<p>Please follow the steps outlined in model benchmarking to ensure that the application builds and runs properly. </p> <p>Note that you need to run the video using v4l2loopback in a separate terminal while running the model benchmarking application.</p> <p>Make sure to change the device path in the <code>model_benchmarking/python/model_benchmarking.yaml</code> file to match the values you provided in the <code>modprobe</code> command when following the v4l2loopback instructions.</p>","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#performance-benchmark-setup","title":"Performance Benchmark Setup","text":"<p>To gather performance metrics for the model benchmarking application, follow the steps outlined in Holoscan Flow Benchmarking. </p> <p>If you are running within a container, please complete Step-3 before launching the container</p> <p>We use the following steps:</p> <p>1. Patch Application:</p> <pre><code>./benchmarks/holoscan_flow_benchmarking/patch_application.sh model_benchmarking\n</code></pre> <p>2. Build Application for Benchmarking:</p> <pre><code>./run build model_benchmarking python --configure-args -DCMAKE_CXX_FLAGS=-I$PWD/benchmarks/holoscan_flow_benchmarking`\n</code></pre> <p>3. Set Up V4l2Loopback Devices:</p> <p>i. Install <code>v4l2loopback</code> and <code>v4l2loopback</code>:</p> <pre><code>sudo apt-get install v4l2loopback-dkms ffmpeg\n</code></pre> <p>ii. Determine the number of instances you would like to benchmark and set that as the value of <code>devices</code>. Then, load the <code>v4l2loopback</code> kernel module on virtual devices <code>/dev/video[*]</code>. This enables each instance to get its input from a separate virtual device.</p> <p>Example: For 3 instances, the <code>v4l2loopback</code> kernel module can be loaded on <code>/dev/video1</code>, <code>/dev/video2</code> and <code>/dev/video3</code>:</p> <pre><code>sudo modprobe v4l2loopback devices=3 video_nr=1 max_buffers=4\n</code></pre> <p>Now open 3 separate terminals.</p> <p>In terminal-1, run: <pre><code>ffmpeg -stream_loop -1 -re -i /data/ultrasound_segmentation/ultrasound_256x256.avi -pix_fmt yuyv422 -f v4l2 /dev/video1\n</code></pre></p> <p>In terminal-2, run: <pre><code>ffmpeg -stream_loop -1 -re -i /data/ultrasound_segmentation/ultrasound_256x256.avi -pix_fmt yuyv422 -f v4l2 /dev/video2\n</code></pre></p> <p>In terminal-3, run: <pre><code>ffmpeg -stream_loop -1 -re -i /data/ultrasound_segmentation/ultrasound_256x256.avi -pix_fmt yuyv422 -f v4l2 /dev/video3\n</code></pre></p> <p>4. Benchmark Application:</p> <pre><code>python benchmarks/holoscan_flow_benchmarking/benchmark.py --run-command=\"python applications/model_benchmarking/python/model_benchmarking.py -l &lt;number of parallel inferences&gt; -i\"  --language python -i &lt;number of instances&gt; -r &lt;number of runs&gt; -m &lt;number of messages&gt; --sched greedy -d &lt;outputs folder&gt; -u\n</code></pre> <p>The command executes <code>&lt;number of runs&gt;</code> runs of <code>&lt;number of instances&gt;</code> instances of the model benchmarking application with <code>&lt;number of messages&gt;</code> messages. Each instance runs <code>&lt;number of parallel inferences&gt;</code> parallel model benchmarking inferences with no post-processing and visualization (<code>-i</code>).</p> <p>Please refer to Model benchmarking options and Holoscan flow benchmarking options for more information on the various command options.</p> <p>Example: After Step-3, to benchmark 3 instances for 10 runs with 1000 messages, run:</p> <pre><code>python benchmarks/holoscan_flow_benchmarking/benchmark.py --run-command=\"python applications/model_benchmarking/python/model_benchmarking.py -l 7 -i\"  --language python -i 3 -r 10 -m 1000 --sched greedy -d myoutputs -u`\n</code></pre>","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#performance-benefits-on-igx-orin-w-discrete-gpu","title":"Performance Benefits on IGX Orin w/ Discrete GPU","text":"<p>We look at the performance benefits of MPS by varying the number of instances and number of inferences. We use the RTX A6000 GPU for our experiments. From our experiments, we observe that enabling MPS results in upto 12% improvement in maximum latency compared to the default setting.</p>","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#varying-number-of-instances","title":"Varying Number of Instances","text":"<p>We fix the number of parallel inferences to 7, number of runs to 10 and number of messages to 1000 and vary the number of instances from 3 to 7 using the <code>-i</code> parameter. Please refer to Performance Benchmark Setup for benchmarking commands.</p> <p>The graph below shows the maximum end-to-end latency of model benchmarking application with and without CUDA MPS, where the active thread percentage was set to <code>80/(number of instances)</code>. For example, for 5 instances, we set the active thread percentage to <code>80/5 = 16</code>. By provisioning resources this way, we leave some resources idle in case a client should require to use it. Please refer to CUDA MPS Resource Provisioning for more details regarding this.</p> <p>The graph is missing a bar for the case of 7 instances and 7 parallel inferences as we were unable to get the baseline to execute. However, we were able to run when MPS was enabled, highlighting the advantage of using MPS for large workloads. We see that the maximum end-to-end latency improves when MPS is enabled and the improvement is more pronounced as the number of instances increases. This is because, as the number of concurrent processes increases, MPS confines CUDA workloads to a certain predefined set of SMs. MPS combines multiple CUDA contexts from multiple processes into one, while simultaneously running them together. It reduces the number of context switches and related inferences, resulting in improved GPU utilization.</p> <p>| Maximum end-to-end Latency | | :-------------------------:| .png)</p> <p>We also notice minor improvements in the 99.9<sup>th</sup> percentile latency and similar improvements in the 99<sup>th</sup> percentile latency.</p> 99.9<sup>th</sup> Percentile Latency 99<sup>th</sup> Percentile Latency .png) .png)","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#varying-number-of-parallel-inferences","title":"Varying Number of Parallel Inferences","text":"<p>We vary the number of parallel inferences to show that MPS may not be beneficial if the workload is insufficient to offset the overhead of running the MPS server. The graph below shows the result of increasing the number of parallel inferences from 3 to 7 while the number of instances is constant. </p> <p>As the number of parallel inferences increases, so does the workload, and the benefit of MPS is more evident. However, when the workload is low, CUDA MPS may not be beneficial. </p> Maximum Latency for 5 Instances .png)","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#igx-orin-w-integrated-gpu","title":"IGX Orin w/ Integrated GPU","text":"","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#mps-setup-on-igx-igpu","title":"MPS Setup on IGX-iGPU","text":"<p>Note that we run all commands as root</p> <p>1. Please add cuda-12.5+ to <code>$PATH</code> and <code>$LD_LIBRARY_PATH</code></p> <p>If you have multiple CUDA installations, check it at <code>/usr/local/</code> directory.</p> <pre><code>echo $PATH\n/usr/local/cuda-12.6/compat:/usr/local/cuda-12.6/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin\n\necho $LD_LIBRARY_PATH\n/usr/local/cuda-12.6/compat/lib:/usr/local/cuda-12.6/compat:/usr/local/cuda-12.6/lib64:\n</code></pre> <p>2. Be sure to pass <code>-v /tmp/nvidia-mps:/tmp/nvidia-mps  -v /tmp/nvidia-log:/tmp/nvidia-log -v /usr/local/cuda-12.6:/usr/local/cuda-12.6</code> to <code>./dev_container launch</code> command to ensure that the container is connected to the MPS control and server</p> <p>Example: <pre><code>./dev_container launch --img holohub:v2.1 --docker_opts \"-v /tmp/nvidia-mps:/tmp/nvidia-mps  -v /tmp/nvidia-log:/tmp/nvidia-log -v /usr/local/cuda-12.6:/usr/local/cuda-12.6\"\n</code></pre></p> <p>3. Inside the container, be sure to set the following environment variables: <pre><code>export CUDA_VISIBLE_DEVICES=0\nexport CUDA_MPS_PIPE_DIRECTORY=/tmp/nvidia-mps\nexport CUDA_MPS_LOG_DIRECTORY=/tmp/nvidia-log\n\nexport PATH=/usr/local/cuda-12.6/bin:$PATH\nexport PATH=/usr/local/cuda-12.6/compat:$PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.6/lib64:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.6/compat:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.6/compat/lib:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=/usr/lib/aarch64-linux-gnu/nvidia:$LD_LIBRARY_PATH\n</code></pre> Our <code>$PATH</code> and <code>$LD_LIBRARY_PATH</code> values inside the container are: <pre><code>echo $PATH\n/usr/local/cuda-12.6/bin:/opt/tensorrt/bin:/usr/local/mpi/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/ucx/bin:/opt/nvidia/holoscan/bin\n\necho $LD_LIBRARY_PATH\n/usr/local/cuda-12.6/compat/lib:/usr/local/cuda-12.6/compat:/usr/local/cuda-12.6/lib64:/usr/local/cuda/compat/lib.real:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/nvidia/holoscan/lib\n</code></pre></p> <p>4. Start MPS server and control <pre><code>sudo -i\nexport CUDA_MPS_ACTIVE_THREAD_PERCENTAGE=20\nnvidia-cuda-mps-control -d\n</code></pre></p> <p>5. After steps 1-4, follow the benchmark insructions to benchmark the application</p>","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#performance-benefits-on-igx-orin-w-integrated-gpu","title":"Performance Benefits on IGX Orin w/ Integrated GPU","text":"<p>We look at the performance benefits of MPS by varying the number of application instances. We run the model benchmarking application in a mode where the inputs are always available and being read from the disk with the video replayer operator. For every instance of the application, we run 1 inference (<code>-l 1</code>) as iGPU is a smaller GPU. In this experiment, we also oversubscribe the GPU to provide the instances more opportunity to utilize the available SMs (IGX Orin iGPU has 16 SMs).</p> <p>From our experiments, we observe that enabling MPS results in 22-50%, 13-49% and 6-37% improvement in maximum latency, 99.9 percentile latency and average latency, respectively. The graphs below capture the result. In the X-axis, number of instances is show to increase from 2 to 7, and the number in the parenthesis shows the number of SMs per instance enabled by <code>CUDA_ACTIVE_THREAD_PERCENTAGE</code>.</p> <p>| Maximum end-to-end Latency | | :-------------------------:| </p> 99.9 Percentile Latency Average Latency <p>We use different number of SMs for different instances to ensure the total number of SMs requested by all the instances exceed the number of available SMs.</p>","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/debugging/cli_debugging/","title":"Interactively Debugging a Holoscan Application","text":"<p> Authors: Tom Birdsong (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: May 13, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 2.2.0 Tested Holoscan SDK versions: 2.2.0, 2.3.0 Contribution metric: Level 1 - Highly Reliable</p> <p>Holoscan SDK is a platform for rapidly developing low-latency AI pipelines. As part of software development we often find it useful to inspect pipeline operations and data contexts during execution. This tutorial walks through a few common scenarios to illustrate how common command line interface tools can be used in debugging an application based on Holoscan SDK.</p>","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#tutorial-sections","title":"Tutorial Sections","text":"<ol> <li>Prerequisites</li> <li>Debugging a C++ or Python application with <code>gdb</code></li> <li>Debugging a Python application with <code>pdb</code></li> <li>Debugging Symbols for Legacy Holoscan SDK</li> <li>Logging</li> </ol>","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#background","title":"Background","text":"","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#what-is-debugging","title":"What is Debugging?","text":"<p>Software debugging is the process of identifying and rectifying issues in software. - Just-in-time debugging lets us pause execution and inspect the state while a program is running.   For a C++ library such as Holoscan SDK, we rely on debugging symbols generated by the compiler   at runtime to map the execution state back to human-readable source code, which lets us understand what the program was doing when paused. Tools such as <code>gdb</code> and <code>Visual Studio Code</code> allow us to manage application   execution by setting breakpoints and watches, as well as inspect the program state such as local variable   values when paused. - Logging is one process through which we record events that occur during real-time execution of   a pipeline without interrupting the flow of execution. Log messages can provide a view of the application   state and flow that is limited to the messages the developer chooses to log. Holoscan SDK supports   logging based on popular libraries such as <code>spdlog</code> and the Python <code>logging</code> module.</p> <p>In general, both logging and just-in-time debugging are useful tools for prototyping and general development, while logging is usually better suited for application deployment. In this guide, we focus on using just-in-time debugging tools to inspect applications built on Holoscan SDK.</p>","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#what-are-debugging-symbols","title":"What are debugging symbols?","text":"<p>Debugging symbols are generated by a C++ compiler at build time to provide additional information for application debugging such as file names and line numbers. Debugging symbols must be present for debugging tools to provide meaningful information to a developer when inspecting an application's execution state.</p> <p>Holoscan SDK uses the following build profiles in its <code>run</code> script: - <code>release</code>: Instructs the C++ compiler to optimize where possible and not generate debugging symbols. We use this build type to create minimum-footprint binaries such as those in the Holoscan SDK Debian package or Python wheel distributions. - <code>rel-debug</code>: Instructs the C++ compiler to optimize where possible and generate debugging symbols. We use this build type to create developer-friendly binaries packaged in the Holoscan SDK NGC containers. Debug symbols may occasionally have inaccuracies due to release optimizations. - <code>debug</code>: Instructs the C++ compiler to minimize optimizations and prioritize debugging symbol accuracy. As of v2.3, we do not release Holoscan SDK builds of this type, but we do provide a <code>run</code> script to help developers generate their own <code>debug</code> builds on demand.</p>","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#what-are-some-common-tools-i-can-use-for-debugging-my-application","title":"What are some common tools I can use for debugging my application?","text":"<p>There are a wide variety of free and/or open source software tools available for general C++ and Python debugging, including: - NVIDIA's debugging solutions, including NVIDIA NSight and CUDA-GDB; - The GNU project DeBugger (GDB); - The built-in Python Debugger (pdb) module; - Microsoft Visual Studio Code, with a wide variety of community extensions.</p> <p>In this tutorial we will focus on the <code>GDB</code> and <code>pdb</code> command line tools. These require minimal setup and can be run via a simple terminal without a dedicated display (\"headless\"). For advanced development we recommend reviewing Visual Studio Code Development Containers with custom launch profiles, with HoloHub support coming soon.</p>","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#references","title":"References","text":"<p>To get started with debugging your Holoscan SDK application, visit the Debugging user guide section for common topics, including: - Generating debug symbols with VSCode - Live debugging for C++ and Python applications - Inspecting application crashes - Profiling and code coverage</p> <p>Visit the Logging user guide section for a thorough overview on how to set up Holoscan SDK logging in your C++ or Python application.</p>","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#prerequisites","title":"Prerequisites","text":"<p>The steps for getting started with <code>gdb</code> depend on how you are consuming Holoscan SDK. - We encourage using Holoscan SDK containers from NGC for development and we take this approach for most of the tutorial. If you are using an NGC container for Holoscan SDK v2.3.0 or later, you already have access   to debug symbols and can get started right away. - If you are using an older Holoscan SDK container from NGC, or if you are consuming Holoscan SDK through another means such as Debian packages, Python wheels, or custom installation, you will need to build Holoscan SDK with debugging symbols in order to step through Holoscan SDK code during debugging. Jump to the Legacy Holoscan SDK section to get started.</p> <p>Review the HoloHub Prerequisites along with the Endoscopy Tool Tracking requirements C++, Python to get started before continuing.</p> <p>If you have previously built the Endoscopy Tool Tracking application, you should clear your build directory before proceeding. <pre><code>./run clear-cache\n</code></pre></p>","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#debugging-a-c-application-with-gdb","title":"Debugging a C++ Application with <code>gdb</code>","text":"","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#background_1","title":"Background","text":"<p>GDB (the GNU project DeBugger) is a widely used tool for headless just-in-time debugging of C++ applications. GDB distributions are available for most Holoscan SDK supported platforms and do not require a display to set up. Refer to the GDB User Manual to get started.</p>","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#getting-started","title":"Getting Started","text":"<p>For this tutorial we will debug the Endoscopy Tool Tracking application. The tutorial <code>debug_gdb.sh</code> script is a self-contained example that will build the C++ application and launch into a <code>gdb</code> debugging session.</p> <p>Run the script to get started: <pre><code>./tutorials/cli_debugging/debug_gdb.sh\n</code></pre></p> <p>The script runs through the following steps: 1. Builds the tutorial container environment with <code>gdb</code> based on the Holoscan SDK NGC container. 2. Builds the Endoscopy Tool Tracking application in the container environment. By default we use the   <code>debug</code> build mode to generate detailed debugging symbols for the Endoscopy Tool Tracking application.   Note that this does not regenerate build symbols for Holoscan SDK, which are already packaged in   Holoscan SDK binaries in <code>rel-debug</code> mode. 3. Launches the Endoscopy Tool Tracking application with <code>gdb</code>. This step prefixes the launch command   given by <code>./run launch endoscopy_tool_tracking</code> to run with <code>gdb</code>. The command sets a few actions for <code>gdb</code> to take on startup:     - Sets a breakpoint in the <code>main</code> function of Endoscopy Tool Tracking;     - Runs the program with custom arguments until the breakpoint is hit;     - Sets a breakpoint in Holoscan SDK's <code>add_flow</code> function.</p> <p>At this point <code>gdb</code> enters an interactive session where we can inspect the Endoscopy Tool Tracking program state and advance execution.</p> <p>GDB can also be used to interactively debug C++ symbols underlying a Holoscan Python pipeline. Run the tutorial <code>debug_gdb.sh</code> script to inspect the Endoscopy Tool Tracking Python application:</p> <pre><code>./tutorials/cli_debugging/debug_gdb.sh debug python\n</code></pre> <p>When the <code>python</code> argument is provided, the script builds the Endoscopy Tool Tracking application with Python bindings and initiates debugging with GDB from the Python script entrypoint. Once symbols are loaded in GDB, we can set breakpoints and inspect the underlying state of Holoscan SDK C++ operator implementations at runtime.</p> <p>From this point we recommend referring to the GDB Manual or online tutorials to get started with interactive debugging commands.</p>","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#frequently-asked-questions-and-troubleshooting","title":"Frequently Asked Questions and Troubleshooting","text":"","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#how-can-i-verify-that-holoscan-sdk-debugging-symbols-have-been-loaded-in-gdb","title":"How can I verify that Holoscan SDK debugging symbols have been loaded in <code>gdb</code>?","text":"<p><code>gdb</code> loads debugging symbols for Holoscan SDK only when the application loads Holoscan SDK binaries. Before that time, we can set breakpoints in Holoscan SDK files, but <code>gdb</code> will not understand them yet.</p> <p>We can use <code>info sharedlibrary</code> to inspect the shared libraries that have been dynamically loaded for execution.</p> <pre><code>(gdb) info sharedlibrary\nFrom                To                  Syms Read   Shared Object Library\n0x00007ffff7fc5090  0x00007ffff7fee315  Yes         /lib64/ld-linux-x86-64.so.2\n0x00007ffff7ca3ba0  0x00007ffff7ec655d  Yes         /opt/nvidia/holoscan/lib/libholoscan_op_aja.so.2\n0x00007ffff7f90ac0  0x00007ffff7faee1f  Yes         /opt/nvidia/holoscan/lib/libholoscan_op_video_stream_replayer.so.2\n0x00007ffff7bd9810  0x00007ffff7bf4e3f  Yes         /opt/nvidia/holoscan/lib/libholoscan_op_video_stream_recorder.so.2\n...\n</code></pre> <p>We can use <code>info sources</code> to inspect the Holoscan SDK symbols are available. Note: Source paths are loaded from Holoscan SDK binaries and respect source file locations at the time the Holoscan SDK distribution was built. These paths may not reflect your filesystem if you have mounted <code>holoscan-sdk</code> somewhere other than <code>/workspace/holoscan-sdk</code>.</p> <pre><code>(gdb) info sources /workspace/holoscan-sdk/src/core\n/workspace/holoscan-sdk/src/core/application.cpp, /workspace/holoscan-sdk/src/core/services/generated/system_resource.grpc.pb.cc,\n/workspace/holoscan-sdk/src/core/services/generated/system_resource.pb.h, /workspace/holoscan-sdk/src/core/services/generated/system_resource.pb.cc,\n/workspace/holoscan-sdk/src/core/services/generated/result.grpc.pb.cc, /workspace/holoscan-sdk/src/core/services/generated/result.pb.h,\n...\n</code></pre>","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#how-can-i-pause-a-running-holoscan-sdk-application-for-debugging","title":"How can I pause a running Holoscan SDK application for debugging?","text":"<p>After launching the application with <code>gdb</code> as done in <code>debug_gdb.sh</code>, use <code>continue</code> to allow the application to run. Then, press <code>Ctrl+C</code> to force the application to pause when you want to enter interactive debugging. Use <code>backtrace</code> to view stack frames at the point where the application paused.</p>","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#how-can-i-manage-breakpoints-to-pause-the-application-sometime-in-the-future","title":"How can I manage breakpoints to pause the application sometime in the future?","text":"<ol> <li>To add a breakpoint <code>in holoscan/operators/format_converter/format_converter</code>: <pre><code>(gdb) break /workspace/holoscan-sdk/src/operators/format_converter/format_converter.cpp:compute\"\n</code></pre></li> <li>To list all current breakpoints: <pre><code>(gdb) info breakpoints\n</code></pre></li> <li>To remove breakpoints: <pre><code>(gdb) delete &lt;number(s) of breakpoint(s)&gt;\n</code></pre></li> </ol>","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#how-can-i-inspect-local-variables","title":"How can I inspect local variables?","text":"<p>Use the <code>info</code> command to inspect the values of local variables.</p> <pre><code>(gdb) info locals\n</code></pre>","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#how-can-i-attach-to-a-running-holoscan-sdk-session","title":"How can I attach to a running Holoscan SDK session?","text":"<p>Do the following to attach to a HoloHub application (C++ or Python):</p> <ol> <li> <p>Launch the container with root permissions and start the process in the background: <pre><code>./dev_container launch --img holohub:debugging --as_root\n\n# Run inside the container\n&gt;&gt;&gt; ./run launch endoscopy_tool_tracking &amp;\n</code></pre></p> </li> <li> <p>Press <code>Ctrl+C</code> to return to your interactive shell</p> </li> <li> <p>Find the process ID (PID) of the running application: <pre><code># Inside the container\n&gt;&gt;&gt; ps -ef | grep endoscopy_tool_tracking\nuser+     292     203 28 13:17 pts/9    00:00:04 /workspace/holohub/build/endoscopy_tool_tracking/applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking --data /workspace/holohub/data/endoscopy\n</code></pre></p> </li> <li> <p>Attach to the running process with <code>gdb -p</code>: <pre><code># Inside the container\n&gt;&gt;&gt; gdb -p 292\n</code></pre></p> </li> </ol> <p>From this point you can use Ctrl+C to pause application execution, then use the interactive GDB console to set breakpoints and inspect application state as usual.</p>","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#i-see-a-gdb-python-exception-in-the-gdb-log","title":"I see a <code>gdb</code> Python Exception in the <code>gdb</code> log.","text":"<p><code>gdb</code> relies on a Python module for operations such as unwinding. If the Python module is not properly referenced in the container, you may see <code>gdb</code> errors appear in the console log such as the following: <pre><code>\"Python Exception &lt;class 'NameError'&gt;: Installation error: gdb._execute_unwinders function is missing\"\n</code></pre> To resolve the error, update your <code>PYTHONPATH</code> variable to include your GDB Python directory: <pre><code>PYTHONPATH=${PYTHONPATH}:/usr/share/gdb/python\n</code></pre></p>","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#debugging-a-python-application-with-pdb","title":"Debugging a Python application with <code>pdb</code>","text":"","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#background_2","title":"Background","text":"<p>The Python Debugger module is a built-in interactive debugging tool for Python programs. Similar to <code>gdb</code>, it supports setting breakpoints for interactive, headless, just-in-time debugging. You can use <code>pdb</code> to debug Holoscan SDK Python programs on any platform supported by Holoscan SDK.</p> <p>Holoscan SDK Python libraries serve as wrappers around Holoscan SDK C++ libraries. While <code>pdb</code> may load C++ symbols, it is not well suited for setting breakpoints or stepping into underlying C++ code. <code>pdb</code> is best suited for debugging Holoscan SDK operators whose implementation lies in a Python-native <code>compute</code> method.</p>","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#prerequisites_1","title":"Prerequisites","text":"<p>The <code>pdb</code> module is built in to modern Python versions. No additional installation is required.</p> <p>We will continue to use the Holoscan SDK v2.3 development container from NGC for this section, which includes pre-installed versions of Python and Holoscan SDK.</p>","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#getting-started_1","title":"Getting Started","text":"<p>We will continue to debug the Endoscopy Tool Tracking application. The tutorial <code>debug_pdb.sh</code> script is a self-contained example that will build the application and launch into a <code>pdb</code> debugging session.</p> <p>Run the script to get started: <pre><code>./tutorials/cli_debugging/debug_pdb.sh\n</code></pre></p> <p>The script runs through the following steps: 1. Builds the HoloHub container environment based on the Holoscan SDK NGC container. 2. Builds the Endoscopy Tool Tracking Python application in the container environment. By default we use the   <code>debug</code> build mode to generate detailed C++ debugging symbols for the Endoscopy Tool Tracking application. 3. Launches the Endoscopy Tool Tracking application with <code>pdb</code>, which is invoked via the Python interpreter: <pre><code>python3 -m pdb &lt;command --args ...&gt;\n</code></pre></p> <p>This command launches the Python version of the Endoscopy Tool Tracking application with a breakpoint set on the very first line. From this point you can set additional breakpoints, inspect application state, and control program execution.</p> <p>For instance, the following snippet sets a breakpoint and then inspects the value of <code>self.source.lower()</code> during pipeline setup in the app <code>compose</code> method: <pre><code>(Pdb) break endoscopy_tool_tracking.py:77\nBreakpoint 1 at /workspace/holohub/applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking.py:77\n(Pdb) continue\n...\n&gt; /workspace/holohub/applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking.py(77)compose()\n-&gt; if self.source.lower() == \"aja\":\n(Pdb) p self.source.lower()\n'replayer'\n(Pdb)\n...\n</code></pre></p> <p>You can also add a breakpoint directly in the <code>.py</code> source code before running a program by adding a <code>breakpoint()</code> statement.</p> <p>From here we recommend referring to <code>pdb</code> documentation for common commands and debugging strategies.</p>","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#debugging-symbols-for-legacy-holoscan-sdk-versions","title":"Debugging Symbols for Legacy Holoscan SDK Versions","text":"<p>Starting from Holoscan SDK v2.3, we package debugging symbols as part of the libraries distributed in NGC Holoscan Containers, with the goal of improving ease of development and debugging. This change comes at a small cost to size and performance of the Holoscan SDK binary distribution. But what about older versions of Holoscan SDK containers?</p> <p>If you are using a legacy Holoscan SDK container (earlier than v2.3) for your development, your container does not come with debugging symbols pre-packaged. However, you can rebuild the Holoscan SDK from its source code to enable interactive debugging. We provide utilities as part of Holoscan SDK open source code to help you generate debugging symbols that will allow you to use tools such as <code>gdb</code> and <code>pdb</code> with legacy Holoscan SDK.</p> <p>The following script provides the necessary steps to rebuild Holoscan SDK version with debugging symbols and then set up for debugging with <code>gdb</code>:</p> <pre><code>./tutorials/cli_debugging/debug_legacy.sh\n</code></pre> <p>The script does the following: 1. Builds the specified Holoscan SDK version with the specified build type in a temporary tutorial folder. Refer to   background discussion for an overview of the different build types. 2. Builds the Endoscopy Tool Tracking application against the custom Holoscan SDK debug build. 3. Runs the Endoscopy Tool Tracking application with <code>gdb</code> for interactive debugging.</p> <p>For convenience, the <code>debug_legacy.sh</code> script mounts your custom Holoscan SDK installation at <code>/opt/nvidia/holoscan</code>, the default library path in the Holoscan SDK container. This effectively hides the Holoscan SDK build otherwise distributed inside the container and instead makes your custom debugging build available to build downstream applications.</p>","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#frequently-asked-questions-and-troubleshooting_1","title":"Frequently Asked Questions and Troubleshooting","text":"","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#how-can-i-use-a-custom-container-path-for-my-holoscan-sdk-debugging-build-other-than-optnvidiaholoscan","title":"How can I use a custom container path for my Holoscan SDK debugging build other than <code>/opt/nvidia/holoscan</code>?","text":"<p>You can choose to mount your custom Holoscan SDK debugging build at another path in the container with the Docker <code>-v</code> option or the HoloHub <code>dev_container</code> script <code>--local_sdk_root</code> or <code>--mount-volume</code> options. If you are mounting your build at a custom path in the Holoscan SDK container for general development, consider the following details when building and debugging: - <code>LD_LIBRARY_PATH</code> is an environment variable with a list of locations to look up for dynamic loading. By default   the Holoscan SDK container sets <code>LD_LIBRARY_PATH</code> to include <code>/opt/nvidia/holoscan</code>, and then the HoloHub <code>run</code> script   sets it again when launching an application. Edit this variable and launch your application directly to load libraries   from your custom mount by default. - <code>RPATH</code> or <code>RUNPATH</code> is an ELF header field that embeds shared library lookup locations in an executable.   HoloHub applications set <code>RPATH</code> to include <code>/opt/nvidia/holoscan</code> by default. Edit the value of <code>CMAKE_INSTALL_RPATH</code> in  <code>CMakeLists.txt</code> to remove the reference to <code>/opt/nvidia/holoscan</code> or reference your preferred mount path.</p>","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#how-can-i-launch-the-tutorial-application","title":"How can I launch the tutorial application?","text":"<p>You can simply re-run the tutorial script to rebuild and relaunch the application:</p> <pre><code>./tutorials/cli_debugging/debug_legacy.sh\n</code></pre> <p>Alternatively, run the following to relaunch the application in the debugging container without rebuilding: <pre><code># Find the custom Holoscan SDK debugging build\nINSTALL_DIR=$(realpath $(find ./tutorials/cli_debugging/tmp -type d -name \"install-*\"))\n\n# Launch the debugging container\n./dev_container launch --docker_opts \"-v $INSTALL_DIR:/opt/nvidia/holoscan --security-opt seccomp=unconfined\" --img holohub:debugging\n\n# Inside the container\n&gt;&gt;&gt; gdb -q \\\n    -ex \"break main\" \\\n    -ex \"run --data /workspace/holohub/data/endoscopy\" \\\n    -ex \"break /workspace/holoscan-sdk/src/core/application.cpp:add_flow\" \\\n    /workspace/holohub/build/endoscopy_tool_tracking/applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking\n</code></pre></p> <p>Refer to the <code>debug_legacy.sh</code> script for more details.</p>","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#logging","title":"Logging","text":"<p>Just-in-time debugging is not well suited to problems that require real-time performance analysis. Logging is usually the better choice to debug performance related issues in your Holoscan application.</p> <p>The Holoscan SDK User Guide Logging section presents a detailed overview of how to get started with logging from your application.</p>","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#logging-from-a-c-application","title":"Logging from a C++ application","text":"<p>C++ applications based on Holoscan should use the <code>HOLOSCAN_LOG_LEVEL</code> environment variable or <code>holoscan::set_log_level</code> function to set the global level of detail to log in the application. You can add inline macros such as <code>HOLOSCAN_LOG_INFO</code> AND <code>HOLOSCAN_LOG_TRACE</code> in your application code to print out log messages at runtime according to the current logging level of detail.</p> <pre><code>export HOLOSCAN_LOG_LEVEL=\"Debug\"\n./run launch endoscopy_tool_tracking\n</code></pre>","tags":["Container"]},{"location":"tutorials/debugging/cli_debugging/#logging-from-a-python-application","title":"Logging from a Python application","text":"<p>Python applications based on Holoscan should use the Python <code>logging</code> module. Holoscan observes the standard logging module interface with statements such as <code>logger.info</code> and <code>logger.debug</code>. Refer to the Python <code>logging</code> module for more information.</p>","tags":["Container"]},{"location":"tutorials/debugging/holoscan_container_vscode/","title":"Holoscan SDK Visual Studio Code Dev Container Template","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Last modified: May 13, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.3.0 Tested Holoscan SDK versions: 2.3.0 Contribution metric: Level 1 - Highly Reliable</p>","tags":["VS Code"]},{"location":"tutorials/debugging/holoscan_container_vscode/#overview","title":"Overview","text":"<p>A Dev Container (short for Development Container) is a lightweight, isolated environment for developing software. It's a self-contained directory that contains all the dependencies and tools needed to develop a software project without polluting the host machine or interfering with other projects.</p> <p>This directory contains a pre-configured Dev Container designed for Holoscan SDK using the Holoscan NGC Container with Visual Studio Code. This Dev Container comes with the complete Holoscan SDK and source code, sample applications, and all the tools needed to build your next application using Holoscan SDK.</p> <p>Follow this step-by-step guide to get started!</p>","tags":["VS Code"]},{"location":"tutorials/debugging/holoscan_container_vscode/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker</li> <li>VS Code with the Dev Container Extension Pack</li> <li>Install Dev Container Extension Pack via command line     <pre><code>code --install-extension ms-vscode-remote.remote-containers\n</code></pre></li> <li>NVIDIA Container Toolkit</li> <li>NVIDIA CUDA Toolkit</li> <li>Holoscan SDK container 2.3 or later</li> </ul>","tags":["VS Code"]},{"location":"tutorials/debugging/holoscan_container_vscode/#dev-container-setup","title":"Dev Container Setup","text":"<ol> <li>Download everything in this directory to a folder of your choice. This folder can also be used to store your project(s) later. See Directory Structure.</li> <li>Open .devcontainer/devcontainer.json, find and change the value of <code>HOLOSCAN_SDK_IMAGE</code> to a version of Holoscan SDK container image you want to use.   \ud83d\udca1  Important: Holoscan SDK container v2.3 or later is required to step into Holoscan SDK source code in debug mode.</li> <li>Start VS Code, open the Command Palette by pressing <code>F1</code>, <code>Ctrl+Shift+P</code>, or navigating to <code>View &gt; Command Palette</code>, type to select <code>Dev Containers: Open Folder in Container...</code> and select the folder where you stored step 1.</li> <li>Wait for the Dev Container to start up. Building the container and installing all the required extensions will take a while. (Switch to the <code>Output</code> view and select the <code>Server</code> option from the dropdown to check the status of extension installations.)</li> </ol> <p>When everything is ready, you should see the following in the VS Code Explorer sidebar.</p> <p></p> <p>The <code>My Workspace</code> section contains all the files that you've copied. The <code>Holoscan SDK</code> section includes the Holoscan example applications from <code>~/examples</code>.</p>","tags":["VS Code"]},{"location":"tutorials/debugging/holoscan_container_vscode/#directory-structure","title":"Directory Structure","text":"<pre><code>/workspace\n   \u251c\u2500\u2500 .vscode/            # Visual Studio Code debugging configuration files\n   \u251c\u2500\u2500 holoscan-sdk/       # Holoscan SDK source code\n   \u2514\u2500\u2500 my/                 # Your workspace directory - this is the directory created and mounted from step 1 above.\n       \u251c\u2500\u2500 .devcontainer/  # Dev Container configurations\n       \u251c\u2500\u2500 src/            # A folder set up to store your project(s)\n       \u2514\u2500\u2500 README.md       # This file\n</code></pre> <p>\ud83d\udca1 Note: The Dev Container is configured with a default user account <code>holoscan</code> using user ID <code>1000</code> and group ID <code>1000</code> in the devcontainer.json file.   - To change the user name, find and replace <code>USERNAME</code> and <code>remoteUser</code>.  - To change the user ID, find and replace <code>USER_UID</code> and <code>userUid</code>.  - To change the group ID, find and replace <code>USER_GID</code> and <code>userGid</code>.</p>","tags":["VS Code"]},{"location":"tutorials/debugging/holoscan_container_vscode/#debugging-applications","title":"Debugging Applications","text":"","tags":["VS Code"]},{"location":"tutorials/debugging/holoscan_container_vscode/#debugging-a-c-application","title":"Debugging a C++ Application","text":"<p>\ud83d\udca1 Tip: Open this <code>README.md</code> file in the Dev Container will help you navigate to the linked files faster.      This file can be found inside the Dev Container under <code>/workspace/my/README.md</code>.</p> <p>This section will walk you through the steps to debug the Hello World application using VS Code.</p> <p>Open the hello_world.cpp file in VS Code. (If the link doesn't work, click on Explorer from the sidebar and expand the holoscan-sdk folder. Find and open <code>hello_world.cpp</code> under the <code>examples/hello_world/cpp</code> directory. Let's put a breakpoint on this line: <code>auto app = holoscan::make_application&lt;HelloWorldApp&gt;();</code> (Feel free to put more breakpoints wherever you want).</p> <p>Now, let's switch to the Run and Debug panel on the sidebar, and then click on the dropdown box to the right of the Start button.</p> <p>Select <code>(gdb) examples/hello_world/cpp</code> from the list of available launch configurations.</p> <p>Hit F5 on the keyboard or click the green arrow to start debugging. VS Code shall hit the breakpoint and stop as the screenshot shows below: </p> <p>\ud83d\udca1 Tip: What happens when you hit F5? VS Code looks up the launch profile for <code>(gdb) examples/hello_world/cpp</code> in the .vscode/launch.json file and starts the debugger with the appropriate configurations and arguments.</p>","tags":["VS Code"]},{"location":"tutorials/debugging/holoscan_container_vscode/#debugging-a-python-application","title":"Debugging a Python Application","text":"<p>There are a few options when debugging a Python application. In the .vscode/launch.json file, you may find the following options to debug the Hello World application:</p> <ul> <li>Python: Debug Current File: with this option selected, open hello_world.py file and hit F5. It shall stop at any breakpoints selected.</li> <li>Python C++ Debug: similar to the previous option, this launch profile allows you to debug both the Python and C++ code.   Open holoscan-sdk/src/core/application.cpp and find the <code>Application::run()</code> function. Let's put a breakpoint inside this function. Navigate back to the hello_world.py file and hit F5. When the debug session starts, it stops at the top of the main application file and brings up a prompt in the terminal asking for superuser access.</li> </ul> <pre><code>Superuser access is required to attach to a process. Attaching as superuser can potentially harm your computer. Do you want to continue? [y/N]\n</code></pre> <p>You may answer <code>Y</code> or <code>y</code> to continue the debug session. The debugger shall now stop at the breakpoint you've set in the <code>application.cpp</code> file.</p> <ul> <li>(gdb) examples/hello_world/python: this third launch profile option allows you to debug the C++ code only.  Put a breakpoint in the <code>Application::run()</code> function inside the holoscan-sdk/src/core/application.cpp file.</li> </ul> <p>\ud83d\udca1 Tip: you must open a Python file and make sure the file tab is active to debug when using the first two launch profiles.*</p>","tags":["VS Code"]},{"location":"tutorials/debugging/holoscan_container_vscode/#troubleshooting","title":"Troubleshooting","text":"","tags":["VS Code"]},{"location":"tutorials/debugging/holoscan_container_vscode/#cannot-start-debugger","title":"Cannot Start Debugger","text":"<ul> <li>Configured debug type 'cppdbg' is not supported.</li> <li>Configured debug type 'debugpy' is not supported.</li> <li>Configured debug type 'pythoncpp' is not supported.</li> <li>Configured debug type 'python' is not supported.</li> </ul> <p>If you encounter the above errors, please ensure all the required extensions are installed in VS Code. It may take a while to install them for the first time.</p>","tags":["VS Code"]},{"location":"tutorials/debugging/holoscan_container_vscode/#cannot-set-breakpoints","title":"Cannot Set Breakpoints","text":"<p>If you cannot set breakpoints, please ensure all the required extensions are installed in VS Code. It may take a while to install them for the first time.</p>","tags":["VS Code"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/","title":"Processing DICOM to USD with MONAI Deploy and Holoscan","text":"<p> Authors: Rahul Choudhury (NVIDIA), Andreas Heumann (NVIDIA), Cristiana Dinea (NVIDIA), Gregory Lee (NVIDIA), Jeroen Stinstra (NVIDIA), Ming Melvin Qi (NVIDIA), Tom Birdsong (NVIDIA), Wendell Hom (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 2 - Trusted</p> <p></p> <p>In this tutorial we demonstrate a method leveraging a combined MONAI Deploy and Holoscan pipeline to process DICOM input data and write a resulting mesh to disk in the OpenUSD file format.</p>","tags":["Healthcare Interop","MONAI","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#demonstrated-technologies","title":"Demonstrated Technologies","text":"","tags":["Healthcare Interop","MONAI","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#dicom","title":"DICOM","text":"<p>The Digital Imaging and Communications in Medicine (DICOM) standard is a comprehensive standard for medical imaging. DICOM covers a wide variety of medical imaging modalities and defines standards for both image storage and communications in medicine. DICOM data often represent 2D or 3D volumes or series of volumes.</p>","tags":["Healthcare Interop","MONAI","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#openusd","title":"OpenUSD","text":"<p>Universal Scene Description (OpenUSD) is an extensible ecosystem of file formats, compositors, renderers, and other plugins for comprehensive 3D scene description.</p> <p>OpenUSD serves as the backbone of the NVIDIA Omniverse cloud computing platform. Omniverse includes a variety of applications such as USD Composer for viewing and manipulating OpenUSD scenes, with features such as: - State-of-the-art cloud rendering - Live collaborative scene editing - Multi-user mixed reality</p> <p>Download the NVIDIA Omniverse launcher to get started with Omniverse. See NVIDIA OpenUSD Tutorials for getting started with the OpenUSD Python libraries we use in this tutorial.</p>","tags":["Healthcare Interop","MONAI","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#monai-deploy-holoscan-pipeline","title":"MONAI Deploy + Holoscan Pipeline","text":"<p>The MONAI Deploy App SDK provides a series of operators to load and select DICOM instances and then decode the pixel data into in-memory NumPy data objects. MONAI Deploy integrates seamlessly with Holoscan pipelines.</p> <ol> <li>The DICOM Loader operator parses a set of DICOM instance files, and loads them into a set of objects representing the logical hierarchical structure of DICOM Study, Series, and Instance. Key DICOM metadata is extracted, while the image pixel data is not loaded in memory or decoded.</li> <li>The DICOM Series Selector selects relevant DICOM Series, e.g, a MR T2 series, using simple configurable selection rules.</li> <li>The Series to Volume converter decodes and combines the pixel data of the instances to a 3D NumPy array with a set of metadata for spacing, orientation, etc.</li> <li>The MONAI Deploy AI Inference Operator accepts the 3D volume and runs AI inference to segment the region of interest, which in this case is pixels representing the spleen.</li> <li>The MONAI STL Conversion Operator converts the output label volume to a mesh in STL format.</li> <li>The Holoscan \"Send Mesh to USD\" operator writes the mesh to disk in the OpenUSD format.</li> </ol>","tags":["Healthcare Interop","MONAI","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#requirements","title":"Requirements","text":"<p>Please review the HoloHub README to get started with HoloHub general requirements before continuing.</p>","tags":["Healthcare Interop","MONAI","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#hardware","title":"Hardware","text":"<p>This tutorial may run on an <code>amd64</code> or <code>arm64</code> workstation. - For <code>amd64</code> we rely on <code>usd-core</code> Python wheels from PyPI for OpenUSD support. - For <code>arm64</code> we rely on NVIDIA Omniverse Python libraries for OpenUSD support.</p>","tags":["Healthcare Interop","MONAI","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#host-software","title":"Host Software","text":"<p>This tutorial should run in a <code>docker</code> container:</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install docker\n</code></pre>","tags":["Healthcare Interop","MONAI","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#building-the-tutorial-container","title":"Building the tutorial container","text":"<p>Run the command below from the top-level HoloHub directory to build the tutorial container on the host workstation:</p> <pre><code>export NGC_CONTAINER_IMAGE_PATH=\"nvcr.io/nvidia/clara-holoscan/holoscan:v1.0.3-dgpu\"\n./dev_container build --docker_file tutorials/dicom_to_usd_with_monai_and_holoscan/Dockerfile --base_img ${NGC_CONTAINER_IMAGE_PATH} --img holohub:dicom-to-usd\n</code></pre>","tags":["Healthcare Interop","MONAI","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#running-the-application","title":"Running the application","text":"<p>Run the commands below on the host workstation to launch the container and run the tutorial. The application will run the MONAI Deploy + Holoscan pipeline for AI segmentation and write results to the <code>.usd</code> output file. The mesh will also be available as a <code>mesh.stl</code> file on disk.</p> <pre><code>./dev_container launch --img holohub:dicom-to-usd # start the container\ncd ./tutorials/dicom_to_usd_with_monai_and_holoscan\npython tutorial.py --output ./output/spleen-segmentation.usd # run the tutorial\n</code></pre> <p>Download the NVIDIA Omniverse launcher to explore applications such as USD Composer for viewing and manipulating the OpenUSD output file.</p>","tags":["Healthcare Interop","MONAI","OpenUSD","STL"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/","title":"Taking advantage of GPU Direct Storage on the latest NVIDIA Edge platform","text":"<p> Authors: Marcus Manos (NVIDIA) Supported platforms: aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 4 - Experimental</p>","tags":["GPUDirect"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#introduction","title":"Introduction","text":"<p>Modern-day edge accelerated computing solutions constantly push the boundaries of what is possible. The success of edge computing is mainly due to a new approach to accelerated computing problems, which are viewed from both a GPU and systems perspective. With innovations such as GPU Direct Storage (GDS) and GPU Direct Storage-over-Fabrics (GDS-OF), we can load data directly onto the GPU at lightning-fast speeds. GDS allows us to transform previously offline batch workloads into online streaming solutions, bringing them into the 21st century. This tutorial demonstrates this with a sample pipeline using the latest industrial-grade edge hardware (IGX) and A6000 workstation GPU. We\u2019ll start by providing an overview of the two main types of workflows, then discuss how to set up GDS, and finally, wrap up with an example application using Nvidia Holoscan and the Nvidia Rapids software suite.</p>","tags":["GPUDirect"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#overview-of-gds-and-gds-of","title":"Overview of GDS and GDS-OF","text":"<p>To give a brief introduction to the GPU Direct Storage paradigm, we first need to understand the traditional file transfer pathway between the storage device and a GPU. Traditionally, to transfer a file from the storage device to the GPU, the CPU would create a temporary cache or bounce buffer out of the system memory (RAM). This buffer would hold data transferred from the storage device. Only after the data has been transferred to the bounce buffer or the buffer is full will the data transfer from the bounce buffer to the GPU. In the case of large file transfers to repetitive file transfers, this will result in increased latency. GPU Direct Storage sends the data directly from the storage device to the GPU without the need for the temporarily allocated CPU system memory to coordinate movement with the PCIE bus. For more information, please visit the blog below. -   Introduction to GPU Direct Storage</p> <p>Before diving into the technical details of setting up GDS, we will first cover the two prominent use cases of ultra-fast GDS. The first case is the consumer use case, those developers that will take advantage of the GDS to read data in real time. These cases include: - Running Digital Signal Processing models in real-time - Streaming Video data for live analytics - Streaming data from scientific instruments such as electron microscopes.</p>","tags":["GPUDirect"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#producer-use-cases","title":"Producer Use Cases","text":"<p>The other GDS use case for developers includes producing or writing data to     a source. Examples of these workflows include: - Writing streams from multiple scientific equipment into a single stream. - Archiving raw or transformed data in real-time. - Writing data to a secondary workflow.</p>","tags":["GPUDirect"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#overview-of-igx","title":"Overview of IGX","text":"<p>IGX is the latest generation of Industrial grade edge AI platform. It boasts an integrated 12-core ARM CPU and 2048 Cuda core + 64 Tensor Core integrated Ampere generation GPU. This powerful duo combines many other features, such as NVME support, RJ45 networking, and more, designed to optimize any edge AI workflow. The 2 PCIE slots connected to the ConnectX 7 chip onboard the IGX are critical. This ConnectX 7 chip uniquely facilitates GDS &amp; GDS-OF for the GPU and any PCI-E expansion card. For this reason, we added a PCIE riser card and compatible SSD to the second PCIE Gen5x8 slot. \u00a0  </p> <p></p>","tags":["GPUDirect"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#setting-up-gds","title":"Setting Up GDS","text":"<p>As previously mentioned, we have two open slots onboard the IGX system. The first x16 slot is for our GPU, which will take advantage of all x16 lanes for communication. The second x8 slot, however, can be used for any PCIE expansion card; we chose to use a PCIE riser to add an NVME-compatible drive to take advantage of GDS. It\u2019s important to note that we couldn\u2019t use the existing M.2 NVME boot drive since it\u2019s not connected to the Connect 7 chip onboard the IGX.</p> <p></p>","tags":["GPUDirect"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#hardware-requirements","title":"Hardware Requirements","text":"<p>The first step in this process is to obtain the correct hardware. In addition to the IGX system you have, you will also need a PCIE riser card and a compatible NVME SSD with the PCIE riser you chose (e.g., if you have a PCIE M.2 riser, then you\u2019ll need an M.2 NVME SSD).</p> <p></p>","tags":["GPUDirect"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#software-installation","title":"Software Installation","text":"<p>The next step is to update the software inside your IGX. Note: You can skip this step if you have a new IGX system or one that has been freshly flashed. An explanation of each command will be available at the end of this post. * Install/update GCC and Linux Headers     * <code>sudo apt update</code> * Install MOFED drivers     * <code>wget --no-check-certificate https://content.mellanox.com/ofed/MLNX_OFED-24.01-0.3.3.1/MLNX_OFED_LINUX-24.01-0.3.3.1-ubuntu22.04-aarch64.iso</code>     * <code>mkdir /mnt/iso</code>     * <code>mount -o loop MLNX_OFED_LINUX-24.01-0.3.3.1-ubuntu22.04-x86_64.iso /mnt/iso</code>     * <code>/mnt/iso/mlnxofedinstall --with-nfsrdma --with-nvmf \u2013force</code>     * <code>update-initramfs -u -k $(uname -r)</code>     * <code>sudo reboot</code> * Install cuda toolkit, cuda drivers, and GPU drivers     * <code>sudo apt-get update</code>     * <code>sudo apt-get -y install cuda-toolkit-12-5</code>     * <code>sudo apt-get install -y nvidia-driver-535-open</code>     * <code>sudo apt-get install -y cuda-drivers-535</code></p> <p>After updating the essential software, follow the instructions below to install the Nvidia-GDS application and reboot your machine. * Install the nvidia-gds application     * <code>sudo apt-get install nvidia-gds</code> * Reboot     * <code>sudo reboot</code> * Mount a new directory to the SSD     * Find which SSD is the new one:         *   <code>Lsblk</code>     * Mount a compatible file system (In our case ext4) to the drive         * <code>sudo mount -t ext4 /dev/nvme1n1</code>         * <code>/mnt/nvme/ -o data=ordered</code></p> <p>Once you have completed the software setup process, there are some additional things you can do to ensure GDS is being used correctly:</p> <ul> <li>Modify your cufile.json only to be compatible with GDS file systems<ul> <li>Open <code>/usr/local/cuda/gds/cufile.json</code></li> <li>Change <code>\u201callow_compat_mode\u201d</code> to <code>\u201cfalse\u201d</code></li> </ul> </li> </ul> <p>After this, you can run several checks to ensure that GDS is working: * Run the gdscheck script     * <code>/usr/local/cuda/gds/tools/gdscheck -p</code> * Run the gdsio application to send sample data to the mounted directory     * <code>Sudo /usr/local/cuda/gds/tools/gdsio x 0 -i 1M -s 10M -d 0 -w 1 -I 1 -V -D /mnt/nvme</code></p>","tags":["GPUDirect"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#example-application","title":"Example Application","text":"","tags":["GPUDirect"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#using-nvidia-holoscan-and-rapids","title":"Using Nvidia Holoscan and RAPIDS","text":"<p>Kvikio is a part of the RAPIDS ecosystem of libraries. It gives you access to the GPU Direct Storage API, CuFile. Although you can access CuFile directly, Kvikio is a Python and C++ API that gives you easy access to the underlying CuFile functionality in a straightforward way. \u00a0</p> <p>Holoscan, a powerful sensor processing SDK, plays a crucial role in enabling scientists and engineers to harness accelerated computing for their needs. It provides the essential framework for hosting a scientific pipeline. In the example below, we demonstrate how Holoscan, in conjunction with CuPy and Kvikio, can be used to read electron microscope data and identify electrons in the image.</p> <p>Feel free to run the example application by either providing your own data, or using the data generation script. Once the data is generated you can run the application with the following command:  <code>python3 holoscan_gds.py</code></p>","tags":["GPUDirect"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#conclusion","title":"Conclusion","text":"<p>GPU Direct Storage (GDS) is revolutionizing edge computing by enabling ultra-fast data transfer directly to GPUs. Following the steps outlined in this tutoril, you can set up GDS on your IGX system and leverage its real-time data processing capabilities. Try it out and see the difference it makes in your workflows!</p>","tags":["GPUDirect"]},{"location":"tutorials/gui_for_python_applications/","title":"Adding a GUI to Holoscan Python Applications","text":"<p> Authors: Wendell Hom (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.3.0 Contribution metric: Level 1 - Highly Reliable</p> <p>When developing Holoscan applications, incorporating a graphical user interface (GUI) can enhance usability and allow modification of the application's behavior at runtime.</p> <p>This tutorial demonstrates how GUI controls were integrated into the Florence-2 Python application using PySide6. This addition enables users to dynamically change the vision task performed by the application.</p> <p> </p>","tags":["UI"]},{"location":"tutorials/gui_for_python_applications/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview<ol> <li>Dockerfile</li> <li>Application Code</li> <li>GUI Code</li> </ol> </li> <li>Creating the GUI Widgets and Layout</li> <li>Starting the Holoscan Application Thread</li> <li>Adding a GUI to Your Own Application</li> </ol>","tags":["UI"]},{"location":"tutorials/gui_for_python_applications/#overview","title":"Overview","text":"<p>The Florence-2 application includes the typical components of a Holohub application, with the addition of a GUI component.  The main components are:</p> <ul> <li>Dockerfile: For installing additional dependencies.</li> <li>Application Code: Defines the Holoscan application and its operators.</li> <li>GUI Code: Utilizes PySide6 to add UI controls.</li> </ul>","tags":["UI"]},{"location":"tutorials/gui_for_python_applications/#dockerfile","title":"Dockerfile","text":"<p>The Dockerfile is used in Holohub when the application requires additional dependencies. For GUI functionality, this application's  Dockerfile installs:</p> <ul> <li><code>qt6-base-dev</code> for Qt6 framework</li> <li><code>PySide6</code> for Python bindings for Qt6 (as specified in requirements.txt)</li> </ul>","tags":["UI"]},{"location":"tutorials/gui_for_python_applications/#application-code","title":"Application Code","text":"<p>The Florence-2 application code is organized across several files:</p> <ul> <li><code>florence2_app.py</code>: Main application code.</li> <li><code>florence2_op.py</code>: Florence-2 model inference code.</li> <li><code>florence2_postprocessor_op.py</code>: Post-processing code to send overlays (e.g., bounding boxes, labels, segmentation masks) to Holoviz.</li> <li><code>config.yaml</code>: Default application parameters.</li> </ul> <p>The Florence-2 application can be run independently of the GUI code. E.g., the application can be run with <code>python application/florence-2-vision/florence2_app.py</code> inside the Florence-2 Docker container. This will run the application without the GUI controls.  The only code needed for GUI integration in the application code is the <code>set_parameters()</code> method in the <code>FlorenceApp</code> class. This method updates two fields in the Florence-2 operator:</p> <pre><code>class FlorenceApp(Application):\n    def set_parameters(self, task, prompt):\n        \"\"\"Set parameters for the Florence2Operator.\"\"\"\n        if self.florence_op:\n            self.florence_op.task = task\n            self.florence_op.prompt = prompt\n</code></pre> <p>These updated parameters are passed to the model during the next <code>compute()</code> method execution of the Florence-2 operator.</p>","tags":["UI"]},{"location":"tutorials/gui_for_python_applications/#gui-code","title":"GUI Code","text":"<p>The GUI code resides in qt_app.py. The code in this file defines a class for the main window which calls <code>setupUi()</code> and <code>runHoloscanApp()</code> when the instance is initialized.</p> <pre><code>class Window(QMainWindow):\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.setupUi()  # Setup the UI\n        self.runHoloscanApp()  # Run the Holoscan application\n</code></pre> <p>At a high level, this is all we need to launch a Python Holoscan application with a GUI. The <code>setupUi()</code> method defines the GUI widgets and layout, while <code>runHoloscanApp()</code> runs the Florence-2 application in a separate thread within the process. Details of these methods are explored in the following sections.</p>","tags":["UI"]},{"location":"tutorials/gui_for_python_applications/#creating-the-gui-widgets-and-layout","title":"Creating the GUI Widgets and Layout","text":"<p>The <code>setupUi()</code> method creates the GUI with a few simple widgets using PySide6 APIs.  For those unfamiliar with PySide6, this tutorial provides an introduction.</p> <pre><code>    def setupUi(self):\n        \"\"\"Setup the UI components.\"\"\"\n        self.setWindowTitle(\"Florence-2\")\n        self.resize(400, 150)\n        self.centralWidget = QWidget()\n        self.setCentralWidget(self.centralWidget)\n\n        layout = QVBoxLayout()\n\n        # Create and add dropdown for task selection\n        self.dropdown = QComboBox()\n        self.dropdown.addItems(\n            [\n                \"Object Detection\",\n                \"Caption\",\n                \"Detailed Caption\",\n                \"More Detailed Caption\",\n                \"Dense Region Caption\",\n                \"Region Proposal\",\n                \"Caption to Phrase Grounding\",\n                \"Referring Expression Segmentation\",\n                \"Open Vocabulary Detection\",\n                \"OCR\",\n                \"OCR with Region\",\n            ]\n        )\n        layout.addWidget(QLabel(\"Select an option:\"))\n        layout.addWidget(self.dropdown)\n\n        # Create and add text input for prompt\n        self.text_input = QLineEdit()\n        layout.addWidget(QLabel(\"Enter text:\"))\n        layout.addWidget(self.text_input)\n\n        # Create and add submit button\n        self.submit_button = QPushButton(\"Submit\")\n        self.submit_button.clicked.connect(self.on_submit)\n        layout.addWidget(self.submit_button)\n\n        self.centralWidget.setLayout(layout)\n</code></pre> <p>This code creates the following widgets:</p> <ul> <li>Drop-down Menu: Lists the vision tasks supported by Florence-2.</li> <li>Text input Widget: Allows text input for tasks such as Open Vocabulary Detection.</li> <li>Submit Button: Triggers the <code>on_submit()</code> method when clicked.</li> </ul> <p>When the application is running, the user selects a vision task, enters text (if  needed), and clicks \"Submit\" to change the task performed by the model. The <code>on_submit()</code> method is then invoked, calling the <code>set_parameters()</code> method  in the <code>FlorenceApp</code> class to update the operator's parameters.</p> <pre><code>    def on_submit(self):\n        \"\"\"Handle the submit button click event.\"\"\"\n        selected_option = self.dropdown.currentText()\n        entered_text = self.text_input.text()\n\n        # Set parameters in the Holoscan application\n        global gApp\n        if gApp:\n            gApp.set_parameters(selected_option, entered_text)\n</code></pre>","tags":["UI"]},{"location":"tutorials/gui_for_python_applications/#starting-the-holoscan-application-thread","title":"Starting the Holoscan Application Thread","text":"<p>The <code>runHoloscanApp()</code> method starts the Florence-2 application by creating an instance of <code>FlorenceWorker</code>  and running it in a thread.</p> <pre><code>    def runHoloscanApp(self):\n        \"\"\"Run the Holoscan application in a separate thread.\"\"\"\n        self.thread = QThread()\n        self.worker = FlorenceWorker()\n        self.worker.moveToThread(self.thread)\n        self.thread.started.connect(self.worker.run)\n        self.worker.finished.connect(self.thread.quit)\n        self.worker.finished.connect(self.worker.deleteLater)\n        self.thread.finished.connect(self.thread.deleteLater)\n        self.thread.start()\n</code></pre> <p>When the thread is started, it calls the <code>FlorenceWorker</code> class's <code>run()</code> method which creates and runs the Holoscan application.</p> <pre><code># Worker class to run the Holoscan application in a separate thread\nclass FlorenceWorker(QObject):\n    finished = Signal()  # Signal to indicate the worker has finished\n    progress = Signal(int)  # Signal to indicate progress (if needed)\n\n    def run(self):\n        \"\"\"Run the Holoscan application.\"\"\"\n        config_file = os.path.join(os.path.dirname(__file__), \"config.yaml\")\n        global gApp\n        gApp = app = FlorenceApp()\n        app.config(config_file)\n        app.run()\n</code></pre> <p>This covers the essential steps for creating a GUI to control your Python Holoscan applications.  To try out the application, follow the instructions provided here.</p>","tags":["UI"]},{"location":"tutorials/gui_for_python_applications/#adding-a-gui-to-your-own-application","title":"Adding a GUI to Your Own Application","text":"<p>To integrate a GUI into your Python application using PySide6, follow these steps:</p> <ol> <li>Ensure Qt and PySide6 dependencies are included in your Dockerfile. Verify that Qt and PySide6 package licenses meet your project requirements.</li> <li>Copy the <code>qt_app.py</code> file to your application directory.  Rename and modify the <code>FlorenceWorker</code> class to create an instance of your application. Update the import statement <code>from florence2_app import FlorenceApp</code> as necessary.</li> <li>Customize the <code>setupUi()</code> method to include the controls relevant to your application.</li> <li>Update <code>set_parameters()</code> methoed to reflect the parameters your application needs to update.</li> </ol>","tags":["UI"]},{"location":"tutorials/high_performance_networking/","title":"High Performance Networking with Holoscan","text":"<p> Authors: Alexis Girault (NVIDIA) Supported platforms: x86_64, SBSA, IGX Orin (dGPU) Language: C++ Last modified: May 13, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 3.0.0 Tested Holoscan SDK versions: 3.0.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This tutorial demonstrates how to use the Advanced Network library (referred to as <code>advanced_network</code> in HoloHub) for low latency and high throughput communication through NVIDIA SmartNICs. With a properly tuned system, the Advanced Network library can achieve hundreds of Gbps with latencies in the low microseconds.</p> <p>Note</p> <p>This solution is designed for users who want to create a Holoscan application that will interface with an external system or sensor over Ethernet.</p> <ul> <li>For high performance communication with systems also running Holoscan, refer to the Holoscan distributed application documentation instead.</li> <li>For JESD-compliant sensor without Ethernet support, consider the Holoscan Sensor Bridge for an FPGA-based interface to Holoscan.</li> </ul>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#prerequisites","title":"Prerequisites","text":"<p>Achieving High Performance Networking with Holoscan requires a system with an NVIDIA SmartNIC and a discrete GPU. That is the case of NVIDIA Data Center systems, or edge systems like the NVIDIA IGX platform and the NVIDIA Project DIGITS. <code>x86_64</code> systems equipped with these components are also supported, though the performance will vary greatly depending on the PCIe topology of the system (more on this below).</p> <p>In this tutorial, we will be developing on an NVIDIA IGX Orin platform with IGX SW 1.1 and an NVIDIA RTX 6000 ADA GPU, which is the configuration that is currently actively tested. The concepts should be applicable to other systems based on Ubuntu 22.04 as well. It should also work on other Linux distributions with a glibc version of 2.35 or higher by containerizing the dependencies and applications on top of an Ubuntu 22.04 image, but this is not actively tested at this time.</p> <p>Secure boot conflict</p> <p>If you have secure boot enabled on your system, you might need to disable it as a prerequisite to run some of the configurations below (switching the NIC link layers to Ethernet, updating the MRRS of your NIC ports, updating the BAR1 size of your GPU). Secure boot can be re-enabled after the configurations are completed.</p>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#background","title":"Background","text":"<p>Achieving high performance networking is a complex problem that involves many system components and configurations which we will cover in this tutorial. Two of the core concepts to achieve this are named Kernel Bypass, and GPUDirect.</p>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#kernel-bypass","title":"Kernel Bypass","text":"<p>In this context, Kernel Bypass refers to bypassing the operating system's kernel to directly communicate with the network interface (NIC), greatly reducing the latency and overhead of the Linux network stack. There are multiple technologies that achieve this in different fashions. They're all Ethernet-based, but differ in their implementation and features. The goal of the Advanced Network library in Holoscan Networking is to provide a common higher-level interface to all these backends:</p> <ul> <li>RDMA: Remote Direct Memory Access, using the open-source <code>rdma-core</code> library. It differs from the other Ethernet-based backends with its server/client model and RoCE (RDMA over Ethernet) protocol. Given the extra cost and complexity to setup on both ends, it offers a simpler user interface, orders packets on arrival, and is the only one to offer a high reliability mode.</li> <li>DPDK: the Data Plane Development Kit is an open-source project part of the Linux Foundation with a strong and long-lasting community support. Its RTE Flow capability is generally considered the most flexible solution to split packets ingress and egress data.</li> <li>DOCA GPUNetIO: This NVIDIA proprietary technology differs from the other backends by transmitting and receiving packets from the NIC using a GPU kernel instead of CPU code, which is highly beneficial for CPU-bound applications.</li> <li>NVIDIA Rivermax: NVIDIA's other proprietary kernel bypass technology. For a license fee, it should offer the lowest latency and lowest resource utilization for video streaming (RTP packets).</li> </ul> Work In Progress <p>The Holoscan Advanced Network library integration testing infrastructure is under active development. As such:</p> <ul> <li>The DPDK backend is supported and distributed with the <code>holoscan-networking</code> package, and is the only backend actively tested at this time.</li> <li>The DOCA GPUNetIO backend is supported and distributed with the <code>holoscan-networking</code> package, with testing infrastructure under development.</li> <li>The NVIDIA Rivermax backend is supported for Rx only when building from source, but not yet distributed nor actively tested. Tx support is under development.</li> <li>The RDMA backend is under active development and should be available soon.</li> </ul> <p>Which backend is best for your use case will depend on multiple factors, such as packet size, batch size, data type, and more. The goal of the Advanced Network library is to abstract the interface to these backends, allowing developers to focus on the application logic and experiment with different configurations to identify the best technology for their use case.</p>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#gpudirect","title":"GPUDirect","text":"<p><code>GPUDirect</code> allows the NIC to read and write data from/to a GPU without requiring to copy the data the system memory, decreasing CPU overheads and significantly reducing latency. An implementation of <code>GPUDirect</code> is supported by all the kernel bypass backends listed above.</p> <p>Warning</p> <p><code>GPUDirect</code> is only supported on Workstation/Quadro/RTX GPUs and Data Center GPUs. It is not supported on GeForce cards.</p> How does that relate to peermem or dma-buf? <p>There are two interfaces to enable <code>GPUDirect</code>:</p> <ul> <li>The <code>nvidia-peermem</code> kernel module, distributed with the NVIDIA DKMS GPU drivers.<ul> <li>Supported on Ubuntu kernels 5.4+, deprecated starting with kernel 6.8.</li> <li>Supported on NVIDIA optimized Linux kernels, including IGX OS and DGX OS.</li> <li>Supported by all MOFED drivers (requires rebuilding nvidia-dkms drivers afterwards).</li> </ul> </li> <li><code>DMA Buf</code>, supported on Linux kernels 5.12+ with NVIDIA open-source drivers 515+ and CUDA toolkit 11.7+.</li> </ul>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#1-installing-holoscan-networking","title":"1. Installing Holoscan Networking","text":"<p>We'll start with installing the <code>holoscan-networking</code> package, as it provides some utilities to help tune the system, and requires some dependencies which will help us with the system setup.</p> <p>First, add the DOCA apt repository which holds some of its dependencies:</p> IGX OS 1.1SBSA (Ubuntu 22.04)x86_64 (Ubuntu 22.04) <pre><code>export DOCA_URL=\"https://linux.mellanox.com/public/repo/doca/2.10.0/ubuntu22.04/arm64-sbsa/\"\nwget -qO- https://linux.mellanox.com/public/repo/doca/GPG-KEY-Mellanox.pub | gpg --dearmor - | sudo tee /etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub &gt; /dev/null\necho \"deb [signed-by=/etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub] $DOCA_URL ./\"  | sudo tee /etc/apt/sources.list.d/doca.list &gt; /dev/null\n\nsudo apt update\n</code></pre> <pre><code>export DOCA_URL=\"https://linux.mellanox.com/public/repo/doca/2.10.0/ubuntu22.04/arm64-sbsa/\"\nwget -qO- https://linux.mellanox.com/public/repo/doca/GPG-KEY-Mellanox.pub | gpg --dearmor - | sudo tee /etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub &gt; /dev/null\necho \"deb [signed-by=/etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub] $DOCA_URL ./\"  | sudo tee /etc/apt/sources.list.d/doca.list &gt; /dev/null\n\n# Also need the CUDA repository for holoscan: https://developer.nvidia.com/cuda-downloads?target_os=Linux\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\n\nsudo apt update\n</code></pre> <pre><code>export DOCA_URL=\"https://linux.mellanox.com/public/repo/doca/2.10.0/ubuntu22.04/x86_64/\"\nwget -qO- https://linux.mellanox.com/public/repo/doca/GPG-KEY-Mellanox.pub | gpg --dearmor - | sudo tee /etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub &gt; /dev/null\necho \"deb [signed-by=/etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub] $DOCA_URL ./\"  | sudo tee /etc/apt/sources.list.d/doca.list &gt; /dev/null\n\n# Also need the CUDA repository for holoscan: https://developer.nvidia.com/cuda-downloads?target_os=Linux\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\n\nsudo apt update\n</code></pre> <p>You can then install <code>holoscan-networking</code>:</p> Debian installationFrom source <pre><code>sudo apt install -y holoscan-networking\n</code></pre> <p>You can build the Holoscan Networking libraries and sample applications from source on HoloHub:</p> <pre><code>git clone git@github.com:nvidia-holoscan/holohub.git\ncd holohub\n./dev_container build_and_install holoscan-networking   # Installed in ./install\n</code></pre> <p>If you'd like to generate the debian package from source and install it to ensure all dependencies are then present on your system, you can run:</p> <pre><code>./dev_container build_and_package holoscan-networking\nsudo apt-get install ./holoscan-networking_*.deb        # Installed in /opt/nvidia/holoscan\n</code></pre> <p>Refer to the HoloHub README for more information.</p>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#2-required-system-setup","title":"2. Required System Setup","text":"","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#21-check-your-nic-drivers","title":"2.1 Check your NIC drivers","text":"<p>Ensure your NIC drivers are loaded:</p> <pre><code>lsmod | grep ib_core\n</code></pre> See an example output <p>This would be an expected output, where <code>ib_core</code> is listed on the left.</p> <pre><code>ib_core               442368  8 rdma_cm,ib_ipoib,iw_cm,ib_umad,rdma_ucm,ib_uverbs,mlx5_ib,ib_cm\nmlx_compat             20480  11 rdma_cm,ib_ipoib,mlxdevm,iw_cm,ib_umad,ib_core,rdma_ucm,ib_uverbs,mlx5_ib,ib_cm,mlx5_core\n</code></pre> <p>If this is empty, install the latest OFED drivers from DOCA (the DOCA APT repository should already be configured from the Holoscan Networking installation above), and reboot your system:</p> <pre><code>sudo apt update\nsudo apt install doca-ofed\nsudo reboot\n</code></pre> <p>Note</p> <p>If this is not empty, you can still install the newest OFED drivers from <code>doca-ofed</code> above. If you choose to keep your current drivers, install the following utilities for convenience later on. They include tools like <code>ibstat</code>, <code>ibv_devinfo</code>, <code>ibdev2netdev</code>, <code>mlxconfig</code>:</p> <pre><code>sudo apt update\nsudo apt install infiniband-diags ibverbs-utils mlnx-ofed-kernel-utils mft\n</code></pre> <p>Also upgrade the user space libraries to make sure your tools have all the symbols they need:</p> <pre><code>sudo apt install libibverbs1 librdmacm1 rdma-core\n</code></pre> <p>Running <code>ibstat</code> or <code>ibv_devinfo</code> will confirm your NIC interfaces are recognized by your drivers.</p>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#22-switch-your-nic-link-layers-to-ethernet","title":"2.2 Switch your NIC Link Layers to Ethernet","text":"<p>NVIDIA SmartNICs can function in two separate modes (called link layer):</p> <ul> <li>Ethernet (ETH)</li> <li>Infiniband (IB)</li> </ul> <p>To identify the current mode, run <code>ibstat</code> or <code>ibv_devinfo</code> and look for the <code>Link Layer</code> value.</p> <pre><code>ibv_devinfo\n</code></pre> Couldn't load driver 'libmlx5-rdmav34.so' <p>If you see an error like this, you might have different versions for your OFED tools and libraries. Attempt after upgrading your user space libraries to match the version of your OFED tools like so:</p> <pre><code>sudo apt update\nsudo apt install libibverbs1 librdmacm1 rdma-core\n</code></pre> See an example output <p>In the example below, the <code>mlx5_0</code> interface is in Ethernet mode, while the <code>mlx5_1</code> interface is in Infiniband mode. Do not pay attention to the <code>transport</code> value which is always <code>InfiniBand</code>.</p> <pre><code>hca_id: mlx5_0\n        transport:                      InfiniBand (0)\n        fw_ver:                         28.38.1002\n        node_guid:                      48b0:2d03:00f4:07fb\n        sys_image_guid:                 48b0:2d03:00f4:07fb\n        vendor_id:                      0x02c9\n        vendor_part_id:                 4129\n        hw_ver:                         0x0\n        board_id:                       NVD0000000033\n        phys_port_cnt:                  1\n                port:   1\n                        state:                  PORT_ACTIVE (4)\n                        max_mtu:                4096 (5)\n                        active_mtu:             4096 (5)\n                        sm_lid:                 0\n                        port_lid:               0\n                        port_lmc:               0x00\n                        link_layer:             Ethernet\n\nhca_id: mlx5_1\n        transport:                      InfiniBand (0)\n        fw_ver:                         28.38.1002\n        node_guid:                      48b0:2d03:00f4:07fc\n        sys_image_guid:                 48b0:2d03:00f4:07fb\n        vendor_id:                      0x02c9\n        vendor_part_id:                 4129\n        hw_ver:                         0x0\n        board_id:                       NVD0000000033\n        phys_port_cnt:                  1\n                port:   1\n                        state:                  PORT_ACTIVE (4)\n                        max_mtu:                4096 (5)\n                        active_mtu:             4096 (5)\n                        sm_lid:                 0\n                        port_lid:               0\n                        port_lmc:               0x00\n                        link_layer:             InfiniBand\n</code></pre> <p>For Holoscan Networking, we want the NIC to use the ETH link layer. To switch the link layer mode, there are two possible options:</p> <ol> <li>On IGX Orin developer kits, you can switch that setting through the BIOS: see IGX Orin documentation.</li> <li> <p>On any system with a NVIDIA NIC (including the IGX Orin developer kits), you can run the commands below from a terminal:</p> <ol> <li> <p>Identify the PCI address of your NVIDIA NIC</p> ibdev2netdevlspci <pre><code>nic_pci=$(sudo ibdev2netdev -v | awk '{print $1}' | head -n1)\n</code></pre> <pre><code># `0200` is the PCI-SIG class code for Ethernet controllers\n# `0207` is the PCI-SIG class code for Infiniband controllers\n# `15b3` is the Vendor ID for Mellanox\nnic_pci=$(lspci -n | awk '($2 == \"0200:\" || $2 == \"0207:\") &amp;&amp; $3 ~ /^15b3:/ {print $1; exit}')\n</code></pre> </li> <li> <p>Set both link layers to Ethernet. <code>LINK_TYPE_P1</code> and <code>LINK_TYPE_P2</code> are for <code>mlx5_0</code> and <code>mlx5_1</code> respectively. You can choose to only set one of them. <code>ETH</code> or <code>2</code> is Ethernet mode, and <code>IB</code> or <code>1</code> is for InfiniBand.</p> <pre><code>sudo mlxconfig -d $nic_pci set LINK_TYPE_P1=ETH LINK_TYPE_P2=ETH\n</code></pre> <p>Apply with <code>y</code>.</p> See an example output <pre><code>Device #1:\n----------\n\nDevice type:    ConnectX7\nName:           P3740-B0-QSFP_Ax\nDescription:    NVIDIA Prometheus P3740 ConnectX-7 VPI PCIe Switch Motherboard; 400Gb/s; dual-port QSFP; PCIe switch5.0 X8 SLOT0 ;X16 SLOT2; secure boot;\nDevice:         0005:03:00.0\n\nConfigurations:                                      Next Boot       New\n        LINK_TYPE_P1                                ETH(2)          ETH(2)\n        LINK_TYPE_P2                                IB(1)           ETH(2)\n\nApply new Configuration? (y/n) [n] :\ny\n\nApplying... Done!\n-I- Please reboot machine to load new configurations.\n</code></pre> <ul> <li><code>Next Boot</code> is the current value that was expected to be used at the next reboot.</li> <li><code>New</code> is the value you're about to set to override <code>Next Boot</code>.</li> </ul> ERROR: write counter to semaphore: Operation not permitted <p>Disable secure boot on your system ahead of changing the link type of your NIC ports. It can be re-enabled afterwards.</p> </li> <li> <p>Reboot your system.</p> <pre><code>sudo reboot\n</code></pre> </li> </ol> </li> </ol>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#23-configure-the-ip-addresses-of-the-nic-ports","title":"2.3 Configure the IP addresses of the NIC ports","text":"<p>First, we want to identify the logical names of your NIC interfaces. Connecting an SFP cable in just one of the ports of the NIC will help you identify which port is which. Run the following command once the cable is in place:</p> <pre><code>ibdev2netdev\n</code></pre> See an example output <p>In the example below, only <code>mlx5_1</code> has a cable connected (<code>Up</code>), and its logical ethernet name is <code>eth1</code>:</p> <pre><code>$ ibdev2netdev\nmlx5_0 port 1 ==&gt; eth0 (Down)\nmlx5_1 port 1 ==&gt; eth1 (Up)\n</code></pre> ibdev2netdev does not show the NIC <p>If you have a cable connected but it does not show Up/Down in the output of <code>ibdev2netdev</code>, you can try to parse the output of <code>dmesg</code> instead. The example below shows that <code>0005:03:00.1</code> is plugged, and that it is associated with <code>eth1</code>:</p> <pre><code>$ sudo dmesg | grep -w mlx5_core\n...\n[   11.512808] mlx5_core 0005:03:00.0 eth0: Link down\n[   11.640670] mlx5_core 0005:03:00.1 eth1: Link down\n...\n[ 3712.267103] mlx5_core 0005:03:00.1: Port module event: module 1, Cable plugged\n</code></pre> <p>The next step is to set a static IP on the interface you'd like to use so you can refer to it in your Holoscan applications. First, check if you already have any addresses configured using the ethernet interface names identified above (in our case, <code>eth0</code> and <code>eth1</code>):</p> <pre><code>ip -f inet addr show eth0\nip -f inet addr show eth1\n</code></pre> <p>If nothing appears, or you'd like to change the address, you can set an IP address through the Network Manager user interface, CLI (<code>nmcli</code>), or other IP configuration tools. In the example below, we configure the <code>eth0</code> interface with an address of <code>1.1.1.1/24</code>, and the <code>eth1</code> interface with an address of <code>2.2.2.2/24</code>.</p> One-timePersistent <pre><code>sudo ip addr add 1.1.1.1/24 dev eth0\nsudo ip addr add 2.2.2.2/24 dev eth1\n</code></pre> <p>Set these variables to your desired values:</p> <pre><code>if_name=eth0\nif_static_ip=1.1.1.1/24\n</code></pre> NetworkManagersystemd-networkd <p>Update the IP with <code>nmcli</code>:</p> <pre><code>sudo nmcli connection modify $if_name ipv4.addresses $if_static_ip\nsudo nmcli connection up $if_name\n</code></pre> <p>Create a network config file with the static IP:</p> <pre><code>cat &lt;&lt; EOF | sudo tee /etc/systemd/network/20-$if_name.network\n[Match]\nMACAddress=$(cat /sys/class/net/$if_name/address)\n\n[Network]\nAddress=$if_static_ip\nEOF\n</code></pre> <p>Apply now:</p> <pre><code>sudo systemctl restart systemd-networkd\n</code></pre> <p>Note</p> <p>If you are connecting the NIC to another NIC with an interconnect, do the same on the other system with an IP address on the same network segment. For example, to communicate with <code>1.1.1.1/24</code> above (<code>/24</code> -&gt; <code>255.255.255.0</code> submask), setup your other system with an IP between <code>1.1.1.2</code> and <code>1.1.1.254</code>, and the same <code>/24</code> submask.</p>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#24-enable-gpudirect","title":"2.4 Enable GPUDirect","text":"<p>Assuming you already have NVIDIA drivers installed, check if the <code>nvidia_peermem</code> kernel module is loaded:</p> tune_system.py Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check topo\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check topo\n</code></pre> See an example output <pre><code>2025-03-12 14:15:07 - INFO - GPU 0: NVIDIA RTX A6000 has GPUDirect support.\n2025-03-12 14:15:27 - INFO - nvidia-peermem module is loaded.\n</code></pre> <pre><code>lsmod | grep nvidia_peermem\n</code></pre> <p>If it's not loaded, run the following command, then check again:</p> One-timePersistent <pre><code>sudo modprobe nvidia_peermem\n</code></pre> <pre><code>sudo echo \"nvidia-peermem\" &gt;&gt; /etc/modules\nsudo systemctl restart systemd-modules-load.service\n</code></pre> Error loading the <code>nvidia-peermem</code> kernel module <p>If you run into an error loading the <code>nvidia-peermem</code> kernel module, follow these steps:</p> <ol> <li>Install the <code>doca-ofed</code> package to get the latest drivers for your NIC as documented above.</li> <li>Restart your system.</li> <li>Rebuild your NVIDIA drivers with DKMS like so:</li> </ol> <pre><code>peermem_ko=$(find /lib/modules/$(uname -r) -name \"*peermem*.ko\")\nnv_dkms=$(dpkg -S \"$peermem_ko\" | cut -d: -f1)\nsudo dpkg-reconfigure $nv_dkms\nsudo modprobe nvidia_peermem\n</code></pre> Why peermem and not dma buf? <p><code>peermem</code> is currently the only GPUDirect interface supported by all our networking backends. This section will therefore provide instructions for <code>peermem</code> and not <code>dma buf</code>.</p>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#3-optimal-system-configurations","title":"3. Optimal system configurations","text":"<p>Advanced</p> <p>The section below is for advanced users looking to extract more performance out of their system. You can choose to skip this section and return to it later if performance if your application is not satisfactory.</p> <p>While the configurations above are the minimum requirements to get a NIC and a NVIDIA GPU to communicate while bypassing the OS kernel stack, performance can be further improved in most scenarios by tuning the system as described below.</p> <p>Before diving in each of the setups below, we provide a utility script as part of the <code>holoscan-networking</code> package which provides an overview of the configurations that potentially need to be tuned on your system.</p> Work In Progress <p>This utility script is under active development and will be updated in future releases with additional checks, more actionable recommendations, and automated tuning.</p> Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check all\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check all\n</code></pre> See an example output <p>Our tuned-up IGX system with A6000 can optimize most settings:</p> <pre><code>2025-03-12 14:16:06 - INFO - CPU 0: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 1: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 2: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 3: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 4: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 5: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 6: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 7: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 8: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 9: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 10: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 11: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - cx7_0/0005:03:00.0: MRRS is correctly set to 4096.\n2025-03-12 14:16:06 - INFO - cx7_1/0005:03:00.1: MRRS is correctly set to 4096.\n2025-03-12 14:16:06 - WARNING - cx7_0/0005:03:00.0: PCIe Max Payload Size is not set to 256 bytes. Found: 128 bytes.\n2025-03-12 14:16:06 - WARNING - cx7_1/0005:03:00.1: PCIe Max Payload Size is not set to 256 bytes. Found: 128 bytes.\n2025-03-12 14:16:06 - INFO - HugePages_Total: 3\n2025-03-12 14:16:06 - INFO - HugePage Size: 1024.00 MB\n2025-03-12 14:16:06 - INFO - Total Allocated HugePage Memory: 3072.00 MB\n2025-03-12 14:16:06 - INFO - Hugepages are sufficiently allocated with at least 500 MB.\n2025-03-12 14:16:06 - INFO - GPU 0: SM Clock is correctly set to 1920 MHz (within 500 of the 2100 MHz theoretical Max).\n2025-03-12 14:16:06 - INFO - GPU 0: Memory Clock is correctly set to 8000 MHz.\n2025-03-12 14:16:06 - INFO - GPU 00000005:09:00.0: BAR1 size is 8192 MiB.\n2025-03-12 14:16:06 - INFO - GPU GPU0 has at least one PIX/PXB connection to a NIC\n2025-03-12 14:16:06 - INFO - isolcpus found in kernel boot line\n2025-03-12 14:16:06 - INFO - rcu_nocbs found in kernel boot line\n2025-03-12 14:16:06 - INFO - irqaffinity found in kernel boot line\n2025-03-12 14:16:06 - INFO - Interface cx7_0 has an acceptable MTU of 9000 bytes.\n2025-03-12 14:16:06 - INFO - Interface cx7_1 has an acceptable MTU of 9000 bytes.\n2025-03-12 14:16:06 - INFO - GPU 0: NVIDIA RTX A6000 has GPUDirect support.\n2025-03-12 14:16:06 - INFO - nvidia-peermem module is loaded.\n</code></pre> <p>Based on the results, you can figure out which of the sections below are appropriate to update configurations on your system.</p>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#31-ensure-ideal-pcie-topology","title":"3.1 Ensure ideal PCIe topology","text":"<p>Kernel bypass and GPUDirect rely on PCIe to communicate between the GPU and the NIC at high speeds. As-such, the topology of the PCIe tree on a system is critical to ensure optimal performance.</p> <p>Run the following command to check the GPUDirect communication matrix. You are looking for a <code>PXB</code> or <code>PIX</code> connection between the GPU and the NIC interfaces to get the best performance.</p> tune_system.pynvidia-smi Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check topo\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check topo\n</code></pre> See an example output <p>On IGX developer kits, the board's internal switch is designed to connect the GPU to the NIC interfaces with a <code>PXB</code> connection, offering great performance.</p> <pre><code>2025-03-06 12:07:45 - INFO - GPU GPU0 has at least one PIX/PXB connection to a NIC\n</code></pre> <pre><code>nvidia-smi topo -mp\n</code></pre> See an example output <p>On IGX developer kits, the board's internal switch is designed to connect the GPU to the NIC interfaces with a <code>PXB</code> connection, offering great performance. <pre><code>        GPU0    NIC0    NIC1    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PXB     PXB     0-11    0               N/A\nNIC0    PXB      X      PIX\nNIC1    PXB     PIX      X\n\nLegend:\n\nX    = Self\nSYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\nNODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\nPHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\nPXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\nPIX  = Connection traversing at most a single PCIe bridge\n\nNIC Legend:\n\nNIC0: mlx5_0\nNIC1: mlx5_1\n</code></pre></p> <p>If your connection is not optimal, you might be able to improve it by moving your NIC and/or GPU on a different PCIe port, so that they can share a branch and do not require going back to the Host Bridge (the CPU) to communicate. Refer to your system manufacturer for documentation, or run the following command to inspect the topology of your system:</p> <pre><code>lspci -tv\n</code></pre> See an example output <p>Here is the PCIe tree of an IGX system. Note how the ConnectX-7 and RTX A6000 are connected to the same branch. <pre><code>-+-[0007:00]---00.0-[01-ff]----00.0  Marvell Technology Group Ltd. 88SE9235 PCIe 2.0 x2 4-port SATA 6 Gb/s Controller\n+-[0005:00]---00.0-[01-ff]----00.0-[02-09]--+-00.0-[03]--+-00.0  Mellanox Technologies MT2910 Family [ConnectX-7]\n|                                           |            \\-00.1  Mellanox Technologies MT2910 Family [ConnectX-7]\n|                                           +-01.0-[04-06]----00.0-[05-06]----08.0-[06]--\n|                                           \\-02.0-[07-09]----00.0-[08-09]----00.0-[09]--+-00.0  NVIDIA Corporation GA102GL [RTX A6000]\n|                                                                                        \\-00.1  NVIDIA Corporation GA102 High Definition Audio Controller\n+-[0004:00]---00.0-[01-ff]----00.0  Sandisk Corp WD PC SN810 / Black SN850 NVMe SSD\n+-[0001:00]---00.0-[01-ff]----00.0-[02-fc]--+-01.0-[03-34]----00.0  Realtek Semiconductor Co., Ltd. RTL8111/8168/8411 PCI Express Gigabit Ethernet Controller\n|                                           +-02.0-[35-66]----00.0  Realtek Semiconductor Co., Ltd. RTL8111/8168/8411 PCI Express Gigabit Ethernet Controller\n|                                           +-03.0-[67-98]----00.0  Device 1c00:3450\n|                                           +-04.0-[99-ca]----00.0-[9a]--+-00.0  ASPEED Technology, Inc. ASPEED Graphics Family\n|                                           |                            \\-02.0  ASPEED Technology, Inc. Device 2603\n|                                           \\-05.0-[cb-fc]----00.0  Realtek Semiconductor Co., Ltd. RTL8822CE 802.11ac PCIe Wireless Network Adapter\n\\-[0000:00]-\n</code></pre></p> <p>x86_64 compatibility</p> <p>Most x86_64 systems are not designed for this topology as they lack a discrete PCIe switch. In that case, the best connection they can achieve is <code>NODE</code>.</p>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#32-check-the-nics-pcie-configuration","title":"3.2 Check the NIC's PCIe configuration","text":"<p>Understanding PCIe Configuration for Maximum Performance - May 27, 2022</p> <p>PCIe is used in any system for communication between different modules [including the NIC and the GPU]. This means that in order to process network traffic, the different devices communicating via the PCIe should be well configured. When connecting the network adapter to the PCIe, it auto-negotiates for the maximum capabilities supported between the network adapter and the CPU.</p> <p>The instructions below are meant to understand if your system is able to extract the maximum capabilities of your NIC, but they're not configurable. The two values that we are looking at here are the Max Payload Size (MPS - the maximum size of a PCIe packet) and the Speed (or PCIe generation).</p>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#max-payload-size-mps","title":"Max Payload Size (MPS)","text":"tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check mps\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check mps\n</code></pre> See an example output <p>The PCIe configuration on the IGX Orin developer kit is not able to leverage the max payload size of the NIC:</p> <pre><code>2025-03-10 16:15:54 - WARNING - cx7_0/0005:03:00.0: PCIe Max Payload Size is not set to 256 bytes. Found: 128 bytes.\n2025-03-10 16:15:54 - WARNING - cx7_1/0005:03:00.1: PCIe Max Payload Size is not set to 256 bytes. Found: 128 bytes.\n</code></pre> <p>Identify the PCIe address of your NVIDIA NIC:</p> ibdev2netdevlspci <pre><code>nic_pci=$(sudo ibdev2netdev -v | awk '{print $1}' | head -n1)\n</code></pre> <pre><code># `0200` is the PCI-SIG class code for NICs\n# `15b3` is the Vendor ID for Mellanox\nnic_pci=$(lspci -n | awk '$2 == \"0200:\" &amp;&amp; $3 ~ /^15b3:/ {print $1}' | head -n1)\n</code></pre> <p>Check current and max MPS:</p> <pre><code>sudo lspci -vv -s $nic_pci | awk '/DevCap/{s=1} /DevCtl/{s=0} /MaxPayload /{match($0, /MaxPayload [0-9]+/, m); if(s){print \"Max \" m[0]} else{print \"Current \" m[0]}}'\n</code></pre> See an example output <p>The PCIe configuration on the IGX Orin developer kit is not able to leverage the max payload size of the NIC:</p> <pre><code>Max MaxPayload 512\nCurrent MaxPayload 128\n</code></pre> <p>Note</p> <p>While your NIC might be capable of more, 256 bytes is generally the largest supported by any switch/CPU at this time.</p>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#pcie-speedgeneration","title":"PCIe Speed/Generation","text":"<p>Identify the PCIe address of your NVIDIA NIC:</p> ibdev2netdevlspci <pre><code>nic_pci=$(sudo ibdev2netdev -v | awk '{print $1}' | head -n1)\n</code></pre> <pre><code># `0200` is the PCI-SIG class code for NICs\n# `15b3` is the Vendor ID for Mellanox\nnic_pci=$(lspci -n | awk '$2 == \"0200:\" &amp;&amp; $3 ~ /^15b3:/ {print $1}' | head -n1)\n</code></pre> <p>Check current and max Speeds:</p> <pre><code>sudo lspci -vv -s $nic_pci | awk '/LnkCap/{s=1} /LnkSta/{s=0} /Speed /{match($0, /Speed [0-9]+GT\\/s/, m); if(s){print \"Max \" m[0]} else{print \"Current \" m[0]}}'\n</code></pre> See an example output <p>On IGX, the switch is able to maximize the NIC speed, both being PCIe 5.0:</p> <pre><code>Max Speed 32GT/s\nCurrent Speed 32GT/s\n</code></pre>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#33-maximize-the-nics-max-read-request-size-mrrs","title":"3.3 Maximize the NIC's Max Read Request Size (MRRS)","text":"<p>Understanding PCIe Configuration for Maximum Performance - May 27, 2022</p> <p>PCIe Max Read Request determines the maximal PCIe read request allowed. A PCIe device usually keeps track of the number of pending read requests due to having to prepare buffers for an incoming response. The size of the PCIe max read request may affect the number of pending requests (when using data fetch larger than the PCIe MTU).</p> <p>Unlike the PCIe properties queried in the previous section, the MRRS is configurable. We recommend maxing it to 4096 bytes. Run the following to check your current settings:</p> tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check mrrs\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check mrrs\n</code></pre> <p>Identify the PCIe address of your NVIDIA NIC:</p> ibdev2netdevlspci <pre><code>nic_pci=$(sudo ibdev2netdev -v | awk '{print $1}' | head -n1)\n</code></pre> <pre><code># `0200` is the PCI-SIG class code for NICs\n# `15b3` is the Vendor ID for Mellanox\nnic_pci=$(lspci -n | awk '$2 == \"0200:\" &amp;&amp; $3 ~ /^15b3:/ {print $1}' | head -n1)\n</code></pre> <p>Check current MRRS:</p> <pre><code>sudo lspci -vv -s $nic_pci | grep DevCtl: -A2 | grep -oE \"MaxReadReq [0-9]+\"\n</code></pre> <p>Update MRRS:</p> Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --set mrrs\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --set mrrs\n</code></pre> <p>Note</p> <p>This value is reset on reboot and needs to be set every time the system boots</p> ERROR: pcilib: sysfs_write: write failed: Operation not permitted <p>Disable secure boot on your system ahead of changing the MRRS of your NIC ports. It can be re-enabled afterwards.</p>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#34-enable-huge-pages","title":"3.4 Enable Huge pages","text":"<p>Huge pages are a memory management feature that allows the OS to allocate large blocks of memory (typically 2MB or 1GB) instead of the default 4KB pages. This reduces the number of page table entries and the amount of memory used for translation, improving cache performance and reducing TLB (Translation Lookaside Buffer) misses, which leads to lower latencies.</p> <p>While it is naturally beneficial for CPU packets, it is also needed when routing data packets to the GPU in order to handle metadata (mbufs) on the CPU.</p> hugeadmvanilla <p>We recommend installing the <code>libhugetlbfs-bin</code> package for the <code>hugeadm</code> utility:</p> <pre><code>sudo apt update\nsudo apt install -y libhugetlbfs-bin\n</code></pre> <p>Then, check your huge page pools:</p> <pre><code>hugeadm --pool-list\n</code></pre> See an example output <p>The example below shows that this system supports huge pages of 64K, 2M (default), 32M, and 1G, but that none of them are currently allocated.</p> <pre><code>      Size  Minimum  Current  Maximum  Default\n     65536        0        0        0\n   2097152        0        0        0        *\n  33554432        0        0        0\n1073741824        0        0        0\n</code></pre> <p>And your huge page mount points:</p> <pre><code>hugeadm --list-all-mounts\n</code></pre> See an example output <p>The default huge pages are mounted on <code>/dev/hugepages</code> with a page size of 2M.</p> <pre><code>Mount Point          Options\n/dev/hugepages       rw,relatime,pagesize=2M\n</code></pre> <p>First, check your huge page pools:</p> <pre><code>ls -1 /sys/kernel/mm/hugepages/\ngrep Huge /proc/meminfo\n</code></pre> See an example output <p>The example below shows that this system supports huge pages of 64K, 2M (default), 32M, and 1G, but that none of them are currently allocated.</p> <pre><code>hugepages-1048576kB\nhugepages-2048kB\nhugepages-32768kB\nhugepages-64kB\n</code></pre> <pre><code>HugePages_Total:       0\nHugePages_Free:        0\nHugePages_Rsvd:        0\nHugePages_Surp:        0\nHugepagesize:       2048 kB\nHugetlb:               0 kB\n</code></pre> <p>And your huge page mount points:</p> <pre><code>mount | grep huge\n</code></pre> See an example output <p>The default huge pages are mounted on <code>/dev/hugepages</code> with a page size of 2M.</p> <pre><code>hugetlbfs on /dev/hugepages type hugetlbfs (rw,relatime,pagesize=2M)\n</code></pre> <p>As a rule of thumb, we recommend to start with 3 to 4 GB of total huge pages, with an individual page size of 500 MB to 1 GB (per system availability).</p> <p>There are two ways to allocate huge pages:</p> <ul> <li>in the kernel bootline (recommended to ensure contiguous memory allocation) or</li> <li>dynamically at runtime (risk of fragmentation for large page sizes)</li> </ul> <p>The example below allocates 3 huge pages of 1GB each.</p> Kernel bootlineRuntime <p>Add the flags below to the <code>GRUB_CMDLINE_LINUX</code> variable in <code>/etc/default/grub</code>:</p> <pre><code>default_hugepagesz=1G hugepagesz=1G hugepages=3\n</code></pre> Show explanation <ul> <li><code>default_hugepagesz</code>: the default huge page size to use, making them available from the default mount point, <code>/dev/hugepages</code>.</li> <li><code>hugepagesz</code>: the size of the huge pages to allocate.</li> <li><code>hugepages</code>: the number of huge pages to allocate.</li> </ul> <p>Then rebuild your GRUB configuration and reboot:</p> <pre><code>sudo update-grub\nsudo reboot\n</code></pre> <p>Allocate the 3x 1GB huge pages:</p> hugeadmvanilla <pre><code>sudo hugeadm --pool-pages-min 1073741824:3\n</code></pre> <pre><code>echo 3 | sudo tee /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages\n</code></pre> <p>Create a mount point to access the 1GB huge pages pool since that is not the default size on that system. We will name it <code>/mnt/huge</code> here.</p> One-timePersistent <pre><code>sudo mkdir -p /mnt/huge\nsudo mount -t hugetlbfs -o pagesize=1G none /mnt/huge\n</code></pre> <pre><code>echo \"nodev /mnt/huge hugetlbfs pagesize=1G 0 0\" | sudo tee -a /etc/fstab\nsudo mount /mnt/huge\n</code></pre> <p>Note</p> <p>If you work with containers, remember to mount this directory in your container as well with <code>-v /mnt/huge:/mnt/huge</code>.</p> <p>Rerunning the initial commands should now list 3 hugepages of 1GB each. 1GB will be the default huge page size if updated in the kernel bootline only.</p>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#35-isolate-cpu-cores","title":"3.5 Isolate CPU cores","text":"<p>Note</p> <p>This optimization is less impactful when using the <code>gpunetio</code> backend since the GPU polls the NIC.</p> <p>The CPU interacting with the NIC to route packets is sensitive to perturbations, especially with smaller packet/batch sizes requiring more frequent work. Isolating a CPU in Linux prevents unwanted user or kernel threads from running on it, reducing context switching and latency spikes from noisy neighbors.</p> <p>We recommend isolating the CPU cores you will select to interact with the NIC (defined in the <code>advanced_network</code> configuration described later in this tutorial). This is done by setting additional flags on the kernel bootline.</p> <p>You can first check if any of the recommended flags were already set on the last boot:</p> tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check cmdline\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check cmdline\n</code></pre> <pre><code>cat /proc/cmdline | grep -e isolcpus -e irqaffinity -e nohz_full -e rcu_nocbs -e rcu_nocb_poll\n</code></pre> <p>Decide which cores to isolate based on your configuration. We recommend one core per queue as a rule of thumb. First, identify your core IDs:</p> <pre><code>cat /proc/cpuinfo | grep processor\n</code></pre> See an example output <p>This system has 12 cores, numbered 0 to 11: <pre><code>processor       # 0\nprocessor       # 1\nprocessor       # 2\nprocessor       # 3\nprocessor       # 4\nprocessor       # 5\nprocessor       # 6\nprocessor       # 7\nprocessor       # 8\nprocessor       # 9\nprocessor       # 10\nprocessor       # 11\n</code></pre></p> <p>As an example, the line below will isolate cores 9, 10 and 11, leaving cores 0-8 free for other tasks and hardware interrupts:</p> <pre><code>isolcpus=9-11 irqaffinity=0-8 nohz_full=9-11 rcu_nocbs=9-11 rcu_nocb_poll\n</code></pre> Show explanation Parameter Description <code>isolcpus</code> Isolates specific CPU cores from the Linux scheduler, preventing regular system tasks from running on them. This ensures dedicated cores are available exclusively for your networking tasks, reducing context switches and interruptions that can cause latency spikes. <code>irqaffinity</code> Controls which CPU cores can handle hardware interrupts. By directing network interrupts away from your isolated cores, you prevent networking tasks from being interrupted by hardware events, maintaining consistent processing time. <code>nohz_full</code> Disables regular kernel timer ticks on specified cores when they're running user space applications. This reduces overhead and prevents periodic interruptions, allowing your networking code to run with fewer disturbances. <code>rcu_nocbs</code> Offloads Read-Copy-Update (RCU) callback processing from specified cores. RCU is a synchronization mechanism in the Linux kernel that can cause periodic processing bursts. Moving this work away from your networking cores helps maintain consistent performance. <code>rcu_nocb_poll</code> Works with <code>rcu_nocbs</code> to improve how RCU callbacks are processed on non-callback CPUs. This can reduce latency spikes by changing how the kernel polls for RCU work. <p>Together, these parameters create an environment where specific CPU cores can focus exclusively on network packet processing with minimal interference from the operating system, resulting in lower and more consistent latency.</p> <p>Add these flags to the <code>GRUB_CMDLINE_LINUX</code> variable in <code>/etc/default/grub</code>, then rebuild your GRUB configuration and reboot:</p> <pre><code>sudo update-grub\nsudo reboot\n</code></pre> <p>Verify that the flags were properly set after boot by rerunning the check commands above.</p>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#36-prevent-cpu-cores-from-going-idle","title":"3.6 Prevent CPU cores from going idle","text":"<p>When a core goes idle/to sleep, coming back online to poll the NIC can cause latency spikes and dropped packets. To prevent this, we recommend setting the scaling governor to <code>performance</code> for these CPU cores.</p> <p>Note</p> <p>Cores from a single cluster will always share the same governor.</p> <p>Bug</p> <p>We have witnessed instances where setting the governor to <code>performance</code> on only the isolated cores (dedicated to polling the NIC) does not lead to the performance gains expected. As such, we currently recommend setting the governor to <code>performance</code> for all cores which has shown to be reliably effective.</p> <p>Check the current governor for each of your cores:</p> tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check cpu-freq\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check cpu-freq\n</code></pre> See an example output <pre><code>2025-03-06 12:20:27 - WARNING - CPU 0: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 1: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 2: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 3: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 4: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 5: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 6: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 7: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 8: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 9: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 10: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 11: Governor is set to 'powersave', not 'performance'.\n</code></pre> <pre><code>cat /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor\n</code></pre> See an example output <p>In this example, all cores were defaulted to <code>powersave</code> instead of the recommended <code>performance</code>.</p> <pre><code>powersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\n</code></pre> <p>Install <code>cpupower</code> to more conveniently set the governor:</p> <pre><code>sudo apt update\nsudo apt install -y linux-tools-$(uname -r)\n</code></pre> <p>Set the governor to <code>performance</code> for all cores:</p> One-timePersistent <pre><code>sudo cpupower frequency-set -g performance\n</code></pre> <pre><code>cat &lt;&lt; EOF | sudo tee /etc/systemd/system/cpu-performance.service\n[Unit]\nDescription=Set CPU governor to performance\nAfter=multi-user.target\n\n[Service]\nType=oneshot\nExecStart=/usr/bin/cpupower -c all frequency-set -g performance\n\n[Install]\nWantedBy=multi-user.target\nEOF\nsudo systemctl enable cpu-performance.service\nsudo systemctl start cpu-performance.service\n</code></pre> <p>Running the checks above should now list <code>performance</code> as the governor for all cores. You can also run <code>sudo cpupower -c all frequency-info</code> for more details.</p>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#37-prevent-the-gpu-from-going-idle","title":"3.7 Prevent the GPU from going idle","text":"<p>Similarly to the above, we want to maximize the GPU's clock speed and prevent it from going idle.</p> <p>Run the following command to check your current clocks and whether they're locked (persistence mode):</p> <pre><code>nvidia-smi -q | grep -i \"Persistence Mode\"\nnvidia-smi -q -d CLOCK\n</code></pre> See an example output <pre><code>    Persistence Mode: Enabled\n...\nAttached GPUs                             : 1\nGPU 00000005:09:00.0\n    Clocks\n        Graphics                          : 420 MHz\n        SM                                : 420 MHz\n        Memory                            : 405 MHz\n        Video                             : 1680 MHz\n    Applications Clocks\n        Graphics                          : 1800 MHz\n        Memory                            : 8001 MHz\n    Default Applications Clocks\n        Graphics                          : 1800 MHz\n        Memory                            : 8001 MHz\n    Deferred Clocks\n        Memory                            : N/A\n    Max Clocks\n        Graphics                          : 2100 MHz\n        SM                                : 2100 MHz\n        Memory                            : 8001 MHz\n        Video                             : 1950 MHz\n    ...\n</code></pre> <p>To lock the GPU's clocks to their max values:</p> One-timePersistent <pre><code>sudo nvidia-smi -pm 1\nsudo nvidia-smi -lgc=$(nvidia-smi --query-gpu=clocks.max.sm --format=csv,noheader,nounits)\nsudo nvidia-smi -lmc=$(nvidia-smi --query-gpu=clocks.max.mem --format=csv,noheader,nounits)\n</code></pre> <pre><code>cat &lt;&lt; EOF | sudo tee /etc/systemd/system/gpu-max-clocks.service\n[Unit]\nDescription=Max GPU clocks\nAfter=multi-user.target\n\n[Service]\nType=oneshot\nExecStart=/usr/bin/nvidia-smi -pm 1\nExecStart=/bin/bash -c '/usr/bin/nvidia-smi --lock-gpu-clocks=$(/usr/bin/nvidia-smi --query-gpu=clocks.max.sm --format=csv,noheader,nounits)'\nExecStart=/bin/bash -c '/usr/bin/nvidia-smi --lock-memory-clocks=$(/usr/bin/nvidia-smi --query-gpu=clocks.max.mem --format=csv,noheader,nounits)'\nRemainAfterExit=true\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\nsudo systemctl enable gpu-max-clocks.service\nsudo systemctl start gpu-max-clocks.service\n</code></pre> Show explanation <p>This queries the max clocks for the GPU SM (<code>clocks.max.sm</code>) and memory (<code>clocks.max.mem</code>) and sets them to the current clocks (<code>lock-gpu-clocks</code> and <code>lock-memory-clocks</code> respectively). <code>-pm 1</code> (or <code>--persistence-mode=1</code>) enables persistence mode to lock these values.</p> See an example output <pre><code>GPU clocks set to \"(gpuClkMin 2100, gpuClkMax 2100)\" for GPU 00000005:09:00.0\nAll done.\nMemory clocks set to \"(memClkMin 8001, memClkMax 8001)\" for GPU 00000005:09:00.0\nAll done.\n</code></pre> <p>You can confirm that the clocks are set to the max values by running <code>nvidia-smi -q -d CLOCK</code> again.</p> <p>Note</p> <p>Some max clocks might not be achievable in certain configurations, or due to boost clocks (SM) or rounding errors (Memory),  despite the lock commands indicating it worked. For example - on IGX - the max non-boot SM clock will be 1920 MHz, and the max memory clock will show 8000 MHz, which are satisfying compared to the initial mode.</p>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#38-maximize-gpu-bar1-size","title":"3.8 Maximize GPU BAR1 size","text":"<p>The GPU BAR1 memory is the primary resource consumed by <code>GPUDirect</code>. It allows other PCIe devices (like the CPU and the NIC) to access the GPU's memory space. The larger the BAR1 size, the more memory the GPU can expose to these devices in a single PCIe transaction, reducing the number of transactions needed and improving performance.</p> <p>We recommend a BAR1 size of 1GB or above. Check the current BAR1 size:</p> tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check bar1-size\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check bar1-size\n</code></pre> See an example output <pre><code>2025-03-06 12:22:53 - INFO - GPU 00000005:09:00.0: BAR1 size is 8192 MiB.\n</code></pre> <pre><code>nvidia-smi -q | grep -A 3 BAR1\n</code></pre> See an example output <p>For our RTX A6000, this shows a BAR1 size of 256 MiB:</p> <pre><code>    BAR1 Memory Usage\n    Total                             : 256 MiB\n    Used                              : 13 MiB\n    Free                              : 243 MiB\n</code></pre> <p>Warning</p> <p>Resizing the BAR1 size requires:</p> <ul> <li>A BIOS with resizable BAR support</li> <li>A GPU with physical resizable BAR</li> </ul> <p>If you attempt to go forward with the instructions below without meeting the above requirements, you might render your GPU unusable.</p>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#bios-resizable-bar-support","title":"BIOS Resizable BAR support","text":"<p>First, check if your system and BIOS support resizable BAR. Refer to your system's manufacturer documentation to access the BIOS. The Resizable BAR option is often categorized under <code>Advanced &gt; PCIe</code> settings. Enable this feature if found.</p> <p>Note</p> <p>The IGX Developer kit with IGX OS 1.1+ supports resizable BAR by default.</p>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#gpu-resizable-bar-support","title":"GPU Resizable BAR support","text":"<p>Next, you can check if your GPU has physical resizable BAR by running the following command:</p> <pre><code>sudo lspci -vv -s $(nvidia-smi --query-gpu=pci.bus_id --format=csv,noheader) | grep BAR\n</code></pre> See an example output <p>This RTX A6000 has a resizable BAR1, currently set to 256 MiB:</p> <pre><code>Capabilities: [bb0 v1] Physical Resizable BAR\n    BAR 0: current size: 16MB, supported: 16MB\n    BAR 1: current size: 256MB, supported: 64MB 128MB 256MB 512MB 1GB 2GB 4GB 8GB 16GB 32GB 64GB\n    BAR 3: current size: 32MB, supported: 32MB\n</code></pre> <p>If your GPU is listed on this page, you can download the <code>Display Mode Selector</code> to resize the BAR1 to 8GB.</p> <ol> <li>Press <code>Join Now</code>.</li> <li>Once approved, download the <code>Display Mode Selector</code> archive.</li> <li>Unzip the archive.</li> <li>Access your system without a X-server running, either through SSH or a Virtual Console (<code>Alt+F1</code>).</li> <li>Go down the right OS and architecture folder for your system (<code>linux/aarch64</code> or <code>linux/x64</code>).</li> <li>Run the <code>displaymodeselector</code> command like so:</li> </ol> <pre><code>chmod +x displaymodeselector\nsudo ./displaymodeselector --gpumode physical_display_enabled_8GB_bar1\n</code></pre> <p>Press <code>y</code> to confirm you'd like to continue, then <code>y</code> again to apply to all the eligible adapters.</p> See an example output <pre><code>NVIDIA Display Mode Selector Utility (Version 1.67.0)\nCopyright (C) 2015-2021, NVIDIA Corporation. All Rights Reserved.\n\nWARNING: This operation updates the firmware on the board and could make\n        the device unusable if your host system lacks the necessary support.\n\nAre you sure you want to continue?\nPress 'y' to confirm (any other key to abort):\ny\nSpecified GPU Mode \"physical_display_enabled_8GB_bar1\"\n\n\nUpdate GPU Mode of all adapters to \"physical_display_enabled_8GB_bar1\"?\nPress 'y' to confirm or 'n' to choose adapters or any other key to abort:\ny\n\nUpdating GPU Mode of all eligible adapters to \"physical_display_enabled_8GB_bar1\"\n\nApply GPU Mode &lt;6&gt; corresponds to \"physical_display_enabled_8GB_bar1\"\n\nReading EEPROM (this operation may take up to 30 seconds)\n\n[==================================================] 100 %\nReading EEPROM (this operation may take up to 30 seconds)\n\nSuccessfully updated GPU mode to \"physical_display_enabled_8GB_bar1\" ( Mode 6 ).\n\nA reboot is required for the update to take effect.\n</code></pre> Error: unload the NVIDIA kernel driver first <p>If you see this error:</p> <pre><code>ERROR: In order to avoid the irreparable damage to your graphics adapter it is necessary to unload the NVIDIA kernel driver first:\n\nrmmod nvidia_uvm nvidia_drm nvidia_modeset nvidia_peermem nvidia\n</code></pre> <p>Try to unload the NVIDIA kernel driver listed in the error message above (list may vary):</p> <pre><code>sudo rmmod nvidia_uvm nvidia_drm nvidia_modeset nvidia_peermem nvidia\n</code></pre> <p>If this fails because the drivers are in use, stop the X-server first before trying again:</p> <pre><code>sudo systemctl isolate multi-user\n</code></pre> /dev/mem: Operation not permitted. Access to physical memory denied <p>Disable secure boot on your system ahead of changing your GPU's BAR1 size. It can be re-enabled afterwards.</p> <p>Reboot your system, and check the BAR1 size again to confirm the change.</p> <pre><code>sudo reboot\n</code></pre>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#39-enable-jumbo-frames","title":"3.9 Enable Jumbo Frames","text":"<p>Jumbo frames are Ethernet frames that carry a payload larger than the standard 1500 bytes MTU (Maximum Transmission Unit). They can significantly improve network performance when transferring large amounts of data by reducing the overhead of packet headers and the number of packets that need to be processed.</p> <p>We recommend an MTU of 9000 bytes on all interfaces involved in the data path. You can check the current MTU of your interfaces:</p> tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check mtu\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check mtu\n</code></pre> See an example output <pre><code>2025-03-06 16:51:19 - INFO - Interface eth0 has an acceptable MTU of 9000 bytes.\n2025-03-06 16:51:19 - INFO - Interface eth1 has an acceptable MTU of 9000 bytes.\n</code></pre> <p>For a given <code>if_name</code> interface:</p> <pre><code>if_name=eth0\nip link show dev $if_name | grep -oE \"mtu [0-9]+\"\n</code></pre> See an example output <pre><code>mtu 1500\n</code></pre> <p>You can set the MTU for each interface like so, for a given <code>if_name</code> name identified above:</p> One-timePersistent <pre><code>sudo ip link set dev $if_name mtu 9000\n</code></pre> NetworkManagersystemd-networkd <pre><code>sudo nmcli connection modify $if_name ipv4.mtu 9000\nsudo nmcli connection up $if_name\n</code></pre> <p>Assuming you've set an IP address for the interface above, you can add the MTU to the interface's network configuration file like so:</p> <pre><code>sudo sed -i '/\\[Network\\]/a MTU=9000' /etc/systemd/network/20-$if_name.network\nsudo systemctl restart systemd-networkd\n</code></pre> Can I do more than 9000? <p>While your NIC might have a maximum MTU capability larger than 9000, we typically recommend setting the MTU to 9000 bytes, as that is the standard size for jumbo frames that's widely supported for compatibility with other network equipment. When using jumbo frames, all devices in the communication path must support the same MTU size. If any device in between has a smaller MTU, packets will be fragmented or dropped, potentially degrading performance.</p> <p>Example with the CX-7 NIC:</p> <pre><code>$ ip -d link show dev $if_name | grep -oE \"maxmtu [0-9]+\"\nmaxmtu 9978\n</code></pre>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#4-running-a-test-application","title":"4. Running a test application","text":"<p>Holoscan Networking provides a benchmarking application named <code>adv_networking_bench</code> that can be used to test the performance of the networking configuration. In this section, we'll walk you through the steps needed to configure the application for your NIC for Tx and Rx, and run a loopback test between the two interfaces with a physical SFP cable connecting them.</p> <p>Make sure to install <code>holoscan-networking</code> beforehand.</p>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#41-update-the-loopback-configuration","title":"4.1 Update the loopback configuration","text":"","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#find-the-application-files","title":"Find the application files","text":"<p>Identify the location of the <code>adv_networking_bench</code> executable, and of the configuration file named <code>adv_networking_bench_default_tx_rx.yaml</code>, for your installation:</p> Debian installationFrom source <p>Both located under <code>/opt/nvidia/holoscan/examples/adv_networking_bench/</code>:</p> <pre><code>ls -1 /opt/nvidia/holoscan/examples/adv_networking_bench/\nadv_networking_bench\nadv_networking_bench_default_rx_multi_q.yaml\nadv_networking_bench_default_tx_rx_hds.yaml\nadv_networking_bench_default_tx_rx.yaml\nadv_networking_bench_gpunetio_tx_rx.yaml\nadv_networking_bench_rmax_rx.yaml\nCMakeLists.txt\ndefault_bench_op_rx.h\ndefault_bench_op_tx.h\ndoca_bench_op_rx.h\ndoca_bench_op_tx.h\nkernels.cu\nkernels.cuh\nmain.cpp\n</code></pre> <p>Both located under <code>./install/examples/adv_networking_bench/</code></p> <pre><code>ls -1 ./install/examples/adv_networking_bench\nadv_networking_bench\nadv_networking_bench_default_rx_multi_q.yaml\nadv_networking_bench_default_tx_rx_hds.yaml\nadv_networking_bench_default_tx_rx.yaml\nadv_networking_bench_gpunetio_tx_rx.yaml\nadv_networking_bench.py\nadv_networking_bench_rmax_rx.yaml\nCMakeLists.txt\ndefault_bench_op_rx.h\ndefault_bench_op_tx.h\ndoca_bench_op_rx.h\ndoca_bench_op_tx.h\nkernels.cu\nkernels.cuh\nmain.cpp\n</code></pre> <p>Warning</p> <p>The configuration file is also located alongide the application source code at <code>applications/adv_networking_bench/adv_networking_bench_default_tx_rx.yaml</code>. However, modifying this file will not affect the configuration used by the application executable without rebuilding the application.</p> <p>For this reason, we recommend using the configuration file located in the install tree.</p> <p>Note</p> <p>The fields in this <code>yaml</code> file will be explained in more details in a section below. For now, we'll stick to modifying the strict minimum required fields to run the application as-is on your system.</p>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#identify-your-nics-pcie-addresses","title":"Identify your NIC's PCIe addresses","text":"<p>Retrieve the PCIe addresses of both ports of your NIC. We'll arbitrarily use the first for Tx and the second for Rx here:</p> ibdev2netdevlspci <pre><code>sudo ibdev2netdev -v | awk '{print $1}'\n</code></pre> <pre><code># `0200` is the PCI-SIG class code for NICs\n# `15b3` is the Vendor ID for Mellanox\nlspci -n | awk '$2 == \"0200:\" &amp;&amp; $3 ~ /^15b3:/ {print $1}'\n</code></pre> See an example output <pre><code>0005:03:00.0\n0005:03:00.1\n</code></pre>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#configure-the-nic-for-tx-and-rx","title":"Configure the NIC for Tx and Rx","text":"<p>Set the NIC addresses in the <code>interfaces</code> section of the <code>advanced_network</code> section, making sure to remove the template brackets <code>&lt; &gt;</code>. This configures your NIC independently of your application:</p> <ul> <li>Set the <code>address</code> field of the <code>tx_port</code> interface to one of these addresses. That interface will be able to transmit ethernet packets.</li> <li>Set the <code>address</code> field of the <code>rx_port</code> interface to the other address. This interface will be able to receive ethernet packets.</li> </ul> <pre><code>interfaces:\n    - name: \"tx_port\"\n    address: &lt;0000:00:00.0&gt;       # The BUS address of the interface doing Tx\n    tx:\n        ...\n    - name: \"rx_port\"\n    address: &lt;0000:00:00.0&gt;       # The BUS address of the interface doing Rx\n    rx:\n        ...\n</code></pre> See an example yaml <pre><code>interfaces:\n    - name: \"tx_port\"\n    address: 0005:03:00.0       # The BUS address of the interface doing Tx\n    tx:\n        ...\n    - name: \"rx_port\"\n    address: 0005:03:00.1       # The BUS address of the interface doing Rx\n    rx:\n        ...\n</code></pre>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#configure-the-application","title":"Configure the application","text":"<p>To run the benchmarking application to run a loopback on your system, you'll need to modify the <code>bench_tx</code> section which configures the application itself, to create the packet headers and direct the packets to the NIC. Make sure to remove the template brackets <code>&lt; &gt;</code>.</p> <ul> <li><code>eth_dst_addr</code> with the MAC address (and not the PCIe address) of the NIC interface you want to use for Rx. You can get the MAC address of your <code>if_name</code> interface with <code>cat /sys/class/net/$if_name/address</code>:</li> </ul> <pre><code>bench_tx:\n    interface_name: \"tx_port\" # Name of the TX port from the advanced_network config\n    ...\n    eth_dst_addr: &lt;00:00:00:00:00:00&gt; # Destination MAC address - required when Rx flow_isolation=true\n    ...\n</code></pre> See an example yaml <pre><code>bench_tx:\n    interface_name: \"tx_port\" # Name of the TX port from the advanced_network config\n    ...\n    eth_dst_addr: 48:b0:2d:ee:83:ad # Destination MAC address - required when Rx flow_isolation=true\n    ...\n</code></pre> Show explanation <ul> <li><code>eth_dst_addr</code> - the destination ethernet MAC address - will be embedded in the packet headers by the application. This is required here because the Rx interface above has <code>flow_isolation: true</code> (explained in more details below). In that configuration, only the packets listing the adequate destination MAC address will be accepted by the Rx interface.</li> <li>We ignore the IP fields (<code>ip_src_addr</code>, <code>ip_dst_addr</code>) for now, as we are testing on a layer 2 network by just connecting a cable between the two interfaces on our system, therefore having mock values has no impact.</li> <li>You might have noted the lack of a <code>eth_src_addr</code> field in this <code>bench_tx</code> section. This is because the source Ethernet MAC address can be inferred automatically by the Advanced Network library from the PCIe address of the Tx interface referenced above.</li> </ul>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#42-run-the-loopback-test","title":"4.2 Run the loopback test","text":"<p>After having modified the configuration file, ensure you have connected an SFP cable between the two interfaces of your NIC, then run the application with the command below:</p> Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/examples/adv_networking_bench/adv_networking_bench adv_networking_bench_default_tx_rx.yaml\n</code></pre> Bare MetalContainerized <p>This assumes you have the required dependencies (holoscan, doca, etc.) installed locally on your system.</p> <pre><code>sudo ./install/examples/adv_networking_bench/adv_networking_bench adv_networking_bench_default_tx_rx.yaml\n</code></pre> <pre><code>./dev_container launch \\\n  --img holohub:adv_networking_bench \\\n  --docker_opts \"-u 0 --privileged\" \\\n  -- bash -c \"./install/examples/adv_networking_bench/adv_networking_bench adv_networking_bench_default_tx_rx.yaml\"\n</code></pre> <p>The application will run indefinitely. You can stop it gracefully with <code>Ctrl-C</code>. You can also uncomment and set the <code>max_duration_ms</code> field in the <code>scheduler</code> section of the configuration file to limit the duration of the run automatically.</p> See an example output <pre><code>[info] [fragment.cpp:599] Loading extensions from configs...\n[info] [gxf_executor.cpp:264] Creating context\n[info] [main.cpp:35] Initializing advanced network operator\n[info] [main.cpp:40] Using ANO manager dpdk\n[info] [adv_network_rx.cpp:35] Adding output port bench_rx_out\n[info] [adv_network_rx.cpp:51] AdvNetworkOpRx::initialize()\n[info] [adv_network_common.h:607] Finished reading advanced network operator config\n[info] [adv_network_dpdk_mgr.cpp:373] Attempting to use 2 ports for high-speed network\n[info] [adv_network_dpdk_mgr.cpp:382] Setting DPDK log level to: Info\n[info] [adv_network_dpdk_mgr.cpp:402] DPDK EAL arguments: adv_net_operator --file-prefix=nwlrbbmqbh -l 3,11,9 --log-level=9 --log-level=pmd.net.mlx5:info -a 0005:03:00.0,txq_inline_max=0,dv_flow_en=2 -a 0005:03:00.1,txq_inline_max=0,dv_flow_en=2\nLog level 9 higher than maximum (8)\nEAL: Detected CPU lcores: 12\nEAL: Detected NUMA nodes: 1\nEAL: Detected shared linkage of DPDK\nEAL: Multi-process socket /var/run/dpdk/nwlrbbmqbh/mp_socket\nEAL: Selected IOVA mode 'VA'\nEAL: 1 hugepages of size 1073741824 reserved, but no mounted hugetlbfs found for that size\nEAL: Probe PCI driver: mlx5_pci (15b3:1021) device: 0005:03:00.0 (socket -1)\nmlx5_net: PCI information matches for device \"mlx5_0\"\nmlx5_net: enhanced MPS is enabled\nmlx5_net: port 0 MAC address is 48:B0:2D:EE:83:AC\nEAL: Probe PCI driver: mlx5_pci (15b3:1021) device: 0005:03:00.1 (socket -1)\nmlx5_net: PCI information matches for device \"mlx5_1\"\nmlx5_net: enhanced MPS is enabled\nmlx5_net: port 1 MAC address is 48:B0:2D:EE:83:AD\nTELEMETRY: No legacy callbacks, legacy socket not created\n[info] [adv_network_dpdk_mgr.cpp:298] Port 0 has no RX queues. Creating dummy queue.\n[info] [adv_network_dpdk_mgr.cpp:165] Adjusting buffer size to 9228 for headroom\n[info] [adv_network_dpdk_mgr.cpp:165] Adjusting buffer size to 9128 for headroom\n[info] [adv_network_dpdk_mgr.cpp:165] Adjusting buffer size to 9128 for headroom\n[info] [adv_network_mgr.cpp:116] Registering memory regions\n[info] [adv_network_mgr.cpp:178] Successfully allocated memory region MR_Unused_P0 at 0x100fa0000 type 2 with 9100 bytes (32768 elements @ 9228 bytes total 302383104)\n[info] [adv_network_mgr.cpp:178] Successfully allocated memory region Data_RX_GPU at 0xffff4fc00000 type 3 with 9000 bytes (51200 elements @ 9128 bytes total 467402752)\n[info] [adv_network_mgr.cpp:178] Successfully allocated memory region Data_TX_GPU at 0xffff33e00000 type 3 with 9000 bytes (51200 elements @ 9128 bytes total 467402752)\n[info] [adv_network_mgr.cpp:191] Finished allocating memory regions\n[info] [adv_network_dpdk_mgr.cpp:223] Successfully registered external memory for Data_TX_GPU\n[info] [adv_network_dpdk_mgr.cpp:223] Successfully registered external memory for Data_RX_GPU\n[info] [adv_network_dpdk_mgr.cpp:193] Mapped external memory descriptor for 0xffff4fc00000 to device 0\n[info] [adv_network_dpdk_mgr.cpp:193] Mapped external memory descriptor for 0xffff33e00000 to device 0\n[info] [adv_network_dpdk_mgr.cpp:193] Mapped external memory descriptor for 0xffff4fc00000 to device 1\n[info] [adv_network_dpdk_mgr.cpp:193] Mapped external memory descriptor for 0xffff33e00000 to device 1\n[info] [adv_network_dpdk_mgr.cpp:454] DPDK init (0005:03:00.0) -- RX: ENABLED TX: ENABLED\n[info] [adv_network_dpdk_mgr.cpp:464] Configuring RX queue: UNUSED_P0_Q0 (0) on port 0\n[info] [adv_network_dpdk_mgr.cpp:513] Created mempool RXP_P0_Q0_MR0 : mbufs=32768 elsize=9228 ptr=0x10041c380\n[info] [adv_network_dpdk_mgr.cpp:523] Max packet size needed for RX: 9100\n[info] [adv_network_dpdk_mgr.cpp:564] Configuring TX queue: ADC Samples (0) on port 0\n[info] [adv_network_dpdk_mgr.cpp:607] Created mempool TXP_P0_Q0_MR0 : mbufs=51200 elsize=9000 ptr=0x100c1fc00\n[info] [adv_network_dpdk_mgr.cpp:621] Max packet size needed with TX: 9100\n[info] [adv_network_dpdk_mgr.cpp:632] Setting port config for port 0 mtu:9082\n[info] [adv_network_dpdk_mgr.cpp:663] Initializing port 0 with 1 RX queues and 1 TX queues...\nmlx5_net: port 0 Tx queues number update: 0 -&gt; 1\nmlx5_net: port 0 Rx queues number update: 0 -&gt; 1\n[info] [adv_network_dpdk_mgr.cpp:679] Successfully configured ethdev\n[info] [adv_network_dpdk_mgr.cpp:689] Successfully set descriptors to 8192/8192\n[info] [adv_network_dpdk_mgr.cpp:704] Port 0 not in isolation mode\n[info] [adv_network_dpdk_mgr.cpp:713] Setting up port:0, queue:0, Num scatter:1 pool:0x10041c380\n[info] [adv_network_dpdk_mgr.cpp:734] Successfully setup RX port 0 queue 0\n[info] [adv_network_dpdk_mgr.cpp:756] Successfully set up TX queue 0/0\n[info] [adv_network_dpdk_mgr.cpp:761] Enabling promiscuous mode for port 0\nmlx5_net: [mlx5dr_cmd_query_caps]: Failed to query wire port regc value\nmlx5_net: port 0 Rx queues number update: 1 -&gt; 1\n[info] [adv_network_dpdk_mgr.cpp:775] Successfully started port 0\n[info] [adv_network_dpdk_mgr.cpp:778] Port 0, MAC address: 48:B0:2D:EE:83:AC\n[info] [adv_network_dpdk_mgr.cpp:1111] Applying tx_eth_src offload for port 0\n[info] [adv_network_dpdk_mgr.cpp:454] DPDK init (0005:03:00.1) -- RX: ENABLED TX: DISABLED\n[info] [adv_network_dpdk_mgr.cpp:464] Configuring RX queue: Data (0) on port 1\n[info] [adv_network_dpdk_mgr.cpp:513] Created mempool RXP_P1_Q0_MR0 : mbufs=51200 elsize=9128 ptr=0x125a5b940\n[info] [adv_network_dpdk_mgr.cpp:523] Max packet size needed for RX: 9000\n[info] [adv_network_dpdk_mgr.cpp:621] Max packet size needed with TX: 9000\n[info] [adv_network_dpdk_mgr.cpp:632] Setting port config for port 1 mtu:8982\n[info] [adv_network_dpdk_mgr.cpp:663] Initializing port 1 with 1 RX queues and 0 TX queues...\nmlx5_net: port 1 Rx queues number update: 0 -&gt; 1\n[info] [adv_network_dpdk_mgr.cpp:679] Successfully configured ethdev\n[info] [adv_network_dpdk_mgr.cpp:689] Successfully set descriptors to 8192/8192\n[info] [adv_network_dpdk_mgr.cpp:701] Port 1 in isolation mode\n[info] [adv_network_dpdk_mgr.cpp:713] Setting up port:1, queue:0, Num scatter:1 pool:0x125a5b940\n[info] [adv_network_dpdk_mgr.cpp:734] Successfully setup RX port 1 queue 0\n[info] [adv_network_dpdk_mgr.cpp:764] Not enabling promiscuous mode on port 1 since flow isolation is enabled\nmlx5_net: [mlx5dr_cmd_query_caps]: Failed to query wire port regc value\nmlx5_net: port 1 Rx queues number update: 1 -&gt; 1\n[info] [adv_network_dpdk_mgr.cpp:775] Successfully started port 1\n[info] [adv_network_dpdk_mgr.cpp:778] Port 1, MAC address: 48:B0:2D:EE:83:AD\n[info] [adv_network_dpdk_mgr.cpp:790] Adding RX flow ADC Samples\n[info] [adv_network_dpdk_mgr.cpp:998] Adding IPv4 length match for 1050\n[info] [adv_network_dpdk_mgr.cpp:1018] Adding UDP port match for src/dst 4096/4096\n[info] [adv_network_dpdk_mgr.cpp:814] Setting up RX burst pool with 8191 batches of size 81920\n[info] [adv_network_dpdk_mgr.cpp:833] Setting up RX burst pool with 8191 batches of size 20480\n[info] [adv_network_dpdk_mgr.cpp:875] Setting up TX ring TX_RING_P0_Q0\n[info] [adv_network_dpdk_mgr.cpp:901] Setting up TX burst pool TX_BURST_POOL_P0_Q0 with 10240 pointers at 0x125a0d4c0\n[info] [adv_network_dpdk_mgr.cpp:1186] Config validated successfully\n[info] [adv_network_dpdk_mgr.cpp:1199] Starting advanced network workers\n[info] [adv_network_dpdk_mgr.cpp:1278] Flushing packet on port 1\n[info] [adv_network_dpdk_mgr.cpp:1478] Starting RX Core 9, port 1, queue 0, socket 0\n[info] [adv_network_dpdk_mgr.cpp:1268] Done starting workers\n[info] [default_bench_op_tx.h:79] AdvNetworkingBenchDefaultTxOp::initialize()\n[info] [adv_network_dpdk_mgr.cpp:1637] Starting TX Core 11, port 0, queue 0 socket 0 using burst pool 0x125a0d4c0 ring 0x127690740\n[info] [default_bench_op_tx.h:113] Initialized 4 streams and events\n[info] [default_bench_op_tx.h:130] AdvNetworkingBenchDefaultTxOp::initialize() complete\n[info] [default_bench_op_rx.h:67] AdvNetworkingBenchDefaultRxOp::initialize()\n[info] [gxf_executor.cpp:1797] creating input IOSpec named 'burst_in'\n[info] [default_bench_op_rx.h:104] AdvNetworkingBenchDefaultRxOp::initialize() complete\n[info] [adv_network_tx.cpp:46] AdvNetworkOpTx::initialize()\n[info] [gxf_executor.cpp:1797] creating input IOSpec named 'burst_in'\n[info] [adv_network_common.h:607] Finished reading advanced network operator config\n[info] [gxf_executor.cpp:2208] Activating Graph...\n[info] [gxf_executor.cpp:2238] Running Graph...\n[info] [multi_thread_scheduler.cpp:300] MultiThreadScheduler started worker thread [pool name: default_pool, thread uid: 0]\n[info] [multi_thread_scheduler.cpp:300] MultiThreadScheduler started worker thread [pool name: default_pool, thread uid: 1]\n[info] [multi_thread_scheduler.cpp:300] MultiThreadScheduler started worker thread [pool name: default_pool, thread uid: 2]\n[info] [gxf_executor.cpp:2240] Waiting for completion...\n[info] [multi_thread_scheduler.cpp:300] MultiThreadScheduler started worker thread [pool name: default_pool, thread uid: 3]\n[info] [multi_thread_scheduler.cpp:300] MultiThreadScheduler started worker thread [pool name: default_pool, thread uid: 4]\n^C[info] [multi_thread_scheduler.cpp:636] Stopping multithread scheduler\n[info] [multi_thread_scheduler.cpp:694] Stopping all async jobs\n[info] [multi_thread_scheduler.cpp:218] Dispatcher thread has stopped checking jobs\n[info] [multi_thread_scheduler.cpp:679] Waiting to join all async threads\n[info] [multi_thread_scheduler.cpp:316] Worker Thread [pool name: default_pool, thread uid: 1] exiting.\n[info] [multi_thread_scheduler.cpp:702] *********************** DISPATCHER EXEC TIME : 476345.364000 ms\n\n[info] [multi_thread_scheduler.cpp:316] Worker Thread [pool name: default_pool, thread uid: 0] exiting.\n[info] [multi_thread_scheduler.cpp:316] Worker Thread [pool name: default_pool, thread uid: 3] exiting.\n[info] [multi_thread_scheduler.cpp:371] Event handler thread exiting.\n[info] [multi_thread_scheduler.cpp:703] *********************** DISPATCHER WAIT TIME : 47339.961000 ms\n\n[info] [multi_thread_scheduler.cpp:704] *********************** DISPATCHER COUNT : 197630449\n\n[info] [multi_thread_scheduler.cpp:316] Worker Thread [pool name: default_pool, thread uid: 2] exiting.\n[info] [multi_thread_scheduler.cpp:705] *********************** WORKER EXEC TIME : 983902.800000 ms\n\n[info] [multi_thread_scheduler.cpp:706] *********************** WORKER WAIT TIME : 1634522.159000 ms\n\n[info] [multi_thread_scheduler.cpp:707] *********************** WORKER COUNT : 11817369\n\n[info] [multi_thread_scheduler.cpp:316] Worker Thread [pool name: default_pool, thread uid: 4] exiting.\n[info] [multi_thread_scheduler.cpp:688] All async worker threads joined, deactivating all entities\n[info] [adv_network_rx.cpp:46] AdvNetworkOpRx::stop()\n[info] [adv_network_dpdk_mgr.cpp:1928] DPDK ANO shutdown called 2\n[info] [adv_network_tx.cpp:41] AdvNetworkOpTx::stop()\n[info] [adv_network_dpdk_mgr.cpp:1928] DPDK ANO shutdown called 1\n[info] [adv_network_dpdk_mgr.cpp:1133] Port 0:\n[info] [adv_network_dpdk_mgr.cpp:1135]  - Received packets:    0\n[info] [adv_network_dpdk_mgr.cpp:1136]  - Transmit packets:    6005066864\n[info] [adv_network_dpdk_mgr.cpp:1137]  - Received bytes:      0\n[info] [adv_network_dpdk_mgr.cpp:1138]  - Transmit bytes:      6389391347584\n[info] [adv_network_dpdk_mgr.cpp:1139]  - Missed packets:      0\n[info] [adv_network_dpdk_mgr.cpp:1140]  - Errored packets:     0\n[info] [adv_network_dpdk_mgr.cpp:1141]  - RX out of buffers:   0\n[info] [adv_network_dpdk_mgr.cpp:1143]    ** Extended Stats **\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_good_packets:          6005070000\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_good_bytes:            6389394480000\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_q0_packets:            6005070000\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_q0_bytes:              6389394480000\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_multicast_bytes:               9589\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_multicast_packets:             22\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_unicast_bytes:         6389394480000\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_multicast_bytes:               9589\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_unicast_packets:               6005070000\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_multicast_packets:             22\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_phy_packets:           6005070022\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_phy_packets:           24\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_phy_bytes:             6413414769677\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_phy_bytes:             9805\n[info] [adv_network_dpdk_mgr.cpp:1133] Port 1:\n[info] [adv_network_dpdk_mgr.cpp:1135]  - Received packets:    6004323692\n[info] [adv_network_dpdk_mgr.cpp:1136]  - Transmit packets:    0\n[info] [adv_network_dpdk_mgr.cpp:1137]  - Received bytes:      6388600255072\n[info] [adv_network_dpdk_mgr.cpp:1138]  - Transmit bytes:      0\n[info] [adv_network_dpdk_mgr.cpp:1139]  - Missed packets:      746308\n[info] [adv_network_dpdk_mgr.cpp:1140]  - Errored packets:     0\n[info] [adv_network_dpdk_mgr.cpp:1141]  - RX out of buffers:   5047027287\n[info] [adv_network_dpdk_mgr.cpp:1143]    ** Extended Stats **\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_good_packets:          6004323692\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_good_bytes:            6388600255072\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_missed_errors:         746308\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_mbuf_allocation_errors:                5047027287\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_q0_packets:            6004323692\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_q0_bytes:              6388600255072\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_q0_errors:             5047027287\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_unicast_bytes:         6389394480000\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_multicast_bytes:               9589\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_unicast_packets:               6005070000\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_multicast_packets:             22\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_multicast_bytes:               9589\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_multicast_packets:             22\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_phy_packets:           24\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_phy_packets:           6005070022\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_phy_bytes:             9805\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_phy_bytes:             6413414769677\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_out_of_buffer:         746308\n[info] [adv_network_dpdk_mgr.cpp:1935] ANO DPDK manager shutting down\n[info] [adv_network_dpdk_mgr.cpp:1622] Total packets received by application (port/queue 1/0): 6004323692\n[info] [adv_network_dpdk_mgr.cpp:1698] Total packets transmitted by application (port/queue 0/0): 6005070000\n[info] [multi_thread_scheduler.cpp:645] Multithread scheduler stopped.\n[info] [multi_thread_scheduler.cpp:664] Multithread scheduler finished.\n[info] [gxf_executor.cpp:2243] Deactivating Graph...\n[info] [multi_thread_scheduler.cpp:491] TOTAL EXECUTION TIME OF SCHEDULER : 523694.460857 ms\n\n[info] [gxf_executor.cpp:2251] Graph execution finished.\n[info] [adv_network_dpdk_mgr.cpp:1928] DPDK ANO shutdown called 0\n[info] [default_bench_op_tx.h:51] ANO benchmark TX op shutting down\n[info] [default_bench_op_rx.h:56] Finished receiver with 6388570603520/6004295680 bytes/packets received and 0 packets dropped\n[info] [default_bench_op_rx.h:61] ANO benchmark RX op shutting down\n[info] [default_bench_op_rx.h:108] AdvNetworkingBenchDefaultRxOp::freeResources() start\n[info] [default_bench_op_rx.h:116] AdvNetworkingBenchDefaultRxOp::freeResources() complete\n[info] [gxf_executor.cpp:294] Destroying context\n</code></pre> <p>To inspect the speed the data is moving through the NIC, run <code>mlnx_perf</code> on one of the interfaces in a separate terminal, concurrently with the application running:</p> <pre><code>sudo mlnx_perf -i $if_name\n</code></pre> See an example output <p>On IGX with RTX A6000, we are able to hit close to the 100 Gbps linerate with this configuration: <pre><code>  rx_vport_unicast_packets: 11,614,900\n    rx_vport_unicast_bytes: 12,358,253,600 Bps   = 98,866.2 Mbps\n            rx_packets_phy: 11,614,847\n              rx_bytes_phy: 12,404,657,664 Bps   = 99,237.26 Mbps\n rx_1024_to_1518_bytes_phy: 11,614,936\n            rx_prio0_bytes: 12,404,738,832 Bps   = 99,237.91 Mbps\n          rx_prio0_packets: 11,614,923\n</code></pre></p> Troubleshooting EAL: failed to parse device <p>Make sure to set valid PCIe addresses in the <code>address</code> fields in <code>interfaces</code>, per instructions above.</p> Invalid MAC address format <p>Make sure to set a valid MAC address in the <code>eth_dst_addr</code> field in <code>bench_tx</code>, per instructions above.</p> mlx5_common: Fail to create MR for address [...] Could not DMA map EXT memory <p>Example error:</p> <pre><code>mlx5_common: Fail to create MR for address (0xffff2fc00000)\nmlx5_common: Device 0005:03:00.0 unable to DMA map\n[critical] [adv_network_dpdk_mgr.cpp:188] Could not DMA map EXT memory: -1 err=Invalid argument\n[critical] [adv_network_dpdk_mgr.cpp:430] Failed to map MRs\n</code></pre> <p>Make sure that <code>nvidia-peermem</code> is loaded.</p> EAL: Couldn't get fd on hugepage file [..] error allocating rte services array <p>Example error:</p> <pre><code>EAL: get_seg_fd(): open '/mnt/huge/nwlrbbmqbhmap_0' failed: Permission denied\nEAL: Couldn't get fd on hugepage file\nEAL: error allocating rte services array\nEAL: FATAL: rte_service_init() failed\nEAL: rte_service_init() failed\n</code></pre> <p>Ensure you run as root, using <code>sudo</code>.</p> EAL: Cannot get hugepage information. <pre><code>EAL: x hugepages of size x reserved, no mounted hugetlbfs found for that size\n</code></pre> <p>Ensure your hugepages are mounted.</p> <pre><code>EAL: No free x kB hugepages reported on node 0\n</code></pre> <ul> <li>Ensure you have allocated hugepages.</li> <li> <p>If you have already, check if they are any free left with <code>grep Huge /proc/meminfo</code>.</p> See an example output <p>No more space here!</p> <pre><code>HugePages_Total:       2\nHugePages_Free:        0\nHugePages_Rsvd:        0\nHugePages_Surp:        0\nHugepagesize:    1048576 kB\nHugetlb:         2097152 kB\n</code></pre> </li> <li> <p>If not, you can delete dangling hugepages under your hugepage mount point. That happens when your previous application run crashes.</p> <pre><code>sudo rm -rf /dev/hugepages/* # default mount point\nsudo rm -rf /mnt/huge/*      # custom mount point\n</code></pre> </li> </ul> Could not allocate x MB of GPU memory [...] Failed to allocate GPU memory <p>Check your GPU utilization:</p> <pre><code>nvidia-smi pmon -c 1\n</code></pre> <p>You might need to kill some of the listed processes to free up GPU VRAM.</p>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#5-building-your-own-application","title":"5. Building your own application","text":"<p>This section will guide you through building your own application using the <code>adv_networking_bench</code> as an example. Make sure to install <code>holoscan-networking</code> first.</p>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#51-understand-the-configuration-parameters","title":"5.1 Understand the configuration parameters","text":"<p>Note</p> <p>The configuration below will be analyzed in the context of the application consuming it, as defined in the <code>main.cpp</code> file. You can look it up when the \"sample application code\" is referenced.</p> Debian installationFrom source <pre><code>/opt/nvidia/holoscan/examples/adv_networking_bench/main.cpp\n</code></pre> <pre><code>./applications/adv_networking_bench/cpp/main.cpp\n</code></pre> <p>If you are not yet familiar with how Holoscan applications are constructed, please refer to the Holoscan SDK documentation first.</p> <p>Let's look at the <code>adv_networking_bench_default_tx_rx.yaml</code> file below. Click on the (1) icons below to expand explanations for each annotated line.</p> <ol> <li>The cake is a lie </li> </ol> <pre><code>scheduler: # (1)!\n  check_recession_period_ms: 0\n  worker_thread_number: 5\n  stop_on_deadlock: true\n  stop_on_deadlock_timeout: 500\n  # max_duration_ms: 20000\n\nadvanced_network: # (2)!\n  cfg:\n    version: 1\n    manager: \"dpdk\" # (3)!\n    master_core: 3 # (4)!\n    debug: false\n    log_level: \"info\"\n\n    memory_regions: # (5)!\n    - name: \"Data_TX_GPU\" # (6)!\n      kind: \"device\" # (7)!\n      affinity: 0 # (8)!\n      num_bufs: 51200 # (9)!\n      buf_size: 1064 # (10)!\n    - name: \"Data_RX_GPU\"\n      kind: \"device\"\n      affinity: 0\n      num_bufs: 51200\n      buf_size: 1000\n    - name: \"Data_RX_CPU\"\n      kind: \"huge\"\n      affinity: 0\n      num_bufs: 51200\n      buf_size: 64\n\n    interfaces: # (11)!\n    - name: \"tx_port\" # (12)!\n      address: &lt;0000:00:00.0&gt; # (13)! # The BUS address of the interface doing Tx\n      tx: # (14)!\n        queues: # (15)!\n        - name: \"tx_q_0\" # (16)!\n          id: 0 # (17)!\n          batch_size: 10240 # (18)!\n          cpu_core: 11 # (19)!\n          memory_regions: # (20)!\n            - \"Data_TX_GPU\"\n          offloads: # (21)!\n            - \"tx_eth_src\"\n    - name: \"rx_port\"\n      address: &lt;0000:00:00.0&gt; # (22)! # The BUS address of the interface doing Rx\n      rx:\n        flow_isolation: true # (23)!\n        queues:\n        - name: \"rx_q_0\"\n          id: 0\n          cpu_core: 9\n          batch_size: 10240\n          memory_regions: # (24)!\n            - \"Data_RX_CPU\"\n            - \"Data_RX_GPU\"\n        flows: # (25)!\n        - name: \"flow_0\" # (26)!\n          id: 0 # (27)!\n          action: # (28)!\n            type: queue\n            id: 0\n          match: # (29)!\n            udp_src: 4096\n            udp_dst: 4096\n            ipv4_len: 1050\n\nbench_rx: # (30)!\n  interface_name: \"rx_port\" # Name of the RX port from the advanced_network config\n  gpu_direct: true          # Set to true if using a GPU region for the Rx queues.\n  split_boundary: true      # Whether header and data are split for Rx (Header to CPU)\n  batch_size: 10240\n  max_packet_size: 1064\n  header_size: 64\n\nbench_tx: # (31)!\n  interface_name: \"tx_port\" # Name of the TX port from the advanced_network config\n  gpu_direct: true          # Set to true if using a GPU region for the Tx queues.\n  split_boundary: 0         # Byte boundary where header and data are split for Tx, 0 if no split\n  batch_size: 10240\n  payload_size: 1000\n  header_size: 64\n  eth_dst_addr: &lt;00:00:00:00:00:00&gt; # Destination MAC address - required when Rx flow_isolation=true\n  ip_src_addr: &lt;1.2.3.4&gt;    # Source IP address - required on layer 3 network\n  ip_dst_addr: &lt;5.6.7.8&gt;    # Destination IP address - required on layer 3 network\n  udp_src_port: 4096        # UDP source port\n  udp_dst_port: 4096        # UDP destination port\n</code></pre> <ol> <li>The <code>scheduler</code> section is passed to the multi threaded scheduler we declare in the <code>main()</code> function of this application. See the holoscan SDK documentation and API docs for more details. This is related to the Holoscan core library and is not specific to Holoscan Networking.</li> <li>The <code>advanced_network</code> section is passed to the <code>advanced_network::adv_net_init</code> which is responsible for setting up the NIC. That function should be called in your <code>Application::compose()</code> function.</li> <li><code>manager</code> is the backend networking library. default: <code>dpdk</code>. Other: <code>gpunetio</code> (DOCA GPUNet IO + DOCA Ethernet &amp; Flow). Coming soon: <code>rivermax</code>, <code>rdma</code>.</li> <li><code>master_core</code> is the ID of the CPU core used for setup. It does not need to be isolated, and is recommended to differ differ from the <code>cpu_core</code> fields below used for polling the NIC.</li> <li>The <code>memory_regions</code> section lists where the NIC will write/read data from/to when bypassing the OS kernel. Tip: when using GPU buffer regions, keeping the sum of their buffer sizes lower than 80% of your BAR1 size is generally a good rule of thumb \ud83d\udc4d.</li> <li>A descriptive name for that memory region to refer to later in the <code>interfaces</code> section.</li> <li>The type of memory region. Best options are <code>device</code> (GPU), or <code>huge</code> (pages - CPU). Also supported but not recommended are <code>malloc</code> (CPU) and <code>pinned</code> (CPU).</li> <li>The GPU ID for <code>device</code> memory regions. The NUMA node ID for CPU memory regions.</li> <li>The number of buffers in the memory region. A higher value means more time to process the data, but it takes additional space on the GPU BAR1. Too low increases the risk of dropping packets from the NIC having nowhere to write (Rx) or the risk of higher latency from buffering (Tx). Need a rule of thumb \ud83d\udc4d? 5x the <code>batch_size</code> below is a good starting point.</li> <li>The size of each buffer in the memory region. These should be equal to your maximum packet size, or less if breaking down packets (ex: header data split, see the <code>rx</code> queue below).</li> <li>The <code>interfaces</code> section lists the NIC interfaces that will be configured for the application.</li> <li>A descriptive name for that interface, currently only used for logging.</li> <li>The PCIe/bus address of that interface, as identified in previous sections.</li> <li>Each interface can have a <code>tx</code> (transmitting) or <code>rx</code> (receiving) section, or both if you'd like to configure both Tx and Rx on the same interface.</li> <li>The <code>queues</code> section lists the queues for that interface. Queues are a core concept of NICs: they handle the actual receiving or transmitting of network packets. Rx queues buffer incoming packets until they can be processed by the application, while Tx queues hold outgoing packets waiting to be sent on the network. The simplest setup uses only one receive and one transmit queue. Using more queues allows multiple streams of network traffic to be processed in parallel, as each queue can be assigned to a specific CPU core, and are assigned their own memory regions that are not shared.</li> <li>A descriptive name for that queue, currently only used for logging.</li> <li>The ID of that queue, which can be referred to later in the <code>flows</code> section.</li> <li>The number of packets per batch (or burst). Your Rx operator will have access to packets from the NIC when it receives enough packets for a whole batch/burst. Your Tx operator needs to ensure it does not send more packets than this value on each <code>Operator::compute()</code> call.</li> <li>The ID of the CPU core that this queue will use to poll the NIC. Ideally one isolated core per queue.</li> <li>The list of memory regions where this queue will write/read packets from/to. The order matters: the first memory region will be used first to read/write from until it fills up one buffer (<code>buf_size</code>), after which it will move to the next region in the list and so on until the packet is fully written/read. See the <code>memory_regions</code> for the <code>rx</code> queue below for an example.</li> <li>The <code>offloads</code> section (Tx queues only) lists optional tasks that can be offloaded to the NIC. The only value currently supported is <code>tx_eth_src</code>, that lets the NIC insert the ethernet source mac address in the packet headers. Note: IP, UDP, and Ethernet Checksums or CRC are always done by the NIC currently and are not optional.</li> <li>Same as for <code>tx_port</code>. Each interface in this list should have a unique mac address. This one will do <code>rx</code> per config below.</li> <li>Whether to isolate the Rx flow. If true, any incoming packets that does not match the MAC address of this interface - or isn't directed to a queue when the <code>flows</code> section below is used - will be delegated back to Linux for processing (no kernel bypass). This is useful to let this interface handle ARP, ICMP, etc. Otherwise, any packets sent to this interface (ex: ping) will need to be processed (or dropped) by your application.</li> <li>This scenario is called HDS (Header-Data Split): the packet will first be written to a buffer in the <code>Data_RX_CPU</code> memory region, filling its <code>buf_size</code> of 64 bytes - which is consistent with the size of our header - then the rest of the packet will be written to the <code>Data_RX_GPU</code> memory region. Its <code>buf_size</code> of 1000 bytes is just what we need to write the payload size for our application, no byte wasted!</li> <li>The list of flows. Flows are responsible for routing packets to the correct queue based on various properties. If this field is missing, all packets will be routed to the first queue.</li> <li>The flow name, currently only used for logging.</li> <li>The flow <code>id</code> is used to tag the packets with what flow it arrived on. This is useful when sending multiple flows to a single queue, as the user application can differentiate which flow (i.e. rules) matched the packet based on this ID.</li> <li>What to do with packets that match this flow. The only supported action currently is <code>type: queue</code> to send the packet to a queue given its <code>id</code>.</li> <li>List of rules to match packets against. All rules must be met for a packet to match the flow. Currently supported rules include <code>udp_src</code> and <code>udp_dst</code> (port numbers), <code>ipv4_len</code> (#TODO#) etc.</li> <li>The <code>bench_rx</code> section is passed to the <code>AdvNetworkingBenchDefaultRxOp</code> operator in the <code>Application::compose()</code> function of the sample application. This operator is a custom operator implemented in <code>default_bench_op_rx.h</code> that pulls and aggregates packets received from the NIC, with parameters specific to its own implementation, which can be used as a reference for your own Rx operator. The first parameter, <code>interface_name</code>, is used to specify which NIC interface to use for the Rx operation. The following parameters are should align with how <code>memory_regions</code> and <code>queues</code> were configured for the <code>rx</code> interface.</li> <li>The <code>bench_tx</code> section is passed to the <code>AdvNetworkingBenchDefaultTxOp</code> operator in the <code>Application::compose()</code> function of the sample application. This operator is a custom operator implemented in <code>default_bench_op_tx.h</code> that generates dummy packets to send to the NIC, with parameters specific to its own implementation, which can be used as a reference for your own Tx operator. The first parameter, <code>interface_name</code>, is used to specify which NIC interface to use for the Tx operation. The following parameters up to <code>header_size</code> should align with how <code>memory_regions</code> and <code>queues</code> were configured for the <code>tx</code> interface. The remaining parameters are used to fill-in the ethernet header of the packets (ETH, IP, UDP).</li> </ol>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#52-create-your-own-rx-operator","title":"5.2 Create your own Rx operator","text":"<p>Under construction</p> <p>This section is under construction. Refer to the implementation of the <code>AdvNetworkingBenchDefaultRxOp</code> for an example.</p> Debian installationFrom source <pre><code>/opt/nvidia/holoscan/examples/adv_networking_bench/default_bench_op_rx.h\n</code></pre> <pre><code>./applications/adv_networking_bench/cpp/default_bench_op_rx.h\n</code></pre> <p>Note</p> <p>Design investigations are expected soon for a generic packet aggregator operator.</p>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#53-create-your-own-tx-operator","title":"5.3 Create your own Tx operator","text":"<p>Under construction</p> <p>This section is under construction. Refer to the implementation of the <code>AdvNetworkingBenchDefaultTxOp</code> for an example.</p> Debian installationFrom source <pre><code>/opt/nvidia/holoscan/examples/adv_networking_bench/default_bench_op_tx.h\n</code></pre> <pre><code>./applications/adv_networking_bench/cpp/default_bench_op_tx.h\n</code></pre> <p>Note</p> <p>Designs investigations are expected soon for a generic way to prepare packets to send to the NIC.</p>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/#54-build-with-cmake","title":"5.4 Build with CMake","text":"Debian installationFrom source <ol> <li>Create a source directory and write your source file(s) for your application and custom operators.</li> <li> <p>Create a <code>CMakeLists.txt</code> file in your source directory like this one:</p> <pre><code>cmake_minimum_required(VERSION 3.20)\nproject(my_app CXX) # Add CUDA if writing .cu kernels\n\nfind_package(holoscan 2.6 REQUIRED CONFIG PATHS \"/opt/nvidia/holoscan\")\nfind_package(holoscan-networking REQUIRED CONFIG PATHS \"/opt/nvidia/holoscan\")\n\n# Create an executable\nadd_executable(my_app\n    my_app.cpp\n    ...\n)\ntarget_include_directories(my_app\n    PRIVATE\n        my_include_dirs/\n        ...\n)\ntarget_link_libraries(my_app\n    PRIVATE\n        holoscan::core\n        holoscan::ops::advanced_network_rx\n        holoscan::ops::advanced_network_tx\n        my_other_dependencies\n        ...\n)\n\n# Copy the config file to the build directory for convenience referring to it\nadd_custom_target(my_app_config_yaml\n    COMMAND ${CMAKE_COMMAND} -E copy_if_different \"${CMAKE_CURRENT_SOURCE_DIR}/my_app_config.yaml\" ${CMAKE_CURRENT_BINARY_DIR}\n    DEPENDS \"${CMAKE_CURRENT_SOURCE_DIR}/my_app_config.yaml\"\n)\nadd_dependencies(my_app my_app_config_yaml)\n</code></pre> </li> <li> <p>Build your application like so:</p> <pre><code># Your chosen paths\nsrc_dir=\".\"\nbuild_dir=\"build\"\n\n# Configure the build\ncmake -S \"$src_dir\" -B \"$build_dir\"\n\n# Build the application\ncmake --build \"$build_dir\" -j\n</code></pre> Failed to detect a default CUDA architecture. <p>Add the path to your installation of <code>nvcc</code> to your <code>PATH</code>, or pass its to the cmake configuration command like so (adjust to your CUDA/nvcc installation path):</p> <pre><code>cmake -S \"$src_dir\" -B \"$build_dir\" -D CMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc\n</code></pre> </li> <li> <p>Run your application like so:</p> <pre><code>\"./$build_dir/my_app my_app_config.yaml\"\n</code></pre> </li> </ol> <ol> <li>Create an application directory under <code>applications/</code> in your clone of the HoloHub repository, and write your source file(s) for your application and custom operators.</li> <li> <p>Add the following to the <code>application/CMakeLists.txt</code> file:</p> <pre><code>add_holohub_application(my_app DEPENDS OPERATORS advanced_network)\n</code></pre> </li> <li> <p>Create a <code>CMakeLists.txt</code> file in your application directory like this one:</p> <pre><code>cmake_minimum_required(VERSION 3.20)\nproject(my_app CXX) # Add CUDA if writing .cu kernels\n\nfind_package(holoscan 2.6 REQUIRED CONFIG PATHS \"/opt/nvidia/holoscan\")\n\n# Create an executable\nadd_executable(my_app\n    my_app.cpp\n    ...\n)\ntarget_include_directories(my_app\n    PRIVATE\n        my_include_dirs/\n        ...\n)\ntarget_link_libraries(my_app\n    PRIVATE\n        holoscan::core\n        holoscan::ops::advanced_network_rx\n        holoscan::ops::advanced_network_tx\n        my_other_dependencies\n        ...\n)\n\n# Copy the config file to the build directory for convenience referring to it\nadd_custom_target(my_app_config_yaml\n    COMMAND ${CMAKE_COMMAND} -E copy_if_different \"${CMAKE_CURRENT_SOURCE_DIR}/my_app_config.yaml\" ${CMAKE_CURRENT_BINARY_DIR}\n    DEPENDS \"${CMAKE_CURRENT_SOURCE_DIR}/my_app_config.yaml\"\n)\nadd_dependencies(my_app my_app_config_yaml)\n</code></pre> </li> <li> <p>Build your application like so:</p> <pre><code>./dev_container build_and_run my_app --no_run\n</code></pre> </li> <li> <p>Run your application like so:</p> <pre><code>./dev_container launch --img holohub:my_app --docker_opts \"-u 0 --privileged\" --bash -c \"./build/my_app/applications/my_app my_app_config.yaml\"\n</code></pre> <p>or, if you have set up a shortcut to run your application with its config file through its <code>metadata.json</code> (see other apps for examples):</p> <pre><code>./dev_container build_and_run --no_build --container_args \" -u 0 --privileged\"\n</code></pre> </li> </ol>","tags":["DPDK","RDMA","GPUNetIO","GPUDirect","Networking and Distributed Computing","HPC"]},{"location":"tutorials/high_performance_networking/0.1/","title":"High Performance Networking with Holoscan","text":"<p> Authors: Alexis Girault (NVIDIA) Supported platforms: x86_64, SBSA, IGX Orin (dGPU) Language: C++ Last modified: March 17, 2025 Archive version: 0.1 Minimum Holoscan SDK version: 3.0.0 Tested Holoscan SDK versions: 3.0.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This tutorial demonstrates how to use the advanced networking Holoscan operator (often referred to as ANO or <code>advanced_network</code> in HoloHub) for low latency and high throughput communication through NVIDIA SmartNICs. With a properly tuned system, the advanced network operator can achieve hundreds of Gbps with latencies in the low microseconds.</p> <p>Note</p> <p>This solution is designed for users who want to create a Holoscan application that will interface with an external system or sensor over Ethernet.</p> <ul> <li>For high performance communication with systems also running Holoscan, refer to the Holoscan distributed application documentation instead.</li> <li>For JESD-compliant sensor without Ethernet support, consider the Holoscan Sensor Bridge for an FPGA-based interface to Holoscan.</li> </ul>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#prerequisites","title":"Prerequisites","text":"<p>Achieving High Performance Networking with Holoscan requires a system with an NVIDIA SmartNIC and a discrete GPU. That is the case of NVIDIA Data Center systems, or edge systems like the NVIDIA IGX platform and the NVIDIA Project DIGITS. <code>x86_64</code> systems equipped with these components are also supported, though the performance will vary greatly depending on the PCIe topology of the system (more on this below).</p> <p>In this tutorial, we will be developing on an NVIDIA IGX Orin platform with IGX SW 1.1 and an NVIDIA RTX 6000 ADA GPU, which is the configuration that is currently actively tested. The concepts should be applicable to other systems based on Ubuntu 22.04 as well. It should also work on other Linux distributions with a glibc version of 2.35 or higher by containerizing the dependencies and applications on top of an Ubuntu 22.04 image, but this is not actively tested at this time.</p> <p>Secure boot conflict</p> <p>If you have secure boot enabled on your system, you might need to disable it as a prerequisite to run some of the configurations below (switching the NIC link layers to Ethernet, updating the MRRS of your NIC ports, updating the BAR1 size of your GPU). Secure boot can be re-enabled after the configurations are completed.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#background","title":"Background","text":"<p>Achieving high performance networking is a complex problem that involves many system components and configurations which we will cover in this tutorial. Two of the core concepts to achieve this are named Kernel Bypass, and GPUDirect.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#kernel-bypass","title":"Kernel Bypass","text":"<p>In this context, Kernel Bypass refers to bypassing the operating system's kernel to directly communicate with the network interface (NIC), greatly reducing the latency and overhead of the Linux network stack. There are multiple technologies that achieve this in different fashions. They're all Ethernet-based, but differ in their implementation and features. The goal of the <code>advanced_network</code> operator in Holoscan Networking is to provide a common higher-level interface to all these backends:</p> <ul> <li>RDMA: Remote Direct Memory Access, using the open-source <code>rdma-core</code> library. It differs from the other Ethernet-based backends with its server/client model and RoCE (RDMA over Ethernet) protocol. Given the extra cost and complexity to setup on both ends, it offers a simpler user interface, orders packets on arrival, and is the only one to offer a high reliability mode.</li> <li>DPDK: the Data Plane Development Kit is an open-source project part of the Linux Foundation with a strong and long-lasting community support. Its RTE Flow capability is generally considered the most flexible solution to split packets ingress and egress data.</li> <li>DOCA GPUNetIO: This NVIDIA proprietary technology differs from the other backends by transmitting and receiving packets from the NIC using a GPU kernel instead of CPU code, which is highly beneficial for CPU-bound applications.</li> <li>NVIDIA Rivermax: NVIDIA's other proprietary kernel bypass technology. For a license fee, it should offer the lowest latency and lowest resource utilization for video streaming (RTP packets).</li> </ul> Work In Progress <p>The Holoscan Advanced Networking Operator integration testing infrastructure is under active development. As such:</p> <ul> <li>The DPDK backend is supported and distributed with the <code>holoscan-networking</code> package, and is the only backend actively tested at this time.</li> <li>The DOCA GPUNetIO backend is supported and distributed with the <code>holoscan-networking</code> package, with testing infrastructure under development.</li> <li>The NVIDIA Rivermax backend is supported for Rx only when building from source, but not yet distributed nor actively tested. Tx support is under development.</li> <li>The RDMA backend is under active development and should be available soon.</li> </ul> <p>Which backend is best for your use case will depend on multiple factors, such as packet size, batch size, data type, and more. The goal of the Advanced Networking Operator is to abstract the interface to these backends, allowing developers to focus on the application logic and experiment with different configurations to identify the best technology for their use case.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#gpudirect","title":"GPUDirect","text":"<p><code>GPUDirect</code> allows the NIC to read and write data from/to a GPU without requiring to copy the data the system memory, decreasing CPU overheads and significantly reducing latency. An implementation of <code>GPUDirect</code> is supported by all the kernel bypass backends listed above.</p> <p>Warning</p> <p><code>GPUDirect</code> is only supported on Workstation/Quadro/RTX GPUs and Data Center GPUs. It is not supported on GeForce cards.</p> How does that relate to peermem or dma-buf? <p>There are two interfaces to enable <code>GPUDirect</code>:</p> <ul> <li>The <code>nvidia-peermem</code> kernel module, distributed with the NVIDIA DKMS GPU drivers.<ul> <li>Supported on Ubuntu kernels 5.4+, deprecated starting with kernel 6.8.</li> <li>Supported on NVIDIA optimized Linux kernels, including IGX OS and DGX OS.</li> <li>Supported by all MOFED drivers (requires rebuilding nvidia-dkms drivers afterwards).</li> </ul> </li> <li><code>DMA Buf</code>, supported on Linux kernels 5.12+ with NVIDIA open-source drivers 515+ and CUDA toolkit 11.7+.</li> </ul>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#1-installing-holoscan-networking","title":"1. Installing Holoscan Networking","text":"<p>We'll start with installing the <code>holoscan-networking</code> package, as it provides some utilities to help tune the system, and requires some dependencies which will help us with the system setup.</p> <p>First, add the DOCA apt repository which holds some of its dependencies:</p> IGX OS 1.1SBSA (Ubuntu 22.04)x86_64 (Ubuntu 22.04) <pre><code>export DOCA_URL=\"https://linux.mellanox.com/public/repo/doca/2.10.0/ubuntu22.04/arm64-sbsa/\"\nwget -qO- https://linux.mellanox.com/public/repo/doca/GPG-KEY-Mellanox.pub | gpg --dearmor - | sudo tee /etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub &gt; /dev/null\necho \"deb [signed-by=/etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub] $DOCA_URL ./\"  | sudo tee /etc/apt/sources.list.d/doca.list &gt; /dev/null\n\nsudo apt update\n</code></pre> <pre><code>export DOCA_URL=\"https://linux.mellanox.com/public/repo/doca/2.10.0/ubuntu22.04/arm64-sbsa/\"\nwget -qO- https://linux.mellanox.com/public/repo/doca/GPG-KEY-Mellanox.pub | gpg --dearmor - | sudo tee /etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub &gt; /dev/null\necho \"deb [signed-by=/etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub] $DOCA_URL ./\"  | sudo tee /etc/apt/sources.list.d/doca.list &gt; /dev/null\n\n# Also need the CUDA repository for holoscan: https://developer.nvidia.com/cuda-downloads?target_os=Linux\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\n\nsudo apt update\n</code></pre> <pre><code>export DOCA_URL=\"https://linux.mellanox.com/public/repo/doca/2.10.0/ubuntu22.04/x86_64/\"\nwget -qO- https://linux.mellanox.com/public/repo/doca/GPG-KEY-Mellanox.pub | gpg --dearmor - | sudo tee /etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub &gt; /dev/null\necho \"deb [signed-by=/etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub] $DOCA_URL ./\"  | sudo tee /etc/apt/sources.list.d/doca.list &gt; /dev/null\n\n# Also need the CUDA repository for holoscan: https://developer.nvidia.com/cuda-downloads?target_os=Linux\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\n\nsudo apt update\n</code></pre> <p>You can then install <code>holoscan-networking</code>:</p> Debian installationFrom source <pre><code>sudo apt install -y holoscan-networking\n</code></pre> <p>You can build the Holoscan Networking libraries and sample applications from source on HoloHub:</p> <pre><code>git clone git@github.com:nvidia-holoscan/holohub.git\ncd holohub\n./dev_container build_and_install holoscan-networking   # Installed in ./install\n</code></pre> <p>If you'd like to generate the debian package from source and install it to ensure all dependencies are then present on your system, you can run:</p> <pre><code>./dev_container build_and_package holoscan-networking\nsudo apt-get install ./holoscan-networking_*.deb        # Installed in /opt/nvidia/holoscan\n</code></pre> <p>Refer to the HoloHub README for more information.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#2-required-system-setup","title":"2. Required System Setup","text":"","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#21-check-your-nic-drivers","title":"2.1 Check your NIC drivers","text":"<p>Ensure your NIC drivers are loaded:</p> <pre><code>lsmod | grep ib_core\n</code></pre> See an example output <p>This would be an expected output, where <code>ib_core</code> is listed on the left.</p> <pre><code>ib_core               442368  8 rdma_cm,ib_ipoib,iw_cm,ib_umad,rdma_ucm,ib_uverbs,mlx5_ib,ib_cm\nmlx_compat             20480  11 rdma_cm,ib_ipoib,mlxdevm,iw_cm,ib_umad,ib_core,rdma_ucm,ib_uverbs,mlx5_ib,ib_cm,mlx5_core\n</code></pre> <p>If this is empty, install the latest OFED drivers from DOCA (the DOCA APT repository should already be configured from the Holoscan Networking installation above), and reboot your system:</p> <pre><code>sudo apt update\nsudo apt install doca-ofed\nsudo reboot\n</code></pre> <p>Note</p> <p>If this is not empty, you can still install the newest OFED drivers from <code>doca-ofed</code> above. If you choose to keep your current drivers, install the following utilities for convenience later on. They include tools like <code>ibstat</code>, <code>ibv_devinfo</code>, <code>ibdev2netdev</code>, <code>mlxconfig</code>:</p> <pre><code>sudo apt update\nsudo apt install infiniband-diags ibverbs-utils mlnx-ofed-kernel-utils mft\n</code></pre> <p>Also upgrade the user space libraries to make sure your tools have all the symbols they need:</p> <pre><code>sudo apt install libibverbs1 librdmacm1 rdma-core\n</code></pre> <p>Running <code>ibstat</code> or <code>ibv_devinfo</code> will confirm your NIC interfaces are recognized by your drivers.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#22-switch-your-nic-link-layers-to-ethernet","title":"2.2 Switch your NIC Link Layers to Ethernet","text":"<p>NVIDIA SmartNICs can function in two separate modes (called link layer):</p> <ul> <li>Ethernet (ETH)</li> <li>Infiniband (IB)</li> </ul> <p>To identify the current mode, run <code>ibstat</code> or <code>ibv_devinfo</code> and look for the <code>Link Layer</code> value.</p> <pre><code>ibv_devinfo\n</code></pre> Couldn't load driver 'libmlx5-rdmav34.so' <p>If you see an error like this, you might have different versions for your OFED tools and libraries. Attempt after upgrading your user space libraries to match the version of your OFED tools like so:</p> <pre><code>sudo apt update\nsudo apt install libibverbs1 librdmacm1 rdma-core\n</code></pre> See an example output <p>In the example below, the <code>mlx5_0</code> interface is in Ethernet mode, while the <code>mlx5_1</code> interface is in Infiniband mode. Do not pay attention to the <code>transport</code> value which is always <code>InfiniBand</code>.</p> <pre><code>hca_id: mlx5_0\n        transport:                      InfiniBand (0)\n        fw_ver:                         28.38.1002\n        node_guid:                      48b0:2d03:00f4:07fb\n        sys_image_guid:                 48b0:2d03:00f4:07fb\n        vendor_id:                      0x02c9\n        vendor_part_id:                 4129\n        hw_ver:                         0x0\n        board_id:                       NVD0000000033\n        phys_port_cnt:                  1\n                port:   1\n                        state:                  PORT_ACTIVE (4)\n                        max_mtu:                4096 (5)\n                        active_mtu:             4096 (5)\n                        sm_lid:                 0\n                        port_lid:               0\n                        port_lmc:               0x00\n                        link_layer:             Ethernet\n\nhca_id: mlx5_1\n        transport:                      InfiniBand (0)\n        fw_ver:                         28.38.1002\n        node_guid:                      48b0:2d03:00f4:07fc\n        sys_image_guid:                 48b0:2d03:00f4:07fb\n        vendor_id:                      0x02c9\n        vendor_part_id:                 4129\n        hw_ver:                         0x0\n        board_id:                       NVD0000000033\n        phys_port_cnt:                  1\n                port:   1\n                        state:                  PORT_ACTIVE (4)\n                        max_mtu:                4096 (5)\n                        active_mtu:             4096 (5)\n                        sm_lid:                 0\n                        port_lid:               0\n                        port_lmc:               0x00\n                        link_layer:             InfiniBand\n</code></pre> <p>For Holoscan Networking, we want the NIC to use the ETH link layer. To switch the link layer mode, there are two possible options:</p> <ol> <li>On IGX Orin developer kits, you can switch that setting through the BIOS: see IGX Orin documentation.</li> <li> <p>On any system with a NVIDIA NIC (including the IGX Orin developer kits), you can run the commands below from a terminal:</p> <ol> <li> <p>Identify the PCI address of your NVIDIA NIC</p> ibdev2netdevlspci <pre><code>nic_pci=$(sudo ibdev2netdev -v | awk '{print $1}' | head -n1)\n</code></pre> <pre><code># `0200` is the PCI-SIG class code for Ethernet controllers\n# `0207` is the PCI-SIG class code for Infiniband controllers\n# `15b3` is the Vendor ID for Mellanox\nnic_pci=$(lspci -n | awk '($2 == \"0200:\" || $2 == \"0207:\") &amp;&amp; $3 ~ /^15b3:/ {print $1; exit}')\n</code></pre> </li> <li> <p>Set both link layers to Ethernet. <code>LINK_TYPE_P1</code> and <code>LINK_TYPE_P2</code> are for <code>mlx5_0</code> and <code>mlx5_1</code> respectively. You can choose to only set one of them. <code>ETH</code> or <code>2</code> is Ethernet mode, and <code>IB</code> or <code>1</code> is for InfiniBand.</p> <pre><code>sudo mlxconfig -d $nic_pci set LINK_TYPE_P1=ETH LINK_TYPE_P2=ETH\n</code></pre> <p>Apply with <code>y</code>.</p> See an example output <pre><code>Device #1:\n----------\n\nDevice type:    ConnectX7\nName:           P3740-B0-QSFP_Ax\nDescription:    NVIDIA Prometheus P3740 ConnectX-7 VPI PCIe Switch Motherboard; 400Gb/s; dual-port QSFP; PCIe switch5.0 X8 SLOT0 ;X16 SLOT2; secure boot;\nDevice:         0005:03:00.0\n\nConfigurations:                                      Next Boot       New\n        LINK_TYPE_P1                                ETH(2)          ETH(2)\n        LINK_TYPE_P2                                IB(1)           ETH(2)\n\nApply new Configuration? (y/n) [n] :\ny\n\nApplying... Done!\n-I- Please reboot machine to load new configurations.\n</code></pre> <ul> <li><code>Next Boot</code> is the current value that was expected to be used at the next reboot.</li> <li><code>New</code> is the value you're about to set to override <code>Next Boot</code>.</li> </ul> ERROR: write counter to semaphore: Operation not permitted <p>Disable secure boot on your system ahead of changing the link type of your NIC ports. It can be re-enabled afterwards.</p> </li> <li> <p>Reboot your system.</p> <pre><code>sudo reboot\n</code></pre> </li> </ol> </li> </ol>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#23-configure-the-ip-addresses-of-the-nic-ports","title":"2.3 Configure the IP addresses of the NIC ports","text":"<p>First, we want to identify the logical names of your NIC interfaces. Connecting an SFP cable in just one of the ports of the NIC will help you identify which port is which. Run the following command once the cable is in place:</p> <pre><code>ibdev2netdev\n</code></pre> See an example output <p>In the example below, only <code>mlx5_1</code> has a cable connected (<code>Up</code>), and its logical ethernet name is <code>eth1</code>:</p> <pre><code>$ ibdev2netdev\nmlx5_0 port 1 ==&gt; eth0 (Down)\nmlx5_1 port 1 ==&gt; eth1 (Up)\n</code></pre> ibdev2netdev does not show the NIC <p>If you have a cable connected but it does not show Up/Down in the output of <code>ibdev2netdev</code>, you can try to parse the output of <code>dmesg</code> instead. The example below shows that <code>0005:03:00.1</code> is plugged, and that it is associated with <code>eth1</code>:</p> <pre><code>$ sudo dmesg | grep -w mlx5_core\n...\n[   11.512808] mlx5_core 0005:03:00.0 eth0: Link down\n[   11.640670] mlx5_core 0005:03:00.1 eth1: Link down\n...\n[ 3712.267103] mlx5_core 0005:03:00.1: Port module event: module 1, Cable plugged\n</code></pre> <p>The next step is to set a static IP on the interface you'd like to use so you can refer to it in your Holoscan applications. First, check if you already have any addresses configured using the ethernet interface names identified above (in our case, <code>eth0</code> and <code>eth1</code>):</p> <pre><code>ip -f inet addr show eth0\nip -f inet addr show eth1\n</code></pre> <p>If nothing appears, or you'd like to change the address, you can set an IP address through the Network Manager user interface, CLI (<code>nmcli</code>), or other IP configuration tools. In the example below, we configure the <code>eth0</code> interface with an address of <code>1.1.1.1/24</code>, and the <code>eth1</code> interface with an address of <code>2.2.2.2/24</code>.</p> One-timePersistent <pre><code>sudo ip addr add 1.1.1.1/24 dev eth0\nsudo ip addr add 2.2.2.2/24 dev eth1\n</code></pre> <p>Set these variables to your desired values:</p> <pre><code>if_name=eth0\nif_static_ip=1.1.1.1/24\n</code></pre> NetworkManagersystemd-networkd <p>Update the IP with <code>nmcli</code>:</p> <pre><code>sudo nmcli connection modify $if_name ipv4.addresses $if_static_ip\nsudo nmcli connection up $if_name\n</code></pre> <p>Create a network config file with the static IP:</p> <pre><code>cat &lt;&lt; EOF | sudo tee /etc/systemd/network/20-$if_name.network\n[Match]\nMACAddress=$(cat /sys/class/net/$if_name/address)\n\n[Network]\nAddress=$if_static_ip\nEOF\n</code></pre> <p>Apply now:</p> <pre><code>sudo systemctl restart systemd-networkd\n</code></pre> <p>Note</p> <p>If you are connecting the NIC to another NIC with an interconnect, do the same on the other system with an IP address on the same network segment. For example, to communicate with <code>1.1.1.1/24</code> above (<code>/24</code> -&gt; <code>255.255.255.0</code> submask), setup your other system with an IP between <code>1.1.1.2</code> and <code>1.1.1.254</code>, and the same <code>/24</code> submask.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#24-enable-gpudirect","title":"2.4 Enable GPUDirect","text":"<p>Assuming you already have NVIDIA drivers installed, check if the <code>nvidia_peermem</code> kernel module is loaded:</p> tune_system.py Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check topo\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check topo\n</code></pre> See an example output <pre><code>2025-03-12 14:15:07 - INFO - GPU 0: NVIDIA RTX A6000 has GPUDirect support.\n2025-03-12 14:15:27 - INFO - nvidia-peermem module is loaded.\n</code></pre> <pre><code>lsmod | grep nvidia_peermem\n</code></pre> <p>If it's not loaded, run the following command, then check again:</p> One-timePersistent <pre><code>sudo modprobe nvidia_peermem\n</code></pre> <pre><code>sudo echo \"nvidia-peermem\" &gt;&gt; /etc/modules\nsudo systemctl restart systemd-modules-load.service\n</code></pre> Error loading the <code>nvidia-peermem</code> kernel module <p>If you run into an error loading the <code>nvidia-peermem</code> kernel module, follow these steps:</p> <ol> <li>Install the <code>doca-ofed</code> package to get the latest drivers for your NIC as documented above.</li> <li>Restart your system.</li> <li>Rebuild your NVIDIA drivers with DKMS like so:</li> </ol> <pre><code>peermem_ko=$(find /lib/modules/$(uname -r) -name \"*peermem*.ko\")\nnv_dkms=$(dpkg -S \"$peermem_ko\" | cut -d: -f1)\nsudo dpkg-reconfigure $nv_dkms\nsudo modprobe nvidia_peermem\n</code></pre> Why peermem and not dma buf? <p><code>peermem</code> is currently the only GPUDirect interface supported by all our networking backends. This section will therefore provide instructions for <code>peermem</code> and not <code>dma buf</code>.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#3-optimal-system-configurations","title":"3. Optimal system configurations","text":"<p>Advanced</p> <p>The section below is for advanced users looking to extract more performance out of their system. You can choose to skip this section and return to it later if performance if your application is not satisfactory.</p> <p>While the configurations above are the minimum requirements to get a NIC and a NVIDIA GPU to communicate while bypassing the OS kernel stack, performance can be further improved in most scenarios by tuning the system as described below.</p> <p>Before diving in each of the setups below, we provide a utility script as part of the <code>holoscan-networking</code> package which provides an overview of the configurations that potentially need to be tuned on your system.</p> Work In Progress <p>This utility script is under active development and will be updated in future releases with additional checks, more actionable recommendations, and automated tuning.</p> Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check all\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check all\n</code></pre> See an example output <p>Our tuned-up IGX system with A6000 can optimize most settings:</p> <pre><code>2025-03-12 14:16:06 - INFO - CPU 0: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 1: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 2: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 3: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 4: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 5: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 6: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 7: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 8: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 9: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 10: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 11: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - cx7_0/0005:03:00.0: MRRS is correctly set to 4096.\n2025-03-12 14:16:06 - INFO - cx7_1/0005:03:00.1: MRRS is correctly set to 4096.\n2025-03-12 14:16:06 - WARNING - cx7_0/0005:03:00.0: PCIe Max Payload Size is not set to 256 bytes. Found: 128 bytes.\n2025-03-12 14:16:06 - WARNING - cx7_1/0005:03:00.1: PCIe Max Payload Size is not set to 256 bytes. Found: 128 bytes.\n2025-03-12 14:16:06 - INFO - HugePages_Total: 3\n2025-03-12 14:16:06 - INFO - HugePage Size: 1024.00 MB\n2025-03-12 14:16:06 - INFO - Total Allocated HugePage Memory: 3072.00 MB\n2025-03-12 14:16:06 - INFO - Hugepages are sufficiently allocated with at least 500 MB.\n2025-03-12 14:16:06 - INFO - GPU 0: SM Clock is correctly set to 1920 MHz (within 500 of the 2100 MHz theoretical Max).\n2025-03-12 14:16:06 - INFO - GPU 0: Memory Clock is correctly set to 8000 MHz.\n2025-03-12 14:16:06 - INFO - GPU 00000005:09:00.0: BAR1 size is 8192 MiB.\n2025-03-12 14:16:06 - INFO - GPU GPU0 has at least one PIX/PXB connection to a NIC\n2025-03-12 14:16:06 - INFO - isolcpus found in kernel boot line\n2025-03-12 14:16:06 - INFO - rcu_nocbs found in kernel boot line\n2025-03-12 14:16:06 - INFO - irqaffinity found in kernel boot line\n2025-03-12 14:16:06 - INFO - Interface cx7_0 has an acceptable MTU of 9000 bytes.\n2025-03-12 14:16:06 - INFO - Interface cx7_1 has an acceptable MTU of 9000 bytes.\n2025-03-12 14:16:06 - INFO - GPU 0: NVIDIA RTX A6000 has GPUDirect support.\n2025-03-12 14:16:06 - INFO - nvidia-peermem module is loaded.\n</code></pre> <p>Based on the results, you can figure out which of the sections below are appropriate to update configurations on your system.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#31-ensure-ideal-pcie-topology","title":"3.1 Ensure ideal PCIe topology","text":"<p>Kernel bypass and GPUDirect rely on PCIe to communicate between the GPU and the NIC at high speeds. As-such, the topology of the PCIe tree on a system is critical to ensure optimal performance.</p> <p>Run the following command to check the GPUDirect communication matrix. You are looking for a <code>PXB</code> or <code>PIX</code> connection between the GPU and the NIC interfaces to get the best performance.</p> tune_system.pynvidia-smi Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check topo\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check topo\n</code></pre> See an example output <p>On IGX developer kits, the board's internal switch is designed to connect the GPU to the NIC interfaces with a <code>PXB</code> connection, offering great performance.</p> <pre><code>2025-03-06 12:07:45 - INFO - GPU GPU0 has at least one PIX/PXB connection to a NIC\n</code></pre> <pre><code>nvidia-smi topo -mp\n</code></pre> See an example output <p>On IGX developer kits, the board's internal switch is designed to connect the GPU to the NIC interfaces with a <code>PXB</code> connection, offering great performance. <pre><code>        GPU0    NIC0    NIC1    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PXB     PXB     0-11    0               N/A\nNIC0    PXB      X      PIX\nNIC1    PXB     PIX      X\n\nLegend:\n\nX    = Self\nSYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\nNODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\nPHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\nPXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\nPIX  = Connection traversing at most a single PCIe bridge\n\nNIC Legend:\n\nNIC0: mlx5_0\nNIC1: mlx5_1\n</code></pre></p> <p>If your connection is not optimal, you might be able to improve it by moving your NIC and/or GPU on a different PCIe port, so that they can share a branch and do not require going back to the Host Bridge (the CPU) to communicate. Refer to your system manufacturer for documentation, or run the following command to inspect the topology of your system:</p> <pre><code>lspci -tv\n</code></pre> See an example output <p>Here is the PCIe tree of an IGX system. Note how the ConnectX-7 and RTX A6000 are connected to the same branch. <pre><code>-+-[0007:00]---00.0-[01-ff]----00.0  Marvell Technology Group Ltd. 88SE9235 PCIe 2.0 x2 4-port SATA 6 Gb/s Controller\n+-[0005:00]---00.0-[01-ff]----00.0-[02-09]--+-00.0-[03]--+-00.0  Mellanox Technologies MT2910 Family [ConnectX-7]\n|                                           |            \\-00.1  Mellanox Technologies MT2910 Family [ConnectX-7]\n|                                           +-01.0-[04-06]----00.0-[05-06]----08.0-[06]--\n|                                           \\-02.0-[07-09]----00.0-[08-09]----00.0-[09]--+-00.0  NVIDIA Corporation GA102GL [RTX A6000]\n|                                                                                        \\-00.1  NVIDIA Corporation GA102 High Definition Audio Controller\n+-[0004:00]---00.0-[01-ff]----00.0  Sandisk Corp WD PC SN810 / Black SN850 NVMe SSD\n+-[0001:00]---00.0-[01-ff]----00.0-[02-fc]--+-01.0-[03-34]----00.0  Realtek Semiconductor Co., Ltd. RTL8111/8168/8411 PCI Express Gigabit Ethernet Controller\n|                                           +-02.0-[35-66]----00.0  Realtek Semiconductor Co., Ltd. RTL8111/8168/8411 PCI Express Gigabit Ethernet Controller\n|                                           +-03.0-[67-98]----00.0  Device 1c00:3450\n|                                           +-04.0-[99-ca]----00.0-[9a]--+-00.0  ASPEED Technology, Inc. ASPEED Graphics Family\n|                                           |                            \\-02.0  ASPEED Technology, Inc. Device 2603\n|                                           \\-05.0-[cb-fc]----00.0  Realtek Semiconductor Co., Ltd. RTL8822CE 802.11ac PCIe Wireless Network Adapter\n\\-[0000:00]-\n</code></pre></p> <p>x86_64 compatibility</p> <p>Most x86_64 systems are not designed for this topology as they lack a discrete PCIe switch. In that case, the best connection they can achieve is <code>NODE</code>.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#32-check-the-nics-pcie-configuration","title":"3.2 Check the NIC's PCIe configuration","text":"<p>Understanding PCIe Configuration for Maximum Performance - May 27, 2022</p> <p>PCIe is used in any system for communication between different modules [including the NIC and the GPU]. This means that in order to process network traffic, the different devices communicating via the PCIe should be well configured. When connecting the network adapter to the PCIe, it auto-negotiates for the maximum capabilities supported between the network adapter and the CPU.</p> <p>The instructions below are meant to understand if your system is able to extract the maximum capabilities of your NIC, but they're not configurable. The two values that we are looking at here are the Max Payload Size (MPS - the maximum size of a PCIe packet) and the Speed (or PCIe generation).</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#max-payload-size-mps","title":"Max Payload Size (MPS)","text":"tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check mps\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check mps\n</code></pre> See an example output <p>The PCIe configuration on the IGX Orin developer kit is not able to leverage the max payload size of the NIC:</p> <pre><code>2025-03-10 16:15:54 - WARNING - cx7_0/0005:03:00.0: PCIe Max Payload Size is not set to 256 bytes. Found: 128 bytes.\n2025-03-10 16:15:54 - WARNING - cx7_1/0005:03:00.1: PCIe Max Payload Size is not set to 256 bytes. Found: 128 bytes.\n</code></pre> <p>Identify the PCIe address of your NVIDIA NIC:</p> ibdev2netdevlspci <pre><code>nic_pci=$(sudo ibdev2netdev -v | awk '{print $1}' | head -n1)\n</code></pre> <pre><code># `0200` is the PCI-SIG class code for NICs\n# `15b3` is the Vendor ID for Mellanox\nnic_pci=$(lspci -n | awk '$2 == \"0200:\" &amp;&amp; $3 ~ /^15b3:/ {print $1}' | head -n1)\n</code></pre> <p>Check current and max MPS:</p> <pre><code>sudo lspci -vv -s $nic_pci | awk '/DevCap/{s=1} /DevCtl/{s=0} /MaxPayload /{match($0, /MaxPayload [0-9]+/, m); if(s){print \"Max \" m[0]} else{print \"Current \" m[0]}}'\n</code></pre> See an example output <p>The PCIe configuration on the IGX Orin developer kit is not able to leverage the max payload size of the NIC:</p> <pre><code>Max MaxPayload 512\nCurrent MaxPayload 128\n</code></pre> <p>Note</p> <p>While your NIC might be capable of more, 256 bytes is generally the largest supported by any switch/CPU at this time.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#pcie-speedgeneration","title":"PCIe Speed/Generation","text":"<p>Identify the PCIe address of your NVIDIA NIC:</p> ibdev2netdevlspci <pre><code>nic_pci=$(sudo ibdev2netdev -v | awk '{print $1}' | head -n1)\n</code></pre> <pre><code># `0200` is the PCI-SIG class code for NICs\n# `15b3` is the Vendor ID for Mellanox\nnic_pci=$(lspci -n | awk '$2 == \"0200:\" &amp;&amp; $3 ~ /^15b3:/ {print $1}' | head -n1)\n</code></pre> <p>Check current and max Speeds:</p> <pre><code>sudo lspci -vv -s $nic_pci | awk '/LnkCap/{s=1} /LnkSta/{s=0} /Speed /{match($0, /Speed [0-9]+GT\\/s/, m); if(s){print \"Max \" m[0]} else{print \"Current \" m[0]}}'\n</code></pre> See an example output <p>On IGX, the switch is able to maximize the NIC speed, both being PCIe 5.0:</p> <pre><code>Max Speed 32GT/s\nCurrent Speed 32GT/s\n</code></pre>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#33-maximize-the-nics-max-read-request-size-mrrs","title":"3.3 Maximize the NIC's Max Read Request Size (MRRS)","text":"<p>Understanding PCIe Configuration for Maximum Performance - May 27, 2022</p> <p>PCIe Max Read Request determines the maximal PCIe read request allowed. A PCIe device usually keeps track of the number of pending read requests due to having to prepare buffers for an incoming response. The size of the PCIe max read request may affect the number of pending requests (when using data fetch larger than the PCIe MTU).</p> <p>Unlike the PCIe properties queried in the previous section, the MRRS is configurable. We recommend maxing it to 4096 bytes. Run the following to check your current settings:</p> tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check mrrs\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check mrrs\n</code></pre> <p>Identify the PCIe address of your NVIDIA NIC:</p> ibdev2netdevlspci <pre><code>nic_pci=$(sudo ibdev2netdev -v | awk '{print $1}' | head -n1)\n</code></pre> <pre><code># `0200` is the PCI-SIG class code for NICs\n# `15b3` is the Vendor ID for Mellanox\nnic_pci=$(lspci -n | awk '$2 == \"0200:\" &amp;&amp; $3 ~ /^15b3:/ {print $1}' | head -n1)\n</code></pre> <p>Check current MRRS:</p> <pre><code>sudo lspci -vv -s $nic_pci | grep DevCtl: -A2 | grep -oE \"MaxReadReq [0-9]+\"\n</code></pre> <p>Update MRRS:</p> Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --set mrrs\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --set mrrs\n</code></pre> <p>Note</p> <p>This value is reset on reboot and needs to be set every time the system boots</p> ERROR: pcilib: sysfs_write: write failed: Operation not permitted <p>Disable secure boot on your system ahead of changing the MRRS of your NIC ports. It can be re-enabled afterwards.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#34-enable-huge-pages","title":"3.4 Enable Huge pages","text":"<p>Huge pages are a memory management feature that allows the OS to allocate large blocks of memory (typically 2MB or 1GB) instead of the default 4KB pages. This reduces the number of page table entries and the amount of memory used for translation, improving cache performance and reducing TLB (Translation Lookaside Buffer) misses, which leads to lower latencies.</p> <p>While it is naturally beneficial for CPU packets, it is also needed when routing data packets to the GPU in order to handle metadata (mbufs) on the CPU.</p> hugeadmvanilla <p>We recommend installing the <code>libhugetlbfs-bin</code> package for the <code>hugeadm</code> utility:</p> <pre><code>sudo apt update\nsudo apt install -y libhugetlbfs-bin\n</code></pre> <p>Then, check your huge page pools:</p> <pre><code>hugeadm --pool-list\n</code></pre> See an example output <p>The example below shows that this system supports huge pages of 64K, 2M (default), 32M, and 1G, but that none of them are currently allocated.</p> <pre><code>      Size  Minimum  Current  Maximum  Default\n     65536        0        0        0\n   2097152        0        0        0        *\n  33554432        0        0        0\n1073741824        0        0        0\n</code></pre> <p>And your huge page mount points:</p> <pre><code>hugeadm --list-all-mounts\n</code></pre> See an example output <p>The default huge pages are mounted on <code>/dev/hugepages</code> with a page size of 2M.</p> <pre><code>Mount Point          Options\n/dev/hugepages       rw,relatime,pagesize=2M\n</code></pre> <p>First, check your huge page pools:</p> <pre><code>ls -1 /sys/kernel/mm/hugepages/\ngrep Huge /proc/meminfo\n</code></pre> See an example output <p>The example below shows that this system supports huge pages of 64K, 2M (default), 32M, and 1G, but that none of them are currently allocated.</p> <pre><code>hugepages-1048576kB\nhugepages-2048kB\nhugepages-32768kB\nhugepages-64kB\n</code></pre> <pre><code>HugePages_Total:       0\nHugePages_Free:        0\nHugePages_Rsvd:        0\nHugePages_Surp:        0\nHugepagesize:       2048 kB\nHugetlb:               0 kB\n</code></pre> <p>And your huge page mount points:</p> <pre><code>mount | grep huge\n</code></pre> See an example output <p>The default huge pages are mounted on <code>/dev/hugepages</code> with a page size of 2M.</p> <pre><code>hugetlbfs on /dev/hugepages type hugetlbfs (rw,relatime,pagesize=2M)\n</code></pre> <p>As a rule of thumb, we recommend to start with 3 to 4 GB of total huge pages, with an individual page size of 500 MB to 1 GB (per system availability).</p> <p>There are two ways to allocate huge pages:</p> <ul> <li>in the kernel bootline (recommended to ensure contiguous memory allocation) or</li> <li>dynamically at runtime (risk of fragmentation for large page sizes)</li> </ul> <p>The example below allocates 3 huge pages of 1GB each.</p> Kernel bootlineRuntime <p>Add the flags below to the <code>GRUB_CMDLINE_LINUX</code> variable in <code>/etc/default/grub</code>:</p> <pre><code>default_hugepagesz=1G hugepagesz=1G hugepages=3\n</code></pre> Show explanation <ul> <li><code>default_hugepagesz</code>: the default huge page size to use, making them available from the default mount point, <code>/dev/hugepages</code>.</li> <li><code>hugepagesz</code>: the size of the huge pages to allocate.</li> <li><code>hugepages</code>: the number of huge pages to allocate.</li> </ul> <p>Then rebuild your GRUB configuration and reboot:</p> <pre><code>sudo update-grub\nsudo reboot\n</code></pre> <p>Allocate the 3x 1GB huge pages:</p> hugeadmvanilla <pre><code>sudo hugeadm --pool-pages-min 1073741824:3\n</code></pre> <pre><code>echo 3 | sudo tee /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages\n</code></pre> <p>Create a mount point to access the 1GB huge pages pool since that is not the default size on that system. We will name it <code>/mnt/huge</code> here.</p> One-timePersistent <pre><code>sudo mkdir -p /mnt/huge\nsudo mount -t hugetlbfs -o pagesize=1G none /mnt/huge\n</code></pre> <pre><code>echo \"nodev /mnt/huge hugetlbfs pagesize=1G 0 0\" | sudo tee -a /etc/fstab\nsudo mount /mnt/huge\n</code></pre> <p>Note</p> <p>If you work with containers, remember to mount this directory in your container as well with <code>-v /mnt/huge:/mnt/huge</code>.</p> <p>Rerunning the initial commands should now list 3 hugepages of 1GB each. 1GB will be the default huge page size if updated in the kernel bootline only.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#35-isolate-cpu-cores","title":"3.5 Isolate CPU cores","text":"<p>Note</p> <p>This optimization is less impactful when using the <code>gpunetio</code> backend since the GPU polls the NIC.</p> <p>The CPU interacting with the NIC to route packets is sensitive to perturbations, especially with smaller packet/batch sizes requiring more frequent work. Isolating a CPU in Linux prevents unwanted user or kernel threads from running on it, reducing context switching and latency spikes from noisy neighbors.</p> <p>We recommend isolating the CPU cores you will select to interact with the NIC (defined in the <code>advanced_network</code> configuration described later in this tutorial). This is done by setting additional flags on the kernel bootline.</p> <p>You can first check if any of the recommended flags were already set on the last boot:</p> tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check cmdline\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check cmdline\n</code></pre> <pre><code>cat /proc/cmdline | grep -e isolcpus -e irqaffinity -e nohz_full -e rcu_nocbs -e rcu_nocb_poll\n</code></pre> <p>Decide which cores to isolate based on your configuration. We recommend one core per queue as a rule of thumb. First, identify your core IDs:</p> <pre><code>cat /proc/cpuinfo | grep processor\n</code></pre> See an example output <p>This system has 12 cores, numbered 0 to 11: <pre><code>processor       # 0\nprocessor       # 1\nprocessor       # 2\nprocessor       # 3\nprocessor       # 4\nprocessor       # 5\nprocessor       # 6\nprocessor       # 7\nprocessor       # 8\nprocessor       # 9\nprocessor       # 10\nprocessor       # 11\n</code></pre></p> <p>As an example, the line below will isolate cores 9, 10 and 11, leaving cores 0-8 free for other tasks and hardware interrupts:</p> <pre><code>isolcpus=9-11 irqaffinity=0-8 nohz_full=9-11 rcu_nocbs=9-11 rcu_nocb_poll\n</code></pre> Show explanation Parameter Description <code>isolcpus</code> Isolates specific CPU cores from the Linux scheduler, preventing regular system tasks from running on them. This ensures dedicated cores are available exclusively for your networking tasks, reducing context switches and interruptions that can cause latency spikes. <code>irqaffinity</code> Controls which CPU cores can handle hardware interrupts. By directing network interrupts away from your isolated cores, you prevent networking tasks from being interrupted by hardware events, maintaining consistent processing time. <code>nohz_full</code> Disables regular kernel timer ticks on specified cores when they're running user space applications. This reduces overhead and prevents periodic interruptions, allowing your networking code to run with fewer disturbances. <code>rcu_nocbs</code> Offloads Read-Copy-Update (RCU) callback processing from specified cores. RCU is a synchronization mechanism in the Linux kernel that can cause periodic processing bursts. Moving this work away from your networking cores helps maintain consistent performance. <code>rcu_nocb_poll</code> Works with <code>rcu_nocbs</code> to improve how RCU callbacks are processed on non-callback CPUs. This can reduce latency spikes by changing how the kernel polls for RCU work. <p>Together, these parameters create an environment where specific CPU cores can focus exclusively on network packet processing with minimal interference from the operating system, resulting in lower and more consistent latency.</p> <p>Add these flags to the <code>GRUB_CMDLINE_LINUX</code> variable in <code>/etc/default/grub</code>, then rebuild your GRUB configuration and reboot:</p> <pre><code>sudo update-grub\nsudo reboot\n</code></pre> <p>Verify that the flags were properly set after boot by rerunning the check commands above.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#36-prevent-cpu-cores-from-going-idle","title":"3.6 Prevent CPU cores from going idle","text":"<p>When a core goes idle/to sleep, coming back online to poll the NIC can cause latency spikes and dropped packets. To prevent this, we recommend setting the scaling governor to <code>performance</code> for these CPU cores.</p> <p>Note</p> <p>Cores from a single cluster will always share the same governor.</p> <p>Bug</p> <p>We have witnessed instances where setting the governor to <code>performance</code> on only the isolated cores (dedicated to polling the NIC) does not lead to the performance gains expected. As such, we currently recommend setting the governor to <code>performance</code> for all cores which has shown to be reliably effective.</p> <p>Check the current governor for each of your cores:</p> tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check cpu-freq\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check cpu-freq\n</code></pre> See an example output <pre><code>2025-03-06 12:20:27 - WARNING - CPU 0: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 1: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 2: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 3: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 4: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 5: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 6: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 7: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 8: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 9: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 10: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 11: Governor is set to 'powersave', not 'performance'.\n</code></pre> <pre><code>cat /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor\n</code></pre> See an example output <p>In this example, all cores were defaulted to <code>powersave</code> instead of the recommended <code>performance</code>.</p> <pre><code>powersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\n</code></pre> <p>Install <code>cpupower</code> to more conveniently set the governor:</p> <pre><code>sudo apt update\nsudo apt install -y linux-tools-$(uname -r)\n</code></pre> <p>Set the governor to <code>performance</code> for all cores:</p> One-timePersistent <pre><code>sudo cpupower frequency-set -g performance\n</code></pre> <pre><code>cat &lt;&lt; EOF | sudo tee /etc/systemd/system/cpu-performance.service\n[Unit]\nDescription=Set CPU governor to performance\nAfter=multi-user.target\n\n[Service]\nType=oneshot\nExecStart=/usr/bin/cpupower -c all frequency-set -g performance\n\n[Install]\nWantedBy=multi-user.target\nEOF\nsudo systemctl enable cpu-performance.service\nsudo systemctl start cpu-performance.service\n</code></pre> <p>Running the checks above should now list <code>performance</code> as the governor for all cores. You can also run <code>sudo cpupower -c all frequency-info</code> for more details.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#37-prevent-the-gpu-from-going-idle","title":"3.7 Prevent the GPU from going idle","text":"<p>Similarly to the above, we want to maximize the GPU's clock speed and prevent it from going idle.</p> <p>Run the following command to check your current clocks and whether they're locked (persistence mode):</p> <pre><code>nvidia-smi -q | grep -i \"Persistence Mode\"\nnvidia-smi -q -d CLOCK\n</code></pre> See an example output <pre><code>    Persistence Mode: Enabled\n...\nAttached GPUs                             : 1\nGPU 00000005:09:00.0\n    Clocks\n        Graphics                          : 420 MHz\n        SM                                : 420 MHz\n        Memory                            : 405 MHz\n        Video                             : 1680 MHz\n    Applications Clocks\n        Graphics                          : 1800 MHz\n        Memory                            : 8001 MHz\n    Default Applications Clocks\n        Graphics                          : 1800 MHz\n        Memory                            : 8001 MHz\n    Deferred Clocks\n        Memory                            : N/A\n    Max Clocks\n        Graphics                          : 2100 MHz\n        SM                                : 2100 MHz\n        Memory                            : 8001 MHz\n        Video                             : 1950 MHz\n    ...\n</code></pre> <p>To lock the GPU's clocks to their max values:</p> One-timePersistent <pre><code>sudo nvidia-smi -pm 1\nsudo nvidia-smi -lgc=$(nvidia-smi --query-gpu=clocks.max.sm --format=csv,noheader,nounits)\nsudo nvidia-smi -lmc=$(nvidia-smi --query-gpu=clocks.max.mem --format=csv,noheader,nounits)\n</code></pre> <pre><code>cat &lt;&lt; EOF | sudo tee /etc/systemd/system/gpu-max-clocks.service\n[Unit]\nDescription=Max GPU clocks\nAfter=multi-user.target\n\n[Service]\nType=oneshot\nExecStart=/usr/bin/nvidia-smi -pm 1\nExecStart=/bin/bash -c '/usr/bin/nvidia-smi --lock-gpu-clocks=$(/usr/bin/nvidia-smi --query-gpu=clocks.max.sm --format=csv,noheader,nounits)'\nExecStart=/bin/bash -c '/usr/bin/nvidia-smi --lock-memory-clocks=$(/usr/bin/nvidia-smi --query-gpu=clocks.max.mem --format=csv,noheader,nounits)'\nRemainAfterExit=true\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\nsudo systemctl enable gpu-max-clocks.service\nsudo systemctl start gpu-max-clocks.service\n</code></pre> Show explanation <p>This queries the max clocks for the GPU SM (<code>clocks.max.sm</code>) and memory (<code>clocks.max.mem</code>) and sets them to the current clocks (<code>lock-gpu-clocks</code> and <code>lock-memory-clocks</code> respectively). <code>-pm 1</code> (or <code>--persistence-mode=1</code>) enables persistence mode to lock these values.</p> See an example output <pre><code>GPU clocks set to \"(gpuClkMin 2100, gpuClkMax 2100)\" for GPU 00000005:09:00.0\nAll done.\nMemory clocks set to \"(memClkMin 8001, memClkMax 8001)\" for GPU 00000005:09:00.0\nAll done.\n</code></pre> <p>You can confirm that the clocks are set to the max values by running <code>nvidia-smi -q -d CLOCK</code> again.</p> <p>Note</p> <p>Some max clocks might not be achievable in certain configurations, or due to boost clocks (SM) or rounding errors (Memory),  despite the lock commands indicating it worked. For example - on IGX - the max non-boot SM clock will be 1920 MHz, and the max memory clock will show 8000 MHz, which are satisfying compared to the initial mode.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#38-maximize-gpu-bar1-size","title":"3.8 Maximize GPU BAR1 size","text":"<p>The GPU BAR1 memory is the primary resource consumed by <code>GPUDirect</code>. It allows other PCIe devices (like the CPU and the NIC) to access the GPU's memory space. The larger the BAR1 size, the more memory the GPU can expose to these devices in a single PCIe transaction, reducing the number of transactions needed and improving performance.</p> <p>We recommend a BAR1 size of 1GB or above. Check the current BAR1 size:</p> tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check bar1-size\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check bar1-size\n</code></pre> See an example output <pre><code>2025-03-06 12:22:53 - INFO - GPU 00000005:09:00.0: BAR1 size is 8192 MiB.\n</code></pre> <pre><code>nvidia-smi -q | grep -A 3 BAR1\n</code></pre> See an example output <p>For our RTX A6000, this shows a BAR1 size of 256 MiB:</p> <pre><code>    BAR1 Memory Usage\n    Total                             : 256 MiB\n    Used                              : 13 MiB\n    Free                              : 243 MiB\n</code></pre> <p>Warning</p> <p>Resizing the BAR1 size requires:</p> <ul> <li>A BIOS with resizable BAR support</li> <li>A GPU with physical resizable BAR</li> </ul> <p>If you attempt to go forward with the instructions below without meeting the above requirements, you might render your GPU unusable.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#bios-resizable-bar-support","title":"BIOS Resizable BAR support","text":"<p>First, check if your system and BIOS support resizable BAR. Refer to your system's manufacturer documentation to access the BIOS. The Resizable BAR option is often categorized under <code>Advanced &gt; PCIe</code> settings. Enable this feature if found.</p> <p>Note</p> <p>The IGX Developer kit with IGX OS 1.1+ supports resizable BAR by default.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#gpu-resizable-bar-support","title":"GPU Resizable BAR support","text":"<p>Next, you can check if your GPU has physical resizable BAR by running the following command:</p> <pre><code>sudo lspci -vv -s $(nvidia-smi --query-gpu=pci.bus_id --format=csv,noheader) | grep BAR\n</code></pre> See an example output <p>This RTX A6000 has a resizable BAR1, currently set to 256 MiB:</p> <pre><code>Capabilities: [bb0 v1] Physical Resizable BAR\n    BAR 0: current size: 16MB, supported: 16MB\n    BAR 1: current size: 256MB, supported: 64MB 128MB 256MB 512MB 1GB 2GB 4GB 8GB 16GB 32GB 64GB\n    BAR 3: current size: 32MB, supported: 32MB\n</code></pre> <p>If your GPU is listed on this page, you can download the <code>Display Mode Selector</code> to resize the BAR1 to 8GB.</p> <ol> <li>Press <code>Join Now</code>.</li> <li>Once approved, download the <code>Display Mode Selector</code> archive.</li> <li>Unzip the archive.</li> <li>Access your system without a X-server running, either through SSH or a Virtual Console (<code>Alt+F1</code>).</li> <li>Go down the right OS and architecture folder for your system (<code>linux/aarch64</code> or <code>linux/x64</code>).</li> <li>Run the <code>displaymodeselector</code> command like so:</li> </ol> <pre><code>chmod +x displaymodeselector\nsudo ./displaymodeselector --gpumode physical_display_enabled_8GB_bar1\n</code></pre> <p>Press <code>y</code> to confirm you'd like to continue, then <code>y</code> again to apply to all the eligible adapters.</p> See an example output <pre><code>NVIDIA Display Mode Selector Utility (Version 1.67.0)\nCopyright (C) 2015-2021, NVIDIA Corporation. All Rights Reserved.\n\nWARNING: This operation updates the firmware on the board and could make\n        the device unusable if your host system lacks the necessary support.\n\nAre you sure you want to continue?\nPress 'y' to confirm (any other key to abort):\ny\nSpecified GPU Mode \"physical_display_enabled_8GB_bar1\"\n\n\nUpdate GPU Mode of all adapters to \"physical_display_enabled_8GB_bar1\"?\nPress 'y' to confirm or 'n' to choose adapters or any other key to abort:\ny\n\nUpdating GPU Mode of all eligible adapters to \"physical_display_enabled_8GB_bar1\"\n\nApply GPU Mode &lt;6&gt; corresponds to \"physical_display_enabled_8GB_bar1\"\n\nReading EEPROM (this operation may take up to 30 seconds)\n\n[==================================================] 100 %\nReading EEPROM (this operation may take up to 30 seconds)\n\nSuccessfully updated GPU mode to \"physical_display_enabled_8GB_bar1\" ( Mode 6 ).\n\nA reboot is required for the update to take effect.\n</code></pre> Error: unload the NVIDIA kernel driver first <p>If you see this error:</p> <pre><code>ERROR: In order to avoid the irreparable damage to your graphics adapter it is necessary to unload the NVIDIA kernel driver first:\n\nrmmod nvidia_uvm nvidia_drm nvidia_modeset nvidia_peermem nvidia\n</code></pre> <p>Try to unload the NVIDIA kernel driver listed in the error message above (list may vary):</p> <pre><code>sudo rmmod nvidia_uvm nvidia_drm nvidia_modeset nvidia_peermem nvidia\n</code></pre> <p>If this fails because the drivers are in use, stop the X-server first before trying again:</p> <pre><code>sudo systemctl isolate multi-user\n</code></pre> /dev/mem: Operation not permitted. Access to physical memory denied <p>Disable secure boot on your system ahead of changing your GPU's BAR1 size. It can be re-enabled afterwards.</p> <p>Reboot your system, and check the BAR1 size again to confirm the change.</p> <pre><code>sudo reboot\n</code></pre>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#39-enable-jumbo-frames","title":"3.9 Enable Jumbo Frames","text":"<p>Jumbo frames are Ethernet frames that carry a payload larger than the standard 1500 bytes MTU (Maximum Transmission Unit). They can significantly improve network performance when transferring large amounts of data by reducing the overhead of packet headers and the number of packets that need to be processed.</p> <p>We recommend an MTU of 9000 bytes on all interfaces involved in the data path. You can check the current MTU of your interfaces:</p> tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check mtu\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check mtu\n</code></pre> See an example output <pre><code>2025-03-06 16:51:19 - INFO - Interface eth0 has an acceptable MTU of 9000 bytes.\n2025-03-06 16:51:19 - INFO - Interface eth1 has an acceptable MTU of 9000 bytes.\n</code></pre> <p>For a given <code>if_name</code> interface:</p> <pre><code>if_name=eth0\nip link show dev $if_name | grep -oE \"mtu [0-9]+\"\n</code></pre> See an example output <pre><code>mtu 1500\n</code></pre> <p>You can set the MTU for each interface like so, for a given <code>if_name</code> name identified above:</p> One-timePersistent <pre><code>sudo ip link set dev $if_name mtu 9000\n</code></pre> NetworkManagersystemd-networkd <pre><code>sudo nmcli connection modify $if_name ipv4.mtu 9000\nsudo nmcli connection up $if_name\n</code></pre> <p>Assuming you've set an IP address for the interface above, you can add the MTU to the interface's network configuration file like so:</p> <pre><code>sudo sed -i '/\\[Network\\]/a MTU=9000' /etc/systemd/network/20-$if_name.network\nsudo systemctl restart systemd-networkd\n</code></pre> Can I do more than 9000? <p>While your NIC might have a maximum MTU capability larger than 9000, we typically recommend setting the MTU to 9000 bytes, as that is the standard size for jumbo frames that's widely supported for compatibility with other network equipment. When using jumbo frames, all devices in the communication path must support the same MTU size. If any device in between has a smaller MTU, packets will be fragmented or dropped, potentially degrading performance.</p> <p>Example with the CX-7 NIC:</p> <pre><code>$ ip -d link show dev $if_name | grep -oE \"maxmtu [0-9]+\"\nmaxmtu 9978\n</code></pre>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#4-running-a-test-application","title":"4. Running a test application","text":"<p>Holoscan Networking provides a benchmarking application named <code>adv_networking_bench</code> that can be used to test the performance of the networking configuration. In this section, we'll walk you through the steps needed to configure the application for your NIC for Tx and Rx, and run a loopback test between the two interfaces with a physical SFP cable connecting them.</p> <p>Make sure to install <code>holoscan-networking</code> beforehand.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#41-update-the-loopback-configuration","title":"4.1 Update the loopback configuration","text":"","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#find-the-application-files","title":"Find the application files","text":"<p>Identify the location of the <code>adv_networking_bench</code> executable, and of the configuration file named <code>adv_networking_bench_default_tx_rx.yaml</code>, for your installation:</p> Debian installationFrom source <p>Both located under <code>/opt/nvidia/holoscan/examples/adv_networking_bench/</code>:</p> <pre><code>ls -1 /opt/nvidia/holoscan/examples/adv_networking_bench/\nadv_networking_bench\nadv_networking_bench_default_rx_multi_q.yaml\nadv_networking_bench_default_tx_rx_hds.yaml\nadv_networking_bench_default_tx_rx.yaml\nadv_networking_bench_gpunetio_tx_rx.yaml\nadv_networking_bench_rmax_rx.yaml\nCMakeLists.txt\ndefault_bench_op_rx.h\ndefault_bench_op_tx.h\ndoca_bench_op_rx.h\ndoca_bench_op_tx.h\nkernels.cu\nkernels.cuh\nmain.cpp\n</code></pre> <p>Both located under <code>./install/examples/adv_networking_bench/</code></p> <pre><code>ls -1 ./install/examples/adv_networking_bench\nadv_networking_bench\nadv_networking_bench_default_rx_multi_q.yaml\nadv_networking_bench_default_tx_rx_hds.yaml\nadv_networking_bench_default_tx_rx.yaml\nadv_networking_bench_gpunetio_tx_rx.yaml\nadv_networking_bench.py\nadv_networking_bench_rmax_rx.yaml\nCMakeLists.txt\ndefault_bench_op_rx.h\ndefault_bench_op_tx.h\ndoca_bench_op_rx.h\ndoca_bench_op_tx.h\nkernels.cu\nkernels.cuh\nmain.cpp\n</code></pre> <p>Warning</p> <p>The configuration file is also located alongide the application source code at <code>applications/adv_networking_bench/adv_networking_bench_default_tx_rx.yaml</code>. However, modifying this file will not affect the configuration used by the application executable without rebuilding the application.</p> <p>For this reason, we recommend using the configuration file located in the install tree.</p> <p>Note</p> <p>The fields in this <code>yaml</code> file will be explained in more details in a section below. For now, we'll stick to modifying the strict minimum required fields to run the application as-is on your system.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#identify-your-nics-pcie-addresses","title":"Identify your NIC's PCIe addresses","text":"<p>Retrieve the PCIe addresses of both ports of your NIC. We'll arbitrarily use the first for Tx and the second for Rx here:</p> ibdev2netdevlspci <pre><code>sudo ibdev2netdev -v | awk '{print $1}'\n</code></pre> <pre><code># `0200` is the PCI-SIG class code for NICs\n# `15b3` is the Vendor ID for Mellanox\nlspci -n | awk '$2 == \"0200:\" &amp;&amp; $3 ~ /^15b3:/ {print $1}'\n</code></pre> See an example output <pre><code>0005:03:00.0\n0005:03:00.1\n</code></pre>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#configure-the-nic-for-tx-and-rx","title":"Configure the NIC for Tx and Rx","text":"<p>Set the NIC addresses in the <code>interfaces</code> section of the <code>advanced_network</code> section, making sure to remove the template brackets <code>&lt; &gt;</code>. This configures your NIC independently of your application:</p> <ul> <li>Set the <code>address</code> field of the <code>tx_port</code> interface to one of these addresses. That interface will be able to transmit ethernet packets.</li> <li>Set the <code>address</code> field of the <code>rx_port</code> interface to the other address. This interface will be able to receive ethernet packets.</li> </ul> <pre><code>interfaces:\n    - name: \"tx_port\"\n    address: &lt;0000:00:00.0&gt;       # The BUS address of the interface doing Tx\n    tx:\n        ...\n    - name: \"rx_port\"\n    address: &lt;0000:00:00.0&gt;       # The BUS address of the interface doing Rx\n    rx:\n        ...\n</code></pre> See an example yaml <pre><code>interfaces:\n    - name: \"tx_port\"\n    address: 0005:03:00.0       # The BUS address of the interface doing Tx\n    tx:\n        ...\n    - name: \"rx_port\"\n    address: 0005:03:00.1       # The BUS address of the interface doing Rx\n    rx:\n        ...\n</code></pre>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#configure-the-application","title":"Configure the application","text":"<p>Modify the <code>bench_tx</code> section which configures the application itself, to create the packet headers and direct them to the NIC. Make sure to remove the template brackets <code>&lt; &gt;</code>.</p> <ul> <li><code>eth_dst_addr</code> with the MAC address (and not the PCIe address) of the NIC interface you want to use for Rx. You can get the MAC address of your <code>if_name</code> interface with <code>cat /sys/class/net/$if_name/address</code>:</li> <li>Replacing <code>address</code> with the PCIe address of the NIC interface you want to use for Tx (same as <code>tx_port</code>'s address above).</li> </ul> <pre><code>bench_tx:\n    ...\n    eth_dst_addr: &lt;00:00:00:00:00:00&gt; # Destination MAC address - required when Rx flow_isolation=true\n    ip_src_addr: &lt;1.2.3.4&gt;  # Source IP address - required on layer 3 network\n    ip_dst_addr: &lt;5.6.7.8&gt;  # Destination IP address - required on layer 3 network\n    udp_src_port: 4096      # UDP source port\n    udp_dst_port: 4096      # UDP destination port\n    address: &lt;0000:00:00.0&gt; # Source NIC Bus ID. Should match the address of the Tx interface above\n</code></pre> See an example yaml <pre><code>bench_tx:\n    ...\n    eth_dst_addr: 48:b0:2d:ee:83:ad # Destination MAC address - required when Rx flow_isolation=true\n    ip_src_addr: &lt;1.2.3.4&gt;  # Source IP address - required on layer 3 network\n    ip_dst_addr: &lt;5.6.7.8&gt;  # Destination IP address - required on layer 3 network\n    udp_src_port: 4096      # UDP source port\n    udp_dst_port: 4096      # UDP destination port\n    address: 0005:03:00.0  # Source NIC Bus ID. Should match the address of the Tx interface above\n</code></pre> Show explanation <ul> <li><code>eth_dst_addr</code> - the destination ethernet MAC address - will be embedded in the packet headers by the application. This is required here because the Rx interface above has <code>flow_isolation: true</code> (explained in more details below). In that configuration, only the packets listing the adequate destination MAC address will be accepted by the Rx interface.</li> <li>We ignore the IP fields (<code>ip_src_addr</code>, <code>ip_dst_addr</code>) for now, as we are testing on a layer 2 network by just connecting a cable between the two interfaces on our system, therefore having mock values has no impact.</li> <li><code>address</code> - the source PCIe address - needs to be defined again to tell the application itself to route the packets to the NIC interface we have configured previously for Tx.</li> <li>You might have noted the lack of a <code>eth_src_addr</code> field in the <code>bench_tx</code> section. This is because the source Ethernet MAC address can be inferred automatically from the PCIe address of the Tx interface (below).</li> </ul>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#42-run-the-loopback-test","title":"4.2 Run the loopback test","text":"<p>After having modified the configuration file, ensure you have connected an SFP cable between the two interfaces of your NIC, then run the application with the command below:</p> Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/examples/adv_networking_bench/adv_networking_bench adv_networking_bench_default_tx_rx.yaml\n</code></pre> Bare MetalContainerized <p>This assumes you have the required dependencies (holoscan, doca, etc.) installed locally on your system.</p> <pre><code>sudo ./install/examples/adv_networking_bench/adv_networking_bench adv_networking_bench_default_tx_rx.yaml\n</code></pre> <pre><code>./dev_container launch \\\n  --img holohub:adv_networking_bench \\\n  --docker_opts \"-u 0 --privileged\" \\\n  -- bash -c \"./install/examples/adv_networking_bench/adv_networking_bench adv_networking_bench_default_tx_rx.yaml\"\n</code></pre> <p>The application will run indefinitely. You can stop it gracefully with <code>Ctrl-C</code>. You can also uncomment and set the <code>max_duration_ms</code> field in the <code>scheduler</code> section of the configuration file to limit the duration of the run automatically.</p> See an example output <pre><code>[info] [fragment.cpp:599] Loading extensions from configs...\n[info] [gxf_executor.cpp:264] Creating context\n[info] [main.cpp:35] Initializing advanced network operator\n[info] [main.cpp:40] Using ANO manager dpdk\n[info] [adv_network_rx.cpp:35] Adding output port bench_rx_out\n[info] [adv_network_rx.cpp:51] AdvNetworkOpRx::initialize()\n[info] [adv_network_common.h:607] Finished reading advanced network operator config\n[info] [adv_network_dpdk_mgr.cpp:373] Attempting to use 2 ports for high-speed network\n[info] [adv_network_dpdk_mgr.cpp:382] Setting DPDK log level to: Info\n[info] [adv_network_dpdk_mgr.cpp:402] DPDK EAL arguments: adv_net_operator --file-prefix=nwlrbbmqbh -l 3,11,9 --log-level=9 --log-level=pmd.net.mlx5:info -a 0005:03:00.0,txq_inline_max=0,dv_flow_en=2 -a 0005:03:00.1,txq_inline_max=0,dv_flow_en=2\nLog level 9 higher than maximum (8)\nEAL: Detected CPU lcores: 12\nEAL: Detected NUMA nodes: 1\nEAL: Detected shared linkage of DPDK\nEAL: Multi-process socket /var/run/dpdk/nwlrbbmqbh/mp_socket\nEAL: Selected IOVA mode 'VA'\nEAL: 1 hugepages of size 1073741824 reserved, but no mounted hugetlbfs found for that size\nEAL: Probe PCI driver: mlx5_pci (15b3:1021) device: 0005:03:00.0 (socket -1)\nmlx5_net: PCI information matches for device \"mlx5_0\"\nmlx5_net: enhanced MPS is enabled\nmlx5_net: port 0 MAC address is 48:B0:2D:EE:83:AC\nEAL: Probe PCI driver: mlx5_pci (15b3:1021) device: 0005:03:00.1 (socket -1)\nmlx5_net: PCI information matches for device \"mlx5_1\"\nmlx5_net: enhanced MPS is enabled\nmlx5_net: port 1 MAC address is 48:B0:2D:EE:83:AD\nTELEMETRY: No legacy callbacks, legacy socket not created\n[info] [adv_network_dpdk_mgr.cpp:298] Port 0 has no RX queues. Creating dummy queue.\n[info] [adv_network_dpdk_mgr.cpp:165] Adjusting buffer size to 9228 for headroom\n[info] [adv_network_dpdk_mgr.cpp:165] Adjusting buffer size to 9128 for headroom\n[info] [adv_network_dpdk_mgr.cpp:165] Adjusting buffer size to 9128 for headroom\n[info] [adv_network_mgr.cpp:116] Registering memory regions\n[info] [adv_network_mgr.cpp:178] Successfully allocated memory region MR_Unused_P0 at 0x100fa0000 type 2 with 9100 bytes (32768 elements @ 9228 bytes total 302383104)\n[info] [adv_network_mgr.cpp:178] Successfully allocated memory region Data_RX_GPU at 0xffff4fc00000 type 3 with 9000 bytes (51200 elements @ 9128 bytes total 467402752)\n[info] [adv_network_mgr.cpp:178] Successfully allocated memory region Data_TX_GPU at 0xffff33e00000 type 3 with 9000 bytes (51200 elements @ 9128 bytes total 467402752)\n[info] [adv_network_mgr.cpp:191] Finished allocating memory regions\n[info] [adv_network_dpdk_mgr.cpp:223] Successfully registered external memory for Data_TX_GPU\n[info] [adv_network_dpdk_mgr.cpp:223] Successfully registered external memory for Data_RX_GPU\n[info] [adv_network_dpdk_mgr.cpp:193] Mapped external memory descriptor for 0xffff4fc00000 to device 0\n[info] [adv_network_dpdk_mgr.cpp:193] Mapped external memory descriptor for 0xffff33e00000 to device 0\n[info] [adv_network_dpdk_mgr.cpp:193] Mapped external memory descriptor for 0xffff4fc00000 to device 1\n[info] [adv_network_dpdk_mgr.cpp:193] Mapped external memory descriptor for 0xffff33e00000 to device 1\n[info] [adv_network_dpdk_mgr.cpp:454] DPDK init (0005:03:00.0) -- RX: ENABLED TX: ENABLED\n[info] [adv_network_dpdk_mgr.cpp:464] Configuring RX queue: UNUSED_P0_Q0 (0) on port 0\n[info] [adv_network_dpdk_mgr.cpp:513] Created mempool RXP_P0_Q0_MR0 : mbufs=32768 elsize=9228 ptr=0x10041c380\n[info] [adv_network_dpdk_mgr.cpp:523] Max packet size needed for RX: 9100\n[info] [adv_network_dpdk_mgr.cpp:564] Configuring TX queue: ADC Samples (0) on port 0\n[info] [adv_network_dpdk_mgr.cpp:607] Created mempool TXP_P0_Q0_MR0 : mbufs=51200 elsize=9000 ptr=0x100c1fc00\n[info] [adv_network_dpdk_mgr.cpp:621] Max packet size needed with TX: 9100\n[info] [adv_network_dpdk_mgr.cpp:632] Setting port config for port 0 mtu:9082\n[info] [adv_network_dpdk_mgr.cpp:663] Initializing port 0 with 1 RX queues and 1 TX queues...\nmlx5_net: port 0 Tx queues number update: 0 -&gt; 1\nmlx5_net: port 0 Rx queues number update: 0 -&gt; 1\n[info] [adv_network_dpdk_mgr.cpp:679] Successfully configured ethdev\n[info] [adv_network_dpdk_mgr.cpp:689] Successfully set descriptors to 8192/8192\n[info] [adv_network_dpdk_mgr.cpp:704] Port 0 not in isolation mode\n[info] [adv_network_dpdk_mgr.cpp:713] Setting up port:0, queue:0, Num scatter:1 pool:0x10041c380\n[info] [adv_network_dpdk_mgr.cpp:734] Successfully setup RX port 0 queue 0\n[info] [adv_network_dpdk_mgr.cpp:756] Successfully set up TX queue 0/0\n[info] [adv_network_dpdk_mgr.cpp:761] Enabling promiscuous mode for port 0\nmlx5_net: [mlx5dr_cmd_query_caps]: Failed to query wire port regc value\nmlx5_net: port 0 Rx queues number update: 1 -&gt; 1\n[info] [adv_network_dpdk_mgr.cpp:775] Successfully started port 0\n[info] [adv_network_dpdk_mgr.cpp:778] Port 0, MAC address: 48:B0:2D:EE:83:AC\n[info] [adv_network_dpdk_mgr.cpp:1111] Applying tx_eth_src offload for port 0\n[info] [adv_network_dpdk_mgr.cpp:454] DPDK init (0005:03:00.1) -- RX: ENABLED TX: DISABLED\n[info] [adv_network_dpdk_mgr.cpp:464] Configuring RX queue: Data (0) on port 1\n[info] [adv_network_dpdk_mgr.cpp:513] Created mempool RXP_P1_Q0_MR0 : mbufs=51200 elsize=9128 ptr=0x125a5b940\n[info] [adv_network_dpdk_mgr.cpp:523] Max packet size needed for RX: 9000\n[info] [adv_network_dpdk_mgr.cpp:621] Max packet size needed with TX: 9000\n[info] [adv_network_dpdk_mgr.cpp:632] Setting port config for port 1 mtu:8982\n[info] [adv_network_dpdk_mgr.cpp:663] Initializing port 1 with 1 RX queues and 0 TX queues...\nmlx5_net: port 1 Rx queues number update: 0 -&gt; 1\n[info] [adv_network_dpdk_mgr.cpp:679] Successfully configured ethdev\n[info] [adv_network_dpdk_mgr.cpp:689] Successfully set descriptors to 8192/8192\n[info] [adv_network_dpdk_mgr.cpp:701] Port 1 in isolation mode\n[info] [adv_network_dpdk_mgr.cpp:713] Setting up port:1, queue:0, Num scatter:1 pool:0x125a5b940\n[info] [adv_network_dpdk_mgr.cpp:734] Successfully setup RX port 1 queue 0\n[info] [adv_network_dpdk_mgr.cpp:764] Not enabling promiscuous mode on port 1 since flow isolation is enabled\nmlx5_net: [mlx5dr_cmd_query_caps]: Failed to query wire port regc value\nmlx5_net: port 1 Rx queues number update: 1 -&gt; 1\n[info] [adv_network_dpdk_mgr.cpp:775] Successfully started port 1\n[info] [adv_network_dpdk_mgr.cpp:778] Port 1, MAC address: 48:B0:2D:EE:83:AD\n[info] [adv_network_dpdk_mgr.cpp:790] Adding RX flow ADC Samples\n[info] [adv_network_dpdk_mgr.cpp:998] Adding IPv4 length match for 1050\n[info] [adv_network_dpdk_mgr.cpp:1018] Adding UDP port match for src/dst 4096/4096\n[info] [adv_network_dpdk_mgr.cpp:814] Setting up RX burst pool with 8191 batches of size 81920\n[info] [adv_network_dpdk_mgr.cpp:833] Setting up RX burst pool with 8191 batches of size 20480\n[info] [adv_network_dpdk_mgr.cpp:875] Setting up TX ring TX_RING_P0_Q0\n[info] [adv_network_dpdk_mgr.cpp:901] Setting up TX burst pool TX_BURST_POOL_P0_Q0 with 10240 pointers at 0x125a0d4c0\n[info] [adv_network_dpdk_mgr.cpp:1186] Config validated successfully\n[info] [adv_network_dpdk_mgr.cpp:1199] Starting advanced network workers\n[info] [adv_network_dpdk_mgr.cpp:1278] Flushing packet on port 1\n[info] [adv_network_dpdk_mgr.cpp:1478] Starting RX Core 9, port 1, queue 0, socket 0\n[info] [adv_network_dpdk_mgr.cpp:1268] Done starting workers\n[info] [default_bench_op_tx.h:79] AdvNetworkingBenchDefaultTxOp::initialize()\n[info] [adv_network_dpdk_mgr.cpp:1637] Starting TX Core 11, port 0, queue 0 socket 0 using burst pool 0x125a0d4c0 ring 0x127690740\n[info] [default_bench_op_tx.h:113] Initialized 4 streams and events\n[info] [default_bench_op_tx.h:130] AdvNetworkingBenchDefaultTxOp::initialize() complete\n[info] [default_bench_op_rx.h:67] AdvNetworkingBenchDefaultRxOp::initialize()\n[info] [gxf_executor.cpp:1797] creating input IOSpec named 'burst_in'\n[info] [default_bench_op_rx.h:104] AdvNetworkingBenchDefaultRxOp::initialize() complete\n[info] [adv_network_tx.cpp:46] AdvNetworkOpTx::initialize()\n[info] [gxf_executor.cpp:1797] creating input IOSpec named 'burst_in'\n[info] [adv_network_common.h:607] Finished reading advanced network operator config\n[info] [gxf_executor.cpp:2208] Activating Graph...\n[info] [gxf_executor.cpp:2238] Running Graph...\n[info] [multi_thread_scheduler.cpp:300] MultiThreadScheduler started worker thread [pool name: default_pool, thread uid: 0]\n[info] [multi_thread_scheduler.cpp:300] MultiThreadScheduler started worker thread [pool name: default_pool, thread uid: 1]\n[info] [multi_thread_scheduler.cpp:300] MultiThreadScheduler started worker thread [pool name: default_pool, thread uid: 2]\n[info] [gxf_executor.cpp:2240] Waiting for completion...\n[info] [multi_thread_scheduler.cpp:300] MultiThreadScheduler started worker thread [pool name: default_pool, thread uid: 3]\n[info] [multi_thread_scheduler.cpp:300] MultiThreadScheduler started worker thread [pool name: default_pool, thread uid: 4]\n^C[info] [multi_thread_scheduler.cpp:636] Stopping multithread scheduler\n[info] [multi_thread_scheduler.cpp:694] Stopping all async jobs\n[info] [multi_thread_scheduler.cpp:218] Dispatcher thread has stopped checking jobs\n[info] [multi_thread_scheduler.cpp:679] Waiting to join all async threads\n[info] [multi_thread_scheduler.cpp:316] Worker Thread [pool name: default_pool, thread uid: 1] exiting.\n[info] [multi_thread_scheduler.cpp:702] *********************** DISPATCHER EXEC TIME : 476345.364000 ms\n\n[info] [multi_thread_scheduler.cpp:316] Worker Thread [pool name: default_pool, thread uid: 0] exiting.\n[info] [multi_thread_scheduler.cpp:316] Worker Thread [pool name: default_pool, thread uid: 3] exiting.\n[info] [multi_thread_scheduler.cpp:371] Event handler thread exiting.\n[info] [multi_thread_scheduler.cpp:703] *********************** DISPATCHER WAIT TIME : 47339.961000 ms\n\n[info] [multi_thread_scheduler.cpp:704] *********************** DISPATCHER COUNT : 197630449\n\n[info] [multi_thread_scheduler.cpp:316] Worker Thread [pool name: default_pool, thread uid: 2] exiting.\n[info] [multi_thread_scheduler.cpp:705] *********************** WORKER EXEC TIME : 983902.800000 ms\n\n[info] [multi_thread_scheduler.cpp:706] *********************** WORKER WAIT TIME : 1634522.159000 ms\n\n[info] [multi_thread_scheduler.cpp:707] *********************** WORKER COUNT : 11817369\n\n[info] [multi_thread_scheduler.cpp:316] Worker Thread [pool name: default_pool, thread uid: 4] exiting.\n[info] [multi_thread_scheduler.cpp:688] All async worker threads joined, deactivating all entities\n[info] [adv_network_rx.cpp:46] AdvNetworkOpRx::stop()\n[info] [adv_network_dpdk_mgr.cpp:1928] DPDK ANO shutdown called 2\n[info] [adv_network_tx.cpp:41] AdvNetworkOpTx::stop()\n[info] [adv_network_dpdk_mgr.cpp:1928] DPDK ANO shutdown called 1\n[info] [adv_network_dpdk_mgr.cpp:1133] Port 0:\n[info] [adv_network_dpdk_mgr.cpp:1135]  - Received packets:    0\n[info] [adv_network_dpdk_mgr.cpp:1136]  - Transmit packets:    6005066864\n[info] [adv_network_dpdk_mgr.cpp:1137]  - Received bytes:      0\n[info] [adv_network_dpdk_mgr.cpp:1138]  - Transmit bytes:      6389391347584\n[info] [adv_network_dpdk_mgr.cpp:1139]  - Missed packets:      0\n[info] [adv_network_dpdk_mgr.cpp:1140]  - Errored packets:     0\n[info] [adv_network_dpdk_mgr.cpp:1141]  - RX out of buffers:   0\n[info] [adv_network_dpdk_mgr.cpp:1143]    ** Extended Stats **\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_good_packets:          6005070000\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_good_bytes:            6389394480000\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_q0_packets:            6005070000\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_q0_bytes:              6389394480000\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_multicast_bytes:               9589\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_multicast_packets:             22\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_unicast_bytes:         6389394480000\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_multicast_bytes:               9589\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_unicast_packets:               6005070000\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_multicast_packets:             22\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_phy_packets:           6005070022\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_phy_packets:           24\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_phy_bytes:             6413414769677\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_phy_bytes:             9805\n[info] [adv_network_dpdk_mgr.cpp:1133] Port 1:\n[info] [adv_network_dpdk_mgr.cpp:1135]  - Received packets:    6004323692\n[info] [adv_network_dpdk_mgr.cpp:1136]  - Transmit packets:    0\n[info] [adv_network_dpdk_mgr.cpp:1137]  - Received bytes:      6388600255072\n[info] [adv_network_dpdk_mgr.cpp:1138]  - Transmit bytes:      0\n[info] [adv_network_dpdk_mgr.cpp:1139]  - Missed packets:      746308\n[info] [adv_network_dpdk_mgr.cpp:1140]  - Errored packets:     0\n[info] [adv_network_dpdk_mgr.cpp:1141]  - RX out of buffers:   5047027287\n[info] [adv_network_dpdk_mgr.cpp:1143]    ** Extended Stats **\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_good_packets:          6004323692\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_good_bytes:            6388600255072\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_missed_errors:         746308\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_mbuf_allocation_errors:                5047027287\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_q0_packets:            6004323692\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_q0_bytes:              6388600255072\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_q0_errors:             5047027287\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_unicast_bytes:         6389394480000\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_multicast_bytes:               9589\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_unicast_packets:               6005070000\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_multicast_packets:             22\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_multicast_bytes:               9589\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_multicast_packets:             22\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_phy_packets:           24\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_phy_packets:           6005070022\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_phy_bytes:             9805\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_phy_bytes:             6413414769677\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_out_of_buffer:         746308\n[info] [adv_network_dpdk_mgr.cpp:1935] ANO DPDK manager shutting down\n[info] [adv_network_dpdk_mgr.cpp:1622] Total packets received by application (port/queue 1/0): 6004323692\n[info] [adv_network_dpdk_mgr.cpp:1698] Total packets transmitted by application (port/queue 0/0): 6005070000\n[info] [multi_thread_scheduler.cpp:645] Multithread scheduler stopped.\n[info] [multi_thread_scheduler.cpp:664] Multithread scheduler finished.\n[info] [gxf_executor.cpp:2243] Deactivating Graph...\n[info] [multi_thread_scheduler.cpp:491] TOTAL EXECUTION TIME OF SCHEDULER : 523694.460857 ms\n\n[info] [gxf_executor.cpp:2251] Graph execution finished.\n[info] [adv_network_dpdk_mgr.cpp:1928] DPDK ANO shutdown called 0\n[info] [default_bench_op_tx.h:51] ANO benchmark TX op shutting down\n[info] [default_bench_op_rx.h:56] Finished receiver with 6388570603520/6004295680 bytes/packets received and 0 packets dropped\n[info] [default_bench_op_rx.h:61] ANO benchmark RX op shutting down\n[info] [default_bench_op_rx.h:108] AdvNetworkingBenchDefaultRxOp::freeResources() start\n[info] [default_bench_op_rx.h:116] AdvNetworkingBenchDefaultRxOp::freeResources() complete\n[info] [gxf_executor.cpp:294] Destroying context\n</code></pre> <p>To inspect the speed the data is moving through the NIC, run <code>mlnx_perf</code> on one of the interfaces in a separate terminal, concurrently with the application running:</p> <pre><code>sudo mlnx_perf -i $if_name\n</code></pre> See an example output <p>On IGX with RTX A6000, we are able to hit close to the 100 Gbps linerate with this configuration: <pre><code>  rx_vport_unicast_packets: 11,614,900\n    rx_vport_unicast_bytes: 12,358,253,600 Bps   = 98,866.2 Mbps\n            rx_packets_phy: 11,614,847\n              rx_bytes_phy: 12,404,657,664 Bps   = 99,237.26 Mbps\n rx_1024_to_1518_bytes_phy: 11,614,936\n            rx_prio0_bytes: 12,404,738,832 Bps   = 99,237.91 Mbps\n          rx_prio0_packets: 11,614,923\n</code></pre></p> Troubleshooting EAL: failed to parse device <p>Make sure to set valid PCIe addresses in the <code>address</code> fields in <code>interfaces</code>, per instructions above.</p> Invalid MAC address format <p>Make sure to set a valid MAC address in the <code>eth_dst_addr</code> field in <code>bench_tx</code>, per instructions above.</p> mlx5_common: Fail to create MR for address [...] Could not DMA map EXT memory <p>Example error:</p> <pre><code>mlx5_common: Fail to create MR for address (0xffff2fc00000)\nmlx5_common: Device 0005:03:00.0 unable to DMA map\n[critical] [adv_network_dpdk_mgr.cpp:188] Could not DMA map EXT memory: -1 err=Invalid argument\n[critical] [adv_network_dpdk_mgr.cpp:430] Failed to map MRs\n</code></pre> <p>Make sure that <code>nvidia-peermem</code> is loaded.</p> EAL: Couldn't get fd on hugepage file [..] error allocating rte services array <p>Example error:</p> <pre><code>EAL: get_seg_fd(): open '/mnt/huge/nwlrbbmqbhmap_0' failed: Permission denied\nEAL: Couldn't get fd on hugepage file\nEAL: error allocating rte services array\nEAL: FATAL: rte_service_init() failed\nEAL: rte_service_init() failed\n</code></pre> <p>Ensure you run as root, using <code>sudo</code>.</p> EAL: Cannot get hugepage information. <pre><code>EAL: x hugepages of size x reserved, no mounted hugetlbfs found for that size\n</code></pre> <p>Ensure your hugepages are mounted.</p> <pre><code>EAL: No free x kB hugepages reported on node 0\n</code></pre> <ul> <li>Ensure you have allocated hugepages.</li> <li> <p>If you have already, check if they are any free left with <code>grep Huge /proc/meminfo</code>.</p> See an example output <p>No more space here!</p> <pre><code>HugePages_Total:       2\nHugePages_Free:        0\nHugePages_Rsvd:        0\nHugePages_Surp:        0\nHugepagesize:    1048576 kB\nHugetlb:         2097152 kB\n</code></pre> </li> <li> <p>If not, you can delete dangling hugepages under your hugepage mount point. That happens when your previous application run crashes.</p> <pre><code>sudo rm -rf /dev/hugepages/* # default mount point\nsudo rm -rf /mnt/huge/*      # custom mount point\n</code></pre> </li> </ul> Could not allocate x MB of GPU memory [...] Failed to allocate GPU memory <p>Check your GPU utilization:</p> <pre><code>nvidia-smi pmon -c 1\n</code></pre> <p>You might need to kill some of the listed processes to free up GPU VRAM.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#5-building-your-own-application","title":"5. Building your own application","text":"<p>This section will guide you through building your own application using the <code>adv_networking_bench</code> as an example. Make sure to install <code>holoscan-networking</code> first.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#51-understand-the-configuration-parameters","title":"5.1 Understand the configuration parameters","text":"<p>Note</p> <p>The configuration below will be analyzed in the context of the application consuming it, as defined in the <code>main.cpp</code> file. You can look it up when the \"sample application code\" is referenced.</p> Debian installationFrom source <pre><code>/opt/nvidia/holoscan/examples/adv_networking_bench/main.cpp\n</code></pre> <pre><code>./applications/adv_networking_bench/cpp/main.cpp\n</code></pre> <p>If you are not yet familiar with how Holoscan applications are constructed, please refer to the Holoscan SDK documentation first.</p> <p>Let's look at the <code>adv_networking_bench_default_tx_rx.yaml</code> file below. Click on the (1) icons below to expand explanations for each annotated line.</p> <ol> <li>The cake is a lie </li> </ol> <pre><code>scheduler: # (1)!\n  check_recession_period_ms: 0\n  worker_thread_number: 5\n  stop_on_deadlock: true\n  stop_on_deadlock_timeout: 500\n  # max_duration_ms: 20000\n\nadvanced_network: # (2)!\n  cfg:\n    version: 1\n    manager: \"dpdk\" # (3)!\n    master_core: 3 # (4)!\n    debug: false\n    log_level: \"info\"\n\n    memory_regions: # (5)!\n    - name: \"Data_TX_GPU\" # (6)!\n      kind: \"device\" # (7)!\n      affinity: 0 # (8)!\n      num_bufs: 51200 # (9)!\n      buf_size: 1064 # (10)!\n    - name: \"Data_RX_GPU\"\n      kind: \"device\"\n      affinity: 0\n      num_bufs: 51200\n      buf_size: 1000\n    - name: \"Data_RX_CPU\"\n      kind: \"huge\"\n      affinity: 0\n      num_bufs: 51200\n      buf_size: 64\n\n    interfaces: # (11)!\n    - name: \"tx_port\" # (12)!\n      address: &lt;0000:00:00.0&gt; # (13)! # The BUS address of the interface doing Tx\n      tx: # (14)!\n        queues: # (15)!\n        - name: \"tx_q_0\" # (16)!\n          id: 0 # (17)!\n          batch_size: 10240 # (18)!\n          cpu_core: 11 # (19)!\n          memory_regions: # (20)!\n            - \"Data_TX_GPU\"\n          offloads: # (21)!\n            - \"tx_eth_src\"\n    - name: \"rx_port\"\n      address: &lt;0000:00:00.0&gt; # (22)! # The BUS address of the interface doing Rx\n      rx:\n        flow_isolation: true # (23)!\n        queues:\n        - name: \"rx_q_0\"\n          id: 0\n          cpu_core: 9\n          batch_size: 10240\n          output_port: \"bench_rx_out\" # (24)!\n          memory_regions: # (25)!\n            - \"Data_RX_CPU\"\n            - \"Data_RX_GPU\"\n        flows: # (26)!\n        - name: \"flow_0\" # (27)!\n          id: 0 # (28)!\n          action: # (29)!\n            type: queue\n            id: 0\n          match: # (30)!\n            udp_src: 4096\n            udp_dst: 4096\n            ipv4_len: 1050\n\nbench_rx: # (31)!\n  gpu_direct: true       # Set to true if using a GPU region for the Rx queues.\n  split_boundary: true   # Whether header and data are split for Rx (Header to CPU)\n  batch_size: 10240\n  max_packet_size: 1064\n  header_size: 64\n\nbench_tx: # (32)!\n  gpu_direct: true        # Set to true if using a GPU region for the Tx queues.\n  split_boundary: 0       # Byte boundary where header and data are split for Tx, 0 if no split\n  batch_size: 10240\n  payload_size: 1000\n  header_size: 64\n  eth_dst_addr: &lt;00:00:00:00:00:00&gt; # Destination MAC address - required when Rx flow_isolation=true\n  ip_src_addr: &lt;1.2.3.4&gt;  # Source IP address - required on layer 3 network\n  ip_dst_addr: &lt;5.6.7.8&gt;  # Destination IP address - required on layer 3 network\n  udp_src_port: 4096      # UDP source port\n  udp_dst_port: 4096      # UDP destination port\n  address: &lt;0000:00:00.0&gt; # Source NIC Bus ID. Should match the address of the Tx interface above\n</code></pre> <ol> <li>The <code>scheduler</code> section is passed to the multi threaded scheduler we declare in the <code>main()</code> function of this application. See the holoscan SDK documentation and API docs for more details. This is related to the Holoscan core library and is not specific to Holoscan Networking.</li> <li>The <code>advanced_network</code> section is passed to the <code>AdvNetworkOpRx</code> and <code>AdvNetworkOpTx</code> operators which are responsible for setting up the NIC.</li> <li><code>manager</code> is the backend networking library. default: <code>dpdk</code>. Other: <code>gpunetio</code> (DOCA GPUNet IO + DOCA Ethernet &amp; Flow). Coming soon: <code>rivermax</code>, <code>rdma</code>.</li> <li><code>master_core</code> is the ID of the CPU core used for setup. It does not need to be isolated, and is recommended to differ differ from the <code>cpu_core</code> fields below used for polling the NIC.</li> <li>The <code>memory_regions</code> section lists where the NIC will write/read data from/to when bypassing the OS kernel. Tip: when using GPU buffer regions, keeping the sum of their buffer sizes lower than 80% of your BAR1 size is generally a good rule of thumb \ud83d\udc4d.</li> <li>A descriptive name for that memory region to refer to later in the <code>interfaces</code> section.</li> <li>The type of memory region. Best options are <code>device</code> (GPU), or <code>huge</code> (pages - CPU). Also supported but not recommended are <code>malloc</code> (CPU) and <code>pinned</code> (CPU).</li> <li>The GPU ID for <code>device</code> memory regions. The NUMA node ID for CPU memory regions.</li> <li>The number of buffers in the memory region. A higher value means more time to process the data, but it takes additional space on the GPU BAR1. Too low increases the risk of dropping packets from the NIC having nowhere to write (Rx) or the risk of higher latency from buffering (Tx). Need a rule of thumb \ud83d\udc4d? 5x the <code>batch_size</code> below is a good starting point.</li> <li>The size of each buffer in the memory region. These should be equal to your maximum packet size, or less if breaking down packets (ex: header data split, see the <code>rx</code> queue below).</li> <li>The <code>interfaces</code> section lists the NIC interfaces that will be configured for the application.</li> <li>A descriptive name for that interface, currently only used for logging.</li> <li>The PCIe/bus address of that interface, as identified in previous sections.</li> <li>Each interface can have a <code>tx</code> (transmitting) or <code>rx</code> (receiving) section, or both if you'd like to configure both Tx and Rx on the same interface.</li> <li>The <code>queues</code> section lists the queues for that interface. Queues are a core concept of NICs: they handle the actual receiving or transmitting of network packets. Rx queues buffer incoming packets until they can be processed by the application, while Tx queues hold outgoing packets waiting to be sent on the network. The simplest setup uses only one receive and one transmit queue. Using more queues allows multiple streams of network traffic to be processed in parallel, as each queue can be assigned to a specific CPU core, and are assigned their own memory regions that are not shared.</li> <li>A descriptive name for that queue, currently only used for logging.</li> <li>The ID of that queue, which can be referred to later in the <code>flows</code> section.</li> <li>The number of packets per batch. The <code>advanced_network</code> Rx operator will forward packets on a timer, or when the NIC receives enough packets for a whole batch per this number. The <code>advanced_network</code> Tx operator needs to ensure it does not send more packets than this value on each <code>Operator::compute()</code> call.</li> <li>The ID of the CPU core that this queue will use to poll the NIC. Ideally one isolated core per queue.</li> <li>The list of memory regions where this queue will write/read packets from/to. The order matters: the first memory region will be used first to read/write from until it fills up one buffer (<code>buf_size</code>), after which it will move to the next region in the list and so on until the packet is fully written/read. See the <code>memory_regions</code> for the <code>rx</code> queue below for an example.</li> <li>The <code>offloads</code> section (Tx queues only) lists optional tasks that can be offloaded to the NIC. The only value currently supported is <code>tx_eth_src</code>, that lets the NIC insert the ethernet source mac address in the packet headers. Note: IP, UDP, and Ethernet Checksums or CRC are always done by the NIC currently and are not optional.</li> <li>Same as for <code>tx_port</code>. Each interface in this list should have a unique mac address. This one will do <code>rx</code> per config below.</li> <li>Whether to isolate the Rx flow. If true, any incoming packets that does not match the MAC address of this interface - or isn't directed to a queue when the <code>flows</code> section below is used - will be delegated back to Linux for processing (no kernel bypass). This is useful to let this interface handle ARP, ICMP, etc. Otherwise, any packets sent to this interface (ex: ping) will need to be processed (or dropped) by your application.</li> <li><code>rx</code> queues have an <code>output_port</code> parameter so you can attach a downstream operator to receive data from this specific queue, as can be seen in the <code>Application::compose()</code> function of the sample application. Multiple <code>rx</code> queues can share the same <code>output_port</code>. In contrast, <code>tx</code> queues have a single non-configurable port (name: <code>burst_in</code>) to which upstream operators will send all packets, which are then routed to the correct queue based on the port/queue in the burst header.</li> <li>This scenario is called HDS (Header-Data Split): the packet will first be written to a buffer in the <code>Data_RX_CPU</code> memory region, filling its <code>buf_size</code> of 64 bytes - which is consistent with the size of our header - then the rest of the packet will be written to the <code>Data_RX_GPU</code> memory region. Its <code>buf_size</code> of 1000 bytes is just what we need to write the payload size for our application, no byte wasted!</li> <li>The list of flows. Flows are responsible for routing packets to the correct queue based on various properties. If this field is missing, all packets will be routed to the first queue.</li> <li>The flow name, currently only used for logging.</li> <li>The flow <code>id</code> is used to tag the packets with what flow it arrived on. This is useful when sending multiple flows to a single queue, as the user application can differentiate which flow (i.e. rules) matched the packet based on this ID.</li> <li>What to do with packets that match this flow. The only supported action currently is <code>type: queue</code> to send the packet to a queue given its <code>id</code>.</li> <li>List of rules to match packets against. All rules must be met for a packet to match the flow. Currently supported rules include <code>udp_src</code> and <code>udp_dst</code> (port numbers), <code>ipv4_len</code> (#TODO#) etc.</li> <li>The <code>bench_rx</code> section is passed to the <code>AdvNetworkingBenchDefaultRxOp</code> operator in the <code>Application::compose()</code> function of the sample application. This operator is a custom operator implemented in <code>default_bench_op_rx.h</code> that aggregates packets received from the NIC. The parameters in this section are specific to this operator, and should align with how <code>memory_regions</code> and <code>queues</code> were configured for the <code>rx</code> interface.</li> <li>The <code>bench_tx</code> section is passed to the <code>AdvNetworkingBenchDefaultTxOp</code> operator in the <code>Application::compose()</code> function of the sample application. This operator is a custom operator implemented in <code>default_bench_op_tx.h</code> that generates dummy packets to send to the NIC. The parameters in this section up to <code>header_size</code> should align with how <code>memory_regions</code> and <code>queues</code> were configured for the <code>tx</code> interface. The following parameters up to <code>udp_dst_port</code> are used to fill-in the ethernet header of the packets. The last parameter, <code>address</code>, is used to specify which NIC interface to use for the Tx operation.</li> </ol>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#52-create-your-own-rx-operator","title":"5.2 Create your own Rx operator","text":"<p>Under construction</p> <p>This section is under construction. Refer to the implementation of the <code>AdvNetworkingBenchDefaultRxOp</code> for an example.</p> Debian installationFrom source <pre><code>/opt/nvidia/holoscan/examples/adv_networking_bench/default_bench_op_rx.h\n</code></pre> <pre><code>./applications/adv_networking_bench/cpp/default_bench_op_rx.h\n</code></pre> <p>Note</p> <p>Design investigations are expected soon for a generic packet aggregator operator.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#53-create-your-own-tx-operator","title":"5.3 Create your own Tx operator","text":"<p>Under construction</p> <p>This section is under construction. Refer to the implementation of the <code>AdvNetworkingBenchDefaultTxOp</code> for an example.</p> Debian installationFrom source <pre><code>/opt/nvidia/holoscan/examples/adv_networking_bench/default_bench_op_tx.h\n</code></pre> <pre><code>./applications/adv_networking_bench/cpp/default_bench_op_tx.h\n</code></pre> <p>Note</p> <p>Designs investigations are expected soon for a generic way to prepare packets to send to the NIC.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#54-build-with-cmake","title":"5.4 Build with CMake","text":"Debian installationFrom source <ol> <li>Create a source directory and write your source file(s) for your application (and custom operators if needed)</li> <li> <p>Create a <code>CMakeLists.txt</code> file in your source directory like this one:</p> <pre><code>cmake_minimum_required(VERSION 3.20)\nproject(my_app CXX) # Add CUDA if writing .cu kernels\n\nfind_package(holoscan 2.6 REQUIRED CONFIG PATHS \"/opt/nvidia/holoscan\")\nfind_package(holoscan-networking REQUIRED CONFIG PATHS \"/opt/nvidia/holoscan\")\n\n# Create an executable\nadd_executable(my_app\n    my_app.cpp\n    ...\n)\ntarget_include_directories(my_app\n    PRIVATE\n        my_include_dirs/\n        ...\n)\ntarget_link_libraries(my_app\n    PRIVATE\n        holoscan::core\n        holoscan::ops::advanced_network_rx\n        holoscan::ops::advanced_network_tx\n        my_other_dependencies\n        ...\n)\n\n# Copy the config file to the build directory for convenience referring to it\nadd_custom_target(my_app_config_yaml\n    COMMAND ${CMAKE_COMMAND} -E copy_if_different \"${CMAKE_CURRENT_SOURCE_DIR}/my_app_config.yaml\" ${CMAKE_CURRENT_BINARY_DIR}\n    DEPENDS \"${CMAKE_CURRENT_SOURCE_DIR}/my_app_config.yaml\"\n)\nadd_dependencies(my_app my_app_config_yaml)\n</code></pre> </li> <li> <p>Build your application like so:</p> <pre><code># Your chosen paths\nsrc_dir=\".\"\nbuild_dir=\"build\"\n\n# Configure the build\ncmake -S \"$src_dir\" -B \"$build_dir\"\n\n# Build the application\ncmake --build \"$build_dir\" -j\n</code></pre> Failed to detect a default CUDA architecture. <p>Add the path to your installation of <code>nvcc</code> to your <code>PATH</code>, or pass its to the cmake configuration command like so (adjust to your CUDA/nvcc installation path):</p> <pre><code>cmake -S \"$src_dir\" -B \"$build_dir\" -D CMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc\n</code></pre> </li> <li> <p>Run your application like so:</p> <pre><code>\"./$build_dir/my_app my_app_config.yaml\"\n</code></pre> </li> </ol> <ol> <li>Create an application directory under <code>applications/</code> in your clone of the HoloHub repository, and write your source file(s) for your application (and custom operators if needed).</li> <li> <p>Add the following to the <code>application/CMakeLists.txt</code> file:</p> <pre><code>add_holohub_application(my_app DEPENDS OPERATORS advanced_network)\n</code></pre> </li> <li> <p>Create a <code>CMakeLists.txt</code> file in your application directory like this one:</p> <pre><code>cmake_minimum_required(VERSION 3.20)\nproject(my_app CXX) # Add CUDA if writing .cu kernels\n\nfind_package(holoscan 2.6 REQUIRED CONFIG PATHS \"/opt/nvidia/holoscan\")\n\n# Create an executable\nadd_executable(my_app\n    my_app.cpp\n    ...\n)\ntarget_include_directories(my_app\n    PRIVATE\n        my_include_dirs/\n        ...\n)\ntarget_link_libraries(my_app\n    PRIVATE\n        holoscan::core\n        holoscan::ops::advanced_network_rx\n        holoscan::ops::advanced_network_tx\n        my_other_dependencies\n        ...\n)\n\n# Copy the config file to the build directory for convenience referring to it\nadd_custom_target(my_app_config_yaml\n    COMMAND ${CMAKE_COMMAND} -E copy_if_different \"${CMAKE_CURRENT_SOURCE_DIR}/my_app_config.yaml\" ${CMAKE_CURRENT_BINARY_DIR}\n    DEPENDS \"${CMAKE_CURRENT_SOURCE_DIR}/my_app_config.yaml\"\n)\nadd_dependencies(my_app my_app_config_yaml)\n</code></pre> </li> <li> <p>Build your application like so:</p> <pre><code>./dev_container build_and_run my_app --no_run\n</code></pre> </li> <li> <p>Run your application like so:</p> <pre><code>./dev_container launch --img holohub:my_app --docker_opts \"-u 0 --privileged\" --bash -c \"./build/my_app/applications/my_app my_app_config.yaml\"\n</code></pre> <p>or, if you have set up a shortcut to run your application with its config file through its <code>metadata.json</code> (see other apps for examples):</p> <pre><code>./dev_container build_and_run --no_build --container_args \" -u 0 --privileged\"\n</code></pre> </li> </ol>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/holoscan-bootcamp/","title":"NVIDIA Holoscan Bootcamp","text":"<p> Authors: Denis Leshchev (NVIDIA), Adam Thompson (NVIDIA), Nicolas Lebovitz (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 2.2.0 Tested Holoscan SDK versions: 2.3.0 Contribution metric: Level 1 - Highly Reliable</p>"},{"location":"tutorials/holoscan-bootcamp/#overview","title":"Overview","text":"<p>This is a meta-tutorial for deploying and running NVIDIA Holoscan Bootcamp lab materials.</p>"},{"location":"tutorials/holoscan-bootcamp/#description","title":"Description","text":"<p>NVIDIA Holoscan Bootcamp was conducted by the NVIDIA team in collaboration with OpenHackathons on September 24th, 2024 (event link). The bootcamp included a lecture and a hands-on Python-based lab. The lab materials are freely available for downloading at OpenHackathons repo.</p> <p>NVIDIA Holoscan Bootcamp lab materials include a Jupyter notebook for self-paced studying of the core Holoscan concepts.</p> <p>The notebook covers the following topics: - Creating custom Holoscan operators - Connecting operators to form an application - Build Holoscan applications with GPU-accelerated Python packages - How to build a sensor to visualization pipeline - Measuring and optimizing application performance - How to build complex sensor processing workflows using multiple fragments and the advanced schedulers</p> <p>The notebook guides users through the topics with example code and diagrams, and includes exercises to help them practice the concepts they've learned.</p> <p>To run the lab materials, clone the repo and follow the deployment guide.</p>"},{"location":"tutorials/holoscan-playground-on-aws/","title":"Holoscan Playground on AWS","text":"<p> Authors: Jin Li (NVIDIA) Supported platforms: x86_64, aarch64 Last modified: May 13, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p>","tags":["Cloud"]},{"location":"tutorials/holoscan-playground-on-aws/#overview","title":"Overview","text":"<p>The Holoscan on AWS EC2 experience is an easy way for having a first try at the Holoscan SDK. The Holoscan SDK documentation lists out the hardware prerequisites. If you have a compatible hardware at hand, please get started with the SDK on your hardware. Otherwise, you could utilize an AWS EC2 instance to have a first look at the Holoscan SDK by following this guide.</p> <p>We estimate the time needed to follow this guide is around 1 hour, after which you could feel free to explore more of the SDK examples and applications. Please note that for the g5.xlarge instance type utilized, the cost is $1.006/hour.</p> <ol> <li> <p>The AWS experience is intended as a trial environment of the Holoscan SDK, not as a full time development environment. Some limitations of running the SDK on an EC2 instance are:</p> </li> <li> <p>An EC2 instance does not have the capability of live input sources, including video capture cards like AJA and Deltacast, or the onboard HDMI input port on devkits. An EC2 instance does not have ConnectX networking capabilities available on devkits.</p> </li> <li> <p>Display forwarding from EC2 to your local machine depends on internet connectivity and results in heavy latency, so if you would like to develop applications with display, it is not ideal.</p> </li> </ol>","tags":["Cloud"]},{"location":"tutorials/holoscan-playground-on-aws/#launch-ec2-instance","title":"Launch EC2 Instance","text":"<p>Type in the name that you want to give to the instance.</p> <p></p> <p>In the <code>Application and OS Images (Amazon Machine Image)</code> window, search for <code>NVIDIA</code>.</p> <p></p> <p>From the results, switch to <code>AWS Marketplace AMIs</code> and choose <code>NVIDIA GPU-Optimized AMI</code>.</p> <p></p> <p>Select <code>Continue</code> after viewing the details of this AMI.</p> <p></p> <p>The selected AMI should look like this in the view to create an instance:</p> <p></p> <p>For <code>Instance type</code>, select <code>g5.xlarge</code>. Note: If you see an error similar to <code>The selected instance type is not supported in the zone (us-west-2d). Please select a different instance type or subnet.</code>, go down to <code>Network settings</code>, click on <code>Edit</code>, and try changing the <code>Subnet</code> selection. Note: If <code>g5.xlarge</code> is not available in any region/subnet accessible to you, <code>p3.2xlarge</code> should also work.</p> <p></p> <p>For <code>Key pair</code>, create a new key pair, enter the key pair name as you like, and store the file as prompted. After clicking on <code>Create key pair</code>, the file <code>your-name.pem</code> will be automatically downloaded by the browser to the Downloads folder. Then select the key pair in the view to create an instance.</p> <p></p> <p>Configure the <code>Network settings</code>. Click on <code>Edit</code> to start. * If you got an error in the <code>Instance type</code> selection about <code>g5.xlarge</code> being unavailable, try changing your <code>Subnet</code> selection in <code>Network settings</code>. Otherwise, there is no need to change the <code>Subnet</code> selection. * Make sure your <code>Auto-assign public IP</code> is enabled, otherwise you would have issues ssh\u2019ing into the instance. * Select a security group with a public IP address where you plan to ssh from, if one doesn\u2019t exist yet, select <code>create security group</code>.     * If you\u2019re already on the local machine you plan to ssh from, select <code>My IP</code> under <code>Source Type</code>.     * To add other machines that you plan to ssh from, select <code>Custom</code> under <code>Source Type</code> and enter your public IP address under <code>Source</code>. You can find the public IP address of the machine by going to https://www.whatsmyip.org/ from the machine.</p> <p></p> <p>Keep the default 128 GB specification in <code>Configure storage</code>.</p> <p></p> <p>Your Summary on the right side should look like this:</p> <p></p> <p>Please note that with a different instance type, Storage (volumes) may look different too.</p> <p>Click <code>Launch instance</code>, and you should see a <code>Success</code> notification.</p> <p></p> <p>Now go back to the <code>Instances</code> window to view the <code>Status check</code> of the instance we had just launched. It should show <code>Initializing</code> for a few minutes:</p> <p></p> <p>And later it should show <code>2/2 checks passed</code>:</p> <p></p> <p>Now we\u2019re ready to ssh into the instance.</p>","tags":["Cloud"]},{"location":"tutorials/holoscan-playground-on-aws/#ssh-into-ec2-instance","title":"SSH into EC2 Instance","text":"<p>Click on the instance ID, and you should see this layout for instance details. Click on the <code>Connect</code> button on the top right.</p> <p></p> <p>Under the <code>SSH client</code> tab there are the SSH instructions. Note that the username <code>root</code> is guessed, and for the AMI we chose, it should be <code>ubuntu</code>. The private key file that you saved from when you were configuring the instance should be on the machine that you are ssh\u2019ing from.</p> <p>Add <code>-X</code> to the ssh command to enable display forwarding.</p> <p></p>","tags":["Cloud"]},{"location":"tutorials/holoscan-playground-on-aws/#setting-up-display-forwarding-from-ec2-instance","title":"Setting up Display Forwarding from EC2 Instance","text":"<p>Holoscan SDK has examples and applications that depend on seeing a display. For this experience, we will do X11 forwarding.</p>","tags":["Cloud"]},{"location":"tutorials/holoscan-playground-on-aws/#on-ec2-instance","title":"On EC2 Instance","text":"<p>First,install the package needed for a simple forwarding test, xeyes.</p> <pre><code>sudo apt install -y x11-apps\n</code></pre> <p>Next, run \u201cxeyes\u201d in the terminal, and you should get a display window popping up on the machine you\u2019re ssh\u2019ing from: <pre><code>xeyes\n</code></pre></p> <p>https://github.com/jin-nvidia/holohub/assets/60405124/57c76bed-ca16-458b-8740-1e4351ca63f7</p> <p>If you run into display issues, ensure the machine you\u2019re ssh\u2019ing from has X11 forwarding enabled. Please see the Troubleshooting section.</p>","tags":["Cloud"]},{"location":"tutorials/holoscan-playground-on-aws/#in-a-docker-container-on-ec2-instance","title":"In a Docker Container On EC2 Instance","text":"<p>Now you have enabled display forwarding on the EC2 instance bare metal, let\u2019s take it one step further to enable display forwarding from a Docker container on the EC2 instance.</p> <pre><code>XSOCK=/tmp/.X11-unix\nXAUTH=/tmp/.docker.xauth\n# the error \u201cfile does not exist\u201d is expected at the next command\nxauth nlist $DISPLAY | sed -e 's/^..../ffff/' | sudo xauth -f $XAUTH nmerge -\nsudo chmod 777 $XAUTH\ndocker run -ti -e DISPLAY=$DISPLAY -v $XSOCK:$XSOCK -v $XAUTH:$XAUTH -e XAUTHORITY=$XAUTH --net host ubuntu:latest\n</code></pre> <p>Within the container:</p> <p><pre><code>apt update &amp;&amp; apt install -y x11-apps\nxeyes\n</code></pre> Press  <code>ctrl + D</code> to exit the Docker container. Now we have enabled display forwarding from both EC2 bare metal and containerized environments!</p>","tags":["Cloud"]},{"location":"tutorials/holoscan-playground-on-aws/#run-holoscan","title":"Run Holoscan","text":"","tags":["Cloud"]},{"location":"tutorials/holoscan-playground-on-aws/#install-holoscan","title":"Install Holoscan","text":"<p>There are several ways to install the Holoscan SDK. For the quickest way to get started, we will choose the Holoscan Docker container that already has all dependencies set up. We run nvidia-smi in the EC2 instance to check that there are drivers installed:</p> <p></p> <p>Follow the overview of https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/containers/holoscan. Some modifications are made to the original commands due to the EC2 environment, <code>--gpus all</code> and <code>-v $XAUTH:$XAUTH -e XAUTHORITY=$XAUTH</code>. <pre><code># install xhost util\nsudo apt install -y x11-xserver-utils\n\nxhost +local:docker\n\nnvidia_icd_json=$(find /usr/share /etc -path '*/vulkan/icd.d/nvidia_icd.json' -type f,l -print -quit 2&gt;/dev/null | grep .) || (echo \"nvidia_icd.json not found\" &gt;&amp;2 &amp;&amp; false)\n\nexport NGC_CONTAINER_IMAGE_PATH=\"nvcr.io/nvidia/clara-holoscan/holoscan:v0.6.0-dgpu\"\n\ndocker run -it --rm --net host \\\n  --gpus all \\\n   -v $XAUTH:$XAUTH -e XAUTHORITY=$XAUTH \\\n  -v /tmp/.X11-unix:/tmp/.X11-unix \\\n  -v $nvidia_icd_json:$nvidia_icd_json:ro \\\n  -e NVIDIA_DRIVER_CAPABILITIES=graphics,video,compute,utility,display \\\n  -e DISPLAY=$DISPLAY \\\n  --ipc=host \\\n  --cap-add=CAP_SYS_PTRACE \\\n  --ulimit memlock=-1 \\\n  ${NGC_CONTAINER_IMAGE_PATH}\n</code></pre></p>","tags":["Cloud"]},{"location":"tutorials/holoscan-playground-on-aws/#sanity-check-with-holoscan-hello-world","title":"Sanity Check with Holoscan Hello World","text":"<pre><code>/opt/nvidia/holoscan/examples/hello_world/cpp/hello_world\n</code></pre>","tags":["Cloud"]},{"location":"tutorials/holoscan-playground-on-aws/#examples","title":"Examples","text":"<p>Refer to each one of the Holoscan SDK examples on GitHub. You will find these examples installed under <code>/opt/nvidia/holoscan/examples/</code>.</p>","tags":["Cloud"]},{"location":"tutorials/holoscan-playground-on-aws/#video-replayer-example","title":"Video Replayer Example","text":"<p>Let\u2019s take a look at the video replayer example which is a basic video player app. Since we are in the Docker container, there\u2019s no need to manually download data as it already exists in the container.</p> <p>Run the video_replayer example</p> <p><pre><code>cd /opt/nvidia/holoscan\n./examples/video_replayer/cpp/video_replayer\n</code></pre> You should see a window like below</p> <p>https://github.com/jin-nvidia/holohub/assets/60405124/7ae99409-ca42-4c38-b495-84a59648b671</p> <p>Please note that it is normal for the video stream to be lagging behind since it is forwarded from a docker container on a EC2 instance to your local machine. How much the forwarded video will lag heavily depends on the internet connection. When running Holoscan applications on the edge, we should have significantly less latency lag.</p> <p>You can close the sample application by pressing ctrl +C.</p> <p>Now that we have run a simple video replayer, let\u2019s explore the examples a little more.</p>","tags":["Cloud"]},{"location":"tutorials/holoscan-playground-on-aws/#tensor-interoperability-example","title":"Tensor Interoperability Example","text":"","tags":["Cloud"]},{"location":"tutorials/holoscan-playground-on-aws/#the-c-tensor-interop-example","title":"The C++ Tensor Interop example","text":"<p>Since we used the Debian package install, run the C++ tensor interopability example by</p> <pre><code>/opt/nvidia/holoscan/examples/tensor_interop/cpp/tensor_interop\n</code></pre> <p>Please refer to the README and the source file to see how we can have interoperability between a native operator (<code>ProcessTensorOp</code>) and two wrapped GXF Codelets (<code>SendTensor</code> and <code>ReceiveTensor</code>). For the Holoscan documentation on tensor interop in the C++ API, please see Interoperability between GXF and native C++ operators.</p>","tags":["Cloud"]},{"location":"tutorials/holoscan-playground-on-aws/#the-python-tensor-interop-example","title":"The Python Tensor Interop example","text":"<p>The Python Tensor Interop example demonstrates interoperability between a native Python operator (<code>ImageProcessingOp</code>) and two operators that wrap existing C++ based operators,  (<code>VideoStreamReplayerOp</code> and <code>HolovizOp</code>) through the Holoscan Tensor object.</p> <p>Run the Python example by</p> <p><pre><code>python3 /opt/nvidia/holoscan/examples/tensor_interop/python/tensor_interop.py\n</code></pre> This example applies a Gaussian filtering to each frame of an endoscopy video stream and displays the filtered (blurred) video stream. You should see a window like below</p> <p>https://github.com/jin-nvidia/holohub/assets/60405124/b043637b-5fd9-4ee1-abc5-0dae069e785f</p> <p>The native Python operator is defined at tensor_interop.py#L37. We can see in the initialization <code>__init__()</code> of the operator, <code>self.count</code> was initialize to 1. In the <code>setup()</code> method, the input message, output message and the parameter <code>sigma</code> is defined. The <code>compute()</code> method is what gets called every time. In the <code>compute()</code> method, first we receive the upstream tensor by</p> <pre><code>in_message = op_input.receive(\"input_tensor\")\n</code></pre> <p>Please note that <code>input_tensor</code> is the name defined in <code>setup()</code>.</p> <p><code>cp_array</code> is the CuPy array that holds the output value after the Gaussian filter, and we can see that the way the CuPy array gets transmitted downstream is <pre><code>out_message = dict()\n\u2026\n# add each CuPy array to the out_message dictionary\nout_message[key] = cp_array\n\u2026\nop_output.emit(out_message, \"output_tensor\")\n</code></pre></p> <p>Please note that <code>output_tensor</code> is the name defined in <code>setup()</code>.</p> <p>Since there is only one input and one output port, when connecting the native Python operator <code>ImageProcessingOp</code> to its upstream and downstream operators, we do not need to specify the in/out name for <code>ImageProcessingOp</code>: <pre><code>self.add_flow(source, image_processing)\nself.add_flow(image_processing, visualizer, {(\"\", \"receivers\")})\n</code></pre></p> <p>Otherwise, with each <code>add_flow()</code>, the input and output port names need to be specified when multiple ports are present.</p> <p>For more information on tensor interop in Python API, please see the Holoscan documentation Interoperability between wrapped and native Python operators.</p> <p>Now that we have seen an example of tensor interop for single tensors per port, let\u2019s look at the next example where there are multiple tensors in the native operator\u2019s output port.</p>","tags":["Cloud"]},{"location":"tutorials/holoscan-playground-on-aws/#holoviz-example","title":"Holoviz Example","text":"<p>Let\u2019s take a look at the Holoviz example. Run the example <pre><code>python3 /opt/nvidia/holoscan/examples/holoviz/python/holoviz_geometry.py\n</code></pre></p> <p>You should get something like below on the display</p> <p>https://github.com/jin-nvidia/holohub/assets/60405124/6d79845a-66bd-4448-9646-284b90c5e5f3</p> <p>Please take your time to look through holoviz_geometry.py for how each one of the shapes and text in the native Holoscan Python operator is defined.</p> <p>Let\u2019s also dive into how we can add to <code>out_message</code> and pass to Holoviz various tensors at the same time, including the frame itself, <code>box_coords</code>,  <code>triangle_coords</code>, <code>cross_coords</code>, <code>oval_coords</code>, the time varying <code>point_coords</code>, and <code>label_coords</code>.</p> <pre><code># define the output message dictionary where box_coords is a numpy array and \u201cboxes\u201d is the tensor name\nout_message = {\n            \"boxes\": box_coords,\n            \"triangles\": triangle_coords,\n            \"crosses\": cross_coords,\n            \"ovals\": oval_coords,\n            \"points\": point_coords,\n            \"label_coords\": label_coords,\n            \"dynamic_text\": dynamic_text,\n}\n\n# emit the output message\nop_output.emit(out_message, \"outputs\")\n</code></pre> <p>We can also see that each tensor name is referenced by the <code>tensors</code> parameter in the instantiation of a Holoviz operator at line 249.</p> <p>This is a great example and reference not only for passing different shapes to Holoviz, but also creating and passing multiple tensors within one message from a native Holoscan Python operator to the downstream operators.</p> <p>For more information on the Holoviz module, please see the Holoscan documentation.</p> <p>Exit from the Docker container by ctrl+D.</p>","tags":["Cloud"]},{"location":"tutorials/holoscan-playground-on-aws/#applications","title":"Applications","text":"<p>To run the reference applications on Holoscan, let\u2019s go to HoloHub - a central repository for users and developers to share reusable operators and sample applications.</p> <p>On the EC2 instance, clone the HoloHub repo: <pre><code>cd ~\ngit clone https://github.com/nvidia-holoscan/holohub.git\ncd holohub\n</code></pre> To set up and build HoloHub, we will go with the option <code>Building dev container</code>: Run the following command from the holohub directory to build the development container: <pre><code>./dev_container build\n</code></pre></p> <p>Check the tag for the container we had just build:</p> <p><pre><code>docker images\n</code></pre> There should be an image with repository:tag similar to <code>holohub:ngc-vx.y.z-dgpu</code> where <code>x.y.z</code> is the latest SDK version. We will set this as <code>HOLOHUB_IMAGE</code>: <pre><code># make sure to replace 0.6.0 with the actual SDK version\nexport HOLOHUB_IMAGE=holohub:ngc-v0.6.0-dgpu\n</code></pre> Next, launch the dev container for HoloHub. On a regular machine we can do so by <code>./dev_container launch</code>, however we need to make a few adjustments to the command again since we\u2019re running on an EC2 instance: <pre><code>docker run -it --rm --net host  -v /etc/group:/etc/group:ro -v /etc/passwd:/etc/passwd:ro -v $PWD:/workspace/holohub -w /workspace/holohub --gpus all -e NVIDIA_DRIVER_CAPABILITIES=graphics,video,compute,utility,display -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY --group-add video -v /etc/vulkan/icd.d/nvidia_icd.json:/etc/vulkan/icd.d/nvidia_icd.json:ro  -v $XAUTH:$XAUTH -e XAUTHORITY=$XAUTH $HOLOHUB_IMAGE\n</code></pre> Please refer to HoloHub for instructions on building each application.</p>","tags":["Cloud"]},{"location":"tutorials/holoscan-playground-on-aws/#endoscopy-tool-tracking-application","title":"Endoscopy Tool Tracking Application","text":"<p>Build the sample app and run: <pre><code>./run build endoscopy_tool_tracking\n./run launch endoscopy_tool_tracking cpp\n</code></pre> You should see a window like:</p> <p>https://github.com/jin-nvidia/holohub/assets/60405124/8eb93c50-d893-4b2c-897b-57de94b91371</p> <p>Note: Be prepared to wait a few minutes as we\u2019re running the app for the first time, and it will convert the ONNX model to a TensorRT engine. The conversion happens only for the first time, after that, each time we run the app the TensorRT engine is already present.</p> <p>Please visit HoloHub to see the application graph, different input types (although on the EC2 instance we can not use a live source such as the AJA capture card), and the construction of the same application in C++ vs in Python.</p>","tags":["Cloud"]},{"location":"tutorials/holoscan-playground-on-aws/#multi-ai-ultrasound-application","title":"Multi AI Ultrasound Application","text":"<p>In the last application we saw how to run AI inference on the video source. Next, let\u2019s see how we can run inference with multiple AI models at the same time within a Holoscan application, enabled by Holoscan Inference Module. Build the application in applications/multiai_ultrasound <pre><code>./run build multiai_ultrasound\n</code></pre></p> <p>Launch the Python application: <pre><code>./run launch multiai_ultrasound python\n</code></pre> You should see a window like below:</p> <p>https://github.com/jin-nvidia/holohub/assets/60405124/9d347b44-d635-4cc6-b013-7d26e3e4e2be</p> <p>You can find more information on Holoscan Inference Module here, including the parameters you can specify to define inference configuration, how to specify the multiple (or single) model(s) you want to run, and how the Holoscan Inference Module functions as an operator within the Holoscan SDK framework.</p> <p>Please see the application graph and more on HoloHub for how the multi AI inference connects to the rest of the operators, the definition of the same application in Python vs in C++, and how the multiai_ultrasound.yaml config file defines parameters for each operator especially the Holoscan Inference Module.</p>","tags":["Cloud"]},{"location":"tutorials/holoscan-playground-on-aws/#stop-ec2-instance","title":"Stop EC2 Instance","text":"<p>Now that you have run several Holoscan Examples and HoloHub Applications, please continue exploring the rest of Examples and Applications, and when you\u2019re ready, stop the instance by going back to EC2 page with the list of <code>Instances</code>, select the launched instance, and select <code>Stop Instance</code> in the dropdown <code>Instance state</code>.</p> <p></p>","tags":["Cloud"]},{"location":"tutorials/holoscan-playground-on-aws/#troubleshooting","title":"Troubleshooting","text":"<p>If you receive a display forwarding error such as <pre><code>unable to open display \"localhost:10.0\"\n</code></pre> <pre><code>Glfw Error 65544: X11: Failed to open display localhost:10.0\nterminate called after throwing an instance of 'std::runtime_error'\n  what():  Failed to initialize glfw\n</code></pre> Please see below to find the suggestion for your OS.</p>","tags":["Cloud"]},{"location":"tutorials/holoscan-playground-on-aws/#from-a-linux-local-machine","title":"From a Linux Local Machine","text":"<ul> <li>Ensure that <code>-X</code> is added to the ssh command when connecting to the EC2 instance.</li> </ul>","tags":["Cloud"]},{"location":"tutorials/holoscan-playground-on-aws/#from-a-windows-local-machine","title":"From a Windows Local Machine","text":"<ul> <li>Ensure that <code>-X</code> is added to the ssh command when connecting to the EC2 instance.</li> <li>Try using MobaXTerm to establish a SSH connection with X11 forwarding enabled.</li> </ul>","tags":["Cloud"]},{"location":"tutorials/holoscan-playground-on-aws/#from-a-mac-local-machine","title":"From a Mac Local Machine","text":"<ul> <li>Download Quartz, reboot, and enable the following.</li> </ul> <p>Once Quartz is downloaded it will automatically launch when running display forwarding apps like <code>xeyes</code>.</p> <ul> <li>Ensure that <code>-X</code> is added to the ssh command when connecting to the EC2 instance.</li> </ul>","tags":["Cloud"]},{"location":"tutorials/holoscan_response_time_analysis/","title":"Holoscan SDK Response-Time Analysis","text":"<p> Authors: Philip Schowitz (The University of British Columbia), Arpan Gujarati (The University of British Columbia), Soham Sinha (NVIDIA) Supported platforms: x86_64, aarch64 Last modified: May 13, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.2.0 Tested Holoscan SDK versions: 2.2.0 Contribution metric: Level 2 - Trusted</p> <p>We have performed a theoretical response-time analysis of applications created using Holoscan SDK in an RTSS paper [1]. This work accounts for different queuing delays due to different types of connections and dependencies between the operators of a Holoscan application. This directory contains helpful scripts for the timing analysis of Holoscan applications, based on the paper.</p> <p>Detailed instructions for how to reproduce the results of the paper, along with the code, can be found in the <code>artifact</code> directory.</p>","tags":["Real-Time","Performance","Optimization"]},{"location":"tutorials/holoscan_response_time_analysis/#scripts","title":"Scripts","text":"<p>The scripts in current directory (<code>computeWCRT.py</code> and <code>runsimulation.py</code>) are written in Python and require the <code>networkx</code> and <code>pydot</code> packages, which can be easily installed with pip.</p> <pre><code>pip install networkx\npip install pydot\n</code></pre> <p>Each script takes a representation of an application graph in the form of a DOT file. An example DOT file is provided in <code>exampledot.dot</code>. The DOT file defines a Holoscan application graph in two parts. The first section consists of the names of the operators, with their worst-case execution times as attributes. The second section captures the connections between operators.</p> <p>We assume that there is only one root operator and one leaf operator in the application graph, without losing any generality. More information on why this works for more than one root and leaf can be found in the paper [1].</p>","tags":["Real-Time","Performance","Optimization"]},{"location":"tutorials/holoscan_response_time_analysis/#computewcrtpy","title":"<code>computeWCRT.py</code>","text":"<p>This script takes a path to a DOT file, representing a Holoscan application graph and computes an upper bound of worst-case response time (WCRT) for the application. The script does not run a Holoscan application on actual hardware. Instead, it computes the WCRT following the timing analysis done in our paper [1]. The algorithm runs reasonably fast (within quadratic time) for graphs with up to 20-30 nodes.</p> <pre><code>python computeWCRT.py examplegraph.dot\nWorst-case response time: 1600\n</code></pre> <p>This script can also, optionally, account for extra scheduling overheads, empirically observed on Jetson AGX Orin. The <code>-overhead</code> argument can include this overhead in the WCRT analysis. Kindly note that this is a heuristic based on measurements using a Jetson AGX Orin and will not be accurate for all systems.</p> <pre><code>python computeWCRT.py examplegraph.dot --overhead\nWorst-case response time: 1612\n</code></pre>","tags":["Real-Time","Performance","Optimization"]},{"location":"tutorials/holoscan_response_time_analysis/#runsimulationpy","title":"<code>runsimulation.py</code>","text":"<p>This script takes a path to a DOT file representing a Holoscan application graph, an expected runtime, and a root operator period (in this order), and runs a discrete-event simulation of the execution of the Holoscan application under the given conditions, printing the results. The runtime argument determines how long the simulation will run. For example, if an application takes 1000 time units to process an input, and the runtime is 1100 time units, then only one iteration will be simulated. The period argument determines how often the source operator can execute. If the source operator could execute every 50 time units, but period is 100 time units, then the source will be constrained in this script. </p> <p>The period will affect response times, though not necessarily the worst-case response time. For example, lowering the period may increase the queuing time of early iterations with response times still converging to the same value. In the  output below, changing the period to <code>5</code> would increase the response time of iteration 1 by 5, but leave the WCRT unchanged.</p> <pre><code>python runsimulation.py examplegraph.dot 3000 10\nIteration 0\nSource start:  0\nSink finish:   900\nResponse time: 900\n\nIteration 1\nSource start:  10\nSink finish:   1300\nResponse time: 1290\n\nIteration 2\nSource start:  200\nSink finish:   1700\nResponse time: 1500\n\nIteration 3\nSource start:  500\nSink finish:   2100\nResponse time: 1600\n\nIteration 4\nSource start:  900\nSink finish:   2500\nResponse time: 1600\n\nIteration 5\nSource start:  1300\nSink finish:   2900\nResponse time: 1600\n\nWorst-case response time: 1600\n</code></pre> <p>For this application, response times converge to the worst-case response time of 1600. Note that not all applications will converge to single value in this manner, and response times may increase or decrease periodically even after reaching the worst-case response time. The previously mentioned <code>computeWCRT.py</code> script provides a theoretical upper bound, even though it can be more pessimistic than simulated or real-world observed times. More details are available in our paper [1].</p>","tags":["Real-Time","Performance","Optimization"]},{"location":"tutorials/holoscan_response_time_analysis/#citation","title":"Citation","text":"<p>[1] P. Schowitz, S. Sinha, and A. Gujarati, \u201cResponse-Time Analysis  of a Soft Real-time NVIDIA Holoscan Application,\u201d in IEEE Real-Time  Systems Symposium, 2024.</p> <p>BibTeX:</p> <pre><code>@inproceedings{Schowitz2024,\nauthor    = {P. Schowitz and S. Sinha and A. Gujarati},\ntitle     = {Response-Time Analysis of a Soft Real-time NVIDIA Holoscan Application},\nbooktitle = {Proceedings of the IEEE Real-Time Systems Symposium},\nyear      = {2024},\n}\n</code></pre>","tags":["Real-Time","Performance","Optimization"]},{"location":"tutorials/integrate_external_libs_into_pipeline/","title":"Best Practices to integrate external libraries into Holoscan pipelines","text":"<p> Authors: Meiran Peng (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The Holoscan SDK is part of NVIDIA Holoscan, the AI sensor processing platform that combines hardware systems for low-latency sensor and network connectivity, optimized libraries for data processing and AI, and core microservices to run streaming, imaging, and other applications, from embedded to edge to cloud. It can be used to build streaming AI pipelines for a variety of domains, including medical devices, high-performance computing at the edge, industrial inspection, and more.</p> <p>With the Holoscan SDK, one can develop an end-to-end GPU-accelerated pipeline with RDMA support. However, with increasing requirements for pre-processing and post-processing beyond inference-only pipelines, integration with other powerful, GPU-accelerated libraries is needed.</p> <p>One of the key features of the Holoscan SDK is its seamless interoperability with other libraries.</p> <p>This tutorial explains how to leverage this capability in your applications. For detailed examples of integrating various libraries with Holoscan applications, refer to the following sections: - Tensor Interoperability   - Integrate MatX library - DLPack support in C++   - Integrate RAPIDS cuCIM library   - Integrate CV-CUDA library   - Integrate OpenCV with CUDA Module   - Integrate PyTorch library - CUDA Interoperability   - Integrate CUDA Python library   - Integrate CuPy library</p>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#interoperability-features","title":"Interoperability Features","text":"","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#dlpack-support","title":"DLPack Support","text":"<p>The Holoscan SDK supports DLPack, enabling efficient data exchange between deep learning frameworks.</p>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#array-interface-support","title":"Array Interface Support","text":"<p>The SDK also supports the array interface, including: - <code>__array_interface__</code> - <code>__cuda_array_interface__</code></p> <p>This allows for seamless integration with various Python libraries such as: - CuPy - PyTorch - JAX - TensorFlow - Numba</p>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#technical-details","title":"Technical Details","text":"<p>The <code>Tensor</code> class is a wrapper around the <code>DLManagedTensorContext</code> struct, which holds the <code>DLManagedTensor</code> object (a DLPack structure).</p> <p>For more information on interoperability, refer to the following sections in the Holoscan SDK documentation: - Interoperability between GXF and native C++ operators - Interoperability between wrapped and native Python operators</p>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#cuda-array-interfacedlpack-support","title":"CUDA Array Interface/DLPack Support","text":"<p>The following Python libraries have adopted the CUDA Array Interface and/or DLPack standards, enabling seamless interoperability with Holoscan Tensors:</p> <ul> <li>CuPy</li> <li>CV-CUDA</li> <li>PyTorch</li> <li>Numba</li> <li>PyArrow</li> <li>mpi4py</li> <li>ArrayViews</li> <li>JAX</li> <li>PyCUDA</li> <li>DALI: the NVIDIA Data Loading Library\u00a0:</li> <li>TensorGPU objects\u00a0expose the CUDA Array Interface.</li> <li>The External Source operator\u00a0consumes objects exporting the CUDA Array Interface.</li> <li>The RAPIDS stack:</li> <li>cuCIM</li> <li>cuDF</li> <li>cuML</li> <li>cuSignal</li> <li>RMM</li> </ul> <p>For more details on using the CUDA Array Interface and DLPack with various libraries, see CuPy's Interoperability guide.</p>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#using-holoscan-tensors-in-python","title":"Using Holoscan Tensors in Python","text":"<p>The Holoscan SDK's Python API provides the <code>holoscan.as_tensor()</code> method to convert objects supporting the (CUDA) Array Interface or DLPack to a Holoscan Tensor. The <code>holoscan.Tensor</code> object itself also supports these interfaces, allowing for easy integration with compatible libraries.</p> <p>Example usage:</p> <pre><code>import cupy as cp\nimport numpy as np\nimport torch\nimport holoscan as hs\n\n# Create tensors using different libraries\ntorch_cpu_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\ntorch_gpu_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]], device=\"cuda\")\nnumpy_tensor = np.array([[1, 2, 3], [4, 5, 6]])\ncupy_tensor = cp.array([[1, 2, 3], [4, 5, 6]])\n\n# Convert to Holoscan Tensors\ntorch_cpu_to_holoscan = hs.as_tensor(torch_cpu_tensor)\ntorch_gpu_to_holoscan = hs.as_tensor(torch_gpu_tensor)\nnumpy_to_holoscan = hs.as_tensor(numpy_tensor)\ncupy_to_holoscan = hs.as_tensor(cupy_tensor)\n</code></pre>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#tensor-interoperability","title":"Tensor Interoperability","text":"","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#integrate-matx-library","title":"Integrate MatX library","text":"<p>MatX library (An efficient C++17 GPU numerical computing library with Python-like syntax) is an open-source, efficient C++17 GPU numerical computing library created by NVIDIA. It provides a NumPy-like interface for GPU-accelerated numerical computing, enabling developers to write high-performance, GPU-accelerated code with ease.</p>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#installation","title":"Installation","text":"<p>MatX is a header-only library. Using it in your own projects is as simple as including only the core <code>matx.h</code> file.</p> <p>Please refer to the MatX documentation for detailed instructions on building and using the MatX library.</p> <p>The following is a sample CMakeLists.txt file for a project that uses MatX:</p> <pre><code>cmake_minimum_required(VERSION 3.20)\nproject(my_app CXX)\n\n# Holoscan\nfind_package(holoscan 2.2 REQUIRED CONFIG\n             PATHS \"/opt/nvidia/holoscan\" \"/workspace/holoscan-sdk/install\")\n\n# Enable cuda language\nset(CMAKE_CUDA_ARCHITECTURES \"70;80\")\nenable_language(CUDA)\n\n# Download MatX (from 'main' branch)\ninclude(FetchContent)\nFetchContent_Declare(\n  MatX\n  GIT_REPOSITORY https://github.com/NVIDIA/MatX.git\n  GIT_TAG main\n)\nFetchContent_MakeAvailable(MatX)\n\nadd_executable(my_app\n  my_app.cpp\n)\n\ntarget_link_libraries(my_app\n  PRIVATE\n  holoscan::core\n  # ...\n  matx::matx\n)\n</code></pre>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#sample-code","title":"Sample code","text":"<p>The following are the sample applications that use the MatX library to integrate with Holoscan SDK.</p> <ul> <li>Multi AI Application with SSD Detection and MONAI Endoscopic Tool Segmentation</li> <li><code>applications/multiai_endoscopy</code></li> <li>Network Radar Pipeline</li> <li><code>applications/network_radar_pipeline/cpp</code></li> <li>Simple Radar Pipeline Application</li> <li><code>applications/simple_radar_pipeline/cpp</code></li> </ul> <p>On the GPU</p> <ul> <li>https://github.com/nvidia-holoscan/holohub/blob/main/applications/multiai_endoscopy/cpp/post-proc-matx-gpu/multi_ai.cu</li> </ul> <pre><code>#include &lt;holoscan/holoscan.hpp&gt;\n#include &lt;matx.h&gt;\n\n// ...\n\nvoid compute(InputContext&amp; op_input, OutputContext&amp; op_output,\n             ExecutionContext&amp; context) override {\n  // Get input message and make output message\n  auto in_message = op_input.receive&lt;gxf::Entity&gt;(\"in\").value();\n  // ...\n  auto boxes = in_message.get&lt;Tensor&gt;(\"inference_output_detection_boxes\");\n  auto scores = in_message.get&lt;Tensor&gt;(\"inference_output_detection_scores\");\n  int32_t Nb = scores-&gt;shape()[1];  // Number of boxes\n  auto Nl = matx::make_tensor&lt;int&gt;({});  // Number of label boxes\n  // ...\n  auto boxesl_mx = matx::make_tensor&lt;float&gt;({1, Nl(), 4});\n  (boxesl_mx = matx::remap&lt;1&gt;(boxes_ix_mx, ixl_mx)).run();\n  // ...\n  // Holoscan tensors to MatX tensors\n  auto boxes_mx = matx::make_tensor&lt;float&gt;((float*)boxes-&gt;data(), {1, Nb, 4});\n  // ...\n  // MatX to Holoscan tensor\n  auto boxes_hs = std::make_shared&lt;holoscan::Tensor&gt;(boxesls_mx.GetDLPackTensor());\n  // ...\n}\n</code></pre> <p>On the CPU</p> <ul> <li>https://github.com/nvidia-holoscan/holohub/blob/main/applications/multiai_endoscopy/cpp/post-proc-matx-cpu/multi_ai.cpp</li> </ul> <p>MatX library usage on the CPU is similar to the GPU version, but the <code>run()</code> function is called with <code>matx::SingleThreadHostExecutor()</code> to run the operation on the CPU.</p> <pre><code>#include &lt;holoscan/holoscan.hpp&gt;\n#include &lt;matx.h&gt;\n\n// ...\n\nvoid compute(InputContext&amp; op_input, OutputContext&amp; op_output,\n             ExecutionContext&amp; context) override {\n  // Get input message and make output message\n  auto in_message = op_input.receive&lt;gxf::Entity&gt;(\"in\").value();\n  // ...\n  auto boxesh = in_message.get&lt;Tensor&gt;(\"inference_output_detection_boxes\");  // (1, num_boxes, 4)\n  auto scoresh = in_message.get&lt;Tensor&gt;(\"inference_output_detection_scores\");  // (1, num_boxes)\n  int32_t Nb = scoresh-&gt;shape()[1];  // Number of boxes\n  auto Nl = matx::make_tensor&lt;int&gt;({});  // Number of label boxes\n  // ...\n  auto boxes = copy_device2vec&lt;float&gt;(boxesh);\n  // Holoscan tensors to MatX tensors\n  auto boxes_mx = matx::make_tensor&lt;float&gt;(boxes.data(), {1, Nb, 4});\n  // ...\n  auto boxesl_mx = matx::make_tensor&lt;float&gt;({1, Nl(), 4});\n  (boxesl_mx = matx::remap&lt;1&gt;(boxes_ix_mx, ixl_mx)).run(matx::SingleThreadHostExecutor());\n  // ...\n  // MatX to Holoscan tensor\n  auto boxes_hs = std::make_shared&lt;holoscan::Tensor&gt;(boxesls_mx.GetDLPackTensor());\n  // ...\n}\n</code></pre>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#integrate-rapids-cucim-library","title":"Integrate RAPIDS cuCIM library","text":"<p>RAPIDS cuCIM (Compute Unified Device Architecture Clara IMage) is an open-source, accelerated computer vision and image processing software library for multidimensional images used in biomedical, geospatial, material and life science, and remote sensing use cases.</p> <p>See the supported Operators in cuCIM documentation.</p> <p>cuCIM offers interoperability with CuPy. We can initialize CuPy arrays directly from Holoscan Tensors and use the arrays in cuCIM operators for processing without memory transfer between host and device.</p>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#installation_1","title":"Installation","text":"<p>Follow the cuCIM documentation to install the RAPIDS cuCIM library.</p>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#sample-code_1","title":"Sample code","text":"<pre><code>import cupy as cp\nimport cucim.skimage.exposure as cu_exposure\nfrom cucim.skimage.util import img_as_ubyte\nfrom cucim.skimage.util import img_as_float\n\ndef CustomizedcuCIMOperator(Operator):\n    ### Other implementation of __init__, setup()... etc.\n\n    def compute(self, op_input, op_output, context):\n        message = op_input.receive(\"input_tensor\")\n        input_tensor = message.get()\n        # Directly use Holoscan tensor to initialize CuPy array\n        cp_array = cp.asarray(input_tensor)\n\n        cp_array = img_as_float(cp_array)\n        cp_res=cu_exposure.equalize_adapthist(cp_array)\n        cp_array = img_as_ubyte(cp_res)\n\n        # Emit CuPy array memory as an item in a `holoscan.TensorMap`\n        op_output.emit(dict(out_tensor=cp_array), \"out\")\n</code></pre>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#integrate-cv-cuda-library","title":"Integrate CV-CUDA library","text":"<p>CV-CUDA is an open-source, graphics processing unit (GPU)-accelerated library for cloud-scale image processing and computer vision developed jointly by NVIDIA and the ByteDance Applied Machine Learning teams. CV-CUDA helps developers build highly efficient pre- and post-processing pipelines that can improve throughput by more than 10x while lowering cloud computing costs.</p> <p>See the supported CV-CUDA Operators in the CV-CUDA developer guide</p>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#installation_2","title":"Installation","text":"<p>Follow the CV-CUDA documentation to install the CV-CUDA library.</p> <p>Requirement: CV-CUDA &gt;= 0.2.1 (From which version DLPack interop is supported)</p>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#sample-code_2","title":"Sample code","text":"<p>CV-CUDA is implemented with DLPack standards. A CV-CUDA tensor can directly access a Holoscan Tensor.</p> <p>Refer to the Holoscan CV-CUDA sample application for an example of how to use CV-CUDA with Holoscan SDK.</p> <pre><code>import cvcuda\n\nclass CustomizedCVCUDAOp(Operator):\n    def __init__(self, *args, **kwargs):\n\n        # Need to call the base class constructor last\n        super().__init__(*args, **kwargs)\n\n    def setup(self, spec: OperatorSpec):\n        spec.input(\"input_tensor\")\n        spec.output(\"output_tensor\")\n\n    def compute(self, op_input, op_output, context):\n        message = op_input.receive(\"input_tensor\")\n        input_tensor = message.get()\n\n        cvcuda_input_tensor = cvcuda.as_tensor(input_tensor,\"HWC\")\n\n        cvcuda_resize_tensor = cvcuda.resize(\n                    cvcuda_input_tensor,\n                    (\n                        640,\n                        640,\n                        3,\n                    ),\n                    cvcuda.Interp.LINEAR,\n                )\n\n        buffer = cvcuda_resize_tensor.cuda()\n\n        # Emits an `holoscan.TensorMap` with a single entry `out_tensor`\n        op_output.emit(dict(out_tensor=buffer), \"output_tensor\")\n</code></pre>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#integrate-opencv-with-cuda-module","title":"Integrate OpenCV with CUDA Module","text":"<p>OpenCV (Open Source Computer Vision Library) is a comprehensive open-source library that contains over 2500 algorithms covering Image &amp; Video Manipulation, Object and Face Detection, OpenCV Deep Learning Module and much more.</p> <p>OpenCV also supports GPU acceleration and includes a CUDA module which is a set of classes and functions to utilize CUDA computational capabilities. It is implemented using NVIDIA CUDA Runtime API and provides utility functions, low-level vision primitives, and high-level algorithms.</p>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#installation_3","title":"Installation","text":"<p>Prerequisites: - OpenCV &gt;= 4.8.0 (From which version, OpenCV GpuMat supports initialization with GPU Memory pointer)</p> <p>Install OpenCV with its CUDA module following the guide in opencv/opencv_contrib</p> <p>We also recommend referring to the Holoscan Endoscopy Depth Estimation application container as an example of how to build an image with Holoscan SDK and OpenCV CUDA.</p>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#sample-code_3","title":"Sample code","text":"<p>The data type of OpenCV is GpuMat which implements neither the cuda_array_interface nor the standard DLPack. To achieve the end-to-end GPU accelerated pipeline/application, we need to implement 2 functions to convert the GpuMat to CuPy array which can be accessed directly with Holoscan Tensor and vice versa.</p> <p>Refer to the Holoscan Endoscopy Depth Estimation sample application for an example of how to use the OpenCV operator with Holoscan SDK.</p> <ol> <li>Conversion from GpuMat to CuPy Array</li> </ol> <p>The GpuMat object of OpenCV Python bindings provides a cudaPtr method that can be used to access the GPU memory address of a GpuMat object. This memory pointer can be utilized to initialize a CuPy array directly, allowing for efficient data handling by avoiding unnecessary data transfers between the host and device.</p> <p>Refer to the function below, which is used to create a CuPy array from a GpuMat. For more details, see the source code in holohub/applications/endoscopy_depth_estimation-gpumat_to_cupy.</p> <pre><code>import cv2\nimport cupy as cp\n\ndef gpumat_to_cupy(gpu_mat: cv2.cuda.GpuMat) -&gt; cp.ndarray:\n    w, h = gpu_mat.size()\n    size_in_bytes = gpu_mat.step * w\n    shapes = (h, w, gpu_mat.channels())\n    assert gpu_mat.channels() &lt;=3, \"Unsupported GpuMat channels\"\n\n    dtype = None\n    if gpu_mat.type() in [cv2.CV_8U,cv2.CV_8UC1,cv2.CV_8UC2,cv2.CV_8UC3]:\n        dtype = cp.uint8\n    elif gpu_mat.type() == cv2.CV_8S:\n        dtype = cp.int8\n    elif gpu_mat.type() == cv2.CV_16U:\n        dtype = cp.uint16\n    elif gpu_mat.type() == cv2.CV_16S:\n        dtype = cp.int16\n    elif gpu_mat.type() == cv2.CV_32S:\n        dtype = cp.int32\n    elif gpu_mat.type() == cv2.CV_32F:\n        dtype = cp.float32\n    elif gpu_mat.type() == cv2.CV_64F:\n        dtype = cp.float64\n\n    assert dtype is not None, \"Unsupported GpuMat type\"\n\n    mem = cp.cuda.UnownedMemory(gpu_mat.cudaPtr(), size_in_bytes, owner=gpu_mat)\n    memptr = cp.cuda.MemoryPointer(mem, offset=0)\n    cp_out = cp.ndarray(\n        shapes,\n        dtype=dtype,\n        memptr=memptr,\n        strides=(gpu_mat.step, gpu_mat.elemSize(), gpu_mat.elemSize1()),\n    )\n    return cp_out\n</code></pre> <p>Note: In this function, we used the UnownedMemory API to create the CuPy array. In some cases, the OpenCV operators will allocate new device memory which needs to be handled by CuPy and the lifetime is not limited to one operator but the whole pipeline. In this case, the CuPy array initiated from the GpuMat shall know the owner and keep the reference to the object. Check the CuPy documentation for more details on CuPy interoperability.</p> <ol> <li>Conversion from Holoscan Tensor to GpuMat via CuPy array</li> </ol> <p>With the release of OpenCV 4.8, the Python bindings for OpenCV now support the initialization of GpuMat objects directly from GPU memory pointers. This capability facilitates more efficient data handling and processing by allowing direct interaction with GPU-resident data, bypassing the need for data transfer between host and device memory.</p> <p>Within pipeline applications based on Holoscan SDK, the GPU Memory pointer can be obtained through the <code>__cuda_array_interface__</code> interface provided by CuPy arrays.</p> <p>Refer to the functions outlined below for creating GpuMat objects utilizing CuPy arrays. For a detailed implementation, see the source code provided in holohub/applications/endoscopy_depth_estimation-gpumat_from_cp_array.</p> <pre><code>import cv2\nimport cupy as cp\nimport holoscan as hs\nfrom holoscan.gxf import Entity\n\ndef gpumat_from_cp_array(arr: cp.ndarray) -&gt; cv2.cuda.GpuMat:\n    assert len(arr.shape) in (2, 3), \"CuPy array must have 2 or 3 dimensions to be a valid GpuMat\"\n    type_map = {\n        cp.dtype('uint8'): cv2.CV_8U,\n        cp.dtype('int8'): cv2.CV_8S,\n        cp.dtype('uint16'): cv2.CV_16U,\n        cp.dtype('int16'): cv2.CV_16S,\n        cp.dtype('int32'): cv2.CV_32S,\n        cp.dtype('float32'): cv2.CV_32F,\n        cp.dtype('float64'): cv2.CV_64F\n    }\n    depth = type_map.get(arr.dtype)\n    assert depth is not None, \"Unsupported CuPy array dtype\"\n    channels = 1 if len(arr.shape) == 2 else arr.shape[2]\n    mat_type = depth + ((channels - 1) &lt;&lt; 3)\n\n     mat = cv2.cuda.createGpuMatFromCudaMemory(\n      arr.__cuda_array_interface__['shape'][1::-1],\n      mat_type,\n      arr.__cuda_array_interface__['data'][0]\n  )\n    return mat\n</code></pre> <ol> <li>Integrate OpenCV Operators inside customized Operator</li> </ol> <p>The demonstration code is provided below. For the complete source code, please refer to the holohub/applications/endoscopy_depth_estimation-customized Operator.</p> <pre><code>   def compute(self, op_input, op_output, context):\n        stream = cv2.cuda_Stream()\n        message = op_input.receive(\"in\")\n\n        cp_frame = cp.asarray(message.get(\"\"))  # CuPy array\n        cv_frame = gpumat_from_cp_array(cp_frame)  # GPU OpenCV mat\n\n        ## Call OpenCV Operator\n        cv_frame = cv2.cuda.XXX(hsv_merge, cv2.COLOR_HSV2RGB)\n\n        cp_frame = gpumat_to_cupy(cv_frame)\n        cp_frame = cp.ascontiguousarray(cp_frame)\n\n        op_output.emit(dict(out_tensor=cp_frame), \"out\")\n</code></pre>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#integrate-pytorch-library","title":"Integrate PyTorch library","text":"<p>PyTorch is a popular open-source machine learning library developed by Facebook's AI Research lab. It provides a flexible and dynamic computational graph that allows for easy model building and training. PyTorch also supports GPU acceleration, making it ideal for deep learning applications that require high-performance computing.</p> <p>Since PyTorch tensors support the array interface and DLPack (link), they can be interoperable with other array/tensor objects including Holoscan Tensors.</p>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#installation_4","title":"Installation","text":"<p>Follow the PyTorch documentation to install the PyTorch library.</p> <p>e.g., for CUDA 12.x with pip:</p> <pre><code>python3 -m pip install torch torchvision torchaudio\n</code></pre>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#sample-code_4","title":"Sample code","text":"<p>The following is a sample application that demonstrates how to use PyTorch with Holoscan SDK:</p> <pre><code>import torch\n\ndef CustomizedTorchOperator(Operator):\n    ### Other implementation of __init__, setup()... etc.\n\n    def compute(self, op_input, op_output, context):\n        message = op_input.receive(\"input_tensor\")\n        input_tensor = message.get()\n        # Directly use Holoscan tensor to initialize PyTorch tensor\n        torch_tensor = torch.as_tensor(input_tensor, device=\"cuda\")\n\n        torch_tensor *= 2\n\n        # Emit PyTorch tensor memory as a `holoscan.Tensor` item in a `holoscan.TensorMap`\n        op_output.emit(dict(out_tensor=torch_tensor), \"out\")\n</code></pre>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#cuda-interoperability","title":"CUDA Interoperability","text":"","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#integrate-cuda-python-library","title":"Integrate CUDA Python library","text":"<p>CUDA Python is a Python library that provides Cython/Python wrappers for CUDA driver and runtime APIs. It offers a convenient way to leverage GPU acceleration for complex computations, making it ideal for high-performance applications that require intensive numerical processing.</p> <p>When using CUDA Python with the Holoscan SDK, you need to use the Primary context (CUDA doc link) by calling <code>cuda.cuDevicePrimaryCtxRetain()</code> (link.</p> <p>Since the Holoscan Operator is executed in an arbitrary non-main thread, you may need to set the CUDA context using the cuda.cuCtxSetCurrent() method in the <code>Operator.compute()</code> method.</p>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#installation_5","title":"Installation","text":"<p>Follow the instructions in the CUDA Python documentation to install the CUDA Python library.</p> <p>CUDA Python can be installed using <code>pip</code>:</p> <pre><code>python3 -m pip install cuda-python\n</code></pre>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#sample-code_5","title":"Sample code","text":"<p>Please see the example application (cuda_example.py) that demonstrates how to use CUDA Python with the Holoscan SDK.</p> <p>In this example, we define a <code>CudaOperator</code> class that encapsulates the CUDA context, stream, and memory management. The <code>CudaOperator</code> class provides methods for allocating device memory, building CUDA kernels, launching kernels, and cleaning up. We also define three operators: <code>CudaTxOp</code>, <code>ApplyGainOp</code>, and <code>CudaRxOp</code>, which perform data initialization, apply a gain operation, and process the output data, respectively. The output of the <code>CudaRxOp</code> operator is passed to both a <code>ProbeOp</code> operator, which inspects the data and prints the metadata information, and a <code>HolovizOp</code> operator, which visualizes the data using the Holoviz module.</p> <p>There are four examples in the <code>CudaRxOp.compute()</code> method that demonstrate different ways to handle data conversion and transfer between tensor libraries. These examples include creating 1) a NumPy array from CUDA memory, 2) converting a NumPy array to a CuPy array, 3) creating a CUDA array interface object, and 4) creating a CuPy array from CUDA memory.</p> <p><code>__cuda_array_interface__</code> is a dictionary that provides a standard interface for exchanging array data between different libraries in Python. It contains metadata such as the shape, data type, and memory location of the array. By using this interface, you can efficiently transfer tensor data between two libraries without copying the data.</p> <p>In the following example, we create a <code>CudaArray</code> class to represent a CUDA array interface object and populate it with the necessary metadata. This object can then be passed to the <code>op_output.emit()</code> method to transfer the data to downstream operators.</p> <p>In the <code>__cuda_array_interface__</code> dictionary, the <code>stream</code> field is the CUDA stream associated with the data. When passing a <code>cuda.cuda.CUstream</code> object (the variable named <code>stream</code>) to the <code>stream</code> field, you need to convert it to an integer using <code>int(stream)</code>:</p> <pre><code>class CudaRxOp(CudaOperator):\n    # ...\n    def compute(self, op_input, op_output, context):\n        # ...\n        class CudaArray:\n            \"\"\"Class to represent a CUDA array interface object.\"\"\"\n\n            pass\n\n        cuda_array = CudaArray()\n\n        # Reference: https://numba.readthedocs.io/en/stable/cuda/cuda_array_interface.html\n        cuda_array.__cuda_array_interface__ = {\n            \"shape\": (self._frame_height, self._frame_width, self._frame_channels),\n            \"typestr\": np.dtype(np.uint8).str,  # \"|u1\"\n            \"descr\": [(\"\", np.dtype(np.uint8).str)],\n            \"stream\": int(stream),\n            \"version\": 3,\n            \"strides\": None,\n            \"data\": (int(value), False),\n        }\n        # ...\n</code></pre> <p>Please don't confuse this with the <code>cuda.cuda.CUstream.getPtr()</code> method. If you use the <code>stream.getPtr()</code> method, it will return a pointer to the CUDA stream object, not the stream ID. To get the stream ID, you need to convert the <code>stream</code> object to an integer using <code>int(stream)</code>. Otherwise, you will get an error that is difficult to debug, like this:</p> <pre><code>[error] [tensor.cpp:479] Runtime call \"Failure during call to cudaEventRecord\" in line 479 of file ../python/holoscan/core/tensor.cpp failed with 'context is destroyed' (709)\n[error] [gxf_wrapper.cpp:84] Exception occurred for operator: 'rx' - RuntimeError: Error occurred in CUDA runtime API call\n</code></pre>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#integrate-cupy-library","title":"Integrate CuPy library","text":"<p>CuPy is an open-source array library for GPU-accelerated computing with a NumPy-compatible API. It provides a convenient way to perform high-performance numerical computations on NVIDIA GPUs, making it ideal for applications that require efficient data processing and manipulation.</p>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#installation_6","title":"Installation","text":"<p>CuPy can be installed using <code>pip</code>:</p> <pre><code>python3 -m pip install cupy-cuda12x  # for CUDA v12.x\n</code></pre> <p>For more detailed installation instructions, refer to the CuPy documentation.</p>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#sample-code_6","title":"Sample code","text":"<p>Please see the example application (cupy_example.py) that demonstrates how to use CuPy with the Holoscan SDK.</p> <p>This example performs the same operations as the previous example but uses CuPy instead of CUDA Python. The <code>CudaOperator</code> class is modified to use CuPy arrays, and the <code>ApplyGainOp</code> operator is updated to use CuPy functions for array manipulation. The <code>CudaTxOp</code> and <code>CudaRxOp</code> operators are also updated to work with CuPy arrays.</p> <p>With CuPy, you can conveniently perform GPU-accelerated computations on multidimensional arrays, making it an excellent choice for high-performance data processing tasks in Holoscan applications.</p> <p>Please note that CuPy does not fully support certain CUDA APIs, such as <code>cupy.cuda.driver.occupancyMaxPotentialBlockSize()</code>. While the driver API may be available (link), it is currently undocumented (link) and lacks support for calling the API with RawKernel's pointer (link), or using CUDA Python's <code>cuda.cuOccupancyMaxPotentialBlockSize()</code> Driver API with CuPy-generated RawKernel functions.</p> <p>Currently, direct assignment of a non-default CUDA stream to HolovizOp in Holoscan applications is not supported without utilizing the <code>holoscan.resources.CudaStreamPool</code> resource. CuPy also has limited support for custom stream management, necessitating reliance on the default stream in this context.</p> <p>For more detailed information, please refer to the following resources: - New RawKernel Calling Convention / Kernel Occupancy Functions \u00b7 Issue #3684 \u00b7 cupy/cupy \u00b7 GitHub - CUDA Stream Support:   - Enhancing stream support in CuPy's default memory pool \u00b7 Issue #8068 \u00b7 cupy/cupy   - cupy.cuda.ExternalStream \u2014 CuPy 13.1.0 documentation</p>","tags":["CV CUDA","OpenCV","Holoscan"]},{"location":"tutorials/local-llama/","title":"Deploying Llama-2 70b model on the edge with IGX Orin","text":"<p> Authors: Nigel Nelson (NVIDIA) Supported platforms: aarch64 Language: Python Last modified: May 13, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p>","tags":["Natural Language and Conversational AI","CUDA","Huggingface","LLM"]},{"location":"tutorials/local-llama/#introduction","title":"\ud83e\udd99 Introduction","text":"<p>With the recent release of the Llama-2 family of models, there has been an excess of excitement in the LLM community due to these models being released freely for research and commercial use. Upon their release, the 70b version of the Llama-2 model quickly rose to the top place on HuggingFace's Open LLM Leaderboard. Additionally, thanks to the publishing of the model weights, fine-tuned versions of these models are consistently being released and raising the bar for the top performing open-LLM. This most recent release of Llama-2 provides some of the first legitimate open-source alternatives to the previously unparalleled performance of closed-source LLMs. This enables developers to deploy these Llama-2 models locally, and benefit from being able to use some of the most advanced LLMs ever created, while also keeping all of their data on their own host machines.</p> <p>The only edge device that is capable of running the Llama-2 70b locally is the NVIDIA IGX Orin. In order to get the Llama-2 70b model running inference, all you need is an IGX Orin, a mouse, a keyboard, and to follow the tutorial below.</p>","tags":["Natural Language and Conversational AI","CUDA","Huggingface","LLM"]},{"location":"tutorials/local-llama/#overview","title":"\ud83d\udcd6 Overview","text":"<p>This tutorial will walk you through how to run a quantized version of Meta's Llama-2 70b model as the backend LLM for a Gradio chatbot app, all running on an NVIDIA IGX Orin. Specifically, we will use Llama.cpp, a project that ports Llama models into C and C++ with CUDA acceleration, to load and run the quantized Llama-2 models. We will setup Llama.cpp's <code>api_like_OAI.py</code> Flask app that emulates the OpenAI API. This will then enable us to create a Gradio chatbot app that utilizes the popular OpenAI API Python library to interact with our local Llama-2 model. Thus, at the conclusion of this tutorial you will have a chatbot app that rivals the performance of closed-source models, while keeping all of your data local and running everything self-contained on an NVIDIA IGX Orin.</p>","tags":["Natural Language and Conversational AI","CUDA","Huggingface","LLM"]},{"location":"tutorials/local-llama/#hardware-requirements","title":"\ud83d\udcbb Hardware Requirements","text":"<ul> <li>NVIDIA IGX Orin with:</li> <li>RTX A6000 dGPU</li> <li>500 GB SSD</li> </ul>","tags":["Natural Language and Conversational AI","CUDA","Huggingface","LLM"]},{"location":"tutorials/local-llama/#dependencies","title":"\ud83d\udce6 Dependencies","text":"<ul> <li>NVIDIA Drivers</li> <li>CUDA Toolkit &gt;= 11.8</li> <li>Python &gt;= 3.8</li> <li><code>build-essential</code> apt package (gcc, g++, etc.)</li> <li>Cmake &gt;= 3.17</li> </ul>","tags":["Natural Language and Conversational AI","CUDA","Huggingface","LLM"]},{"location":"tutorials/local-llama/#cloning-and-building-llamacpp","title":"\ud83d\udd28 Cloning and building Llama.cpp","text":"<ol> <li> <p>Clone Llama.cpp:</p> <pre><code>git clone https://github.com/ggerganov/llama.cpp.git\n</code></pre> </li> <li> <p>Checkout a stable commit of llama.cpp:</p> <pre><code>cd llama.cpp\ngit checkout e519621010cac02c6fec0f8f3b16cda0591042c0 # Commit date: 9/27/23\n</code></pre> </li> <li> <p>Follow cuBLAS build instructions for Llama.cpp to provide BLAS acceleration using the CUDA cores of your NVIDIA GPU.</p> <p>Navigate to the <code>/Llama.cpp</code> directory: <pre><code>cd llama.cpp\n</code></pre> Using <code>make</code>: <pre><code>make LLAMA_CUBLAS=1\n</code></pre></p> </li> </ol> <p>By successfully executing these commands you will now be able to run Llama models on your local machine with BLAS acceleration!</p>","tags":["Natural Language and Conversational AI","CUDA","Huggingface","LLM"]},{"location":"tutorials/local-llama/#downloading-llama-2-70b","title":"\ud83d\udcbe Downloading Llama-2 70B","text":"<p>In order to use Llama-2 70b as it is provided by Meta, you\u2019d need 140 GB of VRAM (70b params x 2 bytes = 140 GB in FP16). However, by utilizing model quantization, we can reduce the computational and memory costs of running inference by representing the weights and activations as low-precision data types, like int8 and int4, instead of higher-precision data types like FP16 and FP32. To learn more about quantization, check out: The Ultimate Guide to Deep Learning Model Quantization.</p> <p>Llama.cpp uses quantized models that are stored in the GGUF format. Browse to TheBloke on Huggingface.co, who provides hundred of the latest quantized models. Feel free to choose a GGUF model that suits your needs. However, for this tutorial, we will use TheBloke's 4-bit medium GGUF quantization of Meta\u2019s LLama-2-70B-Chat model. 1. Download the GGUF model from Huggingface.co.</p> <p> This model requires ~43 GB of VRAM.</p> <pre><code>cd /media/m2 # Download the model to your SSD drive\nmkdir models # Create a directory for GGUF models\ncd models\nwget https://huggingface.co/TheBloke/Llama-2-70B-chat-GGUF/resolve/main/llama-2-70b-chat.Q4_K_M.gguf\n</code></pre>","tags":["Natural Language and Conversational AI","CUDA","Huggingface","LLM"]},{"location":"tutorials/local-llama/#running-llama-2-70b","title":"\ud83e\udd16 Running Llama-2 70B","text":"<ol> <li> <p>Return to the home directory of Llama.cpp:</p> <pre><code>cd &lt;your_parent_dir&gt;/llama.cpp\n</code></pre> </li> <li> <p>Run Llama.cpp\u2019s example server application to set up a HTTP API server and a simple web front end to interact with our Llama model:</p> <pre><code>./server -m /media/m2/models/llama-2-70b-chat.Q4_K_M.gguf -ngl 1000 -c 4096 --alias llama_2\n</code></pre> </li> <li> <p><code>-m</code>: indicates the location of our model.</p> </li> <li><code>-ngl</code>: the number of layers to offload to the GPU (1000 ensures all layers are).</li> <li><code>-c</code>: the size of the prompt context.</li> <li><code>--alias</code>: name given to our model for access through the API.</li> </ol> <p>After executing, you should see the below output indicating the model being loaded to VRAM and the specs of the model: <pre><code>ggml_init_cublas: found 1 CUDA devices:\n  Device 0: NVIDIA RTX A6000, compute capability 8.6\n{\"timestamp\":1695853185,\"level\":\"INFO\",\"function\":\"main\",\"line\":1294,\"message\":\"build info\",\"build\":1279,\"commit\":\"e519621\"}\n{\"timestamp\":1695853185,\"level\":\"INFO\",\"function\":\"main\",\"line\":1296,\"message\":\"system info\",\"n_threads\":6,\"total_threads\":12,\"system_info\":\"AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \"}\nllama_model_loader: loaded meta data with 19 key-value pairs and 723 tensors from /media/m2/models/llama-2-70b-chat.Q4_K_M.gguf (version GGUF V2 (latest))\n**Verbose llama_model_loader output removed for conciseness**\nllm_load_print_meta: format         = GGUF V2 (latest)\nllm_load_print_meta: arch           = llama\nllm_load_print_meta: vocab type     = SPM\nllm_load_print_meta: n_vocab        = 32000\nllm_load_print_meta: n_merges       = 0\nllm_load_print_meta: n_ctx_train    = 4096\nllm_load_print_meta: n_ctx          = 4096\nllm_load_print_meta: n_embd         = 8192\nllm_load_print_meta: n_head         = 64\nllm_load_print_meta: n_head_kv      = 8\nllm_load_print_meta: n_layer        = 80\nllm_load_print_meta: n_rot          = 128\nllm_load_print_meta: n_gqa          = 8\nllm_load_print_meta: f_norm_eps     = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps = 1.0e-05\nllm_load_print_meta: n_ff           = 28672\nllm_load_print_meta: freq_base      = 10000.0\nllm_load_print_meta: freq_scale     = 1\nllm_load_print_meta: model type     = 70B\nllm_load_print_meta: model ftype    = mostly Q4_K - Medium\nllm_load_print_meta: model params   = 68.98 B\nllm_load_print_meta: model size     = 38.58 GiB (4.80 BPW)\nllm_load_print_meta: general.name   = LLaMA v2\nllm_load_print_meta: BOS token = 1 '&lt;s&gt;'\nllm_load_print_meta: EOS token = 2 '&lt;/s&gt;'\nllm_load_print_meta: UNK token = 0 '&lt;unk&gt;'\nllm_load_print_meta: LF token  = 13 '&lt;0x0A&gt;'\nllm_load_tensors: ggml ctx size =    0.23 MB\nllm_load_tensors: using CUDA for GPU acceleration\nllm_load_tensors: mem required  =  140.86 MB (+ 1280.00 MB per state)\nllm_load_tensors: offloading 80 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloading v cache to GPU\nllm_load_tensors: offloading k cache to GPU\nllm_load_tensors: offloaded 83/83 layers to GPU\nllm_load_tensors: VRAM used: 40643 MB\n....................................................................................................\nllama_new_context_with_model: kv self size  = 1280.00 MB\nllama_new_context_with_model: compute buffer total size =  561.47 MB\nllama_new_context_with_model: VRAM scratch buffer: 560.00 MB\n\nllama server listening at http://127.0.0.1:8080\n\n{\"timestamp\":1695853195,\"level\":\"INFO\",\"function\":\"main\",\"line\":1602,\"message\":\"HTTP server listening\",\"hostname\":\"127.0.0.1\",\"port\":8080}\n</code></pre></p> <p>Now, you can interact with the simple web front end by browsing to http://127.0.0.1:8080. Use the provided chat interface to query the Llama-2 model and experiment with manipulating the provided hyperparameters to tune the responses to your liking.</p>","tags":["Natural Language and Conversational AI","CUDA","Huggingface","LLM"]},{"location":"tutorials/local-llama/#setting-up-a-local-openai-server","title":"\ud83d\udda5\ufe0f Setting up a local OpenAI server","text":"<p>Llama.cpp includes a nifty Flask app <code>api_like_OAI.py</code>. This Flask app sets up a server that emulates the OpenAI API. Its trick is that it converts the OpenAI API requests into the format expected by the Llama model, and forwards the captured requests to our local Llama-2 model. This allows you to use the popular OpenAI Python backend, and thus, countless powerful LLM libraries like LangChain, Scikit-LLM, Haystack, and more. However, instead of your data being sent to OpenAI\u2019s servers, it is all processed locally on your machine!</p> <ol> <li> <p>In order to run the OpenAI API server and our eventual Gradio chat app, we need to open a new terminal and install a few Python dependencies:</p> <pre><code>cd tutorials/local-llama\npip install -r requirements.txt\n</code></pre> </li> <li> <p>This then allows us to run the Flask server:</p> <pre><code>cd &lt;your_parent_dir&gt;/llama.cpp/examples/server/\npython api_like_OAI.py\n</code></pre> </li> <li> <p>The server should begin running almost immediately and give you the following output:</p> </li> </ol>","tags":["Natural Language and Conversational AI","CUDA","Huggingface","LLM"]},{"location":"tutorials/local-llama/#creating-the-gradio-chat-app","title":"\ud83d\udcac Creating the Gradio Chat App","text":"<ol> <li> <p>Create a new project directory and a <code>chatbot.py</code> file that contains the following code:</p> <pre><code>import gradio as gr\nimport openai\n\n# Indicate we'd like to send the request\n# to our local model, not OpenAI's servers\nopenai.api_base = \"http://127.0.0.1:8081\"\nopenai.api_key = \"\"\n\n\ndef to_oai_chat(history):\n    \"\"\"Converts the gradio chat history format to\n    the OpenAI chat history format:\n\n    Gradio format: ['&lt;user message&gt;', '&lt;bot message&gt;']\n    OpenAI format: [{'role': 'user', 'content': '&lt;user message&gt;'},\n                    {'role': 'assistant', 'content': '&lt;bot_message&gt;'}]\n\n    Additionally, this adds the 'system' message to the chat to tell the\n    assistant how to act.\n    \"\"\"\n    chat = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful AI Assistant who ends all of your responses with &lt;/BOT&gt;\",\n        }\n    ]\n\n    for msg_pair in history:\n        if msg_pair[0]:\n            chat.append({\"role\": \"user\", \"content\": msg_pair[0]})\n        if msg_pair[1]:\n            chat.append({\"role\": \"assistant\", \"content\": msg_pair[1]})\n    return chat\n\n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot(height=650)\n    msg = gr.Textbox()\n    clear = gr.Button(\"Clear\")\n\n    def user(user_message, history):\n        \"\"\"Appends a submitted question to the history\"\"\"\n        return \"\", history + [[user_message, None]]\n\n    def bot(history):\n        \"\"\"Sends the chat history to our Llama-2 model server\n        so that the model can respond appropriately\n        \"\"\"\n        # Gradio chat -&gt; OpenAI chat\n        oai_chat = to_oai_chat(history)\n\n        # Send chat history to our Llama-2 server\n        response = openai.ChatCompletion.create(\n            messages=oai_chat,\n            stream=True,\n            model=\"llama_2\",\n            temperature=0,\n            # Used to stop runaway responses\n            stop=[\"&lt;/BOT&gt;\"],\n        )\n\n        history[-1][1] = \"\"\n        for response_chunk in response:\n            # Filter through meta-data in the HTTP response to get response text\n            next_token = response_chunk[\"choices\"][0][\"delta\"].get(\"content\")\n            if next_token:\n                history[-1][1] += next_token\n                # Update the Gradio app with the streamed response\n                yield history\n\n    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(bot, chatbot, chatbot)\n    clear.click(lambda: None, None, chatbot, queue=False)\n\ndemo.queue()\ndemo.launch()\n</code></pre> </li> <li> <p>Begin running the Gradio chat app:</p> <pre><code>python chatbot.py\n</code></pre> </li> <li> <p>The chat app should now be accessible at http://127.0.0.1:7860:</p> </li> </ol> <p>You're now set up to interact with the Llama-2 70b model, with everything running locally! If you want to take this project further, you can experiment with different system messages to suit your needs or add the ability to interact with your local documents using frameworks like LangChain. Enjoy experimenting!</p>","tags":["Natural Language and Conversational AI","CUDA","Huggingface","LLM"]},{"location":"tutorials/local-llama/#sources","title":"Sources:","text":"<ul> <li>https://huggingface.co/meta-llama/Llama-2-70b-chat-hf/tree/main</li> <li>https://huggingface.co/docs/optimum/concept_guides/quantization</li> <li>https://deci.ai/quantization-and-quantization-aware-training/</li> <li>https://www.gradio.app/guides/creating-a-custom-chatbot-with-blocks#add-streaming-to-your-chatbot</li> </ul>","tags":["Natural Language and Conversational AI","CUDA","Huggingface","LLM"]},{"location":"tutorials/pretrained_foundational_models/self_supervised_training/","title":"Self-Supervised Contrastive Learning for Surgical videos","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable The focus of this repo is to walkthrough the process of doing Self-Supervised Learning using Contrastive Pre-training on Surgical Video data.  As part of the walk-through we will guide through the steps needed to pre-process and extract the frames from the public Cholec80 Dataset. This will be required to run the tutorial.</p> <p>The repo is organized as follows -  * <code>Contrastive_learning_Notebook.ipynb</code> walks through the process of SSL in a tutorial style * <code>train_simclr_multiGPU.py</code> enables running of \"pre-training\" on surgical data across multiple GPUs through the CLI * <code>downstream_task_tool_segmentation.py</code> shows the process of \"fine-tuning\" for a downstream task starting from a pretrained checkpoint using MONAI</p>","tags":["Computer Vision and Perception","Learning","Healthcare AI","Surgical AI","Video"]},{"location":"tutorials/pretrained_foundational_models/self_supervised_training/#dataset","title":"Dataset","text":"<p>To run through the full tutorial, it is required that the user downloads Cholec80 dataset. Additional preprocessing of the videos to extract individual frames can be performed using the python helper file as follows:</p> <p><code>python extract_frames.py --datadir &lt;path_to_cholec80_dataset&gt;</code> </p>","tags":["Computer Vision and Perception","Learning","Healthcare AI","Surgical AI","Video"]},{"location":"tutorials/pretrained_foundational_models/self_supervised_training/#adapt-to-your-own-dataset","title":"Adapt to your own dataset","text":"<p>To run this with your own dataset, you will need to extract the frames and modify the <code>Pytorch Dataset/Dataloader</code> accordingly. For SSL pre-training, a really simple CSV file formatted as follows can be used.  <pre><code>&lt;path_to_frame&gt;,&lt;label&gt;\n</code></pre> where <code>&lt;label&gt;</code> can be a class/score for a downstream task, and is NOT used during pre-training.</p> <pre><code># Snippet of csv file\n/workspace/data/cholec80/frames/train/video01/1.jpg,0\n/workspace/data/cholec80/frames/train/video01/2.jpg,0\n/workspace/data/cholec80/frames/train/video01/3.jpg,0\n/workspace/data/cholec80/frames/train/video01/4.jpg,0\n/workspace/data/cholec80/frames/train/video01/5.jpg,0\n....\n</code></pre>","tags":["Computer Vision and Perception","Learning","Healthcare AI","Surgical AI","Video"]},{"location":"tutorials/pretrained_foundational_models/self_supervised_training/#environment","title":"Environment","text":"<p>All environment/dependencies are captured in the Dockerfile. The exact software within the base container are described here.</p>","tags":["Computer Vision and Perception","Learning","Healthcare AI","Surgical AI","Video"]},{"location":"tutorials/pretrained_foundational_models/self_supervised_training/#create-docker-imagecontainer","title":"Create Docker Image/Container","text":"<pre><code>DATA_DIR=\"/mnt/sdb/data\"  # location of Cholec80 dataset\ndocker build -t surg_video_ssl_2202:latest Docker/\n\n# sample Docker command (may need to update based on local setup)\ndocker run -it --gpus=\"device=1\" \\\n    --name=SURGSSL_EXPS \\\n    -v $DATA_DIR:/workspace/data \\\n    -v `pwd`:/workspace/codes -w=/workspace/codes/ \\\n    -p 8888:8888 \\\n    --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 \\\n    surg_video_ssl_2202 jupyter lab\n</code></pre> <p>For environment dependencies refer to the Dockerfile</p>","tags":["Computer Vision and Perception","Learning","Healthcare AI","Surgical AI","Video"]},{"location":"tutorials/pretrained_foundational_models/self_supervised_training/#launch-training","title":"Launch Training","text":"<p>PRE-TRAINING <pre><code># Training on single GPU with `efficientnet_b0` backbone\npython3 train_simclr_multigpu.py --gpus 1 --backbone efficientnet_b0 --batch_size 64\n\n# Training on single GPU with `resnet50` backbone\npython3 train_simclr_multigpu.py --gpus 4 --backbone resnet50 --batch_size 128\n</code></pre></p> <p>DOWNSTREAM TASK - Segmentation This script shows an example of taking the checkpoint above and integrating it into MONAI. </p> <pre><code># Fine-Tuning on \"GPU 1\" with 10% of the dataset, while freezing the encoder\npython3 downstream_task_tool_segmentation.py --gpu 1 --perc 10 --exp simclr --freeze\n</code></pre>","tags":["Computer Vision and Perception","Learning","Healthcare AI","Surgical AI","Video"]},{"location":"tutorials/pretrained_foundational_models/self_supervised_training/#modelcheckpoints-information","title":"Model/Checkpoints information","text":"<p>As part of this tutorial, we are also releasing a few different checkpoints for users. These are detailed below. </p> <p>NOTE : These checkpoints were trained using an internal dataset of Chelecystectomy videos provided by Activ Surgical and NOT the Cholec80 dataset. </p>","tags":["Computer Vision and Perception","Learning","Healthcare AI","Surgical AI","Video"]},{"location":"tutorials/pretrained_foundational_models/self_supervised_training/#pre-trained-backbones","title":"Pre-Trained Backbones","text":"<ul> <li>ResNet18        - link</li> <li>ResNet50        - link</li> <li>efficientnet_b0 - link</li> </ul>","tags":["Computer Vision and Perception","Learning","Healthcare AI","Surgical AI","Video"]},{"location":"tutorials/pretrained_foundational_models/self_supervised_training/#tool-segmentation-model","title":"Tool Segmentation Model","text":"<ul> <li>MONAI - FlexibleUNet (efficientnet_b0) - link</li> </ul>","tags":["Computer Vision and Perception","Learning","Healthcare AI","Surgical AI","Video"]},{"location":"tutorials/pretrained_foundational_models/self_supervised_training/#holoscan-sdk","title":"Holoscan SDK","text":"<p>This tool Segmentation Model can be used to build a Holoscan App, using the process under section \"Bring your own Model\" within the Holoscan SDK User guide.</p>","tags":["Computer Vision and Perception","Learning","Healthcare AI","Surgical AI","Video"]},{"location":"tutorials/pretrained_foundational_models/self_supervised_training/#resources","title":"Resources","text":"<p>[1] Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. (2020). A simple framework for contrastive learning of visual representations. In International conference on machine learning (pp. 1597-1607). PMLR. (link)</p> <p>[2] Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G. (2020). Big self-supervised models are strong semi-supervised learners. NeurIPS 2021 (link).</p> <p>[3][Pytorch Lightning SSL Tutorial](https://pytorch-lightning.readthedocs.io/en/latest/notebooks/course_UvA-DL/13-contrastive-learning.html) | Github</p> <p>[4] Ramesh, S., Srivastav, V., Alapatt, D., Yu, T., Muarli, A., et. al. (2023).  Dissecting Self-Supervised Learning Methods for Surgical Computer Vision. arXiv preprint arXiv:2207.00449. (link)</p>","tags":["Computer Vision and Perception","Learning","Healthcare AI","Surgical AI","Video"]},{"location":"tutorials/windows_vm/","title":"Interoperability between Holoscan and a Windows Application on a Single Machine","text":"<p> Authors: NVIDIA (NVIDIA) Supported platforms: x86_64 Language: bash Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.6.0 Tested Holoscan SDK versions: 2.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p></p>"},{"location":"tutorials/windows_vm/#overview","title":"Overview","text":"<p>This tutorial enables Holoscan and Windows applications to run concurrently on the same machine or node.</p>"},{"location":"tutorials/windows_vm/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Description</li> <li>Platform Requirements</li> <li>Windows VM Setup Instructions</li> <li>Software Pre-requisites</li> <li>GPU Passthrough<ul> <li>Two Different GPUs (e.g., RTX A4000 and RTX A6000)</li> <li>Two Identical GPUs (e.g., 2x RTX A4000)</li> </ul> </li> <li>Windows VM Configuration for Passed-through GPU<ul> <li>Install NVIDIA Driver in Windows VM</li> </ul> </li> <li>Communication Performance between Linux Host and Windows VM</li> <li>Running Holoscan DDS App and Windows VM App</li> </ul>"},{"location":"tutorials/windows_vm/#description","title":"Description","text":"<p>Many legacy graphics applications, particularly in medical devices, are developed and operated on the Windows platform. A significant number of these medical devices rely on Windows OS for their functionality. To integrate AI/ML and sensor processing capabilities from NVIDIA Holoscan into such Windows-based systems, we previously introduced a \"sidecar\" architecture. This architecture involved an AI compute node running Holoscan interoperating with a Windows node via a DDS link, showcased in the Holoscan DDS reference application. This tutorial extends the options for such use-cases by providing developers with a straightforward and clear system design to enable interoperability between Holoscan applications and Windows applications running on the same physical machine. It demonstrates how to achieve efficient communication and processing without the need for separate hardware nodes.</p> <p></p> <p>In this tutorial, Holoscan runs on an x86_64 workstation hosting Ubuntu 22.04. Two RTX A4000 GPUs are plugged into the x86_64 workstation. Using Linux KVM, a Windows VM is created on the host Ubuntu Linux. One RTX A4000 GPU is passed through to the Windows VM using VFIO. The other RTX A4000 GPU is reserved for the host Ubuntu OS and used by Holoscan. We provide a step-by-step guide on how to achieve this setup. Furthermore, we also demonstrate how a Holoscan application reads USB-based camera frames and sends the frames via DDS to an application running on the Windows VM. Finally, the Windows VM application renders the camera frames on the screen using its dedicated RTX A4000 GPU.</p>"},{"location":"tutorials/windows_vm/#platform-requirements","title":"Platform Requirements","text":"<p>The setup described in this tutorial requires an x86_64 workstation and ProViz class of NVIDIA GPUs. The exact platform details are provided below:</p> <ul> <li>CPU: AMD Ryzen 9 7950X 16-Core Processor</li> <li>OS: Ubuntu 22.04</li> <li>Kernel Version: 6.8.0-48-generic</li> <li>GPU: 2x NVIDIA RTX A4000</li> </ul> <p>Although we used two same GPUs in this tutorial, combinations of two different GPUs, for example, RTX A4000 and RTX A6000, can also be used.</p> <p>Since x86_64 workstations are very much diverse, the steps to enable the setup described in this tutorial may not work as-is on all x86_64 workstations. Performances may also vary across different x86_64 workstations.</p>"},{"location":"tutorials/windows_vm/#windows-vm-setup-instructions","title":"Windows VM Setup Instructions","text":""},{"location":"tutorials/windows_vm/#software-pre-requisites","title":"Software Pre-requisites","text":"<p>Install the NVIDIA Holoscan SDK and the NVIDIA GPU driver on the host Ubuntu 22.04.</p> <p>Install KVM and QEMU</p> <pre><code>sudo apt update\nsudo apt install qemu-kvm libvirt-clients libvirt-daemon-system bridge-utils virt-manager ovmf\n</code></pre> <p>Check if KVM works</p> <pre><code>$ kvm-ok\nINFO: /dev/kvm exists\nKVM acceleration can be used\n</code></pre> <p>If KVM does not work, kindly figure out how KVM can be enabled for your specific Linux kernel version.</p>"},{"location":"tutorials/windows_vm/#gpu-passthrough","title":"GPU Passthrough","text":"<p>A GPU can be passed through to a virtual machine using Linux's VFIO driver. The VM is, then, capable of using the passed through GPU directly without any host operating system intervention. If a machine has two different GPUs, then it is much more straightforward to passthrough one of the GPUs to a VM. A bit more configuration is required if two GPUs are same. We show how one of the two identical GPUs can be passed through to a VM with the VFIO driver.</p>"},{"location":"tutorials/windows_vm/#two-different-gpus-eg-rtx-a4000-and-rtx-a6000","title":"Two Different GPUs (e.g., RTX A4000 and RTX A6000)","text":""},{"location":"tutorials/windows_vm/#identify-the-gpu-to-be-passed-through","title":"Identify the GPU to be Passed Through","text":"<pre><code>$ lspci | grep -i NVIDIA\n17:00.0 VGA compatible controller [0300]: NVIDIA Corporation Device [RTX A4000] [10de:24b0] (rev a1)\n17:00.1 Audio device [0403]: NVIDIA Corporation Device [10de:228b] (rev a1)\n65:00.0 VGA compatible controller [0300]: NVIDIA Corporation Device [RTX A6000] [10de:2230] (rev a1)\n65:00.1 Audio device [0403]: NVIDIA Corporation Device [10de:1aef] (rev a1)\n</code></pre> <p>Let's assume that we want to passthrough the RTX A4000 GPU to a VM (in this tutorial it's a Windows VM). We note the PCI ID of RTX A4000 as <code>10de:24b0</code>.</p> <p>Note: Make sure that the RTX A4000 GPU to be passed through is not currently used for displaying to the monitor.</p>"},{"location":"tutorials/windows_vm/#update-grub-configuration","title":"Update GRUB Configuration","text":"<p>Open the <code>/etc/default/grub</code> file and find the following line:</p> <pre><code>GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash\"\n</code></pre> <p>Update the above line to the following to add the previously noted PCI ID of RTX A4000 for the VFIO driver:</p> <pre><code>GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash intel_iommu=on amd_iommu=on iommu=pt vfio-pci.ids=10de:24b0\"\n</code></pre> <p>The above line enables IOMMU and passes the PCI ID of RTX A4000 to the VFIO driver for Linux kernel.</p>"},{"location":"tutorials/windows_vm/#update-linux-kernel-module-configuration","title":"Update Linux Kernel Module Configuration","text":"<p>Create a new file <code>/etc/modprobe.d/vfio.conf</code> and add the following lines:</p> <pre><code>options vfio-pci ids=10de:24b0\nsoftdep nvidia pre: vfio-pci\n</code></pre> <p>The above lines ensure that the NVIDIA driver is loaded after the VFIO driver. Therefore, VFIO takes over the RTX A4000 GPU, and the RTX A6000 GPU is used by the default NVIDIA driver.</p>"},{"location":"tutorials/windows_vm/#update-initial-ramfs-and-grub","title":"Update Initial RAMFS and GRUB","text":"<pre><code>sudo update-initramfs -u # optionally add -k all to update all kernels\nsudo update-grub\n</code></pre>"},{"location":"tutorials/windows_vm/#check-if-vfio-is-loaded-for-passed-through-gpu","title":"Check if VFIO is loaded for Passed-through GPU","text":"<p>Reboot the system: <code>sudo reboot</code>. Now, check the loaded driver for RTX A4000:</p> <pre><code>$ lspci -nnk -d 10de:24b0\n17:00.0 VGA compatible controller [0300]: NVIDIA Corporation [RTX A4000] [10de:24b0] (rev a1)\n    Subsystem: NVIDIA Corporation [RTX A4000] [10de:14ad]\n    Kernel driver in use: vfio-pci\n    Kernel modules: nvidiafb, nouveau, nvidia_drm, nvidia\n</code></pre>"},{"location":"tutorials/windows_vm/#two-identical-gpus-eg-2x-rtx-a4000","title":"Two Identical GPUs (e.g., 2x RTX A4000)","text":"<p>In case of two identical GPUs, the PCI IDs of the GPUs are the same.</p> <pre><code>$ lspci -nn | grep \"NVIDIA\"\n01:00.0 VGA compatible controller [0300]: NVIDIA Corporation GA104GL [RTX A4000] [10de:24b0] (rev a1)\n01:00.1 Audio device [0403]: NVIDIA Corporation GA104 High Definition Audio Controller [10de:228b] (rev a1)\n09:00.0 VGA compatible controller [0300]: NVIDIA Corporation GA104GL [RTX A4000] [10de:24b0] (rev a1)\n09:00.1 Audio device [0403]: NVIDIA Corporation GA104 High Definition Audio Controller [10de:228b] (rev a1)\n</code></pre> <p>Therefore, we cannot just pass PCI ID <code>10de:24b0</code> to the VFIO driver to pass through one RTX A4000 to a VM and keep the other one for the host Linux.</p> <p>To mitigate the issue, we force the Linux kernel to load the VFIO driver for one of the RTX A4000 GPUs by identifying the GPU by its PCI bus address at the time of booting Linux. Let's assume that we want to pass through the RTX A4000 GPU at PCI bus address <code>01:00.0</code> to a VM.</p> <p>Note: Make sure that the RTX A4000 GPU to be passed through is not currently used for displaying to the monitor. <code>nvidia-smi</code> can be used to figure out which GPU is being used for display.</p> <p>The following instructions are only verified for Ubuntu 22.04 and Linux kernel 6.8.0-48-generic. For other Ubuntu and Linux kernel versions, the instructions may not work.</p>"},{"location":"tutorials/windows_vm/#update-grub-configuration_1","title":"Update GRUB Configuration","text":"<p>Open the <code>/etc/default/grub</code> file and find the following line:</p> <pre><code>GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash\"\n</code></pre> <p>Update the above line to the following to enable IOMMU:</p> <pre><code>GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash intel_iommu=on amd_iommu=on iommu=pt vfio-pci.ids=\"\n</code></pre>"},{"location":"tutorials/windows_vm/#create-a-script-to-bind-the-rtx-a4000-gpu-to-vfio-driver","title":"Create a script to bind the RTX A4000 GPU to VFIO driver","text":"<p>Create a script <code>/usr/local/bin/vfio-pci-override.sh</code> with the following content:</p> <pre><code>#!/bin/sh\n\necho \"vfio-pci\" &gt; /sys/bus/pci/devices/0000\\:01\\:00.0/driver_override\necho \"vfio-pci\" &gt; /sys/bus/pci/devices/0000\\:01\\:00.0/iommu_group/devices/0000\\:01\\:00.0/driver_override\necho \"vfio-pci\" &gt; /sys/bus/pci/devices/0000\\:01\\:00.0/iommu_group/devices/0000\\:01\\:00.1/driver_override\nmodprobe -i vfio-pci\n</code></pre> <p>Kindly, note that we are using the PCI bus address <code>0000:01:00.0</code> of the RTX A4000 in the above script. In specific cases, the PCI bus address may be different and needs to be updated.</p>"},{"location":"tutorials/windows_vm/#install-vfio-driver-with-the-above-script","title":"Install VFIO driver with the above script","text":"<p>Create a new file <code>/etc/modprobe.d/vfio.conf</code> and add the following lines:</p> <pre><code>install vfio-pci /usr/local/bin/vfio-pci-override.sh\nsoftdep nvidia pre: vfio-pci\n</code></pre>"},{"location":"tutorials/windows_vm/#update-initial-ramfs-and-grub_1","title":"Update Initial RAMFS and GRUB","text":"<pre><code>sudo update-initramfs -u # optionally add -k all to update all kernels\nsudo update-grub\n</code></pre>"},{"location":"tutorials/windows_vm/#check-if-vfio-is-loaded-for-passed-through-gpu_1","title":"Check if VFIO is loaded for Passed-through GPU","text":"<p>Reboot the system: <code>sudo reboot</code>. Now, check the loaded driver for RTX A4000:</p> <pre><code>$ lspci -nnk -d 10de:24b0\n01:00.0 VGA compatible controller [0300]: NVIDIA Corporation GA104GL [RTX A4000] [10de:24b0] (rev a1)\n    Subsystem: NVIDIA Corporation GA104GL [RTX A4000] [10de:14ad]\n    Kernel driver in use: vfio-pci\n    Kernel modules: nvidiafb, nouveau, nvidia_drm, nvidia\n09:00.0 VGA compatible controller [0300]: NVIDIA Corporation GA104GL [RTX A4000] [10de:24b0] (rev a1)\n    Subsystem: NVIDIA Corporation GA104GL [RTX A4000] [10de:14ad]\n    Kernel driver in use: nvidia\n    Kernel modules: nvidiafb, nouveau, nvidia_drm, nvidia\n</code></pre> <p>In the above output, we can see that different kernel drivers are loaded for the two RTX A4000 GPUs.</p>"},{"location":"tutorials/windows_vm/#windows-vm-configuration-for-passed-through-gpu","title":"Windows VM Configuration for Passed-through GPU","text":"<p>A Windows VM can be created using <code>virt-manager</code> or <code>virsh</code> (see references if needed). We assume that the Windows VM is already created and running. We show how to assign the passed-through RTX A4000 GPU to the Windows VM.</p> <p>In the \"Hardware Details\" tab of <code>virt-manager</code> GUI, add a new PCI device by selecting <code>Add Hardware -&gt; PCI Host Device</code> option. Select the RTX A4000 GPU at PCI bus address <code>01:00.0</code> (and also other functions of the PCI device such as <code>01:00.1</code>).</p> <p></p> <p>If the GPU is assigned to the VM correctly, then the left panel of the VM hardware details and the XML of the assigned GPU in <code>virt-manager</code> window should look like the following:</p> <p></p>"},{"location":"tutorials/windows_vm/#add-vnc-display-for-windows-vm","title":"Add VNC Display for Windows VM","text":"<p>Add VNC display server via \"Add Hardware\" option in <code>virt-manager</code> so that the Windows VM display can be controlled via VNC server running on Linux host.</p> <p></p>"},{"location":"tutorials/windows_vm/#install-nvidia-driver-in-windows-vm","title":"Install NVIDIA Driver in Windows VM","text":"<p>Download the NVIDIA driver for RTX A4000 GPU from the NVIDIA website.</p> <p>After installing the NVIDIA driver, reboot the Windows VM. Then, open command-prompt by typing <code>cmd</code> in the Windows search bar and type <code>nvidia-smi</code>. The following output should be displayed:</p> <p></p>"},{"location":"tutorials/windows_vm/#replicate-monitor-display-to-vnc-display","title":"Replicate Monitor Display to VNC Display","text":"<p>Right now, we have two screen for the Windows VM: physical monitor (VGA) and VNC display. It could be difficult to control mouse and keyboard with both the displays. Therefore, the monitor display can be replicated to the monitor display so that the Windows VM's physical monitor display can be controlled via the VNC display.</p> <p>The following configuration shows that the screens are replicated in Windows VM:</p> <p></p>"},{"location":"tutorials/windows_vm/#communication-performance-between-host-and-vm","title":"Communication Performance between Host and VM","text":"<p>The performance of a Windows VM running on a Linux under KVM-based virtualization is well-studied in the research literature. The performance is dependent on specific hardware and applications. In this tutorial, we provide the data-transfer performance in our hardware configuration.</p> <p>Note: Firewalls in both Windows VM and Linux host need to be properly configured to allow network traffic between the two OS. Configuring exact firewall rules is out-of-scope of this tutorial. However, we provide a few tips below as guiding help on this topic.</p> <p>Allowing Network Traffic between VM and Host</p> <p>Use a \"bridge device\" for the VM's network configuration.</p> <p></p> <p>In Windows VM search bar, type \"Windows Defender Firewall with Advanced Security\" and open the application. For both \"Inbound Rules\" and \"Outbound Rules\", add new rules to allow TCP and UDP traffic on common (or all) port numbers.</p> <p></p> <p>In the host Linux, it may not be needed to configure the firewall to allow network traffic between VM and host. However, in case it does not work, <code>iptables</code> or <code>ufw</code> can be used to configure traffic on specific ports.</p> <p>Configuring firewall</p> <p>Turning off the firewall, either in Windows VM, or in host Linux, is not recommended. However, to test the setup in this tutorial, firewalls can be turned off. In Windows, type \"firewall\" in the search bar and turn off the firewall. The setting will look like below:</p> <p></p>"},{"location":"tutorials/windows_vm/#iperf3-performance-test","title":"iperf3 Performance Test","text":"<p>Install iperf3 in both Windows VM and host Linux.</p> <p>We need the IP address of the Windows VM and the host Linux. In our setup, the Linux server IP is <code>192.168.122.1</code> and the Windows VM client IP is <code>192.168.122.147</code>. To check the host Linux IP address, run <code>ifconfig</code> and the IP address of the <code>virbr0</code> interface is the IP address of the host. The Windows VM IP address can be checked in <code>virt-manager</code> GUI in the NIC setting, or by running <code>ipconfig</code> in the Command Prompt.</p>"},{"location":"tutorials/windows_vm/#host-linux-server-and-windows-vm-client","title":"Host Linux Server and Windows VM Client","text":"<p>In host Linux, run the following command:</p> <pre><code>iperf3 -s -B 192.168.122.1\n</code></pre> <p>In the Windows VM, run the following command for a 5 second test: <pre><code>iperf3.exe -c 192.168.122.1 -t 5\n</code></pre></p> <p>Output in Linux should look like below: <pre><code>$ iperf3 -s -B 192.168.122.1\n-----------------------------------------------------------\nServer listening on 5201\n-----------------------------------------------------------\nAccepted connection from 192.168.122.147, port 50807\n[  5] local 192.168.122.1 port 5201 connected to 192.168.122.147 port 50808\n[ ID] Interval           Transfer     Bitrate\n[  5]   0.00-1.00   sec  1.98 GBytes  17.0 Gbits/sec\n[  5]   1.00-2.00   sec  1.88 GBytes  16.1 Gbits/sec\n[  5]   2.00-3.00   sec  2.01 GBytes  17.3 Gbits/sec\n[  5]   3.00-4.00   sec  2.01 GBytes  17.3 Gbits/sec\n[  5]   4.00-5.00   sec  2.00 GBytes  17.2 Gbits/sec\n[  5]   5.00-5.02   sec  46.6 MBytes  16.0 Gbits/sec\n- - - - - - - - - - - - - - - - - - - - - - - - -\n[ ID] Interval           Transfer     Bitrate\n[  5]   0.00-5.02   sec  9.93 GBytes  17.0 Gbits/sec                  receiver\n</code></pre></p> <p>Output in Windows VM should look like below:</p> <p></p> <p>For Linux host to Windows, we have observed 21.9 Gbits/sec transfer rate for a 1 minute test.</p>"},{"location":"tutorials/windows_vm/#host-linux-client-and-windows-vm-server","title":"Host Linux Client and Windows VM Server","text":"<p>The above commands are reversed for host Linux client and Windows VM server. In the Windows VM, run the following command:</p> <pre><code>iperf3.exe -s -B 192.168.122.147\n</code></pre> <p>In the host Linux, run the following command for a 5 second test:</p> <pre><code>iperf3 -c 192.168.122.147 -t 60\n</code></pre> <p>For a 1 minute test, we have observed 4.33 Gbits/sec transfer rate from Windows VM to host.</p>"},{"location":"tutorials/windows_vm/#running-holoscan-dds-app-and-windows-vm-app","title":"Running Holoscan DDS App and Windows VM App","text":"<p>If the above configurations are done correctly, then interoperating between a Holoscan application and Windows VM application is straightforward. For this tutorial, we will use the Holoscan DDS app and a simple Windows application. The Holoscan DDS app is used in <code>publisher</code> mode where it reads frames from a USB camera and sends the frames via DDS. The Windows application (available upon request) receives the frames via DDS and renders the frame on the screen using the RTX A4000 GPU with the help of OpenGL.</p> <p>In Linux host, run the Holoscan DDS app in <code>publisher</code> mode:</p> <pre><code>./run launch dds_video --extra_args \"-p\"\n</code></pre> <p>In Windows VM, running the renderer application shows the camera input from Linux host:</p> <p></p>"},{"location":"tutorials/windows_vm/#references","title":"References","text":"<ul> <li>https://wiki.archlinux.org/title/PCI_passthrough_via_OVMF</li> <li>https://www.makeuseof.com/create-windows-virtual-machine-in-linux-with-kvm/</li> </ul>"},{"location":"workflows/","title":"Workflows","text":"<p>Workflows are pre-configured sequences of operations that help users accomplish specific tasks efficiently. These workflows combine multiple Holoscan components and settings into ready-to-use solutions, making it easier to get started with common use cases.</p>"},{"location":"workflows/ai_surgical_video/","title":"Real-Time End-to-end AI Surgical Video Workflow","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: May 13, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 3.0.0 Tested Holoscan SDK versions: 3.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p> Fig.1: The overall diagram illustrating the end-to-end pipeline for real-time AI surgical video processing. The pipeline achieves an average end-to-end latency of 37ms (maximum 54ms). Key latency components are shown: Holoscan Sensor Bridge (HSB) latency averages 21ms (max 28ms), and the AI application averages 16ms (median 17ms, 95th percentile 18ms, 99th percentile 22ms, max 26ms). These results demonstrate the solution's high-performance, low-latency capabilities for demanding surgical video applications.</p>","tags":["SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#overview","title":"Overview","text":"<p>This reference application offers developers a modular, end-to-end pipeline that spans the entire sensor processing workflow\u2014from sensor data ingestion and accelerated computing to AI inference, real-time visualization, and data stream output.</p> <p>Specifically, we demonstrate a comprehensive real-time end-to-end AI surgical video pipeline that includes:</p> <ol> <li>Sensor I/O: Integration with Holoscan Sensor Bridge, enabling GPU Direct data ingestion for ultra low-latency input of surgical video feeds.</li> <li>Out-of-body detection to determine if the endoscope is inside or outside the patient's body, ensuring patient privacy by removing identifiable information.</li> <li>Dynamic flow condition based on out-of-body detection results.</li> <li>De-identification: pixelate the image to anonymize outside of body elements like people's faces.</li> <li>Multi-AI: Enabling simultaneous execution of multiple models at inference. Surgical Tool processing with:</li> <li>SSD detection for surgical tool detection</li> <li>MONAI segmentation for endoscopic tool segmentation</li> </ol>","tags":["SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#architecture","title":"Architecture","text":"<p> Fig.2: The workflow diagram representing all the holoscan operators (in green) and holoscan sensor bridge operators (in yellow). The source can be a Holoscan Sensor Bridge, an AJA Card or a video replayer.</p> <p> Fig.3: Endoscopy image from a partial nephrectomy procedure (surgical removal of the diseased portion of the kidney) showing AI tool segmentation results when the camera is inside the body and a deidentified (pixelated) output image when the camera is outside of the body.</p>","tags":["SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#1-out-of-body-detection","title":"1. Out-of-Body Detection","text":"<p>The workflow first determines if the endoscope is inside or outside the patient's body using an AI model.</p>","tags":["SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#2-dynamic-flow-control","title":"2. Dynamic Flow Control","text":"<ul> <li>If outside the body: The video is deidentified through pixelation to protect privacy</li> <li>If inside the body: The video is processed by the multi-AI pipeline</li> </ul>","tags":["SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#3-multi-ai-processing","title":"3. Multi-AI Processing","text":"<p>When inside the body, two AI models run concurrently:</p> <ul> <li>SSD detection model identifies surgical tools with bounding boxes</li> <li>MONAI segmentation model provides pixel-level segmentation of tools</li> </ul>","tags":["SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#4-visualization","title":"4. Visualization","text":"<p>The HolovizOp displays the processed video with overlaid AI results, including:</p> <ul> <li>Bounding boxes around detected tools</li> <li>Segmentation masks for tools</li> <li>Text labels for detected tools</li> </ul>","tags":["SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#requirements","title":"Requirements","text":"","tags":["SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#software","title":"Software","text":"<ul> <li>Holoscan SDK <code>v3.0</code>:   Holohub command takes care of this dependency when using Holohub container. However, you can install the Holoscan SDK via one of the methods specified in the SDK user guide.</li> <li>Holoscan Sensor Bridge <code>v2.0</code>: Please see the Quick start guide for building the Holoscan Sensor Bridge docker container.</li> </ul>","tags":["SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#models","title":"Models","text":"<p>This workflow utilizes the following three AI models:</p> Model Description File \ud83d\udce6\ufe0f Out-of-body Detection Model Detects if endoscope is inside or outside the body <code>anonymization_model.onnx</code> \ud83d\udce6\ufe0f SSD Detection for Endoscopy Surgical Tools Detects surgical tools with bounding boxes <code>epoch24_nms.onnx</code> \ud83d\udce6\ufe0f MONAI Endoscopic Tool Segmentation Provides pixel-level segmentation of tools <code>model_endoscopic_tool_seg_sanitized_nhwc_in_nchw_out.onnx</code>","tags":["SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#sample-data","title":"Sample Data","text":"<ul> <li>\ud83d\udce6\ufe0f Orsi partial nephrectomy procedures - Sample endoscopy video data for use with the <code>replayer</code> source</li> </ul> <p>Note: The directory specified by <code>--data</code> at runtime is assumed to contain three subdirectories, corresponding to the NGC resources specified in Models and Sample Data: <code>orsi</code>, <code>monai_tool_seg_model</code> and <code>ssd_model</code>. These resources will be automatically downloaded to the Holohub data directory when building the application.</p>","tags":["SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#quick-start-guide","title":"Quick Start Guide","text":"","tags":["SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#using-aja-card-or-replayer-as-io","title":"Using AJA Card or Replayer as I/O","text":"<pre><code>./dev_container build_and_run ai_surgical_video\n</code></pre>","tags":["SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#using-holoscan-sensor-bridge-as-io","title":"Using Holoscan Sensor Bridge as I/O","text":"<p>When using the workflow with <code>--source hsb</code>, it requires the Holoscan Sensor Bridge software to be installed. You can build a Holoscan Sensor Bridge container using the following commands:</p> <pre><code>git clone https://github.com/nvidia-holoscan/holoscan-sensor-bridge.git\ncd holoscan-sensor-bridge\ngit checkout hsdk-3.0\n./docker/build.sh\n</code></pre> <p>This will build a docker image called <code>hololink-demo:2.0.0</code>.</p> <p>Once you have built the Holoscan Sensor Bridge container, you can build the Holohub container and run the workflow using the following command:</p> <pre><code>./dev_container build_and_run --base_img hololink-demo:2.0.0 --img holohub:link ai_surgical_video --run_args \" --source hsb\"\n</code></pre>","tags":["SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#advanced-usage","title":"Advanced Usage","text":"","tags":["SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#building-the-application","title":"Building the Application","text":"<p>First, you need to run the Holohub container:</p> <pre><code>./dev_container launch --img holohub:link \n</code></pre> <p>Then, you can build the workflow using the following command:</p> <pre><code>./run build ai_surgical_video\n</code></pre>","tags":["SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#running-the-application","title":"Running the Application","text":"","tags":["SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#use-holohub-container-from-outside-of-the-container","title":"Use Holohub Container from Outside of the Container","text":"<p>Using the Holohub container, you can run the workflow without building it again:</p> <pre><code>./dev_container build_and_run --base_img hololink-demo:2.0.0 --img holohub:link --no_build ai_surgical_video\n</code></pre> <p>However, if you want to build the workflow, you can just remove the <code>--no_build</code> flag:</p> <pre><code>./dev_container build_and_run --base_img hololink-demo:2.0.0 --img holohub:link ai_surgical_video\n</code></pre>","tags":["SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#use-holohub-container-from-inside-the-container","title":"Use Holohub Container from Inside the Container","text":"<p>First, you need to run the Holohub container:</p> <pre><code>./dev_container launch --img holohub:link \n</code></pre> <p>To run the Python application, you can make use of the run script:</p> <pre><code>./run launch ai_surgical_video\n</code></pre> <p>Alternatively, you can run the application directly:</p> <pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;/workflows/ai_surgical_video/python\npython3 ai_surgical_video.py --source hsb --data &lt;DATA_DIR&gt; --config &lt;CONFIG_FILE&gt;\n</code></pre> <p>TIP: You can get the exact \"Run command\" along with \"Run environment\" and \"Run workdir\" by executing:</p> <pre><code>./run launch ai_surgical_video --dryrun\n</code></pre>","tags":["SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#command-line-arguments","title":"Command Line Arguments","text":"<p>The application accepts the following command line arguments:</p> Argument Description Default <code>-s, --source</code> Source of video input: <code>replayer</code>, <code>aja</code>, or <code>hsb</code> <code>replayer</code> <code>-c, --config</code> Path to a custom configuration file <code>config.yaml</code> in the application directory <code>-d, --data</code> Path to the data directory containing model and video files Uses the <code>HOLOHUB_DATA_PATH</code> environment variable <code>--headless</code> Run in headless mode (no visualization) False <code>--fullscreen</code> Run in fullscreen mode False <code>--camera-mode</code> Camera mode to use [0,1,2,3] <code>0</code> <code>--frame-limit</code> Exit after receiving this many frames No limit <code>--hololink</code> IP address of Hololink board <code>192.168.0.2</code> <code>--log-level</code> Logging level to display <code>20</code> <code>--ibv-name</code> IBV device to use First available InfiniBand device <code>--ibv-port</code> Port number of IBV device <code>1</code> <code>--expander-configuration</code> I2C Expander configuration (0 or 1) <code>0</code> <code>--pattern</code> Configure to display a test pattern (0-11) None <code>--ptp-sync</code> After reset, wait for PTP time to synchronize False <code>--skip-reset</code> Don't call reset on the hololink device False","tags":["SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#benchmarking","title":"Benchmarking","text":"<p>Please refer to Holoscan Benchmarking for how to perform benchmarking for this workflow.</p>","tags":["SSD","Detection","MONAI","Segmentation"]}]}