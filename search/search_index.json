{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Holoscan Reference Applications is a central repository for the NVIDIA Holoscan AI sensor processing community  to share reference applications, operators, tutorials and benchmarks. The repository hosts a variety of applications that demonstrate how to use Holoscan for streaming, imaging, and other AI-driven tasks across embedded, edge, and cloud environments. These applications serve as reference implementations, providing developers with examples of best practices and efficient coding techniques to build high-performance, low-latency AI applications. The repository is open to contributions from the community, encouraging developers to share their own applications and extensions to enhance the Holoscan ecosystem.</p> <ul> <li> <p> Workflows (1)</p> <p>Reference workflows demonstrate how capabilities from applications and operators can be combined to achieve complex tasks.</p> <p>Browse Workflows</p> </li> <li> <p> Applications (78)</p> <p>Reference applications demonstrate a specific capability of Holoscan or how a specific operator can be used to perform an optimize task.</p> <p>Browse Applications</p> </li> <li> <p> Operators (50)</p> <p>Operators perform a specific task.</p> <p>Browse Operators</p> </li> <li> <p> Tutorials (14)</p> <p>Tutorials provide hands-on experience.</p> <p>Browse Tutorials</p> </li> <li> <p> Benchmarks (5)</p> <p>Benchmarks provide tools for assessing performance of Holoscan pipelines as well as reference benchmarks for specific releases</p> <p>Browse Benchmarks</p> </li> </ul>"},{"location":"SUMMARY/","title":"SUMMARY","text":"<ul> <li>Home</li> <li>Workflows</li> <li>Applications</li> <li>Operators</li> <li>Tutorials</li> <li>Benchmarks</li> </ul>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#tag:3dslicer","title":"3DSlicer","text":"<ul> <li>            OpenIGTLink 3D Slicer (C++)          </li> <li>            OpenIGTLink 3D Slicer (Python)          </li> <li>            openigtlink          </li> </ul>"},{"location":"tags/#tag:ai","title":"AI","text":"<ul> <li>            medical_imaging          </li> </ul>"},{"location":"tags/#tag:ai-assistant","title":"AI-Assistant","text":"<ul> <li>            HoloChat          </li> </ul>"},{"location":"tags/#tag:aja","title":"AJA","text":"<ul> <li>            AJA Video Capture (C++)          </li> <li>            AJA Video Capture (Python)          </li> <li>            Endoscopy Depth Estimation          </li> <li>            Endoscopy Tool Tracking (C++)          </li> <li>            Endoscopy Tool Tracking (Python)          </li> <li>            H.264 Endoscopy Tool Tracking Distributed (C++)          </li> <li>            aja_source          </li> <li>            gRPC-streaming H.264 Endoscopy Tool Tracking Distributed          </li> </ul>"},{"location":"tags/#tag:asr","title":"ASR","text":"<ul> <li>            FM-ASR          </li> <li>            Riva ASR to local-LLM          </li> </ul>"},{"location":"tags/#tag:aws","title":"AWS","text":"<ul> <li>            Holoscan Playground on AWS          </li> </ul>"},{"location":"tags/#tag:acceleration","title":"Acceleration","text":"<ul> <li>            CUDA MPS Tutorial          </li> </ul>"},{"location":"tags/#tag:aerospace","title":"Aerospace","text":"<ul> <li>            Software Defined Radio FM Demodulation          </li> </ul>"},{"location":"tags/#tag:aerospace-defense-communications","title":"Aerospace, Defense, Communications","text":"<ul> <li>            Simple Classical Radar Pipeline (Python)          </li> </ul>"},{"location":"tags/#tag:amazon-web-services","title":"Amazon Web Services","text":"<ul> <li>            Holoscan Playground on AWS          </li> </ul>"},{"location":"tags/#tag:augmented-reality","title":"Augmented Reality","text":"<ul> <li>            ORSI Academy Surgical Tool Segmentation and AR Overlay (C++)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (C++)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (C++)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (Python)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (Python)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (Python)          </li> </ul>"},{"location":"tags/#tag:automatic-speech-recognition","title":"Automatic Speech Recognition","text":"<ul> <li>            FM-ASR          </li> </ul>"},{"location":"tags/#tag:bt2020","title":"BT.2020","text":"<ul> <li>            Holoviz HDR          </li> </ul>"},{"location":"tags/#tag:beamforming","title":"Beamforming","text":"<ul> <li>            matlab_beamform          </li> </ul>"},{"location":"tags/#tag:benchmarking","title":"Benchmarking","text":"<ul> <li>            CUDA MPS Tutorial          </li> <li>            Exclusive Display Benchmark          </li> <li>            Holoscan Flow Benchmarking          </li> <li>            Holoscan Release Benchmarking          </li> <li>            Model Benchmarking (C++)          </li> <li>            Model Benchmarking (Python)          </li> </ul>"},{"location":"tags/#tag:bootcamp","title":"Bootcamp","text":"<ul> <li>            NVIDIA Holoscan Bootcamp lab materials          </li> </ul>"},{"location":"tags/#tag:browser","title":"Browser","text":"<ul> <li>            webrtc_client          </li> <li>            webrtc_server          </li> </ul>"},{"location":"tags/#tag:cuda","title":"CUDA","text":"<ul> <li>            CUDA MPS Tutorial          </li> <li>            Cuda Quantum          </li> <li>            Deploying Llama-2 70b model on the edge with IGX Orin          </li> <li>            matlab_beamform          </li> <li>            matlab_image_processing          </li> </ul>"},{"location":"tags/#tag:cv","title":"CV","text":"<ul> <li>            cvcuda_holoscan_interop          </li> </ul>"},{"location":"tags/#tag:cv-cuda","title":"CV-CUDA","text":"<ul> <li>            Integrate External Libraries into a Holoscan Pipeline          </li> <li>            cvcuda_holoscan_interop          </li> </ul>"},{"location":"tags/#tag:camera","title":"Camera","text":"<ul> <li>            High Speed Endoscopy (C++)          </li> <li>            High Speed Endoscopy (Python)          </li> <li>            aja_source          </li> <li>            apriltag_detector          </li> <li>            emergent_source          </li> <li>            videomaster          </li> <li>            yuan_qcap          </li> </ul>"},{"location":"tags/#tag:chatbot","title":"Chatbot","text":"<ul> <li>            Deploying Llama-2 70b model on the edge with IGX Orin          </li> </ul>"},{"location":"tags/#tag:claraviz","title":"ClaraViz","text":"<ul> <li>            Volume Rendering (C++)          </li> <li>            Volume Rendering (Python)          </li> <li>            volume_renderer          </li> </ul>"},{"location":"tags/#tag:classification","title":"Classification","text":"<ul> <li>            Colonoscopy segmentation          </li> <li>            Endoscopy Out of Body Detection Pipeline in C++ (C++)          </li> <li>            Endoscopy Out of Body Detection Pipeline in Python (Python)          </li> </ul>"},{"location":"tags/#tag:client","title":"Client","text":"<ul> <li>            WebRTC Video Client          </li> <li>            webrtc_client          </li> </ul>"},{"location":"tags/#tag:cloud","title":"Cloud","text":"<ul> <li>            Holoscan Playground on AWS          </li> </ul>"},{"location":"tags/#tag:colonoscopy","title":"Colonoscopy","text":"<ul> <li>            Colonoscopy segmentation          </li> </ul>"},{"location":"tags/#tag:communications","title":"Communications","text":"<ul> <li>            Software Defined Radio FM Demodulation          </li> </ul>"},{"location":"tags/#tag:computer-vision","title":"Computer Vision","text":"<ul> <li>            Body Pose Estimation          </li> <li>            Depth Anything V2          </li> <li>            Real-Time Face and Text Deidentification App          </li> <li>            Self-Supervised Contrastive Learning for Surgical videos          </li> <li>            TAO PeopleNet Detection Model on Video Stream          </li> <li>            Yolo Detection          </li> <li>            cvcuda_holoscan_interop          </li> <li>            matlab_image_processing          </li> </ul>"},{"location":"tags/#tag:connectx","title":"ConnectX","text":"<ul> <li>            Achieving High Performance Networking with Holoscan          </li> </ul>"},{"location":"tags/#tag:container","title":"Container","text":"<ul> <li>            Interactively Debugging a Holoscan Application          </li> </ul>"},{"location":"tags/#tag:convert","title":"Convert","text":"<ul> <li>            Convert Depth to Screen Space          </li> </ul>"},{"location":"tags/#tag:dds","title":"DDS","text":"<ul> <li>            DDS Video          </li> <li>            dds_base          </li> <li>            dds_shapes          </li> <li>            dds_video_publisher          </li> <li>            dds_video_subscriber          </li> </ul>"},{"location":"tags/#tag:dicom","title":"DICOM","text":"<ul> <li>            DICOM to OpenUSD mesh segmentation with MONAI Deploy and Holoscan          </li> <li>            Imaging AI Whole Body Segmentation          </li> <li>            medical_imaging          </li> </ul>"},{"location":"tags/#tag:dicom-seg","title":"DICOM SEG","text":"<ul> <li>            Imaging AI Whole Body Segmentation          </li> </ul>"},{"location":"tags/#tag:dpdk","title":"DPDK","text":"<ul> <li>            Achieving High Performance Networking with Holoscan          </li> <li>            Advanced Networking Benchmark (C++)          </li> <li>            advanced_network          </li> </ul>"},{"location":"tags/#tag:dsp","title":"DSP","text":"<ul> <li>            data_writer          </li> <li>            fft          </li> <li>            high_rate_psd          </li> <li>            low_rate_psd          </li> <li>            vita49_psd_packetizer          </li> </ul>"},{"location":"tags/#tag:debugging","title":"Debugging","text":"<ul> <li>            Interactively Debugging a Holoscan Application          </li> </ul>"},{"location":"tags/#tag:defence","title":"Defence","text":"<ul> <li>            Software Defined Radio FM Demodulation          </li> </ul>"},{"location":"tags/#tag:deltacast","title":"DeltaCast","text":"<ul> <li>            ORSI Academy Surgical Tool Segmentation and AR Overlay (C++)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (C++)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (C++)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (Python)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (Python)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (Python)          </li> </ul>"},{"location":"tags/#tag:deltacast","title":"Deltacast","text":"<ul> <li>            Videomaster transmitter example          </li> <li>            videomaster          </li> </ul>"},{"location":"tags/#tag:depth","title":"Depth","text":"<ul> <li>            Convert Depth to Screen Space          </li> <li>            Endoscopy Depth Estimation          </li> </ul>"},{"location":"tags/#tag:detection","title":"Detection","text":"<ul> <li>            Multi AI SSD Detection MONAI Endoscopic Tool Segmentation Application (C++)          </li> <li>            Multi AI SSD Detection MONAI Endoscopic Tool Segmentation Application (Python)          </li> <li>            Real-Time AI End-to-End Surgical Video Workflow          </li> <li>            Real-Time Face and Text Deidentification App          </li> <li>            SSD Detection Application          </li> <li>            TAO PeopleNet Detection Model on Video Stream          </li> </ul>"},{"location":"tags/#tag:dev-container","title":"Dev Container","text":"<ul> <li>            Holoscan SDK NGC Container Dev Container          </li> </ul>"},{"location":"tags/#tag:distributed","title":"Distributed","text":"<ul> <li>            Creating Multi-Node Holoscan Applications          </li> </ul>"},{"location":"tags/#tag:drone","title":"Drone","text":"<ul> <li>            Orthorectification with OptiX (Python)          </li> </ul>"},{"location":"tags/#tag:ec2","title":"EC2","text":"<ul> <li>            Holoscan Playground on AWS          </li> </ul>"},{"location":"tags/#tag:ehr","title":"EHR","text":"<ul> <li>            EHR Agent Framework          </li> </ul>"},{"location":"tags/#tag:eotf","title":"EOTF","text":"<ul> <li>            Holoviz HDR          </li> </ul>"},{"location":"tags/#tag:electronic-support","title":"Electronic Support","text":"<ul> <li>            Basic PDW Pipeline          </li> </ul>"},{"location":"tags/#tag:emergent","title":"Emergent","text":"<ul> <li>            High Speed Endoscopy (C++)          </li> <li>            High Speed Endoscopy (Python)          </li> <li>            emergent_source          </li> </ul>"},{"location":"tags/#tag:end-to-end-application","title":"End-to-End Application","text":"<ul> <li>            Real-Time AI End-to-End Surgical Video Workflow          </li> </ul>"},{"location":"tags/#tag:endoscopy","title":"Endoscopy","text":"<ul> <li>            Endoscopy Depth Estimation          </li> <li>            Endoscopy Out of Body Detection Pipeline in C++ (C++)          </li> <li>            Endoscopy Out of Body Detection Pipeline in Python (Python)          </li> <li>            Endoscopy Tool Segmentation from MONAI Model Zoo Application          </li> <li>            Endoscopy Tool Tracking (C++)          </li> <li>            Endoscopy Tool Tracking (Python)          </li> <li>            Endoscopy Tool Tracking Distributed (C++)          </li> <li>            Endoscopy Tool Tracking Distributed (Python)          </li> <li>            H.264 Endoscopy Tool Tracking (Python)          </li> <li>            H.264 Endoscopy Tool Tracking Distributed (C++)          </li> <li>            H.264 Endoscopy Tool Tracking Distributed (Python)          </li> <li>            H264 Endoscopy Tool Tracking (C++)          </li> <li>            ORSI Academy Surgical Tool Segmentation and AR Overlay (C++)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (C++)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (C++)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (Python)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (Python)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (Python)          </li> <li>            Stereo Vision          </li> <li>            VPI Stereo Vision          </li> <li>            gRPC-streaming Endoscopy Tool Tracking (C++)          </li> <li>            gRPC-streaming Endoscopy Tool Tracking (Python)          </li> <li>            gRPC-streaming H.264 Endoscopy Tool Tracking Distributed          </li> </ul>"},{"location":"tags/#tag:ethernet","title":"Ethernet","text":"<ul> <li>            Advanced Networking Benchmark (C++)          </li> <li>            Advanced Networking Benchmark (Python)          </li> <li>            Basic Networking Benchmark (Python)          </li> <li>            OpenIGTLink 3D Slicer (C++)          </li> <li>            OpenIGTLink 3D Slicer (Python)          </li> <li>            advanced_network          </li> <li>            basic_network          </li> <li>            openigtlink          </li> </ul>"},{"location":"tags/#tag:exclusive-display","title":"Exclusive Display","text":"<ul> <li>            Exclusive Display Benchmark          </li> </ul>"},{"location":"tags/#tag:fft","title":"FFT","text":"<ul> <li>            PSD pipeline          </li> </ul>"},{"location":"tags/#tag:fhir","title":"FHIR","text":"<ul> <li>            FHIR Client App          </li> <li>            ehr_query_llm          </li> </ul>"},{"location":"tags/#tag:fhir-service-client","title":"FHIR service client","text":"<ul> <li>            FHIR Client App          </li> <li>            ehr_query_llm          </li> </ul>"},{"location":"tags/#tag:filter","title":"Filter","text":"<ul> <li>            npp_filter          </li> </ul>"},{"location":"tags/#tag:flow","title":"Flow","text":"<ul> <li>            Holoscan Flow Benchmarking          </li> </ul>"},{"location":"tags/#tag:fragment","title":"Fragment","text":"<ul> <li>            Creating Multi-Node Holoscan Applications          </li> </ul>"},{"location":"tags/#tag:gpu-direct-storage","title":"GPU Direct Storage","text":"<ul> <li>            GPU Direct Storage on IGX          </li> </ul>"},{"location":"tags/#tag:gpudirect","title":"GPUDirect","text":"<ul> <li>            Achieving High Performance Networking with Holoscan          </li> <li>            Advanced Networking Benchmark (C++)          </li> <li>            advanced_network          </li> </ul>"},{"location":"tags/#tag:gpunetio","title":"GPUNetIO","text":"<ul> <li>            Achieving High Performance Networking with Holoscan          </li> </ul>"},{"location":"tags/#tag:gui","title":"GUI","text":"<ul> <li>            Adding a GUI to Holoscan Python Applications          </li> </ul>"},{"location":"tags/#tag:gauss","title":"Gauss","text":"<ul> <li>            npp_filter          </li> </ul>"},{"location":"tags/#tag:gaze-tracking","title":"Gaze tracking","text":"<ul> <li>            XR Basic Rendering Operator          </li> </ul>"},{"location":"tags/#tag:h264","title":"H.264","text":"<ul> <li>            H.264 Video Decode Reference Application (C++)          </li> </ul>"},{"location":"tags/#tag:h264","title":"H264","text":"<ul> <li>            H.264 Video Decode Reference Application (Python)          </li> </ul>"},{"location":"tags/#tag:hpc","title":"HPC","text":"<ul> <li>            Achieving High Performance Networking with Holoscan          </li> </ul>"},{"location":"tags/#tag:holoscan","title":"Holoscan","text":"<ul> <li>            Integrate External Libraries into a Holoscan Pipeline          </li> </ul>"},{"location":"tags/#tag:holoviz","title":"Holoviz","text":"<ul> <li>            WebRTC Holoviz Server          </li> </ul>"},{"location":"tags/#tag:holoviz-hdr","title":"Holoviz HDR","text":"<ul> <li>            Holoviz HDR          </li> </ul>"},{"location":"tags/#tag:holoviz-ui","title":"Holoviz UI","text":"<ul> <li>            Holoviz UI          </li> </ul>"},{"location":"tags/#tag:holoviz-yuv","title":"Holoviz YUV","text":"<ul> <li>            Holoviz YUV          </li> </ul>"},{"location":"tags/#tag:holoviz-srgb","title":"Holoviz sRGB","text":"<ul> <li>            Holoviz sRGB          </li> </ul>"},{"location":"tags/#tag:holoviz-vsync","title":"Holoviz vsync","text":"<ul> <li>            Holoviz vsync          </li> </ul>"},{"location":"tags/#tag:huggingface","title":"HuggingFace","text":"<ul> <li>            Deploying Llama-2 70b model on the edge with IGX Orin          </li> </ul>"},{"location":"tags/#tag:human-body-pose-estimation","title":"Human Body Pose Estimation","text":"<ul> <li>            Body Pose Estimation          </li> </ul>"},{"location":"tags/#tag:hyperspectral","title":"Hyperspectral","text":"<ul> <li>            Hyperspectral image segmentation          </li> </ul>"},{"location":"tags/#tag:ip","title":"IP","text":"<ul> <li>            Advanced Networking Benchmark (C++)          </li> <li>            Advanced Networking Benchmark (Python)          </li> <li>            Basic Networking Benchmark (Python)          </li> <li>            Basic Networking Ping (C++)          </li> <li>            Networked Radar Pipeline          </li> <li>            advanced_network          </li> <li>            basic_network          </li> </ul>"},{"location":"tags/#tag:iq-data","title":"IQ data","text":"<ul> <li>            PSD pipeline          </li> </ul>"},{"location":"tags/#tag:image-processing","title":"Image Processing","text":"<ul> <li>            matlab_image_processing          </li> </ul>"},{"location":"tags/#tag:inference","title":"Inference","text":"<ul> <li>            medical_imaging          </li> </ul>"},{"location":"tags/#tag:interactive","title":"Interactive","text":"<ul> <li>            Interactively Debugging a Holoscan Application          </li> <li>            Qt Video Replayer          </li> <li>            qt_video          </li> </ul>"},{"location":"tags/#tag:llm","title":"LLM","text":"<ul> <li>            Deploying Llama-2 70b model on the edge with IGX Orin          </li> <li>            EHR Agent Framework          </li> <li>            HoloChat          </li> </ul>"},{"location":"tags/#tag:lstm","title":"LSTM","text":"<ul> <li>            lstm_tensor_rt_inference          </li> </ul>"},{"location":"tags/#tag:large-language-model","title":"Large Language Model","text":"<ul> <li>            Riva ASR to local-LLM          </li> <li>            Speech-to-text + Large Language Model          </li> </ul>"},{"location":"tags/#tag:large-multimodal-model","title":"Large Multimodal Model","text":"<ul> <li>            VILA Live          </li> </ul>"},{"location":"tags/#tag:large-vision-model","title":"Large Vision Model","text":"<ul> <li>            VILA Live          </li> </ul>"},{"location":"tags/#tag:learning","title":"Learning","text":"<ul> <li>            Self-Supervised Contrastive Learning for Surgical videos          </li> </ul>"},{"location":"tags/#tag:lidar","title":"Lidar","text":"<ul> <li>            Velodyne Lidar Viewer          </li> </ul>"},{"location":"tags/#tag:life-sciences-aerospace-defense-communications","title":"Life Sciences, Aerospace, Defense, Communications","text":"<ul> <li>            Power Spectral Density with cuNumeric          </li> </ul>"},{"location":"tags/#tag:lifesciences","title":"Lifesciences","text":"<ul> <li>            Software Defined Radio FM Demodulation          </li> </ul>"},{"location":"tags/#tag:llama","title":"Llama","text":"<ul> <li>            Deploying Llama-2 70b model on the edge with IGX Orin          </li> </ul>"},{"location":"tags/#tag:load","title":"Load","text":"<ul> <li>            volume_loader          </li> </ul>"},{"location":"tags/#tag:local-llm","title":"Local-LLM","text":"<ul> <li>            Riva ASR to local-LLM          </li> </ul>"},{"location":"tags/#tag:logging","title":"Logging","text":"<ul> <li>            Interactively Debugging a Holoscan Application          </li> </ul>"},{"location":"tags/#tag:matlab","title":"MATLAB","text":"<ul> <li>            matlab_beamform          </li> <li>            matlab_image_processing          </li> </ul>"},{"location":"tags/#tag:mhd","title":"MHD","text":"<ul> <li>            volume_loader          </li> </ul>"},{"location":"tags/#tag:monai","title":"MONAI","text":"<ul> <li>            DICOM to OpenUSD mesh segmentation with MONAI Deploy and Holoscan          </li> <li>            Endoscopy Tool Segmentation from MONAI Model Zoo Application          </li> <li>            Imaging AI Whole Body Segmentation          </li> <li>            Multi AI SSD Detection MONAI Endoscopic Tool Segmentation Application (C++)          </li> <li>            Multi AI SSD Detection MONAI Endoscopic Tool Segmentation Application (Python)          </li> <li>            Real-Time AI End-to-End Surgical Video Workflow          </li> <li>            medical_imaging          </li> </ul>"},{"location":"tags/#tag:monai-deploy","title":"MONAI Deploy","text":"<ul> <li>            DICOM to OpenUSD mesh segmentation with MONAI Deploy and Holoscan          </li> </ul>"},{"location":"tags/#tag:monai-model-zoo","title":"MONAI Model Zoo","text":"<ul> <li>            Imaging AI Whole Body Segmentation          </li> </ul>"},{"location":"tags/#tag:mps","title":"MPS","text":"<ul> <li>            CUDA MPS Tutorial          </li> </ul>"},{"location":"tags/#tag:medeical-imaging","title":"Medeical Imaging","text":"<ul> <li>            medical_imaging          </li> </ul>"},{"location":"tags/#tag:medical","title":"Medical","text":"<ul> <li>            Self-Supervised Contrastive Learning for Surgical videos          </li> </ul>"},{"location":"tags/#tag:mixed","title":"Mixed","text":"<ul> <li>            Holoscan XR Demo Application          </li> <li>            Medical Image viewer in XR          </li> </ul>"},{"location":"tags/#tag:monocular-depth-estimation","title":"Monocular Depth Estimation","text":"<ul> <li>            Depth Anything V2          </li> </ul>"},{"location":"tags/#tag:multiai","title":"MultiAI","text":"<ul> <li>            MultiAI Ultrasound (C++)          </li> <li>            MultiAI Ultrasound (Python)          </li> </ul>"},{"location":"tags/#tag:multiai","title":"Multiai","text":"<ul> <li>            Multi AI SSD Detection MONAI Endoscopic Tool Segmentation Application (C++)          </li> <li>            Multi AI SSD Detection MONAI Endoscopic Tool Segmentation Application (Python)          </li> <li>            Real-Time AI End-to-End Surgical Video Workflow          </li> </ul>"},{"location":"tags/#tag:multimodal-model","title":"Multimodal Model","text":"<ul> <li>            Florence-2          </li> </ul>"},{"location":"tags/#tag:nic","title":"NIC","text":"<ul> <li>            Achieving High Performance Networking with Holoscan          </li> </ul>"},{"location":"tags/#tag:nifti","title":"NIFTI","text":"<ul> <li>            volume_loader          </li> </ul>"},{"location":"tags/#tag:nim","title":"NIM","text":"<ul> <li>            Chat with NVIDIA Inference Microservice (NIM)          </li> <li>            Imaging using NVIDIA Inference Microservice (NIM)          </li> <li>            NV-CLIP NIM          </li> </ul>"},{"location":"tags/#tag:nlp","title":"NLP","text":"<ul> <li>            FM-ASR          </li> </ul>"},{"location":"tags/#tag:npp","title":"NPP","text":"<ul> <li>            npp_filter          </li> </ul>"},{"location":"tags/#tag:nrrd","title":"NRRD","text":"<ul> <li>            volume_loader          </li> </ul>"},{"location":"tags/#tag:network","title":"Network","text":"<ul> <li>            Advanced Networking Benchmark (C++)          </li> <li>            Advanced Networking Benchmark (Python)          </li> <li>            Basic Networking Benchmark (Python)          </li> <li>            Basic Networking Ping (C++)          </li> <li>            Basic PDW Pipeline          </li> <li>            Networked Radar Pipeline          </li> <li>            advanced_network          </li> <li>            basic_network          </li> </ul>"},{"location":"tags/#tag:networking","title":"Networking","text":"<ul> <li>            Achieving High Performance Networking with Holoscan          </li> <li>            Advanced Networking Benchmark (C++)          </li> <li>            Advanced Networking Benchmark (Python)          </li> <li>            Basic Networking Benchmark (Python)          </li> <li>            Basic Networking Ping (C++)          </li> <li>            Networked Radar Pipeline          </li> <li>            advanced_network          </li> <li>            basic_network          </li> </ul>"},{"location":"tags/#tag:object-detection","title":"Object detection","text":"<ul> <li>            Object detection using frcnn based pytorch model in C++          </li> </ul>"},{"location":"tags/#tag:omniverse","title":"Omniverse","text":"<ul> <li>            DICOM to OpenUSD mesh segmentation with MONAI Deploy and Holoscan          </li> </ul>"},{"location":"tags/#tag:openai-api","title":"OpenAI API","text":"<ul> <li>            Chat with NVIDIA Inference Microservice (NIM)          </li> <li>            Imaging using NVIDIA Inference Microservice (NIM)          </li> <li>            NV-CLIP NIM          </li> </ul>"},{"location":"tags/#tag:opencv","title":"OpenCV","text":"<ul> <li>            Integrate External Libraries into a Holoscan Pipeline          </li> </ul>"},{"location":"tags/#tag:openusd","title":"OpenUSD","text":"<ul> <li>            DICOM to OpenUSD mesh segmentation with MONAI Deploy and Holoscan          </li> <li>            mesh_to_usd          </li> </ul>"},{"location":"tags/#tag:openxr","title":"OpenXR","text":"<ul> <li>            Holoscan XR Demo Application          </li> <li>            Medical Image viewer in XR          </li> <li>            XR Basic Rendering Operator          </li> </ul>"},{"location":"tags/#tag:optix","title":"OptiX","text":"<ul> <li>            Orthorectification with OptiX (Python)          </li> </ul>"},{"location":"tags/#tag:orthorectification","title":"Orthorectification","text":"<ul> <li>            Orthorectification with OptiX (Python)          </li> </ul>"},{"location":"tags/#tag:psd","title":"PSD","text":"<ul> <li>            PSD pipeline          </li> </ul>"},{"location":"tags/#tag:pva","title":"PVA","text":"<ul> <li>            PVA Video Filter (Unsharp Mask) Example          </li> </ul>"},{"location":"tags/#tag:performance","title":"Performance","text":"<ul> <li>            Holoscan Release Benchmarking          </li> </ul>"},{"location":"tags/#tag:point-cloud","title":"Point Cloud","text":"<ul> <li>            Velodyne Lidar Viewer          </li> </ul>"},{"location":"tags/#tag:processing","title":"Processing","text":"<ul> <li>            PSD pipeline          </li> </ul>"},{"location":"tags/#tag:prohawk","title":"Prohawk","text":"<ul> <li>            prohawk_video_processing          </li> <li>            prohawk_video_replayer (C++)          </li> <li>            prohawk_video_replayer_py (Python)          </li> </ul>"},{"location":"tags/#tag:pyside6","title":"Pyside6","text":"<ul> <li>            Adding a GUI to Holoscan Python Applications          </li> </ul>"},{"location":"tags/#tag:python","title":"Python","text":"<ul> <li>            Adding a GUI to Holoscan Python Applications          </li> <li>            NVIDIA Holoscan Bootcamp lab materials          </li> </ul>"},{"location":"tags/#tag:qml","title":"QML","text":"<ul> <li>            Qt Video Replayer          </li> <li>            qt_video          </li> </ul>"},{"location":"tags/#tag:qt","title":"Qt","text":"<ul> <li>            Qt Video Replayer          </li> <li>            qt_video          </li> </ul>"},{"location":"tags/#tag:qtquick","title":"QtQuick","text":"<ul> <li>            Qt Video Replayer          </li> <li>            qt_video          </li> </ul>"},{"location":"tags/#tag:quantum-computing","title":"Quantum Computing","text":"<ul> <li>            Cuda Quantum          </li> </ul>"},{"location":"tags/#tag:radar","title":"RADAR","text":"<ul> <li>            Networked Radar Pipeline          </li> <li>            Simple Radar Pipeline in C++ (C++)          </li> </ul>"},{"location":"tags/#tag:rag","title":"RAG","text":"<ul> <li>            EHR Agent Framework          </li> </ul>"},{"location":"tags/#tag:rdma","title":"RDMA","text":"<ul> <li>            Achieving High Performance Networking with Holoscan          </li> <li>            Advanced Networking Benchmark (C++)          </li> <li>            advanced_network          </li> </ul>"},{"location":"tags/#tag:rti-connext","title":"RTI Connext","text":"<ul> <li>            DDS Video          </li> <li>            dds_base          </li> <li>            dds_shapes          </li> <li>            dds_video_publisher          </li> <li>            dds_video_subscriber          </li> </ul>"},{"location":"tags/#tag:radar","title":"Radar","text":"<ul> <li>            Basic PDW Pipeline          </li> <li>            Streaming Synthetic Aperture Radar          </li> </ul>"},{"location":"tags/#tag:radio","title":"Radio","text":"<ul> <li>            vita49_psd_packetizer          </li> </ul>"},{"location":"tags/#tag:radiology-imaging","title":"Radiology Imaging","text":"<ul> <li>            Imaging AI Whole Body Segmentation          </li> </ul>"},{"location":"tags/#tag:reality","title":"Reality","text":"<ul> <li>            Holoscan XR Demo Application          </li> <li>            Medical Image viewer in XR          </li> </ul>"},{"location":"tags/#tag:release","title":"Release","text":"<ul> <li>            Holoscan Release Benchmarking          </li> </ul>"},{"location":"tags/#tag:render","title":"Render","text":"<ul> <li>            Volume Rendering (C++)          </li> <li>            Volume Rendering (Python)          </li> <li>            volume_renderer          </li> </ul>"},{"location":"tags/#tag:rendering","title":"Rendering","text":"<ul> <li>            Holoscan XR Demo Application          </li> <li>            Medical Image viewer in XR          </li> </ul>"},{"location":"tags/#tag:reporting","title":"Reporting","text":"<ul> <li>            Holoscan Release Benchmarking          </li> </ul>"},{"location":"tags/#tag:rivermax","title":"Rivermax","text":"<ul> <li>            Achieving High Performance Networking with Holoscan          </li> </ul>"},{"location":"tags/#tag:sam2-model","title":"SAM2 Model","text":"<ul> <li>            sam2          </li> </ul>"},{"location":"tags/#tag:sar","title":"SAR","text":"<ul> <li>            Streaming Synthetic Aperture Radar          </li> </ul>"},{"location":"tags/#tag:sdr","title":"SDR","text":"<ul> <li>            PSD pipeline          </li> <li>            data_writer          </li> <li>            fft          </li> <li>            high_rate_psd          </li> <li>            low_rate_psd          </li> <li>            vita49_psd_packetizer          </li> </ul>"},{"location":"tags/#tag:ssd","title":"SSD","text":"<ul> <li>            Multi AI SSD Detection MONAI Endoscopic Tool Segmentation Application (C++)          </li> <li>            Multi AI SSD Detection MONAI Endoscopic Tool Segmentation Application (Python)          </li> <li>            Real-Time AI End-to-End Surgical Video Workflow          </li> <li>            SSD Detection Application          </li> </ul>"},{"location":"tags/#tag:st2084","title":"ST.2084","text":"<ul> <li>            Holoviz HDR          </li> </ul>"},{"location":"tags/#tag:stl","title":"STL","text":"<ul> <li>            DICOM to OpenUSD mesh segmentation with MONAI Deploy and Holoscan          </li> <li>            medical_imaging          </li> <li>            mesh_to_usd          </li> </ul>"},{"location":"tags/#tag:screen","title":"Screen","text":"<ul> <li>            Convert Depth to Screen Space          </li> </ul>"},{"location":"tags/#tag:segmentation","title":"Segmentation","text":"<ul> <li>            Endoscopy Tool Segmentation from MONAI Model Zoo Application          </li> <li>            Hyperspectral image segmentation          </li> <li>            Multi AI SSD Detection MONAI Endoscopic Tool Segmentation Application (C++)          </li> <li>            Multi AI SSD Detection MONAI Endoscopic Tool Segmentation Application (Python)          </li> <li>            ORSI Academy Surgical Tool Segmentation and AR Overlay (C++)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (C++)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (C++)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (Python)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (Python)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (Python)          </li> <li>            OpenIGTLink 3D Slicer (C++)          </li> <li>            OpenIGTLink 3D Slicer (Python)          </li> <li>            Real-Time AI End-to-End Surgical Video Workflow          </li> <li>            Ultrasound Segmentation (C++)          </li> <li>            Ultrasound Segmentation (Python)          </li> </ul>"},{"location":"tags/#tag:segmentation-by-ai","title":"Segmentation by AI","text":"<ul> <li>            Imaging AI Whole Body Segmentation          </li> </ul>"},{"location":"tags/#tag:self-supervised","title":"Self-Supervised","text":"<ul> <li>            Self-Supervised Contrastive Learning for Surgical videos          </li> </ul>"},{"location":"tags/#tag:sensor","title":"Sensor","text":"<ul> <li>            PSD pipeline          </li> <li>            Velodyne Lidar Viewer          </li> </ul>"},{"location":"tags/#tag:sensor-processing","title":"Sensor Processing","text":"<ul> <li>            NVIDIA Holoscan Bootcamp lab materials          </li> </ul>"},{"location":"tags/#tag:server","title":"Server","text":"<ul> <li>            WebRTC Holoviz Server          </li> <li>            WebRTC Video Server          </li> <li>            webrtc_server          </li> </ul>"},{"location":"tags/#tag:shapes","title":"Shapes","text":"<ul> <li>            DDS Video          </li> <li>            dds_base          </li> <li>            dds_shapes          </li> </ul>"},{"location":"tags/#tag:signal-processing","title":"Signal Processing","text":"<ul> <li>            FM-ASR          </li> <li>            Networked Radar Pipeline          </li> <li>            Simple Radar Pipeline in C++ (C++)          </li> <li>            matlab_image_processing          </li> </ul>"},{"location":"tags/#tag:sobel","title":"Sobel","text":"<ul> <li>            npp_filter          </li> </ul>"},{"location":"tags/#tag:speech-to-text","title":"Speech-to-text","text":"<ul> <li>            Riva ASR to local-LLM          </li> <li>            Speech-to-text + Large Language Model          </li> </ul>"},{"location":"tags/#tag:stereo","title":"Stereo","text":"<ul> <li>            Stereo Vision          </li> <li>            VPI Stereo Vision          </li> </ul>"},{"location":"tags/#tag:streaming","title":"Streaming","text":"<ul> <li>            OpenIGTLink 3D Slicer (C++)          </li> <li>            OpenIGTLink 3D Slicer (Python)          </li> <li>            openigtlink          </li> </ul>"},{"location":"tags/#tag:surgical","title":"Surgical","text":"<ul> <li>            Self-Supervised Contrastive Learning for Surgical videos          </li> </ul>"},{"location":"tags/#tag:synthetic-aperture","title":"Synthetic Aperture","text":"<ul> <li>            Streaming Synthetic Aperture Radar          </li> </ul>"},{"location":"tags/#tag:tcp","title":"TCP","text":"<ul> <li>            Advanced Networking Benchmark (Python)          </li> <li>            Basic Networking Benchmark (Python)          </li> <li>            basic_network          </li> </ul>"},{"location":"tags/#tag:tensor","title":"Tensor","text":"<ul> <li>            tensor_to_video_buffer          </li> </ul>"},{"location":"tags/#tag:tensorrt","title":"TensorRT","text":"<ul> <li>            lstm_tensor_rt_inference          </li> </ul>"},{"location":"tags/#tag:tools","title":"Tools","text":"<ul> <li>            Interactively Debugging a Holoscan Application          </li> </ul>"},{"location":"tags/#tag:total-segmentator","title":"Total segmentator","text":"<ul> <li>            Imaging AI Whole Body Segmentation          </li> </ul>"},{"location":"tags/#tag:tracking","title":"Tracking","text":"<ul> <li>            Endoscopy Tool Tracking (C++)          </li> <li>            Endoscopy Tool Tracking (Python)          </li> <li>            Endoscopy Tool Tracking Distributed (C++)          </li> <li>            Endoscopy Tool Tracking Distributed (Python)          </li> <li>            H.264 Endoscopy Tool Tracking Distributed (C++)          </li> <li>            gRPC-streaming Endoscopy Tool Tracking (C++)          </li> <li>            gRPC-streaming Endoscopy Tool Tracking (Python)          </li> <li>            gRPC-streaming H.264 Endoscopy Tool Tracking Distributed          </li> </ul>"},{"location":"tags/#tag:tutorial","title":"Tutorial","text":"<ul> <li>            Holoscan SDK NGC Container Dev Container          </li> <li>            Interoperability between Holoscan and a Windows Application on a Single Machine          </li> </ul>"},{"location":"tags/#tag:udp","title":"UDP","text":"<ul> <li>            Advanced Networking Benchmark (C++)          </li> <li>            Advanced Networking Benchmark (Python)          </li> <li>            Basic Networking Benchmark (Python)          </li> <li>            Basic Networking Ping (C++)          </li> <li>            Basic PDW Pipeline          </li> <li>            Networked Radar Pipeline          </li> <li>            advanced_network          </li> <li>            basic_network          </li> </ul>"},{"location":"tags/#tag:ui","title":"UI","text":"<ul> <li>            Qt Video Replayer          </li> <li>            qt_video          </li> </ul>"},{"location":"tags/#tag:ultrasound","title":"Ultrasound","text":"<ul> <li>            MultiAI Ultrasound (C++)          </li> <li>            MultiAI Ultrasound (Python)          </li> <li>            Ultrasound Segmentation (C++)          </li> <li>            Ultrasound Segmentation (Python)          </li> <li>            matlab_beamform          </li> </ul>"},{"location":"tags/#tag:userinterface","title":"Userinterface","text":"<ul> <li>            Qt Video Replayer          </li> <li>            qt_video          </li> </ul>"},{"location":"tags/#tag:utilities","title":"Utilities","text":"<ul> <li>            PSD pipeline          </li> </ul>"},{"location":"tags/#tag:vm","title":"VM","text":"<ul> <li>            Interoperability between Holoscan and a Windows Application on a Single Machine          </li> </ul>"},{"location":"tags/#tag:vqe","title":"VQE","text":"<ul> <li>            Cuda Quantum          </li> </ul>"},{"location":"tags/#tag:vs-code","title":"VS Code","text":"<ul> <li>            Holoscan SDK NGC Container Dev Container          </li> </ul>"},{"location":"tags/#tag:vtk","title":"VTK","text":"<ul> <li>            ORSI Academy Surgical Tool Segmentation and AR Overlay (C++)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (C++)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (C++)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (Python)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (Python)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (Python)          </li> </ul>"},{"location":"tags/#tag:vector-database","title":"Vector Database","text":"<ul> <li>            HoloChat          </li> </ul>"},{"location":"tags/#tag:velodyne","title":"Velodyne","text":"<ul> <li>            Velodyne Lidar Viewer          </li> </ul>"},{"location":"tags/#tag:video","title":"Video","text":"<ul> <li>            AJA Video Capture (C++)          </li> <li>            AJA Video Capture (Python)          </li> <li>            DDS Video          </li> <li>            Qt Video Replayer          </li> <li>            Self-Supervised Contrastive Learning for Surgical videos          </li> <li>            WebRTC Video Client          </li> <li>            WebRTC Video Server          </li> <li>            dds_video_publisher          </li> <li>            dds_video_subscriber          </li> <li>            qt_video          </li> <li>            tensor_to_video_buffer          </li> <li>            webrtc_client          </li> <li>            webrtc_server          </li> </ul>"},{"location":"tags/#tag:video-decoding","title":"Video Decoding","text":"<ul> <li>            H.264 Endoscopy Tool Tracking (Python)          </li> <li>            H.264 Endoscopy Tool Tracking Distributed (C++)          </li> <li>            H.264 Endoscopy Tool Tracking Distributed (Python)          </li> <li>            H.264 Video Decode Reference Application (C++)          </li> <li>            H.264 Video Decode Reference Application (Python)          </li> <li>            H264 Endoscopy Tool Tracking (C++)          </li> <li>            gRPC-streaming H.264 Endoscopy Tool Tracking Distributed          </li> </ul>"},{"location":"tags/#tag:video-encoding","title":"Video Encoding","text":"<ul> <li>            H.264 Endoscopy Tool Tracking (Python)          </li> <li>            H.264 Endoscopy Tool Tracking Distributed (C++)          </li> <li>            H.264 Endoscopy Tool Tracking Distributed (Python)          </li> <li>            H264 Endoscopy Tool Tracking (C++)          </li> <li>            gRPC-streaming H.264 Endoscopy Tool Tracking Distributed          </li> </ul>"},{"location":"tags/#tag:video-processing","title":"Video Processing","text":"<ul> <li>            prohawk_video_replayer (C++)          </li> <li>            prohawk_video_replayer_py (Python)          </li> </ul>"},{"location":"tags/#tag:video-processing","title":"Video processing","text":"<ul> <li>            prohawk_video_processing          </li> </ul>"},{"location":"tags/#tag:videomaster","title":"VideoMaster","text":"<ul> <li>            ORSI Academy Surgical Tool Segmentation and AR Overlay (C++)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (C++)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (C++)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (Python)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (Python)          </li> <li>            ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay (Python)          </li> <li>            Videomaster transmitter example          </li> </ul>"},{"location":"tags/#tag:vision-model","title":"Vision Model","text":"<ul> <li>            Florence-2          </li> </ul>"},{"location":"tags/#tag:visualization","title":"Visualization","text":"<ul> <li>            Velodyne Lidar Viewer          </li> </ul>"},{"location":"tags/#tag:volume","title":"Volume","text":"<ul> <li>            Holoscan XR Demo Application          </li> <li>            Medical Image viewer in XR          </li> <li>            Volume Rendering (C++)          </li> <li>            Volume Rendering (Python)          </li> <li>            volume_loader          </li> <li>            volume_renderer          </li> </ul>"},{"location":"tags/#tag:webrtc","title":"WebRTC","text":"<ul> <li>            WebRTC Holoviz Server          </li> <li>            WebRTC Video Client          </li> <li>            WebRTC Video Server          </li> <li>            webrtc_client          </li> <li>            webrtc_server          </li> </ul>"},{"location":"tags/#tag:windows","title":"Windows","text":"<ul> <li>            Interoperability between Holoscan and a Windows Application on a Single Machine          </li> </ul>"},{"location":"tags/#tag:workflow","title":"Workflow","text":"<ul> <li>            Real-Time AI End-to-End Surgical Video Workflow          </li> </ul>"},{"location":"tags/#tag:xr","title":"XR","text":"<ul> <li>            Holoscan XR Demo Application          </li> <li>            Medical Image viewer in XR          </li> <li>            XR Basic Rendering Operator          </li> <li>            XrBeginFrameOp          </li> <li>            XrEndFrameOp          </li> <li>            XrFrameOp          </li> <li>            XrTransformControlOp          </li> <li>            XrTransformOp          </li> <li>            XrTransformRenderOp          </li> </ul>"},{"location":"tags/#tag:xrframe","title":"XRFrame","text":"<ul> <li>            XR Basic Rendering Operator          </li> <li>            XrBeginFrameOp          </li> <li>            XrEndFrameOp          </li> <li>            XrFrameOp          </li> <li>            XrTransformControlOp          </li> <li>            XrTransformOp          </li> <li>            XrTransformRenderOp          </li> </ul>"},{"location":"tags/#tag:ycbcr","title":"YCbCr","text":"<ul> <li>            Holoviz YUV          </li> </ul>"},{"location":"tags/#tag:yolo-detection","title":"Yolo Detection","text":"<ul> <li>            Yolo Detection          </li> </ul>"},{"location":"tags/#tag:yuan","title":"Yuan","text":"<ul> <li>            yuan_qcap          </li> </ul>"},{"location":"tags/#tag:bounding-box","title":"bounding box","text":"<ul> <li>            Multi AI SSD Detection MONAI Endoscopic Tool Segmentation Application (C++)          </li> <li>            Multi AI SSD Detection MONAI Endoscopic Tool Segmentation Application (Python)          </li> <li>            SSD Detection Application          </li> </ul>"},{"location":"tags/#tag:converter","title":"converter","text":"<ul> <li>            orsi_format_converter          </li> </ul>"},{"location":"tags/#tag:cucim","title":"cuCIM","text":"<ul> <li>            Integrate External Libraries into a Holoscan Pipeline          </li> </ul>"},{"location":"tags/#tag:ehr_query_llm","title":"ehr_query_llm","text":"<ul> <li>            FHIR Client App          </li> <li>            ehr_query_llm          </li> </ul>"},{"location":"tags/#tag:encoder","title":"encoder","text":"<ul> <li>            video_encoder_request          </li> </ul>"},{"location":"tags/#tag:grpc","title":"gRPC","text":"<ul> <li>            gRPC-streaming Endoscopy Tool Tracking (C++)          </li> <li>            gRPC-streaming Endoscopy Tool Tracking (Python)          </li> </ul>"},{"location":"tags/#tag:grpc","title":"grpc","text":"<ul> <li>            grpc_operators          </li> </ul>"},{"location":"tags/#tag:mesh","title":"mesh","text":"<ul> <li>            DICOM to OpenUSD mesh segmentation with MONAI Deploy and Holoscan          </li> <li>            mesh_to_usd          </li> </ul>"},{"location":"tags/#tag:postprocessor","title":"postprocessor","text":"<ul> <li>            orsi_segmentation_postprocessor          </li> </ul>"},{"location":"tags/#tag:preprocessor","title":"preprocessor","text":"<ul> <li>            orsi_segmentation_preprocessor          </li> </ul>"},{"location":"tags/#tag:request","title":"request","text":"<ul> <li>            video_encoder_request          </li> </ul>"},{"location":"tags/#tag:tool-tracking","title":"tool tracking","text":"<ul> <li>            grpc_operators          </li> <li>            tool_tracking_postprocessor          </li> <li>            vtk_renderer          </li> </ul>"},{"location":"tags/#tag:unzip","title":"unzip","text":"<ul> <li>            unzip          </li> </ul>"},{"location":"tags/#tag:video","title":"video","text":"<ul> <li>            video_encoder_request          </li> <li>            visualizer_icardio          </li> </ul>"},{"location":"tags/#tag:visualization","title":"visualization","text":"<ul> <li>            grpc_operators          </li> <li>            tool_tracking_postprocessor          </li> <li>            vtk_renderer          </li> </ul>"},{"location":"tags/#tag:visualizer","title":"visualizer","text":"<ul> <li>            orsi_visualizer          </li> </ul>"},{"location":"applications/","title":"Applications","text":"<p>Holohub features a curated collection of reference applications that demonstrate the platform's capabilities across various domains, from medical imaging to industrial automation. Each application is designed to showcase best practices for integrating Holoscan's optimized libraries and microservices, ensuring high performance and low latency. Whether you are looking to streamline data processing workflows, enhance real-time analytics, or develop cutting-edge AI models, the applications in Holohub provide valuable examples and templates to accelerate your development process. </p>"},{"location":"applications/asr_to_llm/","title":"Real-time ASR to local-LLM","text":"<p> Authors: Nigel Nelson (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 19, 2025 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 4 - Experimental</p> <p>This application streams microphone input to NVIDIA Riva Automatic Speech Recognition (ASR), which once the user specifies they are done speaking, passes the transcribed text to an LLM running locally that then summarizes this text.</p> <p>While this workflow in principle could be used for a number of domains, the app is currently configured to be healthcare specific. The current LLM prompt is created for radiology interpretation, but this can be easily changed in the YAML file to tailor the LLM's output to a wide array of potential use cases.</p>","tags":["Speech-to-text","Large Language Model","ASR","Local-LLM"]},{"location":"applications/asr_to_llm/#example-output","title":"Example output","text":"<p>Example output can be found at example_output.md  Description of output fields: Final Transcription: Riva's transcription of the provided mic input LLM Summary:* The LLM's output summarization</p>","tags":["Speech-to-text","Large Language Model","ASR","Local-LLM"]},{"location":"applications/asr_to_llm/#yaml-configuration","title":"YAML Configuration","text":"<p>The directions for the LLM are determined by the <code>stt_to_nlp.yaml</code> file. As you see from our example, the directions for the LLM are made via natural language, and can result in very different applications.</p> <p>With the current YAML configuration, the resulting prompt to the LLM is:</p> <pre><code>&lt;|system|&gt;\nYou are a veteran radiologist, who can answer any medical related question.\n\n&lt;|user|&gt;\nTranscript from Radiologist:\n{**transcribed text inserted here**}\nRequest(s):\nMake a summary of the transcript (and correct any transcription errors in CAPS).\nCreate a Patient Summary with no medical jargon.\nCreate a full radiological report write-up.\nGive likely ICD-10 Codes.\nSuggest follow-up steps.\n\n&lt;|assistant|&gt;\n</code></pre>","tags":["Speech-to-text","Large Language Model","ASR","Local-LLM"]},{"location":"applications/asr_to_llm/#setup-instructions","title":"Setup Instructions","text":"","tags":["Speech-to-text","Large Language Model","ASR","Local-LLM"]},{"location":"applications/asr_to_llm/#install-riva","title":"Install Riva:","text":"<p>First, you must follow the Riva local deployment quickstart guide. For x86 and ARM64 devices with dGPU follow the \"Data Center\" instructions, for ARM64 devices with iGPU follow the \"Embedded\" instructions.</p> <ul> <li>Note: to minimize the Riva install size you can change the <code>config.sh</code> file in the <code>riva_quickstart_vX.XX.X</code> directory such that it specifies to only install the ASR models (Riva has more features but only ASR is needed for this app). To do this, find the <code>sevice_enabled_*</code> variables and set them as shown below: <pre><code>service_enabled_asr=true\nservice_enabled_nlp=false\nservice_enabled_tts=false\nservice_enabled_nmt=false\n</code></pre></li> </ul> <p>\u26a0\ufe0f Note: If you are using ARM64 w/ iGPU or an x86 platform the quick-start scripts should work as intended. However, if you are using ARM64 w/ dGPU, you will need to make the following modifications to the Riva Quick-start scripts:</p> <p>In <code>riva_init.sh</code> make the following changes to ensure the ARM64 version of NGC-CLI is downloaded and your dGPU is used to run the container: <pre><code># download required models\n-if [[ $riva_target_gpu_family == \"tegra\" ]]; then\n-    docker run -it -d --rm -v $riva_model_loc:/data \\\n+if [[ $riva_target_gpu_family == \"non-tegra\" ]]; then\n+            docker run -it -d --rm --gpus '\"'$gpus_to_use'\"' -v $riva_model_loc:/data \\\n                -e \"NGC_CLI_API_KEY=$NGC_API_KEY\" \\\n</code></pre> Then in <code>riva_start.sh</code> make the changes below to ensure your Riva server has access to your sound devices: <pre><code>docker rm $riva_daemon_speech &amp;&gt; /dev/null\n-if [[ $riva_target_gpu_family == \"tegra\" ]]; then\n+if [[ $riva_target_gpu_family == \"non-tegra\" ]]; then\n    docker_run_args=\"-p 8000:8000 -p 8001:8001 -p 8002:8002 -p 8888:8888 --device /dev/bus/usb --device /dev/snd\"\n</code></pre></p>","tags":["Speech-to-text","Large Language Model","ASR","Local-LLM"]},{"location":"applications/asr_to_llm/#setup-instructions_1","title":"Setup Instructions:","text":"<p>Download the quantized Mistral 7B LLM from HugginFace.co: <pre><code>wget -nc -P &lt;your_model_dir&gt; https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GGUF/resolve/main/mistral-7b-openorca.Q8_0.gguf\n</code></pre></p> <p>From the Holohub main directory run the following command: <pre><code>./dev_container build --docker_file applications/asr_to_llm/Dockerfile --img holohub:asr_to_llm\n</code></pre></p>","tags":["Speech-to-text","Large Language Model","ASR","Local-LLM"]},{"location":"applications/asr_to_llm/#run-instructions","title":"Run instructions","text":"<p>Launch the <code>holohub:asr_to_llm</code> container: <pre><code>./dev_container launch --img holohub:asr_to_llm --add-volume &lt;your_model_dir&gt;\n</code></pre> Run the application and use the <code>--list-devices</code> arg to determine which microphone to use: <pre><code>python &lt;streaming_asr_to_llm_dir&gt;/asr_to_llm.py --list-devices\n</code></pre> Then run the application with the <code>--input-device</code> arg to specify the correct microphone: <pre><code>python &lt;streaming_asr_to_llm_dir&gt;/asr_to_llm.py --input-device &lt;device-index&gt;\n</code></pre></p> <p>Once <code>asr_to_llm.py</code> is running, you will see output from ALSA for loading the selected audio device and also from llama_cpp for loading the LLM onto GPU memory. Once this is complete it will immediately begin printing out the transcribed text. To signal that the audio you wish to transcribe is complete, enter <code>x</code> on the keyboard. This will terminate the ASR and microphone instance, and feed the complete transcribed text into the LLM for summarization.</p>","tags":["Speech-to-text","Large Language Model","ASR","Local-LLM"]},{"location":"applications/asr_to_llm/#stopping-instructions","title":"Stopping Instructions","text":"<p>Note: The <code>python asr_to_llm.py</code> command will complete on its own once the LLM is finished summarizing the transcription * Stopping Riva services: <pre><code>bash &lt;Riva_install_dir&gt;riva_stop.sh\n</code></pre></p>","tags":["Speech-to-text","Large Language Model","ASR","Local-LLM"]},{"location":"applications/asr_to_llm/#asr_to_llm-application-arguments","title":"ASR_To_LLM Application arguments","text":"<p>The <code>asr_to_llm.py</code> can receive several cli arguments:</p> <p><code>--input-device</code>: The index of the input audio device to use. <code>--list-devices</code>: List input audio device indices. <code>--sample-rate-hz</code>: The number of frames per second in audio streamed from the selected microphone. <code>--file-streaming-chunk</code>: A maximum number of frames in a audio chunk sent to server.</p>","tags":["Speech-to-text","Large Language Model","ASR","Local-LLM"]},{"location":"applications/asr_to_llm/#implementation-details","title":"Implementation Details","text":"<p>This application adapted the speech_to_text_llm Holohub application to transcribe audio in real-time using Riva ASR, as well as ensure that the complete app runs 100% locally.</p> <p>The LLM currently used in this application is Mistral-7B-OpenOrca-GGUF, which is a quantized Mistal 7B model that is finetuned on the OpenOrca dataset. However, any model in the GGUF file format will work as long as it can fit within your device's VRAM constraints.</p> <p>The inference engine used to run the LLM is llama-cpp-python, which is a Python binding for llama.cpp. The reason for this is that the underlying llama.cpp library is hardware agnostic, dependency free, and it runs quantized LLMs with very high throughput.</p> <p>The RivaStreamingOp is a Holoscan SDK adaptation of the transcribe_mic.py script that is part of the Riva python-clients repository.</p>","tags":["Speech-to-text","Large Language Model","ASR","Local-LLM"]},{"location":"applications/asr_to_llm/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./dev_container vscode asr_to_llm\n</code></pre>","tags":["Speech-to-text","Large Language Model","ASR","Local-LLM"]},{"location":"applications/asr_to_llm/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"<p>There are two launch profiles configured for this application:</p> <ol> <li>(debugpy) asr_to_llm/python: Launch asr_to_llm using a launch profile that enables debugging of Python code.</li> <li>(pythoncpp) asr_to_llm/python: Launch asr_to_llm using a launch profile that enables debugging of Python and C++ code.</li> </ol>","tags":["Speech-to-text","Large Language Model","ASR","Local-LLM"]},{"location":"applications/body_pose_estimation/","title":"Body Pose Estimation App","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: March 14, 2025 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 2 - Trusted</p> <p>Body pose estimation is a computer vision task that involves recognizing specific points on the human body in images or videos. A model is used to infer the locations of keypoints from the source video which is then rendered by the visualizer. </p>","tags":["Computer Vision","Human Body Pose Estimation"]},{"location":"applications/body_pose_estimation/#model","title":"Model","text":"<p>This application uses YOLOv8 pose model from Ultralytics for body pose estimation. The model is downloaded when building the application.</p>","tags":["Computer Vision","Human Body Pose Estimation"]},{"location":"applications/body_pose_estimation/#data","title":"Data","text":"<p>This application downloads a pre-recorded video from Pexels when the application is built for use with this application.  Please review the license terms from Pexels.</p> <p>NOTE: The user is responsible for checking if the dataset license is fit for the intended purpose.</p>","tags":["Computer Vision","Human Body Pose Estimation"]},{"location":"applications/body_pose_estimation/#input","title":"Input","text":"<p>This app currently supports three different input options:</p> <ol> <li>v4l2 compatible input device (default, see V4L2 Support below)</li> <li>pre-recorded video (see Video Replayer Support below)</li> <li>DDS video stream (see DDS Support below)</li> </ol>","tags":["Computer Vision","Human Body Pose Estimation"]},{"location":"applications/body_pose_estimation/#run-instructions","title":"Run Instructions","text":"","tags":["Computer Vision","Human Body Pose Estimation"]},{"location":"applications/body_pose_estimation/#v4l2-support","title":"V4L2 Support","text":"<p>This application supports v4l2 compatible devices as input.  To run this application with your v4l2 compatible device, please plug in your input device and run: <pre><code>./dev_container build_and_run body_pose_estimation\n</code></pre></p> <p>By default, this application expects the input device to be mounted at <code>/dev/video0</code>.  If this is not the case, please update <code>applications/body_pose_estimation/body_pose_estimation.yaml</code> and set it to use the corresponding input device before running the application.  You can also override the default input device on the command line by running: <pre><code>./dev_container build_and_run body_pose_estimation --run_args \"--video_device /dev/video0\"\n</code></pre></p>","tags":["Computer Vision","Human Body Pose Estimation"]},{"location":"applications/body_pose_estimation/#video-replayer-support","title":"Video Replayer Support","text":"<p>If you don't have a v4l2 compatible device plugged in, you may also run this application on a pre-recorded video. To launch the application using the Video Stream Replayer as the input source, run:</p> <pre><code>./dev_container build_and_run body_pose_estimation --run_args \"--source replayer\"\n</code></pre>","tags":["Computer Vision","Human Body Pose Estimation"]},{"location":"applications/body_pose_estimation/#dds-support","title":"DDS Support","text":"<p>This application supports using a DDS video stream as the input as well as publishing the output video stream back to DDS. To enable DDS, the application must first be built with the DDS operators enabled. Only the subscriber or publisher operators need to be enabled for the sake of input or output video streams, respectively, but to enable both use the following:</p> <pre><code>./run build body_pose_estimation --with \"dds_video_subscriber;dds_video_publisher\"\n</code></pre> <p>Note that building these operators requires RTI Connext be installed. See the DDS Operator Documentation for more information on how to build the operators. If using a development container, see the additional instructions below.</p> <p>To use a DDS video stream as the input to the application, use the <code>-s=dds</code> argument when running the application:</p> <pre><code>./run launch body_pose_estimation --extra_args -s=dds\n</code></pre> <p>To publish the output result to DDS, edit the <code>body_pose_estimation.yaml</code> configuration file so that the <code>dds_publisher</code> <code>enable</code> option is <code>true</code>:</p> <pre><code>dds_publisher:\n  enable: true\n</code></pre> <p>Note that the default DDS video stream IDs use by the application are <code>0</code> for the input and <code>1</code> for the output. These can be changed using the <code>stream_id</code> settings in the <code>dds_source</code> and <code>dds_publisher</code> sections of the configuration file, respectively.</p> <p>To produce the DDS input stream or to view the output stream generated by this application, the dds_video application can be used. For example, the following will use the <code>dds_video</code> application to capture video from the default V4L2 device and publish it to DDS so that it can be received as input by this application:</p> <pre><code>./run build dds_video\n./run launch dds_video --extra_args \"-p -i 0\"\n</code></pre> <p>And the following will use the <code>dds_video</code> application to receive and render the output published by this application:</p> <pre><code>./run launch dds_video --extra_args \"-s -i 1\"\n</code></pre>","tags":["Computer Vision","Human Body Pose Estimation"]},{"location":"applications/body_pose_estimation/#using-a-development-container-with-dds-support","title":"Using a Development Container with DDS Support","text":"<p>Installing RTI Connext into the development container is not currently supported, so enabling DDS support with this application requires RTI Connext be installed onto the host and then mounted into the container at runtime. To mount RTI Connext into the container, ensure that the <code>NDDSHOME</code> and <code>CONNEXTDDS_ARCH</code> environment variables are set (which can be done using the RTI <code>setenv</code> script) then use the following:</p> <pre><code>./dev_container launch --img holohub:bpe --docker_opts \"-v $NDDSHOME:/opt/dds -e NDDSHOME=/opt/dds -e CONNEXTDDS_ARCH=$CONNEXTDDS_ARCH\"\n</code></pre>","tags":["Computer Vision","Human Body Pose Estimation"]},{"location":"applications/body_pose_estimation/#known-issues","title":"Known Issues","text":"<p>There is a known issue running this application on IGX w/ iGPU and on Jetson AGX (see #500). The workaround is to update the device to avoid picking up the libnvv4l2.so library.</p> <pre><code>cd /usr/lib/aarch64-linux-gnu/\nls -l libv4l2.so.0.0.999999\nsudo rm libv4l2.so.0.0.999999\nsudo ln -s libv4l2.so.0.0.0.0  libv4l2.so.0.0.999999\n</code></pre>","tags":["Computer Vision","Human Body Pose Estimation"]},{"location":"applications/colonoscopy_segmentation/","title":"Colonoscopy Polyp Segmentation","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: March 7, 2025 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>Full workflow including a generic visualization of segmentation results from a polyp segmentation models.</p>","tags":["Colonoscopy","Classification"]},{"location":"applications/colonoscopy_segmentation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8+</li> <li>The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the colonoscopy data (replayer). Follow the setup instructions from the user guide to use the AJA capture card.</li> </ul>","tags":["Colonoscopy","Classification"]},{"location":"applications/colonoscopy_segmentation/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI Colonoscopy Segmentation of Polyps</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Colonoscopy","Classification"]},{"location":"applications/colonoscopy_segmentation/#run-instructions","title":"Run Instructions","text":"<p>To run this application, you'll need to configure your PYTHONPATH environment variable to locate the necessary python libraries based on your Holoscan SDK installation type.</p> <p>You should refer to the glossary for the terms defining specific locations within HoloHub.</p> <p>If your Holoscan SDK installation type is:</p> <ul> <li>python wheels:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <ul> <li>otherwise:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOSCAN_INSTALL_DIR&gt;/python/lib:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <p>Next, run the commands of your choice:</p> <ul> <li> <p>Using a pre-recorded video     <pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;/applications/colonoscopy_segmentation\npython3 colonoscopy_segmentation.py --source=replayer --data=&lt;DATA_DIR&gt;/colonoscopy_segmentation --no-contours\n</code></pre></p> </li> <li> <p>Using an AJA card     <pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;/applications/colonoscopy_segmentation\npython3 colonoscopy_segmentation.py --source=aja --no-contours\n</code></pre> Note that segmentation contours can be shown (instead of segmentation masks) by changing <code>--no-contours</code> to <code>--contours</code>.</p> </li> </ul>","tags":["Colonoscopy","Classification"]},{"location":"applications/colonoscopy_segmentation/#holoscan-sdk-version","title":"Holoscan SDK version","text":"<p>Colonoscopy segmentation application in HoloHub requires version 0.6+ of the Holoscan SDK. If the Holoscan SDK version is 0.5 or lower, following code changes must be made in the application:</p> <ul> <li>In python/CMakeLists.txt: update the holoscan SDK version from <code>0.6</code> to <code>0.5</code></li> <li>In python/multiai_ultrasound.py: <code>InferenceOp</code> is replaced with <code>MultiAIInferenceOp</code></li> </ul>","tags":["Colonoscopy","Classification"]},{"location":"applications/colonoscopy_segmentation/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./dev_container vscode\n</code></pre>","tags":["Colonoscopy","Classification"]},{"location":"applications/colonoscopy_segmentation/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"<p>There are two launch profiles configured for this application:</p> <ol> <li>(debugpy) colonoscopy_segmentation/python: Launch colonoscopy_segmentation using a launch profile that enables debugging of Python code.</li> <li>(pythoncpp) colonoscopy_segmentation/python: Launch colonoscopy_segmentation using a launch profile that enables debugging of Python and C++ code.</li> </ol> <p>Note: the launch profile starts the application with Video Replayer. To adjust the arguments of the application, open launch.json, find the launch profile named <code>(debugpy) colonoscopy_segmentation/python</code>, and adjust the <code>args</code> field as needed.</p>","tags":["Colonoscopy","Classification"]},{"location":"applications/cuda_quantum/","title":"Hybrid-Computing Sample App - CUDA Quantum Variational Quantum Eigensolver (VQE) Application","text":"<p> Authors: Sean Huver (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 19, 2025 Language: Python Latest version: 1.0.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 4 - Experimental</p>","tags":["Quantum Computing","CUDA","VQE"]},{"location":"applications/cuda_quantum/#variational-quantum-eigensolver-vqe","title":"Variational Quantum Eigensolver (VQE)","text":"<p>The Variational Quantum Eigensolver (VQE) is a quantum algorithm designed to approximate the ground state energy of quantum systems. This energy, represented by what is called the Hamiltonian of the system, is central to multiple disciplines, including drug discovery, material science, and condensed matter physics. The goal of VQE is to find the state that minimizes the expectation value of this Hamiltonian, which corresponds to the ground state energy.</p> <p>At its core, VQE is a lighthouse example of the synergy between classical and quantum computing, requiring them both to tackle problems traditionally deemed computationally intractable. Even in the current landscape where fault-tolerant quantum computing\u2014a stage where quantum computers are resistant to errors\u2014is not yet realized, VQE is seen as a practical tool. This is due to its design as a 'near-term' algorithm, built to operate on existing noisy quantum hardware. </p>","tags":["Quantum Computing","CUDA","VQE"]},{"location":"applications/cuda_quantum/#key-components-of-vqe","title":"Key Components of VQE","text":"<ol> <li> <p>Hamiltonian: This represents the total energy of the quantum system, which is known ahead of time. In VQE, we aim to find the lowest eigenvalue (ground state energy) of this Hamiltonian.</p> </li> <li> <p>Ansatz (or trial wavefunction): The ansatz is the initial guess for the state of the quantum system, represented by a parameterized quantum circuit. It's crucial for this state to be a good representation, as the quality of the ansatz can heavily influence the final results. VQE iteratively refines the parameters of this ansatz to approximate the true ground state of the Hamiltonian.</p> </li> </ol>","tags":["Quantum Computing","CUDA","VQE"]},{"location":"applications/cuda_quantum/#vqe-mechanism","title":"VQE Mechanism","text":"<p>The VQE operates by employing a hybrid quantum-classical approach:</p> <ol> <li>Quantum Circuit Parameterization: VQE begins with a parameterized quantum circuit, effectively serving as an initial guess or representation of the system's state.</li> <li>Evaluation and Refinement: The quantum system's energy is evaluated using the current quantum circuit parameters. Classical optimization algorithms then adjust these parameters in a quest to minimize the energy.</li> <li>Iterative Process: The combination of quantum evaluation and classical refinement is iterative. Over multiple cycles, the parameters are tuned to get increasingly closer to the true ground state energy.</li> </ol>","tags":["Quantum Computing","CUDA","VQE"]},{"location":"applications/cuda_quantum/#integration-with-holoscan-and-cuda-quantum","title":"Integration with Holoscan and CUDA Quantum","text":"<ul> <li>NVIDIA Holoscan SDK: The Holoscan SDK is designed for efficient handling of high-throughput, low-latency GPU tasks. Within the context of VQE, the Holoscan SDK facilitates the rapid classical computations necessary for parameter adjustments and optimization. The <code>ClassicalComputeOp</code> in the provided code sample is an example of this SDK in action, preparing the quantum circuits efficiently.</li> <li>CUDA Quantum: CUDA Quantum is a framework that manages hybrid quantum-classical workflows. For VQE, CUDA Quantum processes quantum data and executes quantum operations. The <code>QuantumComputeOp</code> operator in the code uses the cuQuantum simulator backend, but the user may optionally switch out the simulator for a real quantum cloud backend provided by either IonQ or Quantinuum (see CUDA Quantum backend documentation).</li> </ul> <p>Holoscan ensures swift and efficient classical computations, while CUDA Quantum manages the quantum components with precision.</p>","tags":["Quantum Computing","CUDA","VQE"]},{"location":"applications/cuda_quantum/#usage","title":"Usage","text":"<p>To run the application, you need to have CUDA Quantum, Qiskit, and Holoscan installed. You also need an IBM Quantum account to use their quantum backends.</p> <ol> <li> <p>Clone the repository and navigate to the <code>cuda_quantum</code> directory containing.</p> </li> <li> <p>Install the requirements <code>pip install -r requirements.txt</code></p> </li> <li> <p>Either use or replace the <code>'hamiltonian'</code> in <code>cuda_quantum.yaml</code> dependent on the physical system you wish to model.</p> </li> <li> <p>Run the application with the command <code>python cuda_quantum.py</code>.</p> </li> </ol>","tags":["Quantum Computing","CUDA","VQE"]},{"location":"applications/cuda_quantum/#operators","title":"Operators","text":"<p>The application uses three types of operators:</p> <ul> <li> <p><code>ClassicalComputeOp</code>: This operator performs classical computations. It also creates a quantum kernel representing the initial ansatz, or guess of the state of the system, and a Hamiltonian.</p> </li> <li> <p><code>QuantumComputeOp</code>: This operator performs quantum computations. It uses the quantum kernel and Hamiltonian from <code>ClassicalComputeOp</code> to iterate towards the ground state energy and parameter using VQE.</p> </li> <li> <p><code>PrintOp</code>: This operator prints the result from <code>QuantumComputeOp</code>.</p> </li> </ul>","tags":["Quantum Computing","CUDA","VQE"]},{"location":"applications/cuda_quantum/#operator-connections","title":"Operator Connections","text":"<p>The operators are connected as follows:</p> <pre><code>flowchart LR\n    ClassicalComputeOp --&gt; QuantumComputeOp\n    QuantumComputeOp --&gt; PrintOp\n</code></pre> <p><code>ClassicalComputeOp</code> sends the quantum kernel and Hamiltonian to <code>QuantumComputeOp</code>, which computes the energy and parameter and sends the result to <code>PrintOp</code>.</p>","tags":["Quantum Computing","CUDA","VQE"]},{"location":"applications/cuda_quantum/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./dev_container vscode cuda_quantum\n</code></pre>","tags":["Quantum Computing","CUDA","VQE"]},{"location":"applications/cuda_quantum/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"<p>There are two launch profiles configured for this application:</p> <ol> <li>(debugpy) cuda_quantum/python: Launch cuda_quantum using a launch profile that enables debugging of Python code.</li> <li>(pythoncpp) cuda_quantum/python: Launch cuda_quantum using a launch profile that enables debugging of Python and C++ code.</li> </ol> <p>Note: to adjust the arguments of the application, open launch.json, find the launch profile named <code>(debugpy) cuda_quantum/python</code>, and adjust the <code>args</code> field as needed.</p>","tags":["Quantum Computing","CUDA","VQE"]},{"location":"applications/cunumeric_integration/","title":"Calculate Power Spectral Density with Holoscan and cuNumeric","text":"<p> Authors: Adam Thompson (NVIDIA) Supported platforms: amd64 Last modified: July 28, 2023 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 2 - Trusted</p> <p>cuNumeric is an drop-in replacement for NumPy that aims to provide a distributed and accelerated drop-in replacement for the NumPy API on top of the Legion runtime. It works best for programs that have very large arrays of data that can't fit in the the memory of a single GPU or node.</p> <p>In this example application, we are using the cuNumeric library within a Holoscan application graph to determine the Power Spectral Density (PSD) of an incoming signal waveform. Notably, this is simply achieved by taking the absolute value of the FFT of a data array.</p> <p>The main objectives of this demonstration are to: - Highlight developer productivity in building an end-to-end streaming application with Holoscan and cuNumeric - Demonstrate how to scale a given workload to multiple GPUs using cuNumeric</p>","tags":["Life Sciences, Aerospace, Defense, Communications"]},{"location":"applications/cunumeric_integration/#running-the-application","title":"Running the Application","text":"<p>Prior to running the application, the user needs to install the necessary dependencies. This is most easily done in an Anaconda environment.</p> <pre><code>conda create --name holoscan-cunumeric-demo python=3.9\nconda activate holoscan-cunumeric-demo\nconda install -c nvidia -c conda-forge -c legate cunumeric cupy\npip install holoscan\n</code></pre> <p>The cuNumeric PSD processing pipeline example can then be run via <pre><code>legate --gpus 2 applications/cunumeric_integration/cunumeric_psd.py\n</code></pre></p> <p>While running the application, you can confirm multi GPU utilization via watching <code>nvidia-smi</code> or using another GPU utilization tool</p> <p>To run the same application without cuNumeric, simply change <code>import cunumeric as np</code> to <code>import cupy as np</code> in the code and run <pre><code>python applications/cunumeric_integration/cunumeric_psd.py\n</code></pre></p>","tags":["Life Sciences, Aerospace, Defense, Communications"]},{"location":"applications/deltacast_transmitter/","title":"Deltacast transmitter","text":"<p> Authors: Laurent Radoux (DELTACAST) Supported platforms: amd64, arm64 Last modified: October 8, 2024 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 2 - Trusted</p> <p>This application demonstrates the use of videomaster_transmitter to transmit a video stream through a dedicated IO device.</p>","tags":["Deltacast","VideoMaster"]},{"location":"applications/deltacast_transmitter/#requirements","title":"Requirements","text":"<p>This application uses the DELTACAST.TV capture card for input stream. Contact DELTACAST.TV for more details on how access the SDK and to setup your environment.</p>","tags":["Deltacast","VideoMaster"]},{"location":"applications/deltacast_transmitter/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>See instructions from the top level README on how to build this application. Note that this application requires to provide the VideoMaster_SDK_DIR if it is not located in a default location on the system. This can be done with the following command, from the top level Holohub source directory:</p> <pre><code>./run build deltacast_transmitter --configure-args -DVideoMaster_SDK_DIR=&lt;Path to VideoMasterSDK&gt;\n</code></pre>","tags":["Deltacast","VideoMaster"]},{"location":"applications/deltacast_transmitter/#run-instructions","title":"Run Instructions","text":"<p>From the build directory, run the command:</p> <pre><code>./applications/deltacast_transmitter/deltacast_transmitter --data &lt;holohub_data_dir&gt;/endoscopy\n</code></pre>","tags":["Deltacast","VideoMaster"]},{"location":"applications/depth_anything_v2/","title":"Depth Anything V2","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: January 9, 2025 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 2.5.0 Tested Holoscan SDK versions: 2.8.0 Contribution metric: Level 2 - Trusted</p> <p>This application uses the Depth Anything V2 model for monocular depth estimation.  Monocular Depth Estimation refers to the task of predicting the distance of objects in a scene from a single 2D image captured by a standard camera.</p>","tags":["Computer Vision","Monocular Depth Estimation"]},{"location":"applications/depth_anything_v2/#model","title":"Model","text":"<p>This application uses the Depth Anything V2 model from DepthAnythingV2 for monocular depth estimation. The model is downloaded when building the Docker image.</p> <p>NOTE: The user is responsible for checking if the model license is suitable for the intended purpose.</p>","tags":["Computer Vision","Monocular Depth Estimation"]},{"location":"applications/depth_anything_v2/#data","title":"Data","text":"<p>This application downloads a pre-recorded video from Pexels when the application is built.  Please review the license terms from Pexels.</p> <p>NOTE: The user is responsible for ensuring the dataset license is suitable for the intended purpose.</p>","tags":["Computer Vision","Monocular Depth Estimation"]},{"location":"applications/depth_anything_v2/#input","title":"Input","text":"<p>This app currently supports two input options:</p> <ol> <li>v4l2 compatible input device (default; see V4L2 Support below)</li> <li>Pre-recorded video (see Video Replayer Support below)</li> </ol>","tags":["Computer Vision","Monocular Depth Estimation"]},{"location":"applications/depth_anything_v2/#run-instructions","title":"Run Instructions","text":"","tags":["Computer Vision","Monocular Depth Estimation"]},{"location":"applications/depth_anything_v2/#v4l2-support","title":"V4L2 Support","text":"<p>This application supports v4l2 compatible devices as input.  To run this application with your v4l2 compatible device, please plug in your input device and run: <pre><code>./dev_container build_and_run depth_anything_v2\n</code></pre></p> <p>By default, this application expects the input device to be mounted at <code>/dev/video0</code>.  If this is not the case, update <code>applications/depth_anything_v2/depth_anything_v2.yaml</code> file to set the corresponding input device before running the application.  You can also override the default input device on the command line by running: <pre><code>./dev_container build_and_run depth_anything_v2 --run_args \"--video_device /dev/video0\"\n</code></pre></p>","tags":["Computer Vision","Monocular Depth Estimation"]},{"location":"applications/depth_anything_v2/#video-replayer-support","title":"Video Replayer Support","text":"<p>If you don't have a v4l2 compatible device plugged in, you can also run this application on a pre-recorded video. To launch the application using the Video Stream Replayer as the input source, run:</p> <pre><code>./dev_container build_and_run depth_anything_v2 --run_args \"--source replayer\"\n</code></pre>","tags":["Computer Vision","Monocular Depth Estimation"]},{"location":"applications/depth_anything_v2/#display-modes","title":"Display Modes","text":"<p>This application has multiple display modes which you can toggle through using the left mouse button.</p> <ul> <li>original: output the original image from input source</li> <li>depth: output the color depthmap based on the depthmap returned from Depth Anything V2 model</li> <li>side-by-side: output a side-by-side view of the original image next to the color depthmap</li> <li>interactive: allow user </li> </ul> <p>In interactive mode, the middle or right mouse button can be used to modify the ratio of original image vs color depthmap is shown.</p>","tags":["Computer Vision","Monocular Depth Estimation"]},{"location":"applications/depth_anything_v2/#acknowledgement","title":"Acknowledgement","text":"<p>This project is based on the following projects: - Depth-Anything-V2 - Depth Anything V2 - depth-anything-tensorrt - Depth Anything TensorRT CLI</p>","tags":["Computer Vision","Monocular Depth Estimation"]},{"location":"applications/depth_anything_v2/#known-issues","title":"Known Issues","text":"<p>There is a known issue running this application on IGX w/ iGPU and on Jetson AGX (see #500). The workaround is to update the device to avoid picking up the libnvv4l2.so library.</p> <pre><code>cd /usr/lib/aarch64-linux-gnu/\nls -l libv4l2.so.0.0.999999\nsudo rm libv4l2.so.0.0.999999\nsudo ln -s libv4l2.so.0.0.0.0  libv4l2.so.0.0.999999\n</code></pre>","tags":["Computer Vision","Monocular Depth Estimation"]},{"location":"applications/endoscopy_depth_estimation/","title":"Endoscopy Depth Estimation","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: January 16, 2025 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 2 - Trusted</p> <p>This application demonstrates the use of custom components for depth estimation and its rendering using Holoviz with triangle interpolation.</p> <p></p>","tags":["Endoscopy","Depth","AJA"]},{"location":"applications/endoscopy_depth_estimation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8+</li> <li>OpenCV 4.8+</li> </ul>","tags":["Endoscopy","Depth","AJA"]},{"location":"applications/endoscopy_depth_estimation/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for Endoscopy</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Endoscopy","Depth","AJA"]},{"location":"applications/endoscopy_depth_estimation/#model","title":"Model","text":"<p>\ud83d\udce6\ufe0f (NGC) App Model for AI-based Endoscopy Depth Estimation</p> <p>The model is automatically downloaded to the same folder as the data in ONNX format.</p>","tags":["Endoscopy","Depth","AJA"]},{"location":"applications/endoscopy_depth_estimation/#opencv-gpu","title":"OpenCV-GPU","text":"<p>This application uses OpenCV with GPU acceleration during the preprocessing stage when it runs with Histogram Equalization (flag <code>--clahe</code> or <code>-c</code>). Histogram equalization reduces the effect of specular reflections and improves the visual performance of the depth estimation overall. However, using regular OpenCV datatypes leads to unnecessary I/O operations to transfer data from Holoscan Tensors to the CPU and back. We show in this application how to blend together Holoscan Tensors and OpenCV's <code>GPUMat</code> datatype to get rid of this issue in the <code>CUDACLAHEOp</code> operator.  Compare it to <code>CPUCLAHEOp</code> for reference.</p> <p>To achieve an end-to-end GPU accelerated pipeline / application, the pre-processing operators shall support accessing the GPU memory (Holoscan Tensor)  directly without memory copy / movement in Holoscan SDK. This means that only libraries which implement the <code>__cuda_array_interface__</code>  and DLPack standards allow conversion from/to Holoscan Tensor, such as cuCIM. OpenCV, however, does not implement neither the <code>__cuda_array_interface__</code> nor the standard DLPack, and a little work is needed yet to use this library.</p> <p>First, we convert CuPy arrays to GPUMat using a fix in OpenCV only available from 4.8.0 on. More information here. This is done in the <code>gpumat_from_cp_array</code> function. With a <code>GPUMat</code>, we can now use any OpenCV-CUDA operations. Once the <code>GPUMat</code> processing has finished, we have to convert it back to a CuPy tensor with <code>gpumat_to_cupy</code>. </p> <p>Important: In order to run this application with CUDA acceleration, one must compile OpenCV with CUDA support. We provide a sample Dockerfile to build a container based on Holoscan v2.1.0 with the latest version of OpenCV and CUDA support. In case you use it, note that the variable <code>CUDA_ARCH_BIN</code>  must be modified according to your specific GPU configuration. Refer to this site to find out your NVIDIA GPU architecture.</p>","tags":["Endoscopy","Depth","AJA"]},{"location":"applications/endoscopy_depth_estimation/#workflows","title":"Workflows","text":"<p>This application can be run with or without Histogram Equalization (CLAHE) by toggling the label <code>--clahe</code>.</p>","tags":["Endoscopy","Depth","AJA"]},{"location":"applications/endoscopy_depth_estimation/#with-clahe","title":"With CLAHE","text":"<p> Fig. 1 Depth Estimation Application with CLAHE enabled</p> <p>The pipeline uses a recorded endoscopy video file (generated by <code>convert_video_to_gxf_entities</code> script) for input frames. Each input frame in the file is loaded by Video Stream Replayer and passed to the following two branches: - In the first branch (top), the input frames are passed to the <code>CUDACLAHEOp</code>,  then fed to the Format Converter to convert their data type from <code>uint8</code> to <code>float32</code>, and finally fed to the <code>InferenceOp</code>. The result is then ingested by the <code>DepthPostProcessingOp</code>, which converts the depth map to <code>uint8</code> and reorders its dimensions for rendering with Holoviz. - In the second branch (bottom), the input frames are passed to a Format Converter that resizes them. Its output is finally fed to the <code>DepthPostProcessingOp</code> for  rendering with Holoviz.</p>","tags":["Endoscopy","Depth","AJA"]},{"location":"applications/endoscopy_depth_estimation/#without-clahe","title":"Without CLAHE","text":"<p> Fig. 2 Depth Estimation Application with CLAHE disabled</p> <p>The pipeline uses a recorded endoscopy video file (generated by <code>convert_video_to_gxf_entities</code> script) for input frames. Each input frame in the file is loaded by Video Stream Replayer and passed to a branch that firstly converts its data type to <code>float32</code> and resizes it with a Format Converter. Then, the preprocessed frames are fed to the <code>InferenceOp</code> and mixed with the original video in the custom <code>DepthPostProcessingOp</code> for rendering with Holoviz.</p>","tags":["Endoscopy","Depth","AJA"]},{"location":"applications/endoscopy_depth_estimation/#run-instructions","title":"Run Instructions","text":"<p>To run this application, you'll need to configure your PYTHONPATH environment variable to locate the necessary python libraries based on your Holoscan SDK installation type.</p> <p>You should refer to the glossary for the terms defining specific locations within HoloHub.</p> <p>If your Holoscan SDK installation type is:</p> <ul> <li>python wheels:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <ul> <li>otherwise:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOSCAN_INSTALL_DIR&gt;/python/lib:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <p>This application should be run in the build directory of Holohub in order to load the GXF extensions. Alternatively, the relative path of the extensions in the corresponding yaml file can be modified to match path of the working directory.</p> <p>Next, run the command to run the application:</p> <pre><code>cd &lt;HOLOHUB_BUILD_DIR&gt;\npython3 &lt;HOLOHUB_SOURCE_DIR&gt;/applications/endoscopy_depth_estimation/endoscopy_depth_estimation.py --data=&lt;DATA_DIR&gt; --model=&lt;MODEL_DIR&gt; --clahe\n</code></pre>","tags":["Endoscopy","Depth","AJA"]},{"location":"applications/endoscopy_depth_estimation/#container-build-run-instructions","title":"Container Build &amp; Run Instructions","text":"<p>Build container using Holoscan 2.0.0 NGC container as base image and built OpenCV with CUDA ARCH 8.6, 8.7 and 8.9 support for IGX Orin and Ampere and Ada Lovelace Architecture dGPUs. This application is currently not supported on iGPU.</p>","tags":["Endoscopy","Depth","AJA"]},{"location":"applications/endoscopy_depth_estimation/#change-directory-to-holohub-source-directory","title":"Change directory to Holohub source directory","text":"<pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;\n</code></pre>","tags":["Endoscopy","Depth","AJA"]},{"location":"applications/endoscopy_depth_estimation/#build-and-run-the-application-using-the-development-container","title":"Build and run the application using the development container","text":"<pre><code>./dev_container build_and_run endoscopy_depth_estimation\n</code></pre>","tags":["Endoscopy","Depth","AJA"]},{"location":"applications/endoscopy_depth_estimation/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./dev_container vscode endoscopy_depth_estimation\n</code></pre> <p>This command will build and configure a Dev Container using a Dockerfile that is ready to run the application.</p>","tags":["Endoscopy","Depth","AJA"]},{"location":"applications/endoscopy_depth_estimation/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"<p>There are two launch profiles configured for this application:</p> <ol> <li>(debugpy) endoscopy_depth_estimation/python: Launch endoscopy_depth_estimation using a launch profile that enables debugging of Python code.</li> <li>(pythoncpp) endoscopy_depth_estimation/python: Launch endoscopy_depth_estimation using a launch profile that enables debugging of Python and C++ code.</li> </ol>","tags":["Endoscopy","Depth","AJA"]},{"location":"applications/florence-2-vision/","title":"\ud83d\udcf7\ud83e\udd16 Florence-2","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: December 17, 2024 Language: Python Latest version: 1.0.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates how to run the Florence-2 models on a live video feed with the possibility of changing the task and optional prompt via a QT UI.</p> <p> </p> <p>Note: This demo currently uses Florence-2-large-ft, but any of the Florence-2 models should work as long as the correct URLs and names are used in Dockerfile and config.yaml: - Florence-2-large-ft - Florence-2-large - Florence-2-base-ft - Florence-2-base</p>","tags":["Multimodal Model","Vision Model"]},{"location":"applications/florence-2-vision/#setup-instructions","title":"\u2699\ufe0f Setup Instructions","text":"<p>The app defaults to using the video device at <code>/dev/video0</code></p> <p>Note: You can use a USB webcam as the video source, or an MP4 video by following the instructions for the V4L2_Camera example app.</p> <p>To debug if this is the correct device download <code>v4l2-ctl</code>: <pre><code>sudo apt-get install v4l-utils\n</code></pre> To check for your devices run: <pre><code>v4l2-ctl --list-devices\n</code></pre> This command will output something similar to this: <pre><code>NVIDIA Tegra Video Input Device (platform:tegra-camrtc-ca):\n        /dev/media0\n\nvi-output, lt6911uxc 2-0056 (platform:tegra-capture-vi:0):\n        /dev/video0\n\nDummy video device (0x0000) (platform:v4l2loopback-000):\n        /dev/video3\n</code></pre> Determine your desired video device and edit the source device in config.yaml</p>","tags":["Multimodal Model","Vision Model"]},{"location":"applications/florence-2-vision/#build-and-run-instructions","title":"\ud83d\ude80 Build and Run Instructions","text":"<p>From the Holohub main directory run the following command: <pre><code>./dev_container build_and_run florence-2-vision\n</code></pre> Note: The first build will take ~1.5 hours if you're on ARM64. This is largely due to building Flash Attention 2 since pre-built wheels are not distributed for ARM64 platforms.</p>","tags":["Multimodal Model","Vision Model"]},{"location":"applications/florence-2-vision/#supported-hardware","title":"\ud83d\udcbb Supported Hardware","text":"<ul> <li>IGX w/ dGPU</li> <li>x86 w/ dGPU</li> <li>IGX w/ iGPU and Jetson AGX supported with workaround   There is a known issue running this application on IGX w/ iGPU and on Jetson AGX (see #500).   The workaround is to update the device to avoid picking up the libnvv4l2.so library.</li> </ul> <pre><code>cd /usr/lib/aarch64-linux-gnu/\nls -l libv4l2.so.0.0.999999\nsudo rm libv4l2.so.0.0.999999\nsudo ln -s libv4l2.so.0.0.0.0  libv4l2.so.0.0.999999\n</code></pre>","tags":["Multimodal Model","Vision Model"]},{"location":"applications/florence-2-vision/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./dev_container vscode florence-2-vision\n</code></pre> <p>This command will build and configure a Dev Container using a Dockerfile that is ready to run the application.</p>","tags":["Multimodal Model","Vision Model"]},{"location":"applications/florence-2-vision/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"<p>There are two launch profiles configured for this application:</p> <ol> <li>(debugpy) florence-2-vision/python: Launch florence-2-vision using a launch profile that enables debugging of Python code.</li> <li>(pythoncpp) florence-2-vision/python: Launch florence-2-vision using a launch profile that enables debugging of Python and C++ code.</li> </ol>","tags":["Multimodal Model","Vision Model"]},{"location":"applications/fm_asr/","title":"FM ASR","text":"<p> Authors: Joshua Martinez (NVIDIA) Supported platforms: amd64 Last modified: July 28, 2023 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 0.4.1 Tested Holoscan SDK versions: 0.4.1, 0.5.0 Contribution metric: Level 3 - Developmental This project is proof-of-concept demo featuring the combination of real-time, low-level signal processing and deep learning inference. It currently supports the RTL-SDR. Specifically, this project demonstrates the demodulation, downsampling, and automatic transcription of live, civilian FM radio broadcasts. The pipeline architecture is shown in the figure below. </p> <p></p> <p>The primary pipeline segments are written in Python. Future improvements will introduce a fully C++ system.</p> <p>This project leverages NVIDIA's Holoscan SDK for performant GPU pipelines, cuSignal package for GPU-accelerated signal processing, and the RIVA SDK for high accuracy automatic speech recognition (ASR).</p>","tags":["Signal Processing","NLP","ASR","Automatic Speech Recognition"]},{"location":"applications/fm_asr/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Install <ul> <li>Local Sensor<ul> <li>x86</li> <li>Jetson - TODO</li> </ul> </li> <li>Remote Sensor - TODO<ul> <li>x86</li> <li>Jetson </li> </ul> </li> <li>Bare Metal - TODO </li> </ul> </li> <li>Startup<ul> <li>Scripted Launch</li> <li>Manual Launch</li> </ul> </li> <li>Configuration Parameters</li> <li>Known Issues</li> </ul>","tags":["Signal Processing","NLP","ASR","Automatic Speech Recognition"]},{"location":"applications/fm_asr/#install","title":"Install","text":"<p>To begin installation, clone this repository using the following: <pre><code>git clone https://github.com/nvidia-holoscan/holohub.git\n</code></pre> NVIDIA Riva is required to perform the automated transcriptions. You will need to install and configure the NGC-CLI tool, if you have not done so already, to obtain the Riva container and API. The Riva installation steps may be found at this link: Riva-Install. Note that Riva performs a TensorRT build during setup and requires access to the targeted GPU.  This project has been tested with RIVA 2.10.0.</p> <p>Container-based development and deployment is supported. The supported configurations are explained in the sections that follow. </p>","tags":["Signal Processing","NLP","ASR","Automatic Speech Recognition"]},{"location":"applications/fm_asr/#local-sensor-basic-configuration","title":"Local Sensor - Basic Configuration","text":"<p>The Local Sensor configuration assumes that the RTL-SDR is connected directly to the GPU-enabled system via USB. I/Q samples are collected from the RTL-SDR directly, using the SoapySDR library. Specialized containers are provided for Jetson devices.</p> <p>Only two containers are used in this configuration:  - The Application Container which includes all the necessary low level libraries, radio drivers, Holoscan SDK for the core application pipeline, and the Riva client API; and - The Riva SDK container that houses the ASR transcription service.</p> <p></p> <p>For convenience, container build scripts are provided to automatically build the application containers for Jetson and x86 systems. The Dockerfiles can be readily modified for ARM based systems with a discrete GPU. To build the container for this configuration, run the following: <pre><code># Starting from FM-ASR root directory\ncd scripts\n./build_application_container.sh # builds Application Container\n</code></pre> Note that this script does not build the Riva container.</p> <p>A script for running the application container is also provided. The run scripts will start the containers and leave the user at a bash terminal for development. Separate launch scripts are provided to automatically run the application. <pre><code># Starting from FM-ASR root directory\n./scripts/run_application_container.sh\n</code></pre></p>","tags":["Signal Processing","NLP","ASR","Automatic Speech Recognition"]},{"location":"applications/fm_asr/#local-jetson-container","title":"Local Jetson Container","text":"<p>Helper scripts will be provided in a future release.</p>","tags":["Signal Processing","NLP","ASR","Automatic Speech Recognition"]},{"location":"applications/fm_asr/#remote-sensor-network-in-the-loop","title":"Remote Sensor - Network in the Loop","text":"<p>This configuration is currently in work and will be provided in a future release. Developers can modify this code base to support this configuration if desired.</p>","tags":["Signal Processing","NLP","ASR","Automatic Speech Recognition"]},{"location":"applications/fm_asr/#bare-metal-install","title":"Bare Metal Install","text":"<p>Will be added in the future. Not currently supported.</p>","tags":["Signal Processing","NLP","ASR","Automatic Speech Recognition"]},{"location":"applications/fm_asr/#startup","title":"Startup","text":"<p>After installation, the following steps are needed to launch the application: 1. Start the Riva ASR service 2. Launch the Application Container</p>","tags":["Signal Processing","NLP","ASR","Automatic Speech Recognition"]},{"location":"applications/fm_asr/#scripted-launch","title":"Scripted Launch","text":"<p>The above steps are automated by some helper scripts. <pre><code># Starting from FM-ASR root directory\n./scripts/lauch_application.sh # Starts Application Container and launches app using the config file defined in the script\n</code></pre></p>","tags":["Signal Processing","NLP","ASR","Automatic Speech Recognition"]},{"location":"applications/fm_asr/#manual-launch","title":"Manual Launch","text":"<p>As an alternative to <code>launch_application.sh</code>, the FM-ASR pipeline can be run from inside the Application Container using the following commands: <pre><code>cd /workspace\nexport CONFIG_FILE=/workspace/params/holoscan.yml # can be edited by user\npython fm_asr_app.py $CONFIG_FILE\n</code></pre></p>","tags":["Signal Processing","NLP","ASR","Automatic Speech Recognition"]},{"location":"applications/fm_asr/#initialize-and-start-the-riva-service","title":"Initialize and Start the Riva Service","text":"<p>Riva can be setup following the Quickstart guide (version 2.10.0 currently supported). To summarize it, run the following: <pre><code>cd &lt;riva_quickstart_download_directory&gt;\nbash riva_init.sh\nbash riva_start.sh\n</code></pre> The initialization step will take a while to complete but only needs to be done once. Riva requires a capable GPU to setup and run properly. If your system has insufficient resources, the initialization script may hang. </p> <p>When starting the service, Riva may output a few \"retrying\" messages. This is normal and not an indication that the service is frozen. You should see a message saying <code>Riva server is ready...</code> once successful. </p> <p>Note for users with multiple GPUs:</p> <p>If you want to specify which GPU Riva uses (defaults to device 0), open and edit <code>&lt;riva_quickstart_download_directory&gt;/config.sh</code>, then change line <pre><code>gpus_to_use=\"device=0\"\n</code></pre> to <pre><code>gpus_to_use=\"device=&lt;your-device-number&gt;\"\n# or, to guarantee a specific device\ngpus_to_use=\"device=&lt;your-GPU-UUID&gt;\"\n</code></pre> You can determine your GPUs' UUIDs by running <code>nvidia-smi -L</code>.</p>","tags":["Signal Processing","NLP","ASR","Automatic Speech Recognition"]},{"location":"applications/fm_asr/#configuration-parameters","title":"Configuration Parameters","text":"<p>A table of the configuration parameters used in this project is shown below, organized by application operator.</p> Parameter Type Description run_time int Number of seconds that pipeline will execute RtlSdrGeneratorOp sample_rate float Reception sample rate used by the radio. RTL-SDR max stable sample rate without dropping is 2.56e6. tune_frequency float Tuning frequency for the radio in Hz. gain float 40.0 PlayAudioOp play_audio bool Flag used to enable simultaneous audio playback of signal. RivaAsrOp sample_rate int Audio sample rate expected by the Riva ASR model. Riva default is to 16000, other values will incurr an additional resample operation within Riva. max_alternatives int Riva - Maximum number of alternative transcripts to return (up to limit configured on server). Setting to 1 returns only the best response. word-time-offsets bool Riva - Option to output word timestamps in transcript. automatic-punctuation bool Riva - Flag that controls if transcript should be automatically punctuated. uri str localhost:50051 no-verbatim-transcripts bool Riva - If specified, text inverse normalization will be applied boosted_lm_words str Riva - words to boost when decoding. Useful for handling jargon and acronyms. boosted_lm_score float Value by which to boost words when decoding language-code str Riva - Language code of the model to be used. US English is en-US. Check Riva docs for more options interim_transcriptions bool Riva - Flag to include interim transcriptions in the output file. ssl_cert str Path to SSL client certificates file. Not currently utilized use_ssl bool Boolean to control if SSL/TLS encryption should be used. Not currently utilized. recognize_interval int Specifies the amount of data RIVA processes per request, in time (s). TranscriptSinkOp output_file str File path to store a transcript. Existing files will be overwritten.","tags":["Signal Processing","NLP","ASR","Automatic Speech Recognition"]},{"location":"applications/fm_asr/#known-issues","title":"Known Issues","text":"<p>This table will be populated as issues are identified.</p> Issue Description Status","tags":["Signal Processing","NLP","ASR","Automatic Speech Recognition"]},{"location":"applications/holochat/","title":"HoloChat","text":"<p> Authors: Nigel Nelson (NVIDIA) Supported platforms: amd64, arm64 Last modified: March 10, 2025 Language: Python Latest version: 0.2.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 4 - Experimental</p>","tags":["LLM","Vector Database","AI-Assistant"]},{"location":"applications/holochat/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Hardware Requirements</li> <li>Run Instructions</li> <li>Intended Use</li> <li>Known Limitations</li> <li>Best Practices</li> </ul> <p>HoloChat is an AI-driven chatbot, built on top of a locally hosted Code-Llama model OR a remote NIM API for Llama-3-70b, which acts as developer's copilot in Holoscan development. The LLM leverages a vector database comprised of the Holoscan SDK repository and user guide, enabling HoloChat to answer general questions about Holoscan, as well act as a Holoscan SDK coding assistant.</p> <p> </p>","tags":["LLM","Vector Database","AI-Assistant"]},{"location":"applications/holochat/#hardware-requirements","title":"Hardware Requirements: \ud83d\udc49\ud83d\udcbb","text":"<ul> <li>Processor: x86/Arm64</li> </ul> <p>If running local LLM: - GPU: NVIDIA dGPU w/ &gt;= 28 GB VRAM - Memory: &gt;= 28 GB of available disk memory   - Needed to download fine-tuned Code Llama 34B and BGE-Large embedding model</p> <p>*Tested using NVIDIA IGX Orin w/ RTX A6000 and Dell Precision 5820 Workstation w/ RTX A6000</p>","tags":["LLM","Vector Database","AI-Assistant"]},{"location":"applications/holochat/#running-holochat","title":"Running HoloChat: \ud83c\udfc3\ud83d\udca8","text":"<p>When running HoloChat, you have two LLM options: - Local: Uses Phind-CodeLlama-34B-v2 running on your local machine using Llama.cpp - Remote: Uses Llama-3-70b-Instruct using the NVIDIA NIM API</p>","tags":["LLM","Vector Database","AI-Assistant"]},{"location":"applications/holochat/#tldr","title":"TLDR; \ud83e\udd71","text":"<p>To run locally: <pre><code>./dev_container build_and_run holochat --run_args --local\n</code></pre> To run using the NVIDIA NIM API: <pre><code>echo \"NVIDIA_API_KEY=&lt;api_key_here&gt;\" &gt; ./applications/holochat/.env\n\n./dev_container build_and_run holochat\n</code></pre></p>","tags":["LLM","Vector Database","AI-Assistant"]},{"location":"applications/holochat/#build-notes","title":"Build Notes: \u2699\ufe0f","text":"<p>Build Time: - HoloChat uses a PyTorch container from NGC and may also download the ~23 GB Phind LLM from HuggingFace. As such, the first time building this application will likely take ~45 minutes depending on your internet speeds. However, this is a one-time set-up and subsequent runs of HoloChat should take seconds to launch.</p> <p>Build Location:</p> <ul> <li>If running locally: HoloChat downloads ~28 GB of model data to the <code>holochat/models</code> directory. As such, it is recommended to only run this application on a disk drive with ample storage (ex: the 500 GB SSD included with NVIDIA IGX Orin).</li> </ul>","tags":["LLM","Vector Database","AI-Assistant"]},{"location":"applications/holochat/#running-instructions","title":"Running Instructions:","text":"<p>If connecting to your machine via SSH, be sure to forward the ports 7860 &amp; 8080: <pre><code>ssh &lt;user_name&gt;@&lt;IP address&gt; -L 7860:localhost:7860 -L 8080:localhost:8080\n</code></pre></p>","tags":["LLM","Vector Database","AI-Assistant"]},{"location":"applications/holochat/#running-w-local-llm","title":"Running w/ Local LLM \ud83d\udcbb","text":"<p>To build and start the app: <pre><code>./dev_container build_and_run holochat --run_args --local\n</code></pre> Once the LLM is loaded on the GPU and the Gradio app is running, HoloChat should be available at http://127.0.0.1:7860/.</p>","tags":["LLM","Vector Database","AI-Assistant"]},{"location":"applications/holochat/#running-w-nim-api","title":"Running w/ NIM API \u2601\ufe0f","text":"<p>To use the NIM API you must create a .env file at: <pre><code>./applications/holochat/.env\n</code></pre> This is where you should place your NVIDIA API key. <pre><code>NVIDIA_API_KEY=&lt;api_key_here&gt;\n</code></pre></p> <p>To build and run the app: <pre><code>./dev_container build_and_run holochat\n</code></pre> Once the Gradio app is running, HoloChat should be available at http://127.0.0.1:7860/.</p>","tags":["LLM","Vector Database","AI-Assistant"]},{"location":"applications/holochat/#usage-notes","title":"Usage Notes: \ud83d\uddd2\ufe0f","text":"","tags":["LLM","Vector Database","AI-Assistant"]},{"location":"applications/holochat/#intended-use","title":"Intended use: \ud83c\udfaf","text":"<p>HoloChat is developed to accelerate and assist Holoscan developers\u2019 learning and development. HoloChat serves as an intuitive chat interface, enabling users to pose natural language queries related to the Holoscan SDK. Whether seeking general information about the SDK or specific coding insights, users can obtain immediate responses thanks to the underlying Large Language Model (LLM) and vector database.</p> <p>HoloChat is given access to the Holoscan SDK repository, the HoloHub repository, and the Holoscan SDK user guide. This essentially allows users to engage in natural language conversations with these documents, gaining instant access to the information they need, thus sparing them the task of sifting through vast amounts of documentation themselves.</p>","tags":["LLM","Vector Database","AI-Assistant"]},{"location":"applications/holochat/#known-limitations","title":"Known Limitations: \u26a0\ufe0f\ud83d\udea7","text":"<p>Before diving into how to make the most of HoloChat, it's crucial to understand and acknowledge its known limitations. These limitations can guide you in adopting the best practices below, which will help you navigate and mitigate these issues effectively. * Hallucinations: Occasionally, HoloChat may provide responses that are not entirely accurate. It's advisable to approach answers with a healthy degree of skepticism. * Memory Loss: LLM's limited attention window may lead to the loss of previous conversation history. To mitigate this, consider restarting the application to clear the chat history when necessary. * Limited Support for Stack Traces: HoloChat's knowledge is based on the Holoscan repository and the user guide, which lack large collections of stack trace data. Consequently, HoloChat may face challenges when assisting with stack traces.</p>","tags":["LLM","Vector Database","AI-Assistant"]},{"location":"applications/holochat/#best-practices","title":"Best Practices: \u2705\ud83d\udc4d","text":"<p>While users should be aware of the above limitations, following the recommended tips will drastically minimize these possible shortcomings. In general, the more detailed and precise a question is, the better the results will be. Some best practices when asking questions are: * Be Verbose: If you want to create an application, specify which operators should be used if possible (HolovizOp, V4L2VideoCaptureOp, InferenceOp, etc.). * Be Specific: The less open-ended a question is the less likely the model will hallucinate. * Specify Programming Language: If asking for code, include the desired language (Python or C++). * Provide Code Snippets: If debugging errors include as much relevant information as possible. Copy and paste the code snippet that produces the error, the abbreviated stack trace, and describe any changes that may have introduced the error.</p> <p>In order to demonstrate how to get the most out of HoloChat two example questions are posed below. These examples illustrate how a user can refine their questions and as a result, improve the responses they receive: </p> <p>Worst\ud83d\udc4e: \u201cCreate an app that predicts the labels associated with a video\u201d</p> <p>Better\ud83d\udc4c: \u201cCreate a Python app that takes video input and sends it through a model for inference.\u201d</p> <p>Best\ud83d\ude4c: \u201cCreate a Python Holoscan application that receives streaming video input, and passes that video input into a pytorch classification model for inference. Then, collect the model\u2019s predicted class and use Holoviz to display the class label on each video frame.\u201d</p> <p>Worst\ud83d\udc4e: \u201cWhat os can I use?\u201d</p> <p>Better\ud83d\udc4c: \u201cWhat operating system can I use with Holoscan?\u201d</p> <p>Best\ud83d\ude4c: \u201cCan I use MacOS with the Holoscan SDK?\u201d</p>","tags":["LLM","Vector Database","AI-Assistant"]},{"location":"applications/holochat/#appendix","title":"Appendix:","text":"","tags":["LLM","Vector Database","AI-Assistant"]},{"location":"applications/holochat/#meta-terms-of-use","title":"Meta Terms of Use:","text":"<p>By using the Code-Llama model, you are agreeing to the terms and conditions of the license, acceptable use policy and Meta\u2019s privacy policy.</p>","tags":["LLM","Vector Database","AI-Assistant"]},{"location":"applications/holochat/#implementation-details","title":"Implementation Details:","text":"<p>HoloChat operates by taking user input and comparing it to the text stored within the vector database, which is comprised of Holoscan SDK information. The most relevant text segments from SDK code and the user guide are then appended to the user's query. This approach allows the chosen LLM to answer questions about the Holoscan SDK, without being explicitly trained on SDK data.</p> <p>However, there is a drawback to this method - the most relevant documentation is not always found within the vector database. Since the user's question serves as the search query, queries that are too simplistic or abbreviated may fail to extract the most relevant documents from the vector database. As a consequence, the LLM will then lack the necessary context, leading to poor and potentially inaccurate responses. This occurs because LLMs strive to provide the most probable response to a question, and without adequate context, they hallucinate to fill in these knowledge gaps.</p>","tags":["LLM","Vector Database","AI-Assistant"]},{"location":"applications/hyperspectral_segmentation/","title":"Hyperspectral Image Segmentation","text":"<p> Authors: Lars Doorenbos (NVIDIA) Supported platforms: amd64, arm64 Last modified: January 16, 2025 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 2 - Trusted</p> <p></p> <p>This application segments endoscopic hyperspectral cubes into 20 organ classes. It visualizes the result together with the RGB image corresponding to the cube.</p>","tags":["Segmentation","Hyperspectral"]},{"location":"applications/hyperspectral_segmentation/#data-and-models","title":"Data and Models","text":"<p>The data is a subset of the HeiPorSPECTRAL dataset. The application loops over the 84 cubes selected. The model is the <code>2022-02-03_22-58-44_generated_default_model_comparison</code> checkpoint from this repository, converted to ONNX with the script in <code>utils/convert_to_onnx.py</code>.</p> <p>\ud83d\udce6\ufe0f (NGC) App Data and Model for Hyperspectral Segmentation.  This resource is automatically downloaded when building the application.</p>","tags":["Segmentation","Hyperspectral"]},{"location":"applications/hyperspectral_segmentation/#run-instructions","title":"Run Instructions","text":"<p>This application requires some python modules to be installed.  For simplicity, a Dockerfile is available.  To generate the container run: <pre><code>./dev_container build --docker_file ./applications/hyperspectral_segmentation/Dockerfile\n</code></pre> The application can then be built by launching this container and using the provided run script. <pre><code>./dev_container launch\n./run build hyperspectral_segmentation\n</code></pre> Once the application is built it can be launched with the run script. <pre><code>./run launch hyperspectral_segmentation\n</code></pre></p>","tags":["Segmentation","Hyperspectral"]},{"location":"applications/hyperspectral_segmentation/#viewing-results","title":"Viewing Results","text":"<p>With the default settings, the results of this application are saved to <code>result.png</code> file in the hyperspectral segmentation app directory. Each time a new image is processed, it overwrites <code>result.png</code>.  By opening this image while the application is running, you can see the results as the updates are made (may depend on your image viewer).</p>","tags":["Segmentation","Hyperspectral"]},{"location":"applications/imaging_ai_segmentator/","title":"AI Segmentation using MONAI re-trained TotalSegmentator model and CT DICOM images as input","text":"<p> Authors: Ming Qin (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 21, 2025 Language: Python Latest version: 1.0.0 Minimum Holoscan SDK version: 2.3.0 Tested Holoscan SDK versions: 2.3.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application uses MONAI re-trained TotalSegmentator model to segment 104 body parts from a DICOM series of a CT scan. It is implemented using Holohub DICOM processing operators and PyTorch inference operators.</p> <p>The input is a DICOM CT Series, and the segmentation results are saved as DICOM Segmentation in Part10 storage format, as well as in NIfTI format. The workflow is summarized below, - load DICOM studies - select series with application defined rules - convert DICOM pixel data to 3D volume image - use MONAI SDK to transform input/output and perform inference - write results as DICOM Segmentation OID instance, re-using study level metadata from the original DICOM study so that the new instance and series can be associated with the original study</p> <p>The following is the screenshot of the 3D volume rendering of the segmentation results in NIfTI format.</p> <p></p> <p>The following is the screenshot of a slice of the segmentation saved in DICOM segmentation instance (without color coding the segments).</p> <p></p>","tags":["Radiology Imaging","DICOM","DICOM SEG","Segmentation by AI","Total segmentator","MONAI","MONAI Model Zoo"]},{"location":"applications/imaging_ai_segmentator/#requirements","title":"Requirements","text":"<ul> <li>On a Holohub supported platform</li> <li>Python 3.8+</li> <li>Python packages on Pypi, including but not limited to torch, monai, nibabel, pydicom, highdicom, and others as specified in the requirements file</li> <li>Nvidia GPU with at least 14GB memory, for a 200 slice CT series</li> </ul>","tags":["Radiology Imaging","DICOM","DICOM SEG","Segmentation by AI","Total segmentator","MONAI","MONAI Model Zoo"]},{"location":"applications/imaging_ai_segmentator/#model","title":"Model","text":"<p>This application uses the MONAI whole-body segmentation model.</p>","tags":["Radiology Imaging","DICOM","DICOM SEG","Segmentation by AI","Total segmentator","MONAI","MONAI Model Zoo"]},{"location":"applications/imaging_ai_segmentator/#data","title":"Data","text":"<p>The input for this application is a folder of DICOM image files from a CT series. For testing, CT scan images can be downloaded from The Cancer Imaging Archive, subject to Data Usage Policies and Restrictions</p> <p>One such data set, a CT Abdomen series described as <code>ABD/PANC_3.0_B31f</code>, was used in testing the application. Other DICOM CT Abdomen series can be downloaded from TCIA as test inputs, and, of course, users' own DICOM seriese shall equally work.</p> <p>Note: Please download, or otherwise make available, DICOM files of a CT Abdomen series and save them in a folder, preferably named <code>data/imaging_ai_segmentator/dicom</code> under the project root, as this folder name is used in the examples in the following steps. Manual download scripts are shown in <code>Run the Application in Dev Environment</code></p>","tags":["Radiology Imaging","DICOM","DICOM SEG","Segmentation by AI","Total segmentator","MONAI","MONAI Model Zoo"]},{"location":"applications/imaging_ai_segmentator/#data-citation","title":"Data Citation","text":"<p>National Cancer Institute Clinical Proteomic Tumor Analysis Consortium (CPTAC). (2018). The Clinical Proteomic Tumor Analysis Consortium Cutaneous Melanoma Collection (CPTAC-CM) (Version 11) [Dataset]. The Cancer Imaging Archive. https://doi.org/10.7937/K9/TCIA.2018.ODU24GZE</p>","tags":["Radiology Imaging","DICOM","DICOM SEG","Segmentation by AI","Total segmentator","MONAI","MONAI Model Zoo"]},{"location":"applications/imaging_ai_segmentator/#run-instructions","title":"Run Instructions","text":"<p>There are a number of ways to build and run this application, as well as packaging this application as a Holoscan Application Package. The following sections describe each in detail.</p>","tags":["Radiology Imaging","DICOM","DICOM SEG","Segmentation by AI","Total segmentator","MONAI","MONAI Model Zoo"]},{"location":"applications/imaging_ai_segmentator/#quick-start-using-holohub-container","title":"Quick Start Using Holohub Container","text":"<p>This is the simplest and fastest way to see the application in action running as a container. The input DICOM files must first be downloaded and saved in the folder <code>$PWD/data/imaging_ai_segmentator/dicom</code>, whereas the PyTorch model is automatically downloaded when container image is built.</p> <p>Use the following to build and run the application:</p> <pre><code>mkdir -p output\nrm -rf output/*\n./dev_container build_and_run imaging_ai_segmentator --container_args \"-v $PWD/output:/var/holoscan/output -v $PWD/data/imaging_ai_segmentator/dicom:/var/holoscan/input\"\n</code></pre> <p>Once the command completes, please check the output folder for the results, e.g. <pre><code>output\n\u251c\u2500\u2500 1.2.826.0.1.3680043.10.511.3.57591117750107235783166330094310669.dcm\n\u2514\u2500\u2500 saved_images_folder\n    \u2514\u2500\u2500 1.3.6.1.4.1.14519.5.2.1.7085.2626\n        \u251c\u2500\u2500 1.3.6.1.4.1.14519.5.2.1.7085.2626.nii\n        \u2514\u2500\u2500 1.3.6.1.4.1.14519.5.2.1.7085.2626_seg.nii\n\n2 directories, 3 files\n</code></pre></p> <p>Note It takes quite a few minutes when this command is run the first time.</p>","tags":["Radiology Imaging","DICOM","DICOM SEG","Segmentation by AI","Total segmentator","MONAI","MONAI Model Zoo"]},{"location":"applications/imaging_ai_segmentator/#run-the-application-in-dev-environment","title":"Run the Application in Dev Environment","text":"<p>It is strongly recommended a Python virtual environment is used for running the application in dev environment.</p> <p>This application only has Python implementation depending on a set of Python packages from Pypi, however, a <code>build_and_install</code> step is needed to automate organizing Python code and downloading the model.</p> <p>Set up the Holohub environment, if not already done <pre><code>./run setup\n</code></pre></p> <p>Set the environment variables for the application <pre><code>source applications/imaging_ai_segmentator/env_settings.sh\n</code></pre></p> <p>If not already done, download images of a CT series from TCIA, unzip if necessary, and save the folder of DICOM files under the folder <code>$HOLOSCAN_INPUT_PATH</code>.</p> <p>Optionally download the AI model from MONAI Model Zoo, or wait till the build step to have it downloaded automatically</p> <pre><code>mkdir -p $HOLOSCAN_MODEL_PATH\npip install gdown\npython -m gdown https://drive.google.com/uc?id=1PHpFWboimEXmMSe2vBra6T8SaCMC2SHT -O $HOLOSCAN_MODEL_PATH/model.pt\n</code></pre> <p>Install Python packages required by the application <pre><code>pip install -r applications/imaging_ai_segmentator/requirements.txt\n</code></pre></p> <p>Build and install the application <pre><code>./dev_container build_and_install imaging_ai_segmentator\n</code></pre></p> <p>Run the application <pre><code>rm -f -r $HOLOSCAN_OUTPUT_PATH\npython install/imaging_ai_segmentator/app.py\n</code></pre></p> <p>Note If desired, run the application with explicitly input, output, and/or model folder path, for example <pre><code>rm -f -r ./output\npython install/imaging_ai_segmentator/app.py -m $HOLOSCAN_MODEL_PATH -i $HOLOSCAN_INPUT_PATH -o ./output\n</code></pre></p> <p>Check output <pre><code>ls $HOLOSCAN_OUTPUT_PATH\n</code></pre></p> <p>There should be a DICOM segmentation file with randomized file name. There should also a <code>saved_images_folder</code> containing folder named after the input DICOM series' instance UID, which in turn contains the input and segmentation image files in NIfTI format, e.g. <pre><code>applications/imaging_ai_segmentator/output\n\u251c\u2500\u2500 1.2.826.0.1.3680043.10.511.3.64271669147396658491950188504278234.dcm\n\u2514\u2500\u2500 saved_images_folder\n    \u2514\u2500\u2500 1.3.6.1.4.1.14519.5.2.1.7085.2626\n        \u251c\u2500\u2500 1.3.6.1.4.1.14519.5.2.1.7085.2626.nii\n        \u2514\u2500\u2500 1.3.6.1.4.1.14519.5.2.1.7085.2626_seg.nii\n</code></pre></p>","tags":["Radiology Imaging","DICOM","DICOM SEG","Segmentation by AI","Total segmentator","MONAI","MONAI Model Zoo"]},{"location":"applications/imaging_ai_segmentator/#run-the-application-in-dev-container","title":"Run the Application in Dev Container","text":"<p>In this mode, there is no need to <code>build</code> and <code>install</code>. The Python code will run in its source folders, and both the model and input DICOM files need to be downloaded manually with the scripts provided below.</p> <p>Also, the <code>PYTHONPATH</code> environment variable must be set to locate the necessary Holohub medical imaging operators. The AI model and input DICOM file paths need defined via environment variables, namely <code>HOLOSCAN_MODEL_PATH</code> and <code>HOLOSCAN_INPUT_PATH</code> respectively, otherwise they must be provided explicitly as command options.</p> <p>First Build and launch the Holohub Container, landing in <code>/workspace/holohub</code></p> <p>Set the <code>PYTHONPATH</code> to include the Holohub source folder <pre><code>export PYTHONPATH=$PYTHONPATH:$PWD\n</code></pre></p> <p>Set the environment variables for the application <pre><code>source applications/imaging_ai_segmentator/env_settings.sh\n</code></pre></p> <p>If not already done, download images of a CT series from TCIA,  unzip if necessary, and save the folder of DICOM files under the folder <code>$HOLOSCAN_INPUT_PATH</code>.</p> <p>Optionally download the AI model from MONAI Model Zoo, or wait till the build step to have it downloaded automatically</p> <pre><code>mkdir -p $HOLOSCAN_MODEL_PATH\npip install gdown\npython -m gdown https://drive.google.com/uc?id=1PHpFWboimEXmMSe2vBra6T8SaCMC2SHT -O $HOLOSCAN_MODEL_PATH/model.pt\n</code></pre> <p>Install Python packages required by the application <pre><code>pip install -r applications/imaging_ai_segmentator/requirements.txt\n</code></pre> Run the application <pre><code>rm -f -r $HOLOSCAN_OUTPUT_PATH\npython applications/imaging_ai_segmentator/\n</code></pre></p> <p>Check output <pre><code>ls $HOLOSCAN_OUTPUT_PATH\n</code></pre></p>","tags":["Radiology Imaging","DICOM","DICOM SEG","Segmentation by AI","Total segmentator","MONAI","MONAI Model Zoo"]},{"location":"applications/imaging_ai_segmentator/#packaging-the-application-for-distribution","title":"Packaging the Application for Distribution","text":"<p>With Holoscan CLI, an applications built with Holoscan SDK can be packaged into a Holoscan Application Package (HAP), which is essentially a Open Container Initiative compliant container image. An HAP is well suited to be distributed for deployment on hosting platforms, be a Docker Compose, Kubernetes, or else. Please refer to Packaging Holoscan Applications in the User Guide for more information.</p> <p>This example application provides all the necessary contents for HAP packaging, and the specific commands are revealed by the specific commands.</p> <p>Note</p> <p>The prerequisite is that the application <code>build_and_install</code> has been performed to stage the source and AI model files for packaging.</p> <pre><code>source applications/imaging_ai_segmentator/packageHAP.sh\n</code></pre>","tags":["Radiology Imaging","DICOM","DICOM SEG","Segmentation by AI","Total segmentator","MONAI","MONAI Model Zoo"]},{"location":"applications/monai_endoscopic_tool_seg/","title":"Endoscopic Tool Segmentation from MONAI Model Zoo","text":"<p> Authors: Jin Li (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 5, 2025 Language: Python Latest version: 1.1 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 2 - Trusted This endoscopy tool segmentation application runs the MONAI Endoscopic Tool Segmentation from MONAI Model Zoo.</p> <p>This HoloHub application has been verified on the GI Genius sandbox and is currently deployable to GI Genius Intelligent Endoscopy Modules. GI Genius is Cosmo Intelligent Medical Devices\u2019 AI-powered endoscopy system. This implementation by Cosmo Intelligent Medical Devices showcases the fast and seamless deployment of HoloHub applications on products/platforms running on NVIDIA Holoscan.</p>","tags":["MONAI","Endoscopy","Segmentation"]},{"location":"applications/monai_endoscopic_tool_seg/#model","title":"Model","text":"<p>We will be deploying the endoscopic tool segmentation model from MONAI Model Zoo.  Note that you could also use the MONAI model zoo repo for training your own semantic segmentation model with your own data, but here we are directly deploying the downloaded MONAI model checkpoint into Holoscan. </p>","tags":["MONAI","Endoscopy","Segmentation"]},{"location":"applications/monai_endoscopic_tool_seg/#model-conversion-to-onnx","title":"Model conversion to ONNX","text":"<p>Before deploying the MONAI Model Zoo's trained model checkpoint in Holoscan SDK, we convert the model checkpoint into ONNX.  You can choose to  - download the MONAI Endoscopic Tool Segmentation Model on NGC directly and skip the rest of this Model section, or  - go through the following conversion steps yourself. </p> <ol> <li>Download the PyTorch model checkpoint linked in the README of endoscopic tool segmentation. We will assume its name to be <code>model.pt</code>.</li> <li> <p>Clone the MONAI Model Zoo repo.  <pre><code>cd [your-workspace]\ngit clone https://github.com/Project-MONAI/model-zoo.git\n</code></pre> and place the downloaded PyTorch model into <code>model-zoo/models/endoscopic_tool_segmentation/</code>.</p> </li> <li> <p>Pull and run the docker image for MONAI. We will use this docker image for converting the PyTorch model to ONNX.  <pre><code>docker pull projectmonai/monai\ndocker run -it --rm --gpus all -v [your-workspace]/model-zoo:/workspace/model-zoo -w /workspace/model-zoo/models/endoscopic_tool_segmentation/ projectmonai/monai\n</code></pre></p> </li> <li>Install onnxruntime within the container  <code>pip install onnxruntime onnx-graphsurgeon</code></li> <li>Convert model</li> </ol> <p>We will first export the model.pt file to ONNX by using the export_to_onnx.py file. Modify the backbone in line 122 to be efficientnet-b2: <pre><code>model = load_model_and_export(modelname, outname, out_channels, height, width, multigpu, backbone=\"efficientnet-b2\")\n</code></pre> Note that the model in the Model Zoo here was trained to have only two output channels: label 1 = tools, label 0 = everything else, but the same Model Zoo repo can be repurposed to train a model with a different dataset that has more than two classes. <pre><code>python scripts/export_to_onnx.py --model model.pt --outpath model_endoscopic_tool_seg.onnx --width 736 --height 480 --out_channels 2\n</code></pre> Fold constants in the ONNX model. <pre><code>polygraphy surgeon sanitize --fold-constants model_endoscopic_tool_seg.onnx -o model_endoscopic_tool_seg_sanitized.onnx\n</code></pre> Finally, modify the input and output channels to have shape [n, height, width, channels], [n, channels, height, width].  <pre><code>python scripts/graph_surgeon_tool_seg.py --orig_model model_endoscopic_tool_seg_sanitized.onnx --new_model model_endoscopic_tool_seg_sanitized_nhwc_in_nchw_out.onnx\n</code></pre></p>","tags":["MONAI","Endoscopy","Segmentation"]},{"location":"applications/monai_endoscopic_tool_seg/#data","title":"Data","text":"<p>For this application we will use the same Endoscopy Sample Data as the Holoscan SDK reference applications.</p>","tags":["MONAI","Endoscopy","Segmentation"]},{"location":"applications/monai_endoscopic_tool_seg/#requirements","title":"Requirements","text":"<p>The only requirement is to make sure the model and data are accessible by the application. At runtime we will need to specify via the <code>--data</code> arg, assuming the directory specified contains two subdirectories <code>endoscopy/</code> (endoscopy video data directory) and <code>monai_tool_seg_model/</code> (model directory).</p>","tags":["MONAI","Endoscopy","Segmentation"]},{"location":"applications/monai_endoscopic_tool_seg/#running-the-application","title":"Running the application","text":"<p>To run this application, you'll need to configure your PYTHONPATH environment variable to locate the necessary python libraries based on your Holoscan SDK installation type.</p> <p>You should refer to the glossary for the terms defining specific locations within HoloHub.</p> <p>If your Holoscan SDK installation type is:</p> <ul> <li>python wheels:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <ul> <li>otherwise:</li> </ul> <p><pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOSCAN_INSTALL_DIR&gt;/python/lib:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> Next, run the application, where  is a directory that contains two subdirectories <code>endoscopy/</code> and <code>monai_tool_seg_model/</code>.: <p><pre><code>python3 tool_segmentation.py --data &lt;DATA_DIR&gt;\n</code></pre> If you'd like the application to run at the input framerate, change the <code>replayer</code> config in the yaml file to <code>realtime: true</code>.</p>","tags":["MONAI","Endoscopy","Segmentation"]},{"location":"applications/network_radar_pipeline/","title":"Network Radar Pipeline","text":"<p> Authors: Dylan Eustice (NVIDIA) Supported platforms: amd64, arm64 Last modified: March 14, 2025 Language: C++ Latest version: 1.1 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 2 - Trusted The Network Radar Pipeline demonstrates signal processing on data streamed via packets over a network. It showcases the use of both the Advanced Network Operator and Basic Network Operator to send or receive data, combined with the signal processing operators implemented in the Simple Radar Pipeline application.</p> <p>Using the GPUDirect capabilities afforded by the Advanced Network Operator, this pipeline has been tested up to 100 Gbps (Tx/Rx) using a ConnectX-7 NIC and A30 GPU.</p> <p>The motivation for building this application is to demonstrate how data arrays can be assembled from packet data in real-time for low-latency, high-throughput sensor processing applications. The main components of this work are defining a message format and writing code connecting the network operators to the signal processing operators.</p> <p>This application supports the Advanced Network Operator DPDK and DOCA GPUNetIO transport layers.</p>","tags":["Networking","Network","UDP","IP","Signal Processing","RADAR"]},{"location":"applications/network_radar_pipeline/#prerequisites","title":"Prerequisites","text":"<p>See the README for the Advanced Network Operator for requirements and system tuning needed to enable high-throughput GPUDirect capabilities.</p>","tags":["Networking","Network","UDP","IP","Signal Processing","RADAR"]},{"location":"applications/network_radar_pipeline/#environment","title":"Environment","text":"<p>Note: Dockerfile should be cross-compatible, but has only been tested on x86. Needs to be edited if different versions / architectures are required.</p>","tags":["Networking","Network","UDP","IP","Signal Processing","RADAR"]},{"location":"applications/network_radar_pipeline/#build","title":"Build","text":"<p>Please refer to the top level Holohub README.md file for information on how to build this application: <code>./run build network_radar_pipeline</code>.</p>","tags":["Networking","Network","UDP","IP","Signal Processing","RADAR"]},{"location":"applications/network_radar_pipeline/#run","title":"Run","text":"<p>Note: must properly configure YAML files before running. To run with DPDK as ANO transport layer: - On Tx machine: <code>./build/applications/network_radar_pipeline/cpp/network_radar_pipeline source.yaml</code> - On Rx machine: <code>./build/applications/network_radar_pipeline/cpp/network_radar_pipeline process.yaml</code></p> <p>To run with DOCA GPUNetIO as ANO transport layer: - On Tx machine: <code>./build/applications/network_radar_pipeline/cpp/network_radar_pipeline source_doca.yaml</code> - On Rx machine: <code>./build/applications/network_radar_pipeline/cpp/network_radar_pipeline process_doca.yaml</code></p> <p>For Holoscan internal reasons (not related to the DOCA library), build the Advanced Network Operator with <code>RX_PERSISTENT_ENABLED</code> set to 1 MAY cause problems to this application on the receive (process) side (receive hangs in process.cu file). If you experience any issue on the receive side, please read carefully in the Advanced Network Operator README about how to solve this problem.</p>","tags":["Networking","Network","UDP","IP","Signal Processing","RADAR"]},{"location":"applications/network_radar_pipeline/#network-operator-connectors","title":"Network Operator Connectors","text":"<p>See each operators' README before using / for more detailed information.</p>","tags":["Networking","Network","UDP","IP","Signal Processing","RADAR"]},{"location":"applications/network_radar_pipeline/#basic-network-operator-connector","title":"Basic Network Operator Connector","text":"<p>Implementation in <code>basic_network_connectors</code>. Only supports CPU packet receipt / transmit. Uses cudaMemcpy to move data between network operator and MatX tensors.</p>","tags":["Networking","Network","UDP","IP","Signal Processing","RADAR"]},{"location":"applications/network_radar_pipeline/#advanced-network-operator-connector","title":"Advanced Network Operator Connector","text":"<p>Implementation in <code>advanced_network_connectors</code>. RX connector is only configured to run with GPUDirect enabled, in header-data split (HDS) mode. TX connector supports both GPUDirect/HDS or CPU-only.</p>","tags":["Networking","Network","UDP","IP","Signal Processing","RADAR"]},{"location":"applications/network_radar_pipeline/#testing-rx-on-generic-packet-data","title":"Testing RX on generic packet data","text":"<p>When using the Advanced network operator, the application supports testing the radar processing component in a \"spoof packets\" mode. This functionality allows for easier benchmarking of the application by ingesting generic packet data and writing in header fields such that the full radar pipeline will still be exercised. When \"SPOOF_PACKET_DATA\" (adv_networking_rx.h) is set to \"true\", the index of the packet will be used to set fields appropriately. This functionality is currently unsupported using the basic network operator connectors.</p>","tags":["Networking","Network","UDP","IP","Signal Processing","RADAR"]},{"location":"applications/network_radar_pipeline/#message-format","title":"Message format","text":"<p>The message format is defined by <code>RFPacket</code>. It is a byte array, represented by <code>RFPacket::payload</code>, where the first 16 bytes are reserved for metadata and the rest are used for representing complex I/Q samples. The metadata is: - Sample index: The starting index for a single pulse/channel of the transmitted samples (2 bytes) - Waveform ID: Index of the transmitted waveform (2 bytes) - Channel index: Index of the channel (2 bytes) - Pulse index: Index of the pulse (2 bytes) - Number samples: Number of I/Q samples transmitted (2 bytes) - End of array: Boolean - true if this is the last message for the waveform (2 bytes)</p>","tags":["Networking","Network","UDP","IP","Signal Processing","RADAR"]},{"location":"applications/object_detection_torch/","title":"Object Detection Application","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: March 7, 2025 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application performs object detection using frcnn resnet50 model from torchvision. The inference is executed using <code>torch</code> backend in <code>holoinfer</code> module in Holoscan SDK.</p> <p><code>object_detection_torch.yaml</code> is the configuration file. Input video file is converted into GXF tensors and the name and location of the GXF tensors are updated in the <code>basename</code> and the <code>directory</code> field in <code>replayer</code>.</p> <p>This application need <code>Libtorch</code> for inferencing. Ensure that the Holoscan SDK is build with <code>build_libtorch</code> flag as true. If not, then rebuild the SDK with following: <code>./run build --build_libtorch true</code> before running this application.</p>","tags":["Object detection"]},{"location":"applications/object_detection_torch/#data","title":"Data","text":"<p>To run this application, you will need the following:</p> <ul> <li>Model name: frcnn_resnet50_t.pt<ul> <li>The model should be converted to torchscript format.  The original pytorch model can be downloaded from pytorch model. <code>frcnn_resnet50_t.pt</code> is used</li> </ul> </li> <li>Model configuration file: frcnn_resnet50_t.yaml<ul> <li>Model config documents input and output nodes, their dimensions and respective datatype.</li> </ul> </li> <li>Labels file: labels.txt<ul> <li>Labels for identified objects.</li> </ul> </li> <li>Postprocessor configuration file: postprocessing.yaml<ul> <li>This configuration stores the number and type of objects to be identified. By default, the application detects and generates bounding boxes for <code>car</code> (max 50), <code>person</code> (max 50), <code>motorcycle</code> (max 10) in the input frame. All remaining identified objects are tagged with label <code>object</code> (max 50).</li> <li>Additionally, color of the bounding box for each identified object can be set.</li> <li>Threshold of scores can be set in the <code>params</code>. Default value is 0.75.</li> </ul> </li> </ul> <p>Sample dataset can be any video file freely available for testing on the web. E.g. Traffic video</p> <p>Once the video is downloaded, it must be converted into GXF entities. As shown in the command below, width and height is set to 1920x1080 by default. To reduce the size of generated tensors a lower resolution can be used. Generated entities must be saved at /object_detection_torch folder. <pre><code>ffmpeg -i &lt;downloaded_video&gt; -pix_fmt rgb24 -f rawvideo pipe:1 | python utilities/convert_video_to_gxf_entities.py --width 1920 --height 1080 --channels 3 --framerate 30\n</code></pre> <p>If resolution is updated in entity generation, it must be updated in the following config files as well: /object_detection_torch/frcnn_resnet50_t.yaml /object_detection_torch/postprocessing.yaml","tags":["Object detection"]},{"location":"applications/object_detection_torch/#quick-start","title":"Quick start","text":"<p>If you want to quickly run this application, you can use the <code>./dev_container build_and_run</code> command.</p> <pre><code>./dev_container build_and_run object_detection_torch\n</code></pre> <p>Otherwise, you can build and run the application using the commands below.</p>","tags":["Object detection"]},{"location":"applications/object_detection_torch/#building-the-application","title":"Building the application","text":"<p>The best way to run this application is inside the container, as it would provide all the required third-party packages:</p> <pre><code># Create the container image for this application\n./dev_container build --docker_file applications/object_detection_torch/Dockerfile --img object_detection_torch\n# Launch the container\n./dev_container launch --img object_detection_torch\n# Build the application. Note that this downloads the video data as well\n./run build object_detection_torch\n# Generate the pytorch model\npython3 applications/object_detection_torch/generate_resnet_model.py  data/object_detection_torch/frcnn_resnet50_t.pt\n# Run the application\n./run launch object_detection_torch\n</code></pre> <p>Please refer to the top level Holohub README.md file for more information on how to build this application.</p>","tags":["Object detection"]},{"location":"applications/object_detection_torch/#running-the-application","title":"Running the application","text":"<pre><code># ensure the current working directory contains the &lt;data_dir&gt;.\n&lt;build_dir&gt;/object_detection_torch\n</code></pre> <p>If application is executed from within the holoscan sdk container and is not able to find <code>libtorch.so</code>, update <code>LD_LIBRARY_PATH</code> as below:</p> <pre><code>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/libtorch/1.13.1/lib\n</code></pre> <p>On aarch64, if application is executed from within the holoscan sdk container and libtorch throws linking errors, update the <code>LD_LIBRARY_PATH</code> as below:</p> <pre><code>export LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:/opt/hpcx/ompi/lib\"\n</code></pre>","tags":["Object detection"]},{"location":"applications/object_detection_torch/#containerize-the-application","title":"Containerize the application","text":"<p>To containerize the application using Holoscan CLI, first build the application using <code>./dev_container build_and_install object_detection_torch</code>, run the <code>package-app.sh</code> script and then follow the generated output to package and run the application.</p> <p>Refer to the Packaging Holoscan Applications section of the Holoscan User Guide to learn more about installing the Holoscan CLI or packaging your application using Holoscan CLI.</p>","tags":["Object detection"]},{"location":"applications/orthorectification_with_optix/","title":"HoloHub Orthorectification Application","text":"<p> Authors: Brent Bartlett (NVIDIA) Supported platforms: amd64 Last modified: September 19, 2023 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 4 - Experimental</p> <p>This application is an example of utilizing the nvidia OptiX SDK via the PyOptix bindings to create per-frame orthorectified imagery. In this example, one can create a visualization of mapping frames from a drone mapping mission processed with Open Drone Map. A typical output of a mapping mission is a single merged mosaic. While this product is useful for GIS applications, it is difficult to apply algorithms on a such a large single image without incurring additional steps like image chipping. Additionally, the mosaic process introduces image artifacts which can negativley impact algorithm performance. </p> <p>Since this holoscan pipeline processes each frame individually, it opens the door for one to apply an algorithm to the original un-modififed imagery and then map the result. If custom image processing is desired, it is recommended to insert custom operators before the Ray Trace Ortho operator in the application flow. </p> <p> Fig. 1 Orthorectification sample application workflow</p> <p>Steps for running the application:</p> <p>a) Download and Prep the ODM Dataset 1. Download the Lafayette Square Dataset and place into ~/Data.</p> <ol> <li>Process the dataset with ODM via docker command:  <code>docker run -ti --rm -v ~/Data/lafayette_square:/datasets/code opendronemap/odm --project-path /datasets --camera-lens perspective --dsm</code></li> </ol> <p>If you run out of memory add the following argument to preserve some memory: <code>--feature-quality medium</code></p> <p>b) Clone holohub and navigate to this application directory</p> <p>c) Download OptiX SDK 7.4.0 and extract the package in the same directory as the source code (i.e. applications/orthorectification_with_optix).</p> <p>d) Build development container  1. <code>DOCKER_BUILDKIT=1 docker build -t holohub-ortho-optix:latest .</code></p> <p>You can now run the docker container by:  1. <code>xhost +local:docker</code> 2. <code>nvidia_icd_json=$(find /usr/share /etc -path '*/vulkan/icd.d/nvidia_icd.json' -type f 2&gt;/dev/null | grep .) || (echo \"nvidia_icd.json not found\" &gt;&amp;2 &amp;&amp; false)</code> 3. <code>docker run -it --rm --net host --runtime=nvidia -v ~/Data:/root/Data  -v .:/work/ -v /tmp/.X11-unix:/tmp/.X11-unix  -v $nvidia_icd_json:$nvidia_icd_json:ro  -e NVIDIA_DRIVER_CAPABILITIES=graphics,video,compute,utility,display -e DISPLAY=$DISPLAY  holohub-ortho-optix</code></p> <p>Finish prepping the input data:  1. <code>gdal_translate -tr 0.25 0.25 -r cubic ~/Data/lafayette_square/odm_dem/dsm.tif ~/Data/lafayette_square/odm_dem/dsm_small.tif</code> 2. <code>gdal_fillnodata.py -md 0 ~/Data/lafayette_square/odm_dem/dsm_small.tif ~/Data/lafayette_square/odm_dem/dsm_small_filled.tif</code></p> <p>Finally run the application:  1. <code>python ./python/ortho_with_pyoptix.py</code></p> <p>You can modify the applications settings in the file \"ortho_with_pyoptix.py\" </p> <pre><code>sensor_resize = 0.25 # resizes the raw sensor pixels\nncpu = 8 # how many cores to use to load sensor simulation\ngsd = 0.25 # controls how many pixels are in the rendering\niterations = 425 # how many frames to render from the source images (in this case 425 is max)\nuse_mosaic_bbox = True # render to a static bounds on the ground as defined by the DEM\nwrite_geotiff = False \nnb=3 # how many bands to write to the GeoTiff\nrender_scale = 0.5 # scale the holoview window up or down\nfps = 8.0 # rate limit the simulated sensor feed to this many frames per second\n</code></pre> <p> Fig. 2 Running the orthorectification sample application</p>","tags":["Orthorectification","Drone","OptiX"]},{"location":"applications/psd_pipeline/","title":"PSD pipeline","text":"","tags":["Sensor","Processing","Utilities","PSD","FFT","SDR","IQ data"]},{"location":"applications/psd_pipeline/#psd-pipeline","title":"PSD Pipeline","text":"","tags":["Sensor","Processing","Utilities","PSD","FFT","SDR","IQ data"]},{"location":"applications/psd_pipeline/#overview","title":"Overview","text":"<p>The PSD pipeline takes in a VITA49 data stream from the advanced network operator, then performs an FFT, PSD, and averaging operation before generating a VITA 49.2 spectral data packet which gets sent to a destination UDP socket.</p> <p></p>","tags":["Sensor","Processing","Utilities","PSD","FFT","SDR","IQ data"]},{"location":"applications/psd_pipeline/#acronyms","title":"Acronyms","text":"Acronym Meaning FFT Fast Fourier Transform NIC Network Interface Card PSD Power Spectral Display VITA 49 Standard for interoperability between RF (radio frequency) devices VRT VITA Radio Tansport (transport-layer protocol)","tags":["Sensor","Processing","Utilities","PSD","FFT","SDR","IQ data"]},{"location":"applications/psd_pipeline/#requirements","title":"Requirements","text":"<ul> <li>ConnectX 6 or 7 NIC for GPUDirect RDMA with packet size steering</li> <li>MatX (dependency)</li> <li>vrtgen (dependency)</li> </ul>","tags":["Sensor","Processing","Utilities","PSD","FFT","SDR","IQ data"]},{"location":"applications/psd_pipeline/#configuration","title":"Configuration","text":"<p>[!IMPORTANT] The settings in <code>config.yaml</code> need to be tailored to your system/radio.</p> <p>Each operator in the pipeline has its own configuration section. The specific options and their meaning are defined in each operator's own README:</p> <ol> <li><code>advanced_network</code></li> <li><code>vita_connector</code></li> <li><code>fft</code></li> <li><code>high_rate_psd</code></li> <li><code>low_rate_psd</code></li> <li><code>vita49_psd_packetizer</code></li> </ol> <p>There is also one option specific to this application:</p> <ol> <li><code>num_psds</code>: Number of PSDs to produce out of the pipeline before exiting.                Passing <code>-1</code> here will cause the pipeline to run indefinitely.</li> </ol>","tags":["Sensor","Processing","Utilities","PSD","FFT","SDR","IQ data"]},{"location":"applications/psd_pipeline/#metadata","title":"Metadata","text":"<p>This pipeline leverages Holoscan's operator metadata dictionaries to pass VITA 49-adjacent metadata through the pipeline.</p> <p>Each operator in the pipeline adds its own metadata to the dictionary. At the end of the pipeline, the packetizer operator uses the metadata to construct VITA 49 context packets to send alongside the spectral data.</p>","tags":["Sensor","Processing","Utilities","PSD","FFT","SDR","IQ data"]},{"location":"applications/psd_pipeline/#memory-layout","title":"Memory Layout","text":"<p>The ANO operates using memory regions that it directs data to. Since VITA49 is somewhat unusual in that signal data packets and context packets arrive at the same IP/port, we want to use the ANO's packet length steering feature to drop packets in the appropriate memory region.</p> <p>First, we want to define our memory regions:</p> <ol> <li>A region for any packets that don't match any of our flows [CPU]</li> <li>A region for frame headers (i.e. Ethernet + IP + UDP) [CPU]</li> <li>These headers are not currently used, so this memory region is      essentially acting as a <code>/dev/null</code> sink.</li> <li>A region for each channel's VRT headers [CPU]</li> <li>We need these headers to grab things like stream ID and      timestamp, but don't need that information in the GPU processing,      so make this a CPU region.</li> <li>A region for each channel's VRT signal data [GPU]</li> <li>These are the raw IQ samples from our radio - we want these to      land in GPU memory via GPUDirect RDMA.</li> <li>A region for all channels' VRT context data [CPU]</li> <li>We need the whole packet in the CPU to fill out our metadata      map for downstream processing/packet assembly.</li> </ol> <p>When an individual packet comes in, the ANO will try to match it against the defined flows. So, for our data packets, we want to define a queue like this:</p> <pre><code>            flows:\n              - name: \"Data packets\"\n                id: 0\n                action:\n                  type: queue\n                  id: 1\n                match:\n                  # Match with the port your SDR is sending to and the\n                  # length of the signal data packets\n                  udp_src: 4991\n                  udp_dst: 4991\n                  ipv4_len: 4148\n</code></pre> <p>This is saying \"if a UDP packet with IPv4 length 4,148 comes in on port 4991, send it to the queue with ID 1\". Now, if we look at our queue with ID 1, we see:</p> <pre><code>              - name: \"Data\"\n                id: 1\n                cpu_core: 5\n                batch_size: 12500\n                output_port: \"bench_rx_out\"\n                memory_regions:\n                  - \"Headers_RX_CPU\"\n                  - \"VRT_Headers_RX_CPU\"\n                  - \"Data_RX_GPU\"\n</code></pre> <p>When multiple <code>memory_regions</code> are specified like this, it means that each packet should be split based on the memory region size. In this case, <code>Headers_RX_CPU</code> has <code>buf_size: 42</code> (the size of the frame header), <code>VRT_Headers_RX_CPU</code> has <code>buf_size: 20</code> (the size of the VRT header), and <code>Data_RX_GPU</code> has <code>buf_size: 4100</code> (the remaining size of the data packet). These numbers may be different depending on the packet size of your radio!</p> <p><code>batch_size: 12500</code> tells the ANO to batch up 12,500 packets before sending the data to downstream operators. In our case, 12,500 packets represents 100ms worth of data and that's how much we want to process on each run of the pipeline.</p>","tags":["Sensor","Processing","Utilities","PSD","FFT","SDR","IQ data"]},{"location":"applications/psd_pipeline/#multiple-channels","title":"Multiple Channels","text":"<p>When working with multiple channels, this pipeline expects all context packets (from every channel) to flow to one queue, but each data channel flows to its own queue.</p> <p>The connector operator also makes the following assumptions:</p> <ol> <li>All context packets flow to queue <code>id: 0</code>.</li> <li>All context packets flow ID matches its channel (e.g., flow ID <code>1</code>    is for context packets from channel <code>1</code>).</li> <li>All data packets arrive on a queue ID one greater than its channel    (e.g., queue ID <code>1</code> is for channel <code>0</code> data).</li> <li>The <code>batch_size</code> of the context queue is equal to the number of    channels.</li> </ol>","tags":["Sensor","Processing","Utilities","PSD","FFT","SDR","IQ data"]},{"location":"applications/psd_pipeline/#ingest-nic","title":"Ingest NIC","text":"<p>The PCIe address of your ingest NIC needs to be specified in <code>config.yaml</code>.</p> <pre><code>    interfaces:\n      - name: sdr_data\n        address: 0000:17:00.0\n</code></pre> <p>You can find the addresses of your devices with: <code>lshw -c network -businfo</code>:</p> <pre><code># lshw -c network -businfo\nBus info          Device     Class          Description\n=======================================================\npci@0000:03:00.0  eth0       network        I210 Gigabit Network Connection\npci@0000:06:00.0  eno1       network        Aquantia Corp.\npci@0000:51:00.0  ens3f0np0  network        MT2910 Family [ConnectX-7]\npci@0000:51:00.1  ens3f1np1  network        MT2910 Family [ConnectX-7]\nusb@1:14.2        usb0       network        Ethernet interface\n</code></pre> <p>In this example, if you wanted to use the <code>ens3f1np1</code> interface, you'd pass <code>0000:51:00.1</code>.</p>","tags":["Sensor","Processing","Utilities","PSD","FFT","SDR","IQ data"]},{"location":"applications/psd_pipeline/#build-run","title":"Build &amp; Run","text":"<ol> <li>Build the development container from the ANO operator's directory:    <pre><code>./dev_container build --docker_file ./operators/advanced_network/Dockerfile\n</code></pre></li> <li>Launch the development container with the command:    <pre><code>./dev_container launch --as_root --img docker.io/library/holohub:ngc-v2.9.0-dgpu --docker_opts \"--privileged\"\n</code></pre></li> </ol> <p>Once you are in the dev container: 1. Build the application using:     <pre><code>./run build psd_pipeline\n</code></pre> 2. Run the application using:     <pre><code>./run launch psd_pipeline --extra_args config.yaml\n</code></pre></p>","tags":["Sensor","Processing","Utilities","PSD","FFT","SDR","IQ data"]},{"location":"applications/pva_video_filter/","title":"PVA-Accelerated Image Sharpening Application","text":"<p> Authors: Soham Sinha (NVIDIA), Mehmet Umut Demircin (NVIDIA), Wendell Hom (NVIDIA) Supported platforms: arm64 Last modified: August 2, 2024 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates the usage of Programmable Vision Accelerator (PVA) within a Holoscan application. It reads a video stream, applies a 2D unsharp mask filter and renders it via the visualizer. The unsharp mask filtering operation is done in PVA. Since the PVA is used for this operation, the GPU workload is minimized. This example is a demonstration of how pre-processing, post-processing, and image processing tasks can be offloaded from a GPU, allowing it to concentrate on more compute-intensive machine learning and artificial intelligence tasks.</p> <p>This example application processes a video stream, displaying two visualizer windows: one for the original stream and another for the stream enhanced with image sharpening via PVA.</p>","tags":["PVA"]},{"location":"applications/pva_video_filter/#about-pva","title":"About PVA","text":"<p>PVA is a highly power-efficient VLIW processor integrated into NVIDIA Tegra platforms, specifically designed for advanced image processing and computer vision algorithms. The Compute Unified Programmable Vision Accelerator (CUPVA) SDK offers a comprehensive and unified programming model for PVA, enabling developers to create and optimize their own algorithms. For access to the SDK and further development opportunities, please contact NVIDIA.</p>","tags":["PVA"]},{"location":"applications/pva_video_filter/#content","title":"Content","text":"<ul> <li><code>main.cpp</code>: This file contains a C++ Holoscan application that demonstrates the use of an operator for loading and executing a pre-compiled PVA library dedicated to performing the unsharp masking algorithm on images. CUPVA SDK and license are not required to run this Holohub application.</li> <li><code>pva_unsharp_mask/</code>: This directory houses the <code>pva_unsharp_mask.hpp</code> header file, which declares the <code>PvaUnsharpMask</code> class. The <code>PvaUnsharpMask</code> class includes an <code>init</code> API, invoked for the initial tensor, and a <code>process</code> API, used for processing input tensors. Pre-compiled algorithm library file, <code>libpva_unsharp_mask.a</code>, and the corresponding allowlist file, <code>cupva_allowlist_pva_unsharp_mask</code>, are automatically downloaded by the CMake scripts. Please note that only PVA executables with signatures included in a secure allowlist database are permitted to execute on the PVA. This ensures that only verified and trusted executables are run, enhancing the security and integrity of the system.</li> </ul>","tags":["PVA"]},{"location":"applications/pva_video_filter/#algorithm-overview","title":"Algorithm Overview","text":"<p>The PVAVideoFilterExecutor operator performs an image sharpening operation in three steps:</p> <ol> <li>Convert the input RGB image to the NV24 color format.</li> <li>Apply a 5x5 unsharp mask filter on the luminance color plane.</li> <li>Convert the enhanced image back to the RGB format.</li> </ol> <p>Numerous algorithm examples leveraging the PVA can be found in the Vision Programming Interface (VPI) library. VPI enables computer vision software developers to utilize multiple compute engines simultaneously\u2014including CPU, GPU, PVA, VIC, NVENC, and OFA\u2014through a unified interface. For comprehensive details, please refer to the VPI Documentation.</p>","tags":["PVA"]},{"location":"applications/pva_video_filter/#compiling-the-application","title":"Compiling the application","text":"<p>Build the application inside docker</p> <pre><code>$ ./dev_container build --img holohub:pva_video_filter --base_img nvcr.io/nvidia/clara-holoscan/holoscan:v2.1.0-dgpu --docker_file ./Dockerfile\n# Check which version of CUPVA is installed on your platform at /opt/nvidia\n$ ./dev_container launch --img holohub:pva_video_filter --docker_opts \"-v /opt/nvidia/cupva-&lt;version&gt;:/opt/nvidia/cupva-&lt;version&gt; --device /dev/nvhost-ctrl-pva0:/dev/nvhost-ctrl-pva0 --device /dev/nvmap:/dev/nvmap --device /dev/dri/renderD129:/dev/dri/renderD129\"\n</code></pre> <p>Inside docker, add to your environment variable the following directories: <pre><code># inside docker\n$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/aarch64-linux-gnu/tegra/:/opt/nvidia/cupva-2.5/lib/aarch64-linux-gnu/\n</code></pre></p> <p>Build the application inside docker: <pre><code>$ ./run build pva_video_filter\n</code></pre></p>","tags":["PVA"]},{"location":"applications/pva_video_filter/#running-the-application","title":"Running the application","text":"<p>The application takes an endoscopy video stream as input, applies the unsharp mask filter, and shows it in HoloViz window.</p> <p>Before running the application, deploy VPU application signature allowlist on target in your host (outside a container): <pre><code>sudo cp &lt;HOLOHUB_BUILD_DIR&gt;/applications/pva_video_filter/cpp/pva_unsharp_mask/cupva_allowlist_pva_unsharp_mask /etc/pva/allow.d/cupva_allowlist_pva_unsharp_mask\nsudo pva_allow\n</code></pre></p> <p>Run the same docker container you used to build your application</p> <pre><code>$ ./dev_container launch --img holohub:pva_video_filter --docker_opts \"-v /opt/nvidia/cupva-&lt;version&gt;:/opt/nvidia/cupva-&lt;version&gt; --device /dev/nvhost-ctrl-pva0:/dev/nvhost-ctrl-pva0 --device /dev/nvmap:/dev/nvmap --device /dev/dri/renderD129:/dev/dri/renderD129\"\n\n# inside docker\n# don't forget the line below to export the environment variables\n$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/aarch64-linux-gnu/tegra/:/opt/nvidia/cupva-2.5/lib/aarch64-linux-gnu/\n$ ./run launch pva_video_filter\n</code></pre> <p></p>","tags":["PVA"]},{"location":"applications/qt_video_replayer/","title":"Qt Video Replayer","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: January 16, 2025 Language: C++ Latest version: 1.0.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p></p> <p>This application demonstrates how to integrate Holoscan with a Qt application. It support displaying the video frames output by a Holoscan operator and changing operator properties using Qt UI elements.</p> <pre><code>flowchart LR\n    subgraph Holoscan application\n        A[(VideoFile)] --&gt; VideostreamReplayerOp\n        VideostreamReplayerOp --&gt; FormatConverterOp\n        FormatConverterOp --&gt; NppFilterOp\n        NppFilterOp --&gt; QtVideoOp\n    end\n    subgraph Qt Window\n        QtVideoOp &lt;-.-&gt; QtHoloscanVideo\n    end\n</code></pre> <p>The application uses the VideostreamReplayerOp to read from a file on disk, the FormatConverterOp to convert the frames from RGB to RGBA, the NppFilterOp to apply a filter to the frame and the QtVideoOp operator to display the video stream in a Qt window.</p> <p>The QtHoloscanApp class, which extends the <code>holoscan::Application</code> class, is used to expose parameters of Holoscan operators as Qt properties.</p> <p>For example the application uses a QML Checkbox is used the set the <code>realtime</code> property of the <code>VideostreamReplayerOp</code> operator.</p> <pre><code>    CheckBox {\n        id: realtime\n        text: \"Use Video Framerate\"\n        checked: holoscanApp.realtime\n        onCheckedChanged: {\n            holoscanApp.realtime = checked;\n        }\n    }\n</code></pre> <p>The QtHoloscanVideo is a QQuickItem which can be use in the QML file. Multiple <code>QtHoloscanVideo</code> items can be placed in a Qt window.</p> <pre><code>import QtHoloscanVideo\nItem {\n    QtHoloscanVideo {\n        objectName: \"video\"\n    }\n}\n</code></pre>","tags":["Qt","QML","QtQuick","Video","UI","Userinterface","Interactive"]},{"location":"applications/qt_video_replayer/#run-instructions","title":"Run Instructions","text":"<p>This application requires Qt. For simplicity a DockerFile is available. To generate the container run:</p> <pre><code>./dev_container build --docker_file ./applications/qt_video_replayer/Dockerfile\n</code></pre> <p>The application can then be built by launching this container and using the provided <code>run</code> script.</p> <pre><code>./dev_container launch\n./run build qt_video_replayer\n</code></pre> <p>Once the application is build it can be launched with the <code>run</code> script.</p> <pre><code>./run launch qt_video_replayer\n</code></pre>","tags":["Qt","QML","QtQuick","Video","UI","Userinterface","Interactive"]},{"location":"applications/realsense_visualizer/","title":"RealSense Visualizer","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: June 28, 2024 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 0 - Core Stable</p> <p>Visualizes frames captured from an Intel RealSense camera. </p>"},{"location":"applications/realsense_visualizer/#build-and-run","title":"Build and Run","text":"<p>This application requires an Intel RealSense camera.</p> <p>At the top level of the holohub run the following command:</p> <pre><code>./dev_container build_and_run realsense_visualizer\n</code></pre>"},{"location":"applications/sam2/","title":"\ud83d\udcf7 Holoscan SAM2","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: December 2, 2024 Language: Python Latest version: 1.0.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates how to run SAM2 models on live video feed with the possibility of changing query points in real-time.</p> <p> </p> <p>The application currently uses a single query point as a foreground point that moves on the perimeter of a circle with a configured angular speed.  The models returns three masks, the best mask is selected based on the model scores. For visualization, two options exist. Select between \"logits\" or \"masks\".  - \"logits\": predictions of the network, mapped onto a colorscale that matches matplotlib.pyplot's \"viridis\"  - \"masks\": binarized predictions</p> <p>SAM2, recently announced by Meta, is the next iteration of the Segment Anything Model (SAM). This new version expands upon its predecessor by adding the capability to segment both videos and images. This sample application wraps the ImageInference class, and applies it on a live video feed.</p> <p>Note: This demo currently uses \"sam2_hiera_l.yaml\", but any of the sam2 models work. You only need to adjust segment_one_thing.yaml.</p>","tags":["SAM2 Model"]},{"location":"applications/sam2/#setup-instructions","title":"\u2699\ufe0f Setup Instructions","text":"<p>The app defaults to using the video device at <code>/dev/video0</code></p> <p>To debug if this is the correct device download <code>v4l2-ctl</code>: <pre><code>sudo apt-get install v4l-utils\n</code></pre> To check for your devices run: <pre><code>v4l2-ctl --list-devices\n</code></pre> This command will output something similar to this: <pre><code>NVIDIA Tegra Video Input Device (platform:tegra-camrtc-ca):\n        /dev/media0\n\nvi-output, lt6911uxc 2-0056 (platform:tegra-capture-vi:0):\n        /dev/video0\n\nDummy video device (0x0000) (platform:v4l2loopback-000):\n        /dev/video3\n</code></pre> Determine your desired video device and edit the source device in segment_one_thing.yaml</p>","tags":["SAM2 Model"]},{"location":"applications/sam2/#build-and-run-instructions","title":"\ud83d\ude80 Build and Run Instructions","text":"","tags":["SAM2 Model"]},{"location":"applications/sam2/#arm64-and-x86","title":"ARM64 and x86","text":"<p>This application uses a custom Dockerfile based on a pytorch container. Build and run the application using <pre><code> ./dev_container build_and_run sam2 --docker_file applications/sam2/Dockerfile --img holohub:sam2.1\n</code></pre> Or first build the container, then launch it and run.</p> <p><pre><code> ./dev_container build --docker_file applications/sam2/Dockerfile --img holohub:sam2.1\n</code></pre> <pre><code>./dev_container launch --img holohub:sam2.1\n</code></pre> <pre><code>./run launch sam2\n</code></pre></p>","tags":["SAM2 Model"]},{"location":"applications/sam2/#x86-only","title":"x86 only","text":"<p>If you are only using an x86 system, you may use a Dockerfile based on the Holoscan container. Replace the Dockerfile with this alternative Dockerfile.  Then, from the Holohub main directory run the following command: <pre><code>./dev_container build_and_run sam2\n</code></pre></p> <p>Alternatively build and run:  <pre><code>./dev_container vscode sam2\n</code></pre> Run the application in debug mode from vscode, or execute it by <pre><code>python applications/sam2/segment_one_thing.py\n</code></pre></p> <p>You can choose to output \"logits\" or \"masks\" in the configuration of the postprocessor and holoviz operator segment_one_thing.yaml</p>","tags":["SAM2 Model"]},{"location":"applications/sam2/#supported-hardware","title":"\ud83d\udcbb Supported Hardware","text":"<ul> <li>x86 w/ dGPU</li> <li>IGX devKit w/ dGPU</li> </ul>","tags":["SAM2 Model"]},{"location":"applications/sam2/#acknowledgements","title":"\ud83d\ude4c Acknowledgements","text":"<ul> <li>Meta, SAM2: for providing these models and inference infrastructure</li> </ul>","tags":["SAM2 Model"]},{"location":"applications/sdr_fm_demodulation/","title":"SDR FM Demodulation Application","text":"<p> Authors: Adam Thompson (NVIDIA) Supported platforms: amd64 Last modified: July 28, 2023 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 0.4.0 Tested Holoscan SDK versions: 0.4.0 Contribution metric: Level 2 - Trusted</p> <p>As the \"Hello World\" application of software defined radio developers, this demonstration highlights real-time FM demodulation, resampling, and playback on GPU with NVIDIA's Holoscan SDK. In this example, we are using an inexpensive USB-based RTL-SDR dongle to feed complex valued Radio Frequency (RF) samples into GPU memory and use cuSignal functions to perform the relevant signal processing. The main objectives of this demonstration are to: - Highlight developer productivity in building an end-to-end streaming application with Holoscan and existing GPU-Accelerated Python libraries - Demonstrate how to construct and connect isolated units of work via Holoscan operators - Emphasize that operators created for this application can be re-used in other ones doing similar tasks</p>","tags":["Communications","Aerospace","Defence","Lifesciences"]},{"location":"applications/sdr_fm_demodulation/#running-the-application","title":"Running the Application","text":"<p>Prior to running the application, the user needs to install the necessary dependencies (and, of course, plug in a SDR into your computer). This is most easily done in an Anaconda environment.</p> <pre><code>conda create --name holoscan-sdr-demo python=3.8\nconda activate holoscan-sdr-demo\nconda install -c conda-forge -c rapidsai -c nvidia cusignal soapysdr soapysdr-module-rtlsdr pyaudio\npip install holoscan\n</code></pre> <p>The FM demodulation example can then be run via <pre><code>python applications/sdr_fm_demodulation/sdr_fm_demodulation.py\n</code></pre></p>","tags":["Communications","Aerospace","Defence","Lifesciences"]},{"location":"applications/simple_pdw_pipeline/","title":"Simple PDW Pipeline","text":"<p>This is a Holoscan pipeline that shows the possibility of using Holoscan as a Pulse Description Word (PDW) generator. This is a process that takes in IQ samples (signals represented using time-series complex numbers) and picks out peaks in the signal that may be transmissions from another source. These PDW processors are used to see what is transmitting in your area, be they radio towers or radars.</p> <p>siggen.c a signal generator written in C that will transmit the input to this pipeline. </p>","tags":["Network","UDP","Radar","Electronic Support"]},{"location":"applications/simple_pdw_pipeline/#basicnetworkoprx","title":"BasicNetworkOpRx","text":"<p>This uses the Basic Network Operator to read udp packets this operator is documented elsewhere. </p>","tags":["Network","UDP","Radar","Electronic Support"]},{"location":"applications/simple_pdw_pipeline/#packettotensorop","title":"PacketToTensorOp","text":"<p>This converts the bytes from the Basic Network Operator into the packets used in the rest of the pipeline. The format of the incoming packets is a 16-bit id followed by 8192 IQ samples each sample has the following format: 16 bits (I) 16 bits (Q)</p>","tags":["Network","UDP","Radar","Electronic Support"]},{"location":"applications/simple_pdw_pipeline/#fftop","title":"FFTOp","text":"<p>Does what it says on the tin. Takes an FFT of the input data. Also shifts data so that 0 Hz is centered.</p>","tags":["Network","UDP","Radar","Electronic Support"]},{"location":"applications/simple_pdw_pipeline/#thresholdingop","title":"ThresholdingOp:","text":"<p>Detects samples over a threshold and then packetizes the runs of samples that are above the threshold as a \u201cpulse\u201d.</p>","tags":["Network","UDP","Radar","Electronic Support"]},{"location":"applications/simple_pdw_pipeline/#pulsedescriptiorop","title":"PulseDescriptiorOp","text":"<p>Takes simple statistics of input pulses. This is where I am most excited for future work, but that is not the point of this particular project.</p>","tags":["Network","UDP","Radar","Electronic Support"]},{"location":"applications/simple_pdw_pipeline/#pulseprinterop","title":"PulsePrinterOp","text":"<p>Prints the pulse to screen. Also optionally sends packets to a BasicNetworkOpTx.  The transmitted network packets have the following format: Each of the following fields are 16bit unsigned integers   id   low bin   high bin   zero bin   sum power   max amplitude   average amplitude</p>","tags":["Network","UDP","Radar","Electronic Support"]},{"location":"applications/speech_to_text_llm/","title":"Speech-to-text + Large Language Model","text":"","tags":["Speech-to-text","Large Language Model"]},{"location":"applications/speech_to_text_llm/#speech-to-text-large-language-model","title":"Speech-to-text + Large Language Model","text":"<p>This application transcribes an audio file using a speech-to-text model (STT), then uses a large language model (LLM) to summarize and generate new relevant information. </p> <p>While this workflow in principle could be used for a number of domains, here we provide a healthcare specific example. A <code>sample.wav</code> file is provided which is an example of a radiology interpretation. An OpenAI whisper model is used to transform the audio into text, then an API call is made to either the GPT3.5-turbo or GPT4 LLM. </p>","tags":["Speech-to-text","Large Language Model"]},{"location":"applications/speech_to_text_llm/#yaml-configuration","title":"YAML Configuration","text":"<p>The input (either audio or video file), specific Whisper model (tiny, small, medium, or large), LLM model, and directions for the LLM are all determined by the <code>stt_to_nlp.yaml</code> file. As you see from our example, the directions for the LLM are made via natural language, and can result in very different applications. </p> <p>For our purposes we specify the directions as:</p> <pre><code>  context: 'Make summary of the transcript (and correct any transcription errors in CAPS).\\n Create a Patient Summary with no medical jargon. \\n \n  Create a full radiological report write-up. \\n Give likely ICD-10 Codes \\n Suggested follow-up steps.'\n</code></pre> <p>This results in the following output from the LLM:</p> <pre><code>LLM Response: \n Summary of Transcript:\nThe patient has full thickness wear on the dorsal half of the second metatarsal head with reactive bone marrow edema and capsulitis. There is also second web space bursitis and a third web space neuroma. The 51-year-old male has multiple gallbladder polyps, with the largest measuring 1.9 x 2 cm, 1.7 x 1.7 cm in the mid portion, and 1.6 x 1.6 cm distally. Other smaller polyps are also present.\n\nPatient Summary (No Medical Jargon):\nThe patient has damage and inflammation in the foot, specifically in the second toe joint and surrounding areas. They also have multiple growths in their gallbladder, with the largest being about the size of a grape. The patient is a 51-year-old male with a family history of abdominal aortic aneurysm.\n\nFull Radiological Report Write-up:\nPatient: 51-year-old male\nFamily History: Abdominal aortic aneurysm\n\nFindings:\n1. Foot: Full thickness wear over the dorsal half of the second metatarsal head with reactive subchondral bone marrow edema and capsulitis. Second web space intermetatarsal bursitis and a third web space neuroma are also present.\n2. Gallbladder: Multiple gallbladder polyps are observed. The largest polyp measures 1.9 x 2 cm, with additional polyps measuring 1.7 x 1.7 cm in the mid portion and 1.6 x 1.6 cm distally. Two smaller polyps measure 0.5 x 0.4 x 0.4 cm and 0.5 x 0.3 x 0.5 cm.\n\nLikely ICD-10 Codes:\n1. M25.572 - Capsulitis, left ankle and foot\n2. M79.671 - Bursitis, right ankle and foot\n3. G57.60 - Lesion of plantar nerve, unspecified lower limb\n4. K82.8 - Other specified diseases of the gallbladder (gallbladder polyps)\n\nSuggested Follow-up Steps:\n1. For the foot issues, the patient may benefit from a consultation with a podiatrist or orthopedic specialist to discuss treatment options, which may include physical therapy, orthotics, or surgery.\n2. For the gallbladder polyps, the patient should consult with a gastroenterologist to determine the need for further evaluation, monitoring, or possible surgical intervention. Regular ultrasound examinations may be recommended to monitor the growth of the polyps.\n</code></pre>","tags":["Speech-to-text","Large Language Model"]},{"location":"applications/speech_to_text_llm/#run-instructions","title":"Run Instructions","text":"<p>Note: To run this application you will need to create an OpenAI account and obtain your own API key with active credits.</p> <p>You should refer to the glossary for the terms defining specific locations within HoloHub.</p> <ul> <li>(Optional) Create and use a virtual environment:</li> </ul> <pre><code>python3 -m venv .venv\nsource .venv/bin/activate\n</code></pre> <ul> <li>Install the python packages</li> </ul> <pre><code>pip install -r applications/speech_to_text_llm/requirements.txt\n</code></pre> <ul> <li>Run the application</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOSCAN_INSTALL_DIR&gt;/python/lib:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\ncd applications/speech_to_text_llm \npython3 stt_to_nlp.py\n</code></pre>","tags":["Speech-to-text","Large Language Model"]},{"location":"applications/speech_to_text_llm/#sample-audio-file","title":"Sample Audio File","text":"<p>Please note the sample audio file included is licensed as CC-BY-4.0 International, copyright NVIDIA 2023. </p>","tags":["Speech-to-text","Large Language Model"]},{"location":"applications/ssd_detection_endoscopy_tools/","title":"SSD Detection Application","text":"<p> Authors: Jin Li (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 5, 2025 Language: Python Latest version: 1.1 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 2 - Trusted</p>","tags":["SSD","bounding box","Detection"]},{"location":"applications/ssd_detection_endoscopy_tools/#model","title":"Model","text":"<p>We can train the SSD model from NVIDIA DeepLearningExamples repo with any data of our choosing. Here for the purpose of demonstrating the deployment process, we will use a SSD model checkpoint that is only trained for the demo video clip. </p> <p>Please download the models at this NGC Resource for <code>epoch_24.pt</code>, <code>epoch24_nms.onnx</code> and <code>epoch24.onnx</code>. You can go through the next steps of Model Conversion to ONNX to convert <code>epoch_24.pt</code> into <code>epoch24_nms.onnx</code> and <code>epoch24.onnx</code>, or use the downloaded ONNX models directly.</p>","tags":["SSD","bounding box","Detection"]},{"location":"applications/ssd_detection_endoscopy_tools/#model-conversion-to-onnx","title":"Model Conversion to ONNX","text":"<p>The scripts we need to export the model from .pt checkpoint to the ONNX format are all within this dir <code>./scripts</code>. It is a two step process.</p> <p>Step 1: Export the trained checkpoint to ONNX.  We use <code>export_to_onnx_ssd.py</code> if we want to use the model as is without NMS, or <code>export_to_onnx_ssd_nms.py</code> to prepare the model with NMS.   Let's assume the re-trained SSD model checkpoint from the repo is saved as <code>epoch_24.pt</code>.  The export process is  <pre><code># For exporting the original ONNX model\n python export_to_onnx_ssd.py --model epoch_24.pt  --outpath epoch24_temp.onnx\n</code></pre> <pre><code># For preparing to add the NMS step to ONNX model\npython export_to_onnx_ssd_nms.py --model epoch_24.pt  --outpath epoch24_nms_temp.onnx\n</code></pre> Step 2: modify input shape.  Step 1 produces a onnx model with input shape <code>[1, 3, 300, 300]</code>, but we will want to modify the input node to have shape <code>[1, 300, 300, 3]</code> or in general <code>[batch_size, height, width, channels]</code> for compatibility and easy of deployment in the Holoscan SDK. If we want to incorporate the NMS operation in the the ONNX model, we could add a <code>EfficientNMS_TRT</code> op, which is documented in <code>graph_surgeon_ssd.py</code>'s nms related block. <pre><code># For exporting the original ONNX model\npython graph_surgeon_ssd.py --orig_model epoch24_temp.onnx --new_model epoch24.onnx\n</code></pre> <pre><code># For adding the NMS step to ONNX model, use --nms\npython graph_surgeon_ssd.py --orig_model epoch24_nms_temp.onnx --new_model epoch24_nms.onnx --nms\n</code></pre></p> <p>Note that  - <code>epoch24.onnx</code> is used in <code>ssd_step1.py</code> and <code>ssd_step2_route1.py</code>  - <code>epoch24_nms.onnx</code> is used in <code>ssd_step2_route2.py</code> and <code>ssd_step2_route2_render_labels.py</code> </p>","tags":["SSD","bounding box","Detection"]},{"location":"applications/ssd_detection_endoscopy_tools/#data","title":"Data","text":"<p>For this application we will use the same Endoscopy Sample Data as the Holoscan SDK reference applications.</p>","tags":["SSD","bounding box","Detection"]},{"location":"applications/ssd_detection_endoscopy_tools/#requirements","title":"Requirements","text":"<p>There are two requirements  1. To run <code>ssd_step1.py</code> and <code>ssd_step2_route1.py</code> with the original exported model, we need the installation of PyTorch and CuPy.   To run <code>ssd_step2_route2.py</code> and <code>ssd_step2_route2_render_labels.py</code> with the exported model with additional NMS layer in ONNX, we need the installation of CuPy.   If you're using the dGPU on the devkit, since there are no prebuilt PyTorch wheels for aarch64 dGPU, the simplest way is to modify the Dockerfile and build from source; if you're on x86 or using the iGPU on the devkit, there should be existing prebuilt PyTorch wheels.  If you choose to build the SDK from source, you can find the modified Dockerfile here to replace the SDK repo Dockerfile to satisfy the installation requirements.   The main changes in Dockerfile for dGPU: the base image changed to <code>nvcr.io/nvidia/pytorch:22.03-py3</code> instead of the <code>nvcr.io/nvidia/tensorrt:22.03-py3</code> as dGPU's base image; adding the installation of NVTX for optional profiling. Build the SDK container following the README instructions.    Make sure the directory containing this application and the directory containing the NGC data and models are mounted in the container. Add the <code>-v</code> mount options to the <code>docker run</code> command launched by <code>./run launch</code> in the SDK repo. </p> <ol> <li>Make sure the model and data are accessible by the application.   Make sure the yaml files <code>ssd_endo_model.yaml</code> and <code>ssd_endo_model_with_NMS.yaml</code> are pointing to the right locations for the ONNX model and data. The assumption in the yaml file is that the <code>epoch24_nms.onnx</code> and <code>epoch24.onnx</code> are located at: <pre><code>model_file_path: /byom/models/endo_ssd/epoch24_nms.onnx \nengine_cache_dir: /byom/models/endo_ssd/epoch24_nms_engines\n</code></pre> and / or <pre><code>model_file_path: /byom/models/endo_ssd/epoch24.onnx\nengine_cache_dir: /byom/models/endo_ssd/epoch24_engines\n</code></pre> The Endoscopy Sample Data is assumed to be at  <pre><code>/workspace/holoscan-sdk/data/endoscopy\n</code></pre> Please check and modify the paths to model and data in the yaml file if needed.</li> </ol>","tags":["SSD","bounding box","Detection"]},{"location":"applications/ssd_detection_endoscopy_tools/#building-the-application","title":"Building the application","text":"<p>Please refer to the README under ./app_dev_process to see the process of building the applications.</p>","tags":["SSD","bounding box","Detection"]},{"location":"applications/ssd_detection_endoscopy_tools/#running-the-application","title":"Running the application","text":"<p>Run the incrementally improved Python applications by: <pre><code>python ssd_step1.py\n\npython ssd_step2_route1.py\n\npython ssd_step2_route2.py\n\npython ssd_step2_route2_render_labels.py --labelfile endo_ref_data_labels.csv\n</code></pre></p>","tags":["SSD","bounding box","Detection"]},{"location":"applications/stereo_vision/","title":"Stereo Vision","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: January 30, 2025 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 2.4.0 Tested Holoscan SDK versions: 2.4.0 Contribution metric: Level 1 - Highly Reliable</p> <p> </p>","tags":["Endoscopy","Stereo"]},{"location":"applications/stereo_vision/#overview","title":"Overview","text":"<p>A demo pipeline showcasing stereo disparity estimation.</p>","tags":["Endoscopy","Stereo"]},{"location":"applications/stereo_vision/#description","title":"Description","text":"<p>This pipeline takes video from a stereo camera and estimates disparity using DNN ESS. The disparity map is displayed through Holoviz.</p>","tags":["Endoscopy","Stereo"]},{"location":"applications/stereo_vision/#requirements","title":"Requirements","text":"<p>This application requires a V4L2 stereo camera or recorded stereo video as input. A video acquired from a StereoLabs ZED camera is downloaded when running the <code>get_data_and_models.sh</code> script when building the application. A script for obtaining the calibration for StereoLabs cameras is also provided. Holoscan SDK &gt;=2.0,&lt;=2.5 is required for TensorRT 8.6 compatibility.</p>","tags":["Endoscopy","Stereo"]},{"location":"applications/stereo_vision/#camera-calibration","title":"Camera Calibration","text":"<p>The default calibration will work for the sample video. If using a stereolabs camera the calibration can be retrieved using <code>get_zed_calibration.py</code> and the devices serial number.</p> <pre><code>python3 get_zed_calibration.py -s [Serial Number]\n</code></pre>","tags":["Endoscopy","Stereo"]},{"location":"applications/stereo_vision/#input-video","title":"Input video","text":"<p>For the input video stream, either use a v4l2 stereo camera such as those produced by stereolabs or included recorded video. The <code>stereo-plants.mp4</code> video is provided here and will be downloaded and converted to the necessary format when building the application.</p> <p>The source device in <code>stereo_vision.yaml</code> should be modified to match the device the v4l2 video is using. This can be found using <code>v4l2-ctl --list-devices</code>.</p>","tags":["Endoscopy","Stereo"]},{"location":"applications/stereo_vision/#models","title":"Models","text":"<p>This demo requires the ESS DNN Stereo Disparity available from the NGC catalog for disparity estimation. This model is downloaded when you build the application.</p>","tags":["Endoscopy","Stereo"]},{"location":"applications/stereo_vision/#ess-dnn","title":"ESS DNN","text":"<p>The ESS engine files generated in this demo application is specific to TRT8.6; make sure you build the devcontainer with a compatible <code>base_img</code> as shown in the Build and Run Instructions section.</p>","tags":["Endoscopy","Stereo"]},{"location":"applications/stereo_vision/#build-and-run-instructions","title":"Build and Run Instructions","text":"<p>Run the following command to build and run application using the recorded video: <pre><code>./dev_container build_and_run stereo_vision --base_img nvcr.io/nvidia/clara-holoscan/holoscan:v2.4.0-dgpu\n</code></pre></p> <p>To run the application using a v4l2 compatible stereo camera, run: <pre><code>./dev_container build_and_run stereo_vision --base_img nvcr.io/nvidia/clara-holoscan/holoscan:v2.4.0-dgpu --run_args \"--source v4l2\"\n</code></pre></p>","tags":["Endoscopy","Stereo"]},{"location":"applications/synthetic_aperture_radar/","title":"Holoscan SAR","text":"<p> Authors: Dan Campbell (NVIDIA), Amanda Butler (NVIDIA) Supported platforms: amd64, arm64 Last modified: July 20, 2023 Language: Python Latest version: 0.1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 4 - Experimental</p>","tags":["Radar","SAR","Synthetic Aperture"]},{"location":"applications/synthetic_aperture_radar/#description","title":"Description","text":"<p>This application is a demonstration of using Holoscan to construct Synthetic Aperture Radar (SAR) imagery from a data collection.  In current form, the data is assumed to be precollected and contained in a particular binary format.  It has been tested with 2 versions of the publicly available GOTCHA volumetric SAR data collection.  Python-based converters are included to manipulate the public datasets into the binary format expected by the application.  The application implements Backprojection for image formation.</p>","tags":["Radar","SAR","Synthetic Aperture"]},{"location":"applications/synthetic_aperture_radar/#requirements","title":"Requirements","text":"<ul> <li>Holoscan (&gt;=0.5)</li> <li>Python implementation:<ul> <li>Python3</li> <li>CuPy or Numpy</li> <li>Pillow</li> </ul> </li> <li>Scripts in <code>deploy/</code> will build and execute a docker environment that meets the requirements for systems using nvidia-docker</li> </ul>","tags":["Radar","SAR","Synthetic Aperture"]},{"location":"applications/synthetic_aperture_radar/#obtain-and-format-gotcha-dataset","title":"Obtain and Format GOTCHA Dataset","text":"<ul> <li>Navigate to https://www.sdms.afrl.af.mil/index.php?collection=gotcha </li> <li>Click the DOWNLOAD link below the images</li> <li>Log in.  You may need to create an account to do so</li> <li>Under \"GOTCHA Volumetric SAR Data Set Challenge Problem\" download \"Disc 1 of 2\".<ul> <li>The data in \"Disc 2 of 2\" is compatible with this demo but not used</li> </ul> </li> <li>Unpack the contents of \"Disc 1 of 2\" into the <code>data/</code> directory.  This should create a subdirectry named <code>GOTCHA-CP_Disc1/</code></li> <li><code>cd data</code></li> <li><code>python3 cp-large_convert.py</code></li> <li>This should create a data file named <code>gotcha-cp-td-os.dat</code> that has a file size 2766987672 bytes, and a md5sum of 554b509c2d5c2c3de8e5643983a9748d</li> </ul>","tags":["Radar","SAR","Synthetic Aperture"]},{"location":"applications/synthetic_aperture_radar/#build-and-use-docker-container-optional","title":"Build and Use Docker Container (Optional)","text":"<ul> <li>This demonstration is distributed with tools to build a docker container that meets the demonstration's system requirements.  This approach will only work properly with nvidia-docker</li> <li>From the demonstration root directory:</li> <li><code>cd deploy</code></li> <li><code>bash build_application_container.sh</code> - this will build the container</li> <li><code>bash run_application_container.sh</code> - this will launch a container that meets the demonstration system requirements</li> </ul>","tags":["Radar","SAR","Synthetic Aperture"]},{"location":"applications/synthetic_aperture_radar/#build-and-execute","title":"Build and Execute","text":"<ul> <li>Python: <ul> <li><code>python3 holosar.py</code></li> </ul> </li> </ul> <p>The application will create a window with the resolved SAR image, and update after each group of 100 pulses received.  The image represents the strength of reflectivity at points on the ground within the imaging window.  The text at the top of the window indicates the (X,Y) position of the collecting radar at the most recent pulse, along with the total count of pulses received.  The red line points in the direction of the collection vehicle's location at the most recent pulse.  </p> <p>A screen grab is included below for reference:</p> <p></p>","tags":["Radar","SAR","Synthetic Aperture"]},{"location":"applications/tao_peoplenet/","title":"TAO PeopleNet Detection Model on V4L2 Video Stream","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: September 23, 2024 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0, 1.0.3, 2.0.0, 2.1.0 Contribution metric: Level 2 - Trusted </p> <p>Use the TAO PeopleNet available on NGC to detect faces and people in a V4L2 supported video stream. HoloViz is used to draw bounding boxes around the detections.</p>","tags":["Computer Vision","Detection"]},{"location":"applications/tao_peoplenet/#model","title":"Model","text":"<p>This application uses the TAO PeopleNet model from NGC for face and person classification. The model is downloaded when building the application.</p>","tags":["Computer Vision","Detection"]},{"location":"applications/tao_peoplenet/#data","title":"Data","text":"<p>This application downloads a pre-recorded video from Pexels when the application is built for use with this application.  Please review the license terms from Pexels.</p> <p>NOTE: The user is responsible for checking if the dataset license is fit for the intended purpose.</p>","tags":["Computer Vision","Detection"]},{"location":"applications/tao_peoplenet/#input","title":"Input","text":"<p>This app currently supports three different input options:</p> <ol> <li>v4l2 compatible input device (default, see V4L2 Support below)</li> <li>pre-recorded video (see Video Replayer Support below)</li> </ol>","tags":["Computer Vision","Detection"]},{"location":"applications/tao_peoplenet/#run-instructions","title":"Run Instructions","text":"","tags":["Computer Vision","Detection"]},{"location":"applications/tao_peoplenet/#v4l2-support","title":"V4L2 Support","text":"<p>This application supports v4l2 compatible devices as input.  To run this application with your v4l2 compatible device, please plug in your input device and run: <pre><code>./dev_container build_and_run tao_peoplenet\n</code></pre></p> <p>By default, this application expects the input device to be mounted at <code>/dev/video0</code>.  If this is not the case, please update <code>applications/tao_peoplenet/tao_peoplenet.yaml</code> and set it to use the corresponding input device before running the application.  You can also override the default input device on the command line by running: <pre><code>./dev_container build_and_run tao_peoplenet --run_args \"--video_device /dev/video0\"\n</code></pre></p>","tags":["Computer Vision","Detection"]},{"location":"applications/tao_peoplenet/#video-replayer-support","title":"Video Replayer Support","text":"<p>If you don't have a v4l2 compatible device plugged in, you may also run this application on a pre-recorded video. To launch the application using the Video Stream Replayer as the input source, run:</p> <pre><code>./dev_container build_and_run tao_peopelnet --run_args \"--source replayer\"\n</code></pre>","tags":["Computer Vision","Detection"]},{"location":"applications/tao_peoplenet/#known-issues","title":"Known Issues","text":"<p>There is a known issue running this application on IGX w/ iGPU and on Jetson AGX (see #500). The workaround is to update the device to avoid picking up the libnvv4l2.so library.</p> <pre><code>cd /usr/lib/aarch64-linux-gnu/\nls -l libv4l2.so.0.0.999999\nsudo rm libv4l2.so.0.0.999999\nsudo ln -s libv4l2.so.0.0.0.0  libv4l2.so.0.0.999999\n</code></pre>","tags":["Computer Vision","Detection"]},{"location":"applications/velodyne_lidar_app/","title":"Velodyne VLP-16 Lidar Viewer Application","text":"<p> Authors: Holoscan Team (NVIDIA), nvMap Team (NVIDIA), nvMap Embedded Team (NVIDIA), Tom Birdsong (NVIDIA), Julien Jomier (NVIDIA), Jiahao Yin (NVIDIA), Marlene Wan (NVIDIA) Supported platforms: amd64, arm64 Last modified: April 29, 2024 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 4 - Experimental</p> <p></p>","tags":["Lidar","Velodyne","Point Cloud","Visualization","Sensor"]},{"location":"applications/velodyne_lidar_app/#overview","title":"Overview","text":"<p>In this application we demonstrate how to use Holoscan SDK for low-latency lidar processing. We receive lidar packets from a Velodyne VLP-16 lidar sensor, convert packet information to a rolling Cartesian point cloud on GPU, then visualize the results with HoloViz.</p>","tags":["Lidar","Velodyne","Point Cloud","Visualization","Sensor"]},{"location":"applications/velodyne_lidar_app/#background","title":"Background","text":"<p>\"Lidar\" (LIght Detection And Ranging) is a technique by which \"light\", typically of wavelengths in the infrared spectrum, is used to determine the position of reflective surfaces surrounding a sensor. A 3D lidar sensor often employs a stacked vertical array of infrared laser emitters and sources that it spins rapidly. Similar to radar, the strength and timing of reflected lasers can be used to generate a 360-degree 3D point cloud view of the surrounding environment, with each point corresponding to an estimated point of reflection.</p> <p>For demonstration purposes we selected the Velodyne VLP-16 lidar sensor as our input source. We adapted existing packet processing code from NVIDIA DeepMap SDK into a custom Holoscan operator, <code>VelodyneLidarOp</code>, and connected it with the existing <code>BasicNetworkOp</code> and <code>HoloVizOp</code> operators to provide a complete viewing pipeline. We performed initial benchmarking on an NVIDIA IGX devkit.</p>","tags":["Lidar","Velodyne","Point Cloud","Visualization","Sensor"]},{"location":"applications/velodyne_lidar_app/#requirements","title":"Requirements","text":"<p>This application is intended to run on a Holoscan SDK support platform, namely a Linux x64 system or an NVIDIA IGX developer kit.</p> <p>To run the application you need a live or replayer source to stream Velodyne VLP-16 packet data to the application. That may be either: - A Velodyne VLP-16 lidar sensor. Review the VLP-16 user manual for setup instructions. - A VLP-16 <code>.pcap</code> recording file and a packet replayer software.   - Visit Kitware's VeloView Velodyne Lidar collection for sample VLP-16 <code>.pcap</code> files.   - Visit the third party Wireshark wiki for a curated list of software options for generating traffic from <code>.pcap</code> files.</p>","tags":["Lidar","Velodyne","Point Cloud","Visualization","Sensor"]},{"location":"applications/velodyne_lidar_app/#running-the-application","title":"Running the Application","text":"<p>First, start your lidar stream source. If you are using a VLP-16 lidar sensor, review the VLP-16 user manual for instructions on how to properly set up your network configuration.</p> <p>Then, build and start the Holoscan lidar viewing application:</p> <pre><code>./dev_container build_and_run velodyne_lidar_app\n</code></pre>","tags":["Lidar","Velodyne","Point Cloud","Visualization","Sensor"]},{"location":"applications/velodyne_lidar_app/#benchmarks","title":"Benchmarks","text":"<p>We performed benchmarking on an NVIDIA IGX developer kit with an A4000 GPU. (Note that an A6000 GPU is standard for IGX.) We used the holoscan_flow_benchmarking project to collect and summarize performance. The performance for each component in the Holoscan SDK pipeline is shown in the image below.</p> <p>Key statistics:</p> Minimum Latency 1.03 milliseconds Average Latency 1.12 milliseconds Maximum Latency 1.34 milliseconds <p>By comparison, the VLP-16 lidar publishes packets at a rate of approximately 1.33 milliseconds per packet.</p> <p></p> <p></p>","tags":["Lidar","Velodyne","Point Cloud","Visualization","Sensor"]},{"location":"applications/velodyne_lidar_app/#frequently-asked-questions-faq","title":"Frequently Asked Questions (FAQ)","text":"","tags":["Lidar","Velodyne","Point Cloud","Visualization","Sensor"]},{"location":"applications/velodyne_lidar_app/#how-does-the-application-work","title":"How does the application work?","text":"<p>The application flow is as follows:</p> <ol> <li>A UDP packet is emitted from the Velodyne VLP-16 lidar sensor and received on port 2368 in the Holoscan <code>BasicNetworkOp</code> operator.</li> <li>The packet payload is forwarded to the Holoscan <code>VelodyneLidarOp</code> operator. The operator decodes the packet according to the Velodyne lidar specification, where the VLP-16 packet defines 384 spherical points from laser firings. The operator converts the spherical points to Cartesian points on the GPU device and adds the resulting cloud to a rolling, accumulated point cloud.</li> <li>The Velodyne operator forwards the rolling point cloud to HoloViz, which renders the GPU point cloud to the screen.</li> </ol>","tags":["Lidar","Velodyne","Point Cloud","Visualization","Sensor"]},{"location":"applications/velodyne_lidar_app/#what-are-some-limitations-of-the-application","title":"What are some limitations of the application?","text":"<p>This application is intended as a simple demonstration of how a lidar sensor can be integrated for input to Holoscan SDK for low latency processing. It does not propose any novel features. Some limitations compared with more complete lidar solutions are: - No cloud filtering -- all zero-ranged points are kept in the buffer and visualized. - No advanced inference techniques -- the cloud is simply translated and visualized. - No RDMA -- VLP-16 lidar packets are received via the host ethernet interface on the IGX or x86_64 machine and then copied to the GPU device. - Monochrome visual -- HoloViz operator cloud support is currently limited to one color.</p> <p>Each of these limitations is merely a result of our scope of work, and could be overcome with additional attention.</p>","tags":["Lidar","Velodyne","Point Cloud","Visualization","Sensor"]},{"location":"applications/velodyne_lidar_app/#why-is-holoviz-not-responding","title":"Why is HoloViz not responding?","text":"<p>In most cases this indicates that the Holoscan application is not receiving UDP packets. There are several reasons that this could be the case: - The VLP-16 lidar sensor is not turned on, or the ethernet cable is disconnected.   The sensor typically takes approximately 30 seconds between powering on and transmitting packets. - The VLP-16 lidar sensor network interface is not properly configured to receive packets. You can use a tool   such as Wireshark to review live packets on the network interface. Review the VLP-16   user manual for troubleshooting. - The HoloHub application is not properly configured. Review the <code>lidar.yaml</code> configuration   and confirm that the port and IP address match the VLP-16 configuration.</p>","tags":["Lidar","Velodyne","Point Cloud","Visualization","Sensor"]},{"location":"applications/velodyne_lidar_app/#acknowledgements","title":"Acknowledgements","text":"<p>This operator was developed in part with support from the NVIDIA nvMap team and adapts portions of the NVIDIA DeepMap SDK.</p>","tags":["Lidar","Velodyne","Point Cloud","Visualization","Sensor"]},{"location":"applications/video_deidentification/","title":"Real-Time Face and Text Deidentification","text":"<p> Authors: Wendell Hom (NVIDIA), Jonathan McLeod (NVIDIA) Supported platforms: amd64, arm64 Last modified: September 23, 2024 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 2 - Trusted </p> <p>This sample application demonstrates the use of face and text detection models to do real-time video deidentification. Regions identified to be face or text are blurred out from the final image.</p> <p>NOTE: This application is a demonstration of real-time face and text deidentification and is not meant to be used in critical applications that has zero error tolerance.  The models used in this sample application have limitations, e.g., in detecting faces and text that are partially occluded, in low lighting situations, when there is motion blur, etc.</p>","tags":["Computer Vision","Detection"]},{"location":"applications/video_deidentification/#models","title":"Models","text":"<p>This application uses TAO PeopleNet model from NGC for detecting faces. The model is downloaded when building the application.</p> <p>For text detection, this application uses EasyOCR python library which uses Character Region Awareness for Text Detection (CRAFT).</p>","tags":["Computer Vision","Detection"]},{"location":"applications/video_deidentification/#data","title":"Data","text":"<p>This application downloads a pre-recorded video from Pexels when the application is built for use with this application.  Please review the license terms from Pexels.</p> <p>NOTE: The user is responsible for checking if the dataset license is fit for the intended purpose.</p>","tags":["Computer Vision","Detection"]},{"location":"applications/video_deidentification/#input","title":"Input","text":"<p>This app currently supports three different input options:</p> <ol> <li>v4l2 compatible input device (default, see V4L2 Support below)</li> <li>pre-recorded video (see Video Replayer Support below)</li> </ol>","tags":["Computer Vision","Detection"]},{"location":"applications/video_deidentification/#run-instructions","title":"Run Instructions","text":"","tags":["Computer Vision","Detection"]},{"location":"applications/video_deidentification/#v4l2-support","title":"V4L2 Support","text":"<p>This application supports v4l2 compatible devices as input.  To run this application with your v4l2 compatible device, please plug in your input device and run: <pre><code>./dev_container build_and_run video_deidentification\n</code></pre></p> <p>By default, this application expects the input device to be mounted at <code>/dev/video0</code>.  If this is not the case, please update <code>applications/video_deidentification/video_deidentification.yaml</code> and set it to use the corresponding input device before running the application.  You can also override the default input device on the command line by running: <pre><code>./dev_container build_and_run video_deidentification --run_args \"--video_device /dev/video0\"\n</code></pre></p>","tags":["Computer Vision","Detection"]},{"location":"applications/video_deidentification/#video-replayer-support","title":"Video Replayer Support","text":"<p>If you don't have a v4l2 compatible device plugged in, you may also run this application on a pre-recorded video. To launch the application using the Video Stream Replayer as the input source, run:</p> <pre><code>./dev_container build_and_run video_deidentification --run_args \"--source replayer\"\n</code></pre>","tags":["Computer Vision","Detection"]},{"location":"applications/video_deidentification/#known-issues","title":"Known Issues","text":"<p>There is a known issue running this application on IGX w/ iGPU and on Jetson AGX (see #500). The workaround is to update the device to avoid picking up the libnvv4l2.so library.</p> <pre><code>cd /usr/lib/aarch64-linux-gnu/\nls -l libv4l2.so.0.0.999999\nsudo rm libv4l2.so.0.0.999999\nsudo ln -s libv4l2.so.0.0.0.0  libv4l2.so.0.0.999999\n</code></pre>","tags":["Computer Vision","Detection"]},{"location":"applications/vila_live/","title":"\ud83d\udcf7\ud83e\udd16 Holoscan VILA Live","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: March 13, 2025 Language: Python Latest version: 1.0.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates how to run VILA 1.5 models on live video feed with the possibility of changing the prompt in real time.</p> <p>VILA 1.5 is a family of Vision Language Models (VLM) created by NVIDIA &amp; MIT. It uses SigLIP to encode images into tokens which are fed into an LLM with an accompanying prompt. This application collects video frames from the V4L2 operator and feeds them to an AWQ-quantized VILA 1.5 for inference using the TinyChat library. This allows users to interact with a Generative AI model that is \"watching\" a chosen video stream in real-time.</p> <p> Note: This demo currently uses Llama-3-VILA1.5-8b-AWQ, but any of the following AWQ-quantized models from the VILA 1.5 familty should work as long as the file names are changed in the Dockerfile and run_vila_live.sh: - VILA1.5-3b-AWQ - VILA1.5-3b-s2-AWQ - Llama-3-VILA1.5-8b-AWQ - VILA1.5-13b-AWQ - VILA1.5-40b-AWQ</p>","tags":["Large Multimodal Model","Large Vision Model"]},{"location":"applications/vila_live/#setup-instructions","title":"\u2699\ufe0f Setup Instructions","text":"<p>The app defaults to using the video device at <code>/dev/video0</code></p> <p>Note: You can use a USB webcam as the video source, or an MP4 video by following the instructions for the V4L2_Camera example app.</p> <p>To debug if this is the correct device download <code>v4l2-ctl</code>: <pre><code>sudo apt-get install v4l-utils\n</code></pre> To check for your devices run: <pre><code>v4l2-ctl --list-devices\n</code></pre> This command will output something similar to this: <pre><code>NVIDIA Tegra Video Input Device (platform:tegra-camrtc-ca):\n        /dev/media0\n\nvi-output, lt6911uxc 2-0056 (platform:tegra-capture-vi:0):\n        /dev/video0\n\nDummy video device (0x0000) (platform:v4l2loopback-000):\n        /dev/video3\n</code></pre> Determine your desired video device and edit the source device in vila_live.yaml</p>","tags":["Large Multimodal Model","Large Vision Model"]},{"location":"applications/vila_live/#build-and-run-instructions","title":"\ud83d\ude80 Build and Run Instructions","text":"<p>From the Holohub main directory run the following command: <pre><code>./dev_container build_and_run vila_live\n</code></pre> Note: The first build will take ~1.5 hours if you're on ARM64. This is largely due to building Flash Attention 2 since pre-built wheels are not distributed for ARM64 platforms.</p> <p>Once the main LMM-based app is running, you will see a link for the app at <code>http://127.0.0.1:8050</code>. To receive the video stream, please also ensure port 49000 is open.</p>","tags":["Large Multimodal Model","Large Vision Model"]},{"location":"applications/vila_live/#supported-hardware","title":"\ud83d\udcbb Supported Hardware","text":"<ul> <li>IGX w/ dGPU</li> <li>x86 w/ dGPU</li> <li>IGX w/ iGPU and Jetson AGX supported with workaround   There is a known issue running this application on IGX w/ iGPU and on Jetson AGX (see #500).   The workaround is to update the device to avoid picking up the libnvv4l2.so library.</li> </ul> <pre><code>cd /usr/lib/aarch64-linux-gnu/\nls -l libv4l2.so.0.0.999999\nsudo rm libv4l2.so.0.0.999999\nsudo ln -s libv4l2.so.0.0.0.0  libv4l2.so.0.0.999999\n</code></pre>","tags":["Large Multimodal Model","Large Vision Model"]},{"location":"applications/vila_live/#video-options","title":"\ud83d\udcf7\u2699\ufe0f Video Options","text":"<p>There are three options to ingest video data.</p> <ol> <li>use a physical device or capture card, such as a v4l2 device as described in the Setup Instructions. Make sure the vila_live.yaml contains the v4l2_source group and specifies the device correctly (<code>pixel_format</code> may be tuned accordingly, e.g. <code>pixel_format: \"auto\"</code>).</li> <li> <p>convert a video file to a gxf-compatible format using the convert_video_to_gxf_entities.py script. See the yolo_model_deployment application for a detailed example. When using the replayer, configure the replayer_source in the yaml file and launch the application with:     <pre><code>./run_vila_live.sh --source \"replayer\"\n</code></pre> This application downloads a pre-recorded video from Pexels when the application is built.  Please review the license terms from Pexels.</p> </li> <li> <p>create a virtual video device, that mounts a video file and replays it, as detailed in the v4l2_camera examples in holoscan-sdk. This approach may require signing the v4l2loopback kernel module, when using a system with secure-boot enabled. Make sure the vila_live.yaml contains the v4l2_source group and specifies the virtual device correctly. replay the video, using for example:     <pre><code>ffmpeg -stream_loop -1 -re -i &lt;your_video_path&gt; -pix_fmt yuyv422 -f v4l2 /dev/video3\n</code></pre></p> </li> </ol>","tags":["Large Multimodal Model","Large Vision Model"]},{"location":"applications/vila_live/#acknowledgements","title":"\ud83d\ude4c Acknowledgements","text":"<ul> <li>Jetson AI Lab, Live LLaVA: for the inspiration to create this app</li> <li>Jetson-Containers repo: For the Flask web-app with WebSockets</li> <li>LLM-AWQ repo: For the example code to create AWQ-powered LLM servers</li> </ul>","tags":["Large Multimodal Model","Large Vision Model"]},{"location":"applications/volume_rendering_xr/","title":"Medical Image Viewer in XR","text":"<p> Authors: Andreas Heumann (NVIDIA), Connor Smith (NVIDIA), Cristiana Dinea (NVIDIA), Tom Birdsong (NVIDIA), Antonio Ospite (Magic Leap), Jiwen Cai (Magic Leap), Jochen Stier (Magic Leap), Korcan Hussein (Magic Leap), Robbie Bridgewater (Magic Leap) Supported platforms: amd64, arm64 Last modified: January 16, 2025 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p></p>","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/volume_rendering_xr/#description","title":"Description","text":"<p>We collaborated with Magic Leap on a proof of concept mixed reality viewer for medical imagery built on the Holoscan platform.</p> <p>Medical imagery is one of the fastest-growing sources of data in any industry. When we think about typical diagnostic imaging, X-ray, CT scans, and MRIs come to mind. X-rays are 2D images, so viewing them on a lightbox or, if they\u2019re digital, a computer, is fine. But CT scans and MRIs are 3D. They\u2019re incredibly important technologies, but our way of interacting with them is flawed. This technology helps physicians in so many ways, from training and education to making more accurate diagnoses and ultimately to planning and even delivering more effective treatments.</p> <p>You can use this viewer to visualize a segmented medical volume with a mixed reality device.</p>","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/volume_rendering_xr/#prerequisites","title":"Prerequisites","text":"","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/volume_rendering_xr/#host-machine","title":"Host Machine","text":"<p>Review the HoloHub README document for supported platforms and software requirements.</p> <p>The application supports x86_64 or IGX dGPU platforms. IGX iGPU, AGX, and RHEL platforms are not fully tested at this time.</p>","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/volume_rendering_xr/#magic-leap-2-device","title":"Magic Leap 2 Device","text":"<p>The following packages and applications are required to run remote rendering with a Magic Leap 2 device:</p> Requirement Platform Version Source Magic Leap Hub Windows or macOS PC latest Magic Leap Website Headset Firmware Magic Leap 2 v1.6.0 Magic Leap Hub Headset Remote Rendering Viewer (.apk) Magic Leap 2 1.11.64 Magic Leap Download Link Windrunner OpenXR Backend HoloHub Container 1.11.74 Included in Container Magic Leap 2 Pro License Magic Leap <p>Refer to the Magic Leap 2 documentation for more information: - Updating your device with Magic Leap Hub; - Installing <code>.apk</code> packages with Magic Leap Hub</p>","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/volume_rendering_xr/#quick-start","title":"Quick Start","text":"<p>Run the following command in the top-level HoloHub folder to build and run the host application:</p> <pre><code>./dev_container build_and_run volume_rendering_xr\n</code></pre> <p>A QR code will be visible in the console log. Refer to Magic Leap 2 Remote Rendering Setup documentation to pair the host and device in preparation for remote viewing. Refer to the Remote Viewer section to regenerate the QR code as needed, or to use the local debugger GUI in place of a physical device.</p> <p>The application supports the following hand or controller interactions by default: - Translate: Reach and grab inside the volume with your hand or with the controller trigger to move the volume. - Scale: Grab any face of the bounding box and move your hand or controller to scale the volume. - Rotate: Grab any edge of the bounding box and move your hand or controller to rotate the volume. - Crop: Grab any vertex of the bounding box and move your hand or controller to translate the cropping planes.</p>","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/volume_rendering_xr/#advanced-setup","title":"Advanced Setup","text":"<p>You can use the <code>--dryrun</code> option to see the individual commands run by the quick start option above: <pre><code>./dev_container build_and_run volume_rendering_xr --dryrun\n</code></pre></p> <p>Alternatively, follow the steps below to set up the interactive container session.</p>","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/volume_rendering_xr/#build-the-container","title":"Build the Container","text":"<p>Run the following commands to build and enter the interactive container environment: <pre><code>./dev_container build --img holohub:volume_rendering_xr --docker_file ./applications/volume_rendering_xr/Dockerfile # Build the dev container\n./dev_container launch --img holohub:volume_rendering_xr # Launch the container\n</code></pre></p>","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/volume_rendering_xr/#build-the-application","title":"Build the Application","text":"<p>Inside the container environment, build the application: <pre><code>./run build volume_rendering_xr # Build the application\n</code></pre></p>","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/volume_rendering_xr/#run-the-application","title":"Run the Application","text":"<p>Inside the container environment, start the application: <pre><code>export ML_START_OPTIONS=&lt;\"\"/\"debug\"&gt; # Defaults to \"debug\" to run XR device simulator GUI\n./run launch volume_rendering_xr\n</code></pre></p>","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/volume_rendering_xr/#deploying-as-a-standalone-application","title":"Deploying as a Standalone Application","text":"<p><code>volume_rendering_xr</code> can be packaged in a self-contained release container with datasets and binaries.</p> <p>To build the release container: <pre><code># Generate HoloHub `volume_rendering_xr` installation in the \"holohub/install\" folder\n./dev_container launch --img holohub:volume_rendering_xr -c ./run build volume_rendering_xr --configure-args \"-DCMAKE_INSTALL_PREFIX:PATH=/workspace/holohub/install\"\n./dev_container launch --img holohub:volume_rendering_xr -c cmake --build ./build --target install\n\n# Copy files into a release container\n./dev_container build --img holohub:volume_rendering_xr_rel --docker_file ./applications/volume_rendering_xr/scripts/Dockerfile.rel --base_img nvcr.io/nvidia/cuda:12.4.1-runtime-ubuntu22.04\n</code></pre></p> <p>To run the release container, first create the container startup script: <pre><code>docker run --rm holohub:volume_rendering_xr_rel &gt; ./render-volume-xr\nchmod +x ./render-volume-xr\n</code></pre></p> <p>Then execute the script to start the Windrunner service and the app: <pre><code>./render-volume-xr\n</code></pre></p> <p>For more options, e.g. list available datasets or to select a different dataset, type <pre><code>./render-volume-xr --help\n</code></pre></p> <p>Options not recognized by the render-volume-xr script are forwarded to the application.</p>","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/volume_rendering_xr/#additional-notes","title":"Additional Notes","text":"","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/volume_rendering_xr/#supported-formats","title":"Supported Formats","text":"<p>This application loads static volume files from the local disk. See HoloHub <code>VolumeLoaderOp</code> documentation for supported volume formats and file conversion tools.</p>","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/volume_rendering_xr/#launch-options","title":"Launch Options","text":"<p>Use the <code>--extra-args</code> to see all options, including how to specify a different dataset or configuration file to use. <pre><code>./run launch volume_rendering_xr --extra_args --help\n...\nHoloscan OpenXR volume renderer.Usage: /workspace/holohub/build/applications/volume_rendering_xr/volume_rendering_xr [options]\nOptions:\n  -h, --help                            Display this information\n  -c &lt;FILENAME&gt;, --config &lt;FILENAME&gt;    Name of the renderer JSON configuration file to load (default '/workspace/holoscan-openxr/data/volume_rendering/config.json')\n  -d &lt;FILENAME&gt;, --density &lt;FILENAME&gt;   Name of density volume file to load (default '/workspace/holoscan-openxr/data/volume_rendering/highResCT.mhd')\n  -m &lt;FILENAME&gt;, --mask &lt;FILENAME&gt;      Name of mask volume file to load (default '/workspace/holoscan-openxr/data/volume_rendering/smoothmasks.seg.mhd')\n</code></pre></p> <p>To use a new dataset with the application, mount its volume location from the host machine when launching the container and pass all required arguments explicitly to the executable: <pre><code>./dev_container launch --as_root --img holohub:openxr-dev --add-volume /host/path/to/data-dir\n&gt;&gt;&gt; ./build/applications/volume_rendering_xr/volume_rendering_xr \\\n      -c /workspace/holohub/data/volume_rendering/config.json \\\n      -d /workspace/volumes/path/to/data-dir/dataset.nii.gz \\\n      -m /workspace/volumes/path/to/data-dir/dataset.seg.nii.gz\n</code></pre></p>","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/volume_rendering_xr/#starting-the-magic-leap-openxr-runtime","title":"Starting the Magic Leap OpenXR runtime","text":"<p>OpenXR runtimes are implementations of the OpenXR API that allow the Holoscan XR operators to create XR sessions and render content. The Magic Leap OpenXR runtime including a CLI are by default installed in the dev container. From a terminal inside the dev container you can execute the following scripts:</p> <p><pre><code>ml_start.sh\n</code></pre> starts the OpenXR runtime service. After executing this command, the remote viewer on the Magic Leap device should connect to this runtime service. If not, then you still have to pair the device with the host computer running the Holoscan application.</p> <p>For rapid iteration without a Magic Leap device, pass the argument <code>debug</code> to <code>ml_start.sh</code> i.e. <pre><code>ml_start.sh debug\n</code></pre> This will enable a debug view on your computer showing what the headset would see. You may click into this window and navigate with the keyboard and mouse to manipulate the virtual head position.</p> <p>If you connect an ML2 while the debug view is active, you can continue to view the content on the debug view but can no longer adjust the virtual position, as the real position is used instead.</p> <p><pre><code>ml_pair.sh\n</code></pre> displays a QR code used to pair the device with the host. Start the QR code reader App on the device and scan the QR code displayed in the terminal. Note that the OpenXR runtime has to have been started using the ml_start command in order for the paring script to execute correctly.</p> <p><pre><code>ml_stop.sh\n</code></pre> stops the OpenXR runtime service.</p>","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/volume_rendering_xr/#starting-the-magic-leap-2-remote-viewer","title":"Starting the Magic Leap 2 Remote Viewer","text":"<p>When using a Magic Leap 2 device for the first time or after a software upgrade, the device must be provided with the IP address of the host running the OpenXR runtime. From a terminal inside the dev container run the</p> <pre><code>ml_pair.sh\n</code></pre> <p>command, which will bring up a QR code that has to be scanned using the QR Code App on the Magic Leap 2 device. Once paired with the host, the device  will automatically start the remote viewer which will then prompt you to start an OpenXR application on the host. Any time thereafter, start the remote viewer via the App menu.</p>","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/volume_rendering_xr/#developing-with-a-different-openxr-backend","title":"Developing with a Different OpenXR Backend","text":"<p><code>volume_renderer_xr</code> is an OpenXR compatible application. The Magic Leap Remote Rendering runtime is installed in the application container by default, but a compatible runtime can be used if appropriate to your use case. See https://www.khronos.org/openxr/ for more information on conformant OpenXR runtimes.</p>","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/volume_rendering_xr/#volume-rendering","title":"Volume Rendering","text":"<p>The application carries out volume rendering via the HoloHub <code>volume_renderer</code> operator, which in turn wraps the NVIDIA ClaraViz rendering project. ClaraViz JSON configurations provided in the config folder are available for specifying default scene parameters.</p> <p>See <code>volume_renderer</code> Configuration section for details on manipulating configuration values, along with how to create a new configuration file to fit custom data.</p>","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/volume_rendering_xr/#troubleshooting","title":"Troubleshooting","text":"<p>Please verify that you are building from the latest HoloHub <code>main</code> branch before reviewing troubleshooting steps.</p> <pre><code>git checkout main\n</code></pre>","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/volume_rendering_xr/#libraries-are-missing-when-building-the-application-vulkan-openxr-etc","title":"Libraries are missing when building the application (Vulkan, OpenXR, etc)","text":"<p>This error may indicate that you are building inside the default HoloHub container instead of the expected <code>volume_rendering_xr</code> container. Review the build steps and ensure that you have launched the container with the appropriate <code>dev_container --img</code> option.</p>","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/volume_rendering_xr/#unexpected-cmake-errors","title":"Unexpected CMake errors","text":"<p>You may need to clear your CMake build cache. See the HoloHub Cleaning section for instructions.</p>","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/volume_rendering_xr/#seccomp-errors","title":"\"Seccomp\" Errors","text":"<p>The Magic Leap Windrunner OpenXR backend and remote rendering host application use seccomp to limit syscalls on Linux platforms. You can exempt individual syscalls for local development by adding them to the application syscall whitelist.</p>","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/volume_rendering_xr/#debug-gui-does-not-appear","title":"Debug GUI does not appear","text":"<p>The <code>./run launch volume_rendering_xr</code> command initializes the Magic Leap Windrunner debug GUI by default. If you do not see the debug GUI appear in your application, or if the application appears to stall with no further output after the pairing QR code appears, try any of the following:</p> <ol> <li> <p>Manually set the <code>ML_START_OPTIONS</code> environment variable so that <code>run launch</code> initializes with the debug view: <pre><code>export ML_START_OPTIONS=\"debug\"\n</code></pre></p> </li> <li> <p>Follow Advanced Setup Instructions and add the <code>--as_root</code> option to launch the container with root permissions. <pre><code>./dev_container launch --img holohub:volume_rendering_xr --as_root\n</code></pre></p> </li> <li> <p>Clear the build cache and any home cache folders in the HoloHub workspace. <pre><code>./run clear_cache\nrm -rf .cache/ .cmake/ .config/ .local/\n</code></pre></p> </li> </ol>","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/vpi_stereo/","title":"VPI Stereo Vision","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 25, 2025 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 2.4.0 Tested Holoscan SDK versions: 2.4.0, 2.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p> </p>","tags":["Endoscopy","Stereo"]},{"location":"applications/vpi_stereo/#overview","title":"Overview","text":"<p>Demo pipeline showing stereo disparity estimation using the Vision Programming Interface VPI.</p>","tags":["Endoscopy","Stereo"]},{"location":"applications/vpi_stereo/#description","title":"Description","text":"<p>This pipeline takes video from a stereo camera and uses VPI's stereo disparity estimation algorithm. The input video and estimate disparity map are displayed using Holoviz.</p> <p>The application will select accelerator backends if available (OFA, PVA and VIC). This demonstrates how VPI can be used to offload stereo disparity processing from the GPU on supported devices such as NVIDIA IGX, AGX, or NX platforms.</p>","tags":["Endoscopy","Stereo"]},{"location":"applications/vpi_stereo/#input-video","title":"Input Video","text":"<p>Requires a V4L2 stereo camera, or recorded stereo video, and matching calibration data. By default, the application will share the same source data as stereo_vision.</p>","tags":["Endoscopy","Stereo"]},{"location":"applications/vpi_stereo/#requirements","title":"Requirements","text":"<p>This demo requires VPI version 3.2 or greater. The included Dockerfile will install VPI and its dependencies for either an amd64 target (with discrete NVIDIA GPU), or arm64 target (NVIDIA IGX, AGX, or NX).</p>","tags":["Endoscopy","Stereo"]},{"location":"applications/vpi_stereo/#build-and-run-instructions","title":"Build and Run Instructions","text":"<p>Using default video source (same as stereo_vision application): <pre><code>./dev_container build_and_run vpi_stereo\n</code></pre> Using a v4l2 video source (live camera or loopback device): <pre><code>./dev_container build_and_run vpi_stereo --run_args \"--source v4l2\"\n</code></pre></p>","tags":["Endoscopy","Stereo"]},{"location":"applications/webrtc_holoviz_server/","title":"WebRTC Holoviz Server","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: March 15, 2024 Language: Python Latest version: 1.0.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p></p> <p>This app generates video frames with user specified content using Holoviz and sends it to a browser using WebRTC. The goal is to show how to remote control operators and view the output of a Holoscan pipeline.</p> <p>The app starts a web server, the pipeline starts when a browser is connected to the web server and the <code>Start</code> button is pressed. The pipeline stops when the <code>Stop</code> button is pressed.</p> <p>The web page has user inputs for specifying text and for the speed the text moves across the screen.</p> <pre><code>flowchart LR\n    subgraph Server\n        GeometryGenerationOp --&gt; HolovizOp\n        HolovizOp --&gt; FormatConverterOp\n        FormatConverterOp --&gt; WebRTCServerOp\n        WebServer\n    end\n    subgraph Client\n        WebServer &lt;--&gt; Browser\n        WebRTCServerOp &lt;--&gt; Browser\n    end\n</code></pre> <p>NOTE: When using VPN there might be a delay of several seconds between pressing the <code>Start</code> button and the first video frames are display. The reason for this is that the STUN server <code>stun.l.google.com:19302</code> used by default might not be available when VPN is active and the missing support for Trickle ICE in the used aiortc library. Trickle ICE is an optimization to speed up connection establishment. Normally, possible connections paths are tested one after another. If connections time out this is blocking the whole process. Trickle ICE checks each possible connection path in parallel so the connection timing out won't block the process.</p>","tags":["WebRTC","Server","Holoviz"]},{"location":"applications/webrtc_holoviz_server/#prerequisites","title":"Prerequisites","text":"<p>The app is using AIOHTTP for the web server and AIORTC for WebRTC. Install both using pip.</p> <pre><code>pip install aiohttp aiortc\n</code></pre>","tags":["WebRTC","Server","Holoviz"]},{"location":"applications/webrtc_holoviz_server/#run-instructions","title":"Run Instructions","text":"<p>Run the command:</p> <pre><code>./run launch webrtc_holoviz_server\n</code></pre> <p>On the same machine open a browser and connect to <code>127.0.0.1:8080</code>. You can also connect from a different machine by connecting to the IP address the app is running on.</p> <p>Press the <code>Start</code> button. Video frames are displayed. To stop, press the <code>Stop</code> button. Pressing <code>Start</code> again will continue the video.</p> <p>Change the text input and the speed slider to control the generated video frame content.</p>","tags":["WebRTC","Server","Holoviz"]},{"location":"applications/webrtc_holoviz_server/#command-line-arguments","title":"Command Line Arguments","text":"<pre><code>usage: webrtc_server.py [-h] [--cert-file CERT_FILE] [--key-file KEY_FILE] [--host HOST] [--port PORT] [--verbose VERBOSE] [--ice-server ICE_SERVER]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --cert-file CERT_FILE\n                        SSL certificate file (for HTTPS)\n  --key-file KEY_FILE   SSL key file (for HTTPS)\n  --host HOST           Host for HTTP server (default: 0.0.0.0)\n  --port PORT           Port for HTTP server (default: 8080)\n  --verbose, -v\n  --ice-server ICE_SERVER\n                        ICE server config in the form of `turn:&lt;ip&gt;:&lt;port&gt;[&lt;username&gt;:&lt;password&gt;]` or `stun:&lt;ip&gt;:&lt;port&gt;`. This option can be specified multiple times to add multiple ICE servers.\n</code></pre>","tags":["WebRTC","Server","Holoviz"]},{"location":"applications/webrtc_holoviz_server/#running-with-turn-server","title":"Running With TURN server","text":"<p>A TURN server may be needed if you're running in a containerized environment without host networking (e.g. Kubernetes or Docker). Here are some basic steps to run this example with a TURN server.</p> <p>Run the TURN server in the same machine that you're running the app on.</p> <p>Note: It is strongly recommended to run the TURN server with docker network=host for best performance</p> <pre><code># This is the external IP address of the machine running the TURN server\nexport TURN_SERVER_EXTERNAL_IP=\"&lt;ip&gt;\"\n\n# Command below use admin:admin as the username and password as an example\ndocker run -d --rm --network=host instrumentisto/coturn \\\n    -n --log-file=stdout \\\n    --external-ip=$TURN_SERVER_EXTERNAL_IP \\\n    --listening-ip=$TURN_SERVER_EXTERNAL_IP \\\n    --lt-cred-mech --fingerprint \\\n    --user=admin:admin \\\n    --no-multicast-peers \\\n    --verbose \\\n    --realm=default.realm.org\n</code></pre> <p>Then you can pass in the TURN server config into the app</p> <pre><code>python webrtc_server.py --ice-server \"turn:&lt;ip&gt;:3478[admin:admin]\"\n</code></pre> <p>This will enable you to access the webRTC browser application from different machines.</p>","tags":["WebRTC","Server","Holoviz"]},{"location":"applications/webrtc_video_client/","title":"WebRTC Video Client","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: November 16, 2023 Language: Python Latest version: 1.0.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p></p> <p>This app receives video frames from a web cam connected to a browser and display them on the screen.</p> <p>The app starts a web server, the pipeline starts when a browser is connected to the web server and the <code>Start</code> button is pressed. The pipeline stops when the <code>Stop</code> button is pressed.</p> <p>The video resolution and video codec can be selected in browser.</p> <pre><code>flowchart LR\n    subgraph Server\n        WebRTCClientOp --&gt; HolovizOp\n        WebServer\n    end\n    subgraph Client\n        Webcam --&gt; Browser\n        Browser &lt;--&gt; WebRTCClientOp\n        Browser &lt;--&gt; WebServer\n    end\n</code></pre> <p>NOTE: When using VPN there might be a delay of several seconds between pressing the <code>Start</code> button and the first video frames are display. The reason for this is that the STUN server <code>stun.l.google.com:19302</code> used by default might not be available when VPN is active and the missing support for Trickle ICE in the used aiortc library. Trickle ICE is an optimization to speed up connection establishment. Normally, possible connections paths are tested one after another. If connections time out this is blocking the whole process. Trickle ICE checks each possible connection path in parallel so the connection timing out won't block the process.</p>","tags":["WebRTC","Client","Video"]},{"location":"applications/webrtc_video_client/#prerequisites","title":"Prerequisites","text":"<p>The app is using AIOHTTP for the web server and AIORTC for WebRTC. Install both using pip.</p> <pre><code>pip install aiohttp aiortc\n</code></pre>","tags":["WebRTC","Client","Video"]},{"location":"applications/webrtc_video_client/#run-instructions","title":"Run Instructions","text":"<p>Run the command:</p> <pre><code>./run launch webrtc_video_client\n</code></pre> <p>On the same machine open a browser and connect to <code>127.0.0.1:8080</code>.</p> <p>Select the video resolution and codec or keep the defaults.</p> <p>Press the <code>Start</code> button. Video frames are displayed. To stop, press the <code>Stop</code> button. Pressing <code>Start</code> again will continue the video.</p> <p>You can also connect from a different machine by connecting to the IP address the app is running on. Chrome disables features such as getUserMedia when it comes from an unsecured origin. <code>http://localhost</code> is considered as a secure origin by default, however if you use an origin that does not have an SSL/TLS certificate then Chrome will consider the origin as unsecured and disable getUserMedia.</p> <p>Solutions</p> <ul> <li>Create an self-signed SSL/TLS certificate with <code>openssl req -new -newkey rsa:4096 -x509 -sha256 -days 365 -nodes -out MyCertificate.crt -keyout MyKey.key</code>. Pass the generated files to the <code>webrtc_client</code> using the <code>--cert-file</code> and <code>--key-file</code> arguments. Connect the browser to <code>https://{YOUR HOST IP}:8080</code>.</li> <li>Go to chrome://flags, search for the flag <code>unsafely-treat-insecure-origin-as-secure</code>, enter the origin you want to treat as secure such as <code>http://{YOUR HOST IP}:8080</code>, enable the feature and relaunch the browser.</li> </ul>","tags":["WebRTC","Client","Video"]},{"location":"applications/webrtc_video_client/#command-line-arguments","title":"Command Line Arguments","text":"<pre><code>usage: webrtc_client.py [-h] [--cert-file CERT_FILE] [--key-file KEY_FILE] [--host HOST] [--port PORT] [--verbose VERBOSE]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --cert-file CERT_FILE\n                        SSL certificate file (for HTTPS)\n  --key-file KEY_FILE   SSL key file (for HTTPS)\n  --host HOST           Host for HTTP server (default: 0.0.0.0)\n  --port PORT           Port for HTTP server (default: 8080)\n  --verbose, -v\n</code></pre>","tags":["WebRTC","Client","Video"]},{"location":"applications/webrtc_video_server/","title":"WebRTC Video Server","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: January 16, 2025 Language: Python Latest version: 1.0.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p></p> <p>This app reads video frames from a file and sends it to a browser using WebRTC.</p> <p>The app starts a web server, the pipeline starts when a browser is connected to the web server and the <code>Start</code> button is pressed. The pipeline stops when the <code>Stop</code> button is pressed.</p> <pre><code>flowchart LR\n    subgraph Server\n        A[(VideoFile)] --&gt; VideoStreamReplayerOp\n        VideoStreamReplayerOp --&gt; FormatConverterOp\n        FormatConverterOp --&gt; WebRTCServerOp\n        WebServer\n    end\n    subgraph Client\n        WebServer &lt;--&gt; Browser\n        WebRTCServerOp &lt;--&gt; Browser\n    end\n</code></pre> <p>NOTE: When using VPN there might be a delay of several seconds between pressing the <code>Start</code> button and the first video frames are display. The reason for this is that the STUN server <code>stun.l.google.com:19302</code> used by default might not be available when VPN is active and the missing support for Trickle ICE in the used aiortc library. Trickle ICE is an optimization to speed up connection establishment. Normally, possible connections paths are tested one after another. If connections time out this is blocking the whole process. Trickle ICE checks each possible connection path in parallel so the connection timing out won't block the process.</p>","tags":["WebRTC","Server","Video"]},{"location":"applications/webrtc_video_server/#prerequisites","title":"Prerequisites","text":"<p>The app is using AIOHTTP for the web server and AIORTC for WebRTC. Install both using pip.</p> <pre><code>pip install aiohttp aiortc\n</code></pre>","tags":["WebRTC","Server","Video"]},{"location":"applications/webrtc_video_server/#run-instructions","title":"Run Instructions","text":"<p>Run the command:</p> <pre><code>./run launch webrtc_video_server\n</code></pre> <p>On the same machine open a browser and connect to <code>127.0.0.1:8080</code>. You can also connect from a different machine by connecting to the IP address the app is running on.</p> <p>Press the <code>Start</code> button. Video frames are displayed. To stop, press the <code>Stop</code> button. Pressing <code>Start</code> again will continue the video.</p>","tags":["WebRTC","Server","Video"]},{"location":"applications/webrtc_video_server/#command-line-arguments","title":"Command Line Arguments","text":"<pre><code>usage: webrtc_server.py [-h] [--cert-file CERT_FILE] [--key-file KEY_FILE] [--host HOST] [--port PORT] [--verbose] [--ice-server ICE_SERVER]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --cert-file CERT_FILE\n                        SSL certificate file (for HTTPS)\n  --key-file KEY_FILE   SSL key file (for HTTPS)\n  --host HOST           Host for HTTP server (default: 0.0.0.0)\n  --port PORT           Port for HTTP server (default: 8080)\n  --verbose, -v\n  --ice-server ICE_SERVER\n                        ICE server config in the form of `turn:&lt;ip&gt;:&lt;port&gt;[&lt;username&gt;:&lt;password&gt;]` or `stun:&lt;ip&gt;:&lt;port&gt;`. This option can be specified multiple times to add multiple ICE servers.\n</code></pre>","tags":["WebRTC","Server","Video"]},{"location":"applications/webrtc_video_server/#running-with-turn-server","title":"Running With TURN server","text":"<p>A TURN server may be needed if you're running in a containerized environment without host networking (e.g. Kubernetes or Docker). Here are some basic steps to run this example with a TURN server.</p> <p>Run the TURN server in the same machine that you're running the app on.</p> <p>Note: It is strongly recommended to run the TURN server with docker network=host for best performance</p> <pre><code># This is the external IP address of the machine running the TURN server\nexport TURN_SERVER_EXTERNAL_IP=\"&lt;ip&gt;\"\n\n# Command below use admin:admin as the username and password as an example\ndocker run -d --rm --network=host instrumentisto/coturn \\\n    -n --log-file=stdout \\\n    --external-ip=$TURN_SERVER_EXTERNAL_IP \\\n    --listening-ip=$TURN_SERVER_EXTERNAL_IP \\\n    --lt-cred-mech --fingerprint \\\n    --user=admin:admin \\\n    --no-multicast-peers \\\n    --verbose \\\n    --realm=default.realm.org\n</code></pre> <p>Then you can pass in the TURN server config into the app</p> <pre><code>python webrtc_server.py --ice-server \"turn:&lt;ip&gt;:3478[admin:admin]\"\n</code></pre> <p>This will enable you to access the webRTC browser application from different machines.</p>","tags":["WebRTC","Server","Video"]},{"location":"applications/xr_hello_holoscan/","title":"XR \"Hello Holoscan\"","text":"<p> Authors: Andreas Heumann (NVIDIA), Connor Smith (NVIDIA), Cristiana Dinea (NVIDIA), Tom Birdsong (NVIDIA), Antonio Ospite (Magic Leap), Jiwen Cai (Magic Leap), Jochen Stier (Magic Leap), Korcan Hussein (Magic Leap), Robbie Bridgewater (Magic Leap) Supported platforms: amd64, arm64 Last modified: November 4, 2024 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>This application provides a simple scene demonstrating mixed reality viewing with Holoscan SDK.</p> <p></p>","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/xr_hello_holoscan/#background","title":"Background","text":"<p>We created this test application as part of a collaboration between the Magic Leap and NVIDIA Holoscan teams. See the <code>volume_rendering_xr</code> application for a demonstration of medical viewing in XR with Holoscan SDK.</p>","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/xr_hello_holoscan/#description","title":"Description","text":"<p>The application provides a blueprint for how to set up a mixed reality scene for viewing with Holoscan SDK and HoloHub components.</p> <p>The mixed reality demonstration scene includes: - Static components such as scene axes and cube primitives; - A primitive overlay on the tracked controller input; - A static UI showcasing sensor inputs and tracking.</p>","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/xr_hello_holoscan/#getting-started","title":"Getting Started","text":"<p>Refer to the <code>volume_rendering_xr</code> README for details on hardware, firmware, and software prerequisites.</p> <p>To run the application, run the following command in the HoloHub folder on your host machine: <pre><code>./dev_container build_and_run xr_hello_holoscan\n</code></pre></p> <p>To pair your Magic Leap 2 device with the host, open the QR Reader application in the ML2 headset and scan the QR code printed in console output on the host machine.</p>","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/xr_hello_holoscan/#frequently-asked-questions","title":"Frequently Asked Questions","text":"","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/xr_hello_holoscan/#can-i-test-the-application-without-a-magic-leap-2-device","title":"Can I test the application without a Magic Leap 2 device?","text":"<p>Yes, a debug GUI not requiring a headset is installed inside the application container by default. Follow the steps below to launch the debug GUI and run the application:</p> <pre><code># Build and launch the container\n./dev_container build --img holohub:xr_hello_holoscan --docker_file ./applications/volume_rendering_xr/Dockerfile\n./dev_container launch --img holohub:xr_hello_holoscan\n\n# Build the application\n./run build xr_hello_holoscan\n\n# Launch the debug GUI and the application\nexport ML_START_OPTIONS=\"debug\"\n./run launch xr_hello_holoscan\n</code></pre> <p>The ImGui debug application will launch. Click and slide the position entries to adjust your view of the scene.</p> <p></p>","tags":["Volume","Rendering","OpenXR","Mixed","Reality","XR"]},{"location":"applications/yolo_model_deployment/","title":"Holoscan-Yolo","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 25, 2025 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 2 - Trusted This project is aiming to provide basic guidance to deploy Yolo-based model to Holoscan SDK as \"Bring Your Own Model\"</p>","tags":["Computer Vision","Yolo Detection"]},{"location":"applications/yolo_model_deployment/#model","title":"Model","text":"<ul> <li>Yolo v8 model: https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt</li> <li>Yolo v8 export repository: https://github.com/triple-Mu/YOLOv8-TensorRT</li> </ul> <p>In this application example, we use the YOLOv8s model, which is converted to ONNX format using the repository mentioned above. Note that if you provide your own ONNX model, ensure it includes the EfficientNMS_TRT layer. You can verify it using Netron. Additionally, we employ the <code>graph_surgeon.py</code> script to modify the input shape. For more details on this script, refer to graph_surgeonpy.</p> <p>The detailed process is documented in the <code>CMakeLists.txt</code> file.</p>","tags":["Computer Vision","Yolo Detection"]},{"location":"applications/yolo_model_deployment/#input-source","title":"Input Source","text":"<p>This app currently supports two input options:</p> <ol> <li>v4l2 compatible input device</li> <li>Pre-recorded video</li> </ol>","tags":["Computer Vision","Yolo Detection"]},{"location":"applications/yolo_model_deployment/#data","title":"Data","text":"<p>This application downloads a pre-recorded video from Pexels when the application is built.  Please review the license terms from Pexels.</p>","tags":["Computer Vision","Yolo Detection"]},{"location":"applications/yolo_model_deployment/#run","title":"Run","text":"<p>Build and launch container. Note that this will use a v4l2 input source as default.</p> <pre><code>./dev_container build_and_run yolo_model_deployment\n</code></pre>","tags":["Computer Vision","Yolo Detection"]},{"location":"applications/yolo_model_deployment/#video-replayer-support","title":"Video Replayer Support","text":"<p>If you don't have a v4l2 compatible device plugged in, you can also run this application on a pre-recorded video. To launch the application using the Video Stream Replayer as the input source, run:</p> <pre><code>./dev_container build_and_run yolo_model_deployment --run_args \"--source replayer\"\n</code></pre>","tags":["Computer Vision","Yolo Detection"]},{"location":"applications/yolo_model_deployment/#configuration","title":"Configuration","text":"<p>For application configuration, please refer to the <code>yolo_detection.yaml</code>.</p>","tags":["Computer Vision","Yolo Detection"]},{"location":"applications/adv_networking_bench/cpp/","title":"Advanced Networking Benchmark","text":"<p> Authors: Cliff Burdick (NVIDIA) Supported platforms: amd64, arm64 Last modified: March 13, 2025 Language: C++ Latest version: 1.3 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This is a simple application to measure a lower bound on performance for the advanced networking operator by receiving packets, optionally doing work on them, and freeing the buffers. While only freeing the packets is an unrealistic workload, it's useful to see at a high level whether the application is able to keep up with the bare minimum amount of work to do. The application contains both a transmitter and receiver that are designed to run on different systems, and may be configured independently.</p> <p>The performance of this application depends heavily on a properly-configured system and choosing the best tuning parameters that are acceptable for the workload. To configure the system please see the documentation for the advanced network operator. With the system tuned, the application performance will be dictated by batching size and whether GPUDirect is enabled.</p> <p>At this time both the transmitter and receiver are written to handle an Ethernet+IP+UDP packet with a configurable payload. Other modes may be added in the future. Also, for simplicity, the transmitter and receiver must be configured to a single packet size.</p>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"applications/adv_networking_bench/cpp/#transmit","title":"Transmit","text":"<p>The transmitter sends a UDP packet with an incrementing sequence of bytes after the UDP header. The batch size configured dictates how many packets the benchmark operator sends to the advanced network operator in each tick. Typically with the same number of CPU cores the transmitter will run faster than the receiver, so this parameter may be used to throttle the sender somewhat by making the batches very small.</p>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"applications/adv_networking_bench/cpp/#receiver","title":"Receiver","text":"<p>The receiver receives the UDP packets in either CPU-only mode or header-data split mode. CPU-only mode will receive the packets in CPU memory, copy the payload contents to a host-pinned staging buffer, and freed. In header-data split mode the user may configure a split point where the bytes before that point are sent to the CPU, and all bytes afterwards are sent to the GPU. Header-data split should achieve higher rates than CPU mode since the amount of data to the CPU can be orders of magnitude lower compared to running in CPU-only mode.</p>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"applications/adv_networking_bench/cpp/#configuration","title":"Configuration","text":"<p>The application is configured using a separate transmit and receive file. The transmit file is called <code>adv_networking_bench_tx.yaml</code> while the receive is named <code>adv_networking_bench_rx.yaml</code>. Configure the advanced networking operator on both transmit and receive per the instructions for that operator.</p>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"applications/adv_networking_bench/cpp/#receive-configuration","title":"Receive Configuration","text":"<ul> <li><code>header_data_split</code>: bool   Turn on GPUDirect header-data split mode</li> <li><code>batch_size</code>: integer   Size in packets for a single batch. This should be a multiple of the advanced network RX operator batch size.   A larger batch size consumes more memory since any work will not start unless this batch size is filled. Consider   reducing this value if errors are occurring.</li> <li><code>max_packet_size</code>: integer   Maximum packet size expected. This value includes all headers up to and including UDP.</li> </ul>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"applications/adv_networking_bench/cpp/#transmit-configuration","title":"Transmit Configuration","text":"<ul> <li><code>batch_size</code>: integer   Size in packets for a single batch. This batch size is used to send to the advanced network TX operator, and   will loop sending that many packets for each burst.</li> <li><code>payload_size</code>: integer   Size of the payload to send after all L2-L4 headers</li> </ul>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"applications/adv_networking_bench/cpp/#requirements","title":"Requirements","text":"<p>This application requires all configuration and requirements from the advanced network operator.</p>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"applications/adv_networking_bench/cpp/#build-instructions","title":"Build Instructions","text":"<p>Please refer to the top level Holohub README.md file for information on how to build this application.</p> <pre><code>./dev_container build_and_run adv_networking_bench\n</code></pre>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"applications/adv_networking_bench/cpp/#run-instructions","title":"Run Instructions","text":"<p>First, go in your <code>build</code> or <code>install</code> directory, then:</p> <pre><code>./build/applications/adv_networking_bench/cpp/adv_networking_bench adv_networking_bench_default_tx_rx.yaml\n</code></pre> <p>With DOCA:</p> <pre><code>./build/applications/adv_networking_bench/cpp/adv_networking_bench adv_networking_bench_gpunetio_tx_rx.yaml\n</code></pre> <p>With RIVERMAX RX:</p> <pre><code>./build/applications/adv_networking_bench/cpp/adv_networking_bench adv_networking_bench_rmax_rx.yaml\n</code></pre> <p>For Holoscan internal reasons (not related to the DOCA library), build the Advanced Network Operator with <code>RX_PERSISTENT_ENABLED</code> set to 1 MAY cause problems to this application on the receive (process) side (receive hangs). If you experience any issue on the receive side, please read carefully in the Advanced Network Operator README how to solve this problem.</p>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"applications/aja_video_capture/cpp/","title":"AJA Capture","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 18, 2025 Language: C++ Latest version: 3.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3, 2.0.0, 2.1.0, 2.2.0, 3.0.0 Contribution metric: Level 0 - Core Stable</p> <p>Minimal example to demonstrate the use of the aja source operator to capture device input and stream to holoviz operator.</p> <p>Visit the SDK User Guide to setup the AJA Card.</p>","tags":["Video","AJA"]},{"location":"applications/aja_video_capture/cpp/#quick-start","title":"Quick Start","text":"<pre><code>dev_container build_and_run aja_video_capture --language &lt;cpp/python&gt;\n</code></pre>","tags":["Video","AJA"]},{"location":"applications/aja_video_capture/cpp/#settings","title":"Settings","text":"<p>To evaluate the AJA example using alternative resolutions, you may modify the aja_capture.yaml configuration file as needed. For instance, to test a resolution format of 1280 x 720 at 60 Hz, you can specify the following parameters in the aja section of the configuration :</p> <pre><code>```bash\n  aja:\n    width: 1280\n    height: 720\n    framerate: 60\n```\n</code></pre>","tags":["Video","AJA"]},{"location":"applications/aja_video_capture/cpp/#migration-notes","title":"Migration Notes","text":"<p>Holoscan SDK AJA support is migrated from the core Holoscan SDK library to the HoloHub community repository in Holoscan SDK v3.0.0. Projects depending on AJA support should accordingly update include and linking paths to reference HoloHub.</p> <p>C++/CMake projects should update <code>holoscan::ops::aja</code> to <code>holoscan::aja</code></p> <p>Python projects should update <code>import holoscan.operators.AJASourceOp</code> to <code>import holohub.aja_source.AJASourceOp</code></p>","tags":["Video","AJA"]},{"location":"applications/aja_video_capture/python/","title":"AJA Capture","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 20, 2025 Language: Python Latest version: 3.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3, 2.0.0, 2.1.0, 2.2.0 Contribution metric: Level 0 - Core Stable</p> <p>Minimal example to demonstrate the use of the aja source operator to capture device input and stream to holoviz operator.</p> <p>Visit the SDK User Guide to setup the AJA Card.</p>","tags":["Video","AJA"]},{"location":"applications/aja_video_capture/python/#quick-start","title":"Quick Start","text":"<pre><code>dev_container build_and_run aja_video_capture --language &lt;cpp/python&gt;\n</code></pre>","tags":["Video","AJA"]},{"location":"applications/aja_video_capture/python/#settings","title":"Settings","text":"<p>To evaluate the AJA example using alternative resolutions, you may modify the aja_capture.yaml configuration file as needed. For instance, to test a resolution format of 1280 x 720 at 60 Hz, you can specify the following parameters in the aja section of the configuration :</p> <pre><code>```bash\n  aja:\n    width: 1280\n    height: 720\n    framerate: 60\n```\n</code></pre>","tags":["Video","AJA"]},{"location":"applications/aja_video_capture/python/#migration-notes","title":"Migration Notes","text":"<p>Holoscan SDK AJA support is migrated from the core Holoscan SDK library to the HoloHub community repository in Holoscan SDK v3.0.0. Projects depending on AJA support should accordingly update include and linking paths to reference HoloHub.</p> <p>C++/CMake projects should update <code>holoscan::ops::aja</code> to <code>holoscan::aja</code></p> <p>Python projects should update <code>import holoscan.operators.AJASourceOp</code> to <code>import holohub.aja_source.AJASourceOp</code></p>","tags":["Video","AJA"]},{"location":"applications/basic_networking_ping/cpp/","title":"Basic Networking Ping","text":"<p> Authors: Cliff Burdick (NVIDIA) Supported platforms: amd64, arm64 Last modified: March 10, 2025 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application takes the existing ping example that runs over Holoscan ports and instead uses the basic network operator to run over a UDP socket.</p> <p>The basic network operator allows users to send and receive UDP messages over a standard Linux socket. Separate transmit and receive operators are provided so they can run independently and better suit the needs of the application.</p>","tags":["Networking","Network","UDP","IP"]},{"location":"applications/basic_networking_ping/cpp/#configuration","title":"Configuration","text":"<p>The application is configured using the file basic_networking_ping_rx.yaml or basic_networking_ping_tx.yaml, where RX will receive packets and TX will transmit. Depending on how the machine is configured, the IP and UDP port likely need to be configured. All other settings do not need to be changed.</p> <p>Please refer to the basic network operator documentation for more configuration information.</p>","tags":["Networking","Network","UDP","IP"]},{"location":"applications/basic_networking_ping/cpp/#requirements","title":"Requirements","text":"<p>This application requires: 1. Linux</p>","tags":["Networking","Network","UDP","IP"]},{"location":"applications/basic_networking_ping/cpp/#quick-start","title":"Quick Start","text":"<p>Use the following to build and run the application:</p> <pre><code># Start the receiver\n./dev_container build_and_run basic_networking_ping --base_img holoscan-dev-container:main --language &lt;cpp|python&gt; --run_args basic_networking_ping_rx.yaml\n# Start the transmitter\n./dev_container build_and_run basic_networking_ping --base_img holoscan-dev-container:main --language &lt;cpp|python&gt; --run_args basic_networking_ping_tx.yaml\n</code></pre>","tags":["Networking","Network","UDP","IP"]},{"location":"applications/basic_networking_ping/cpp/#build-instructions","title":"Build Instructions","text":"<p>Please refer to the top level Holohub README.md file for information on how to build this application.</p>","tags":["Networking","Network","UDP","IP"]},{"location":"applications/basic_networking_ping/cpp/#run-instructions","title":"Run Instructions","text":"<p>Running the sample uses the standard HoloHub <code>run</code> script:</p> <pre><code># Start the receiver\n./run launch basic_networking_ping &lt;language&gt; --extra_args basic_networking_ping_rx.yaml\n# Start the transmitter\n./run launch basic_networking_ping &lt;language&gt; --extra_args basic_networking_ping_tx.yaml\n</code></pre> <p>Language can be either C++ or Python.</p>","tags":["Networking","Network","UDP","IP"]},{"location":"applications/basic_networking_ping/cpp/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./dev_container vscode\n</code></pre>","tags":["Networking","Network","UDP","IP"]},{"location":"applications/basic_networking_ping/cpp/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Networking","Network","UDP","IP"]},{"location":"applications/basic_networking_ping/cpp/#c","title":"C++","text":"<p>There are three launch profiles configured for this application:</p> <ol> <li>(gdb) basic_networking_ping/cpp RX: Launch Basic Networking Ping with the RX configurations.</li> <li>(gdb) basic_networking_ping/cpp TX: Launch Basic Networking Ping with the TX configurations.</li> <li>(compound) basic_networking_ping/cpp TX &amp; RX: Launch both 1 and 2 in parallel.    This launch profile launches the receiver follow by the transmitter.</li> </ol>","tags":["Networking","Network","UDP","IP"]},{"location":"applications/basic_networking_ping/cpp/#python","title":"Python","text":"<p>There are several launch profiles configured for this application:</p> <ol> <li>(debugpy) basic_networking_ping/python RX: Launch Basic Networking Ping with the RX configurations.    This launch profile enables debugging of Python code.</li> <li>(debugpy) basic_networking_ping/python TX: Launch Basic Networking Ping with the TX configurations.    This launch profile enables debugging of Python code.</li> <li>(pythoncpp) basic_networking_ping/python TX: Launch Basic Networking Ping with the RX configurations.    This launch profile enables debugging of Python and C++ code.</li> <li>(pythoncpp) basic_networking_ping/python TX: Launch Basic Networking Ping with the TX configurations.    This launch profile enables debugging of Python and C++ code.</li> <li>(compound) basic_networking_ping/python TX &amp; RX: Launch both 1 and 2 in parallel.    This launch profile launches the receiver follow by the transmitter.</li> </ol>","tags":["Networking","Network","UDP","IP"]},{"location":"applications/basic_networking_ping/python/","title":"Basic Networking Ping","text":"<p> Authors: Cliff Burdick (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 26, 2025 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0, 2.1.0, 2.2.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application takes the existing ping example that runs over Holoscan ports and instead uses the basic network operator to run over a UDP socket.</p> <p>The basic network operator allows users to send and receive UDP messages over a standard Linux socket. Separate transmit and receive operators are provided so they can run independently and better suit the needs of the application.</p>","tags":["Network","Networking","UDP","Ethernet","IP","TCP"]},{"location":"applications/basic_networking_ping/python/#configuration","title":"Configuration","text":"<p>The application is configured using the file basic_networking_ping_rx.yaml or basic_networking_ping_tx.yaml, where RX will receive packets and TX will transmit. Depending on how the machine is configured, the IP and UDP port likely need to be configured. All other settings do not need to be changed.</p> <p>Please refer to the basic network operator documentation for more configuration information.</p>","tags":["Network","Networking","UDP","Ethernet","IP","TCP"]},{"location":"applications/basic_networking_ping/python/#requirements","title":"Requirements","text":"<p>This application requires: 1. Linux</p>","tags":["Network","Networking","UDP","Ethernet","IP","TCP"]},{"location":"applications/basic_networking_ping/python/#quick-start","title":"Quick Start","text":"<p>Use the following to build and run the application:</p> <pre><code># Start the receiver\n./dev_container build_and_run basic_networking_ping --base_img holoscan-dev-container:main --language &lt;cpp|python&gt; --run_args basic_networking_ping_rx.yaml\n# Start the transmitter\n./dev_container build_and_run basic_networking_ping --base_img holoscan-dev-container:main --language &lt;cpp|python&gt; --run_args basic_networking_ping_tx.yaml\n</code></pre>","tags":["Network","Networking","UDP","Ethernet","IP","TCP"]},{"location":"applications/basic_networking_ping/python/#build-instructions","title":"Build Instructions","text":"<p>Please refer to the top level Holohub README.md file for information on how to build this application.</p>","tags":["Network","Networking","UDP","Ethernet","IP","TCP"]},{"location":"applications/basic_networking_ping/python/#run-instructions","title":"Run Instructions","text":"<p>Running the sample uses the standard HoloHub <code>run</code> script:</p> <pre><code># Start the receiver\n./run launch basic_networking_ping &lt;language&gt; --extra_args basic_networking_ping_rx.yaml\n# Start the transmitter\n./run launch basic_networking_ping &lt;language&gt; --extra_args basic_networking_ping_tx.yaml\n</code></pre> <p>Language can be either C++ or Python.</p>","tags":["Network","Networking","UDP","Ethernet","IP","TCP"]},{"location":"applications/basic_networking_ping/python/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./dev_container vscode\n</code></pre>","tags":["Network","Networking","UDP","Ethernet","IP","TCP"]},{"location":"applications/basic_networking_ping/python/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Network","Networking","UDP","Ethernet","IP","TCP"]},{"location":"applications/basic_networking_ping/python/#c","title":"C++","text":"<p>There are three launch profiles configured for this application:</p> <ol> <li>(gdb) basic_networking_ping/cpp RX: Launch Basic Networking Ping with the RX configurations.</li> <li>(gdb) basic_networking_ping/cpp TX: Launch Basic Networking Ping with the TX configurations.</li> <li>(compound) basic_networking_ping/cpp TX &amp; RX: Launch both 1 and 2 in parallel.    This launch profile launches the receiver follow by the transmitter.</li> </ol>","tags":["Network","Networking","UDP","Ethernet","IP","TCP"]},{"location":"applications/basic_networking_ping/python/#python","title":"Python","text":"<p>There are several launch profiles configured for this application:</p> <ol> <li>(debugpy) basic_networking_ping/python RX: Launch Basic Networking Ping with the RX configurations.    This launch profile enables debugging of Python code.</li> <li>(debugpy) basic_networking_ping/python TX: Launch Basic Networking Ping with the TX configurations.    This launch profile enables debugging of Python code.</li> <li>(pythoncpp) basic_networking_ping/python TX: Launch Basic Networking Ping with the RX configurations.    This launch profile enables debugging of Python and C++ code.</li> <li>(pythoncpp) basic_networking_ping/python TX: Launch Basic Networking Ping with the TX configurations.    This launch profile enables debugging of Python and C++ code.</li> <li>(compound) basic_networking_ping/python TX &amp; RX: Launch both 1 and 2 in parallel.    This launch profile launches the receiver follow by the transmitter.</li> </ol>","tags":["Network","Networking","UDP","Ethernet","IP","TCP"]},{"location":"applications/cvcuda_basic/cpp/","title":"Cpp","text":"<p>tags:  - CV-CUDA  - Computer Vision  - CV title: CV-CUDA: Basic Interoperability (C++)</p>"},{"location":"applications/cvcuda_basic/cpp/#simple-cv-cuda-application","title":"Simple CV-CUDA application","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: April 15, 2024 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates seamless interoperability between Holoscan tensors and CV-CUDA tensors. The image processing pipeline is just a simple flip of the video orientation.</p> <p>Note that the C++ version of this application currently requires extra code to handle conversion back and forth between CV-CUDA and Holoscan tensor types. On the Python side, the conversion is trivial due to the support for the DLPack Python specification in both CV-CUDA and Holoscan. We provide two operators to handle the interoperability between CVCUDA and Holoscan tensors.</p>"},{"location":"applications/cvcuda_basic/cpp/#using-the-docker-file","title":"Using the docker file","text":"<p>This application requires a compiled version of CV-CUDA. For simplicity a DockerFile is available. To generate the container run:</p> <pre><code>./dev_container build --docker_file ./applications/cvcuda_basic/Dockerfile\n</code></pre> <p>The C++ version of the application can then be built by launching this container and using the provided <code>run</code> script.</p> <pre><code>./dev_container launch\n./run build cvcuda_basic\n</code></pre>"},{"location":"applications/cvcuda_basic/cpp/#running-the-application","title":"Running the Application","text":"<p>This application uses the endoscopy dataset as an example. The <code>run build</code> command above will automatically download it. This application is then run inside the container.</p> <pre><code>./dev_container launch\n</code></pre> <p>The Python version of the simple CV-CUDA pipeline example can be run via <pre><code>python applications/cvcuda_basic/python/cvcuda_basic.py --data=/workspace/holohub/data/endoscopy\n</code></pre></p> <p>or using the run script</p> <pre><code>./run launch cvcuda_basic python\n</code></pre> <p>The C++ version of the simple CV-CUDA pipeline example can then be run via <pre><code>./build/applications/cvcuda_basic/cpp/cvcuda_basic --data=/workspace/holohub/data/endoscopy\n</code></pre></p> <p>or using the run script</p> <pre><code>./run launch cvcuda_basic cpp\n</code></pre>"},{"location":"applications/cvcuda_basic/cpp/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./dev_container vscode cvcuda_basic\n</code></pre>"},{"location":"applications/cvcuda_basic/cpp/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":""},{"location":"applications/cvcuda_basic/cpp/#c","title":"C++","text":"<p>Use the <code>**(gdb) cvcuda_basic/cpp**</code> launch profile configured for this application to debug the application.</p>"},{"location":"applications/cvcuda_basic/cpp/#python","title":"Python","text":"<p>There are two launch profiles configured for this Python application:</p> <ol> <li>(debugpy) cvcuda_basic/python: Launch cvcuda_basic using a launch profile that enables debugging of Python code.</li> <li>(pythoncpp) cvcuda_basic/python: Launch cvcuda_basic using a launch profile that enables debugging of Python and C++ code.</li> </ol>"},{"location":"applications/cvcuda_basic/python/","title":"Python","text":"<p>tags:  - CV-CUDA  - Computer Vision  - CV title: CV-CUDA: Basic Interoperability (Python)</p>"},{"location":"applications/cvcuda_basic/python/#simple-cv-cuda-application","title":"Simple CV-CUDA application","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: May 13, 2024 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates seamless interoperability between Holoscan tensors and CV-CUDA tensors. The image processing pipeline is just a simple flip of the video orientation.</p> <p>Note that the C++ version of this application currently requires extra code to handle conversion back and forth between CV-CUDA and Holoscan tensor types. On the Python side, the conversion is trivial due to the support for the DLPack Python specification in both CV-CUDA and Holoscan. We provide two operators to handle the interoperability between CVCUDA and Holoscan tensors.</p>"},{"location":"applications/cvcuda_basic/python/#using-the-docker-file","title":"Using the docker file","text":"<p>This application requires a compiled version of CV-CUDA. For simplicity a DockerFile is available. To generate the container run:</p> <pre><code>./dev_container build --docker_file ./applications/cvcuda_basic/Dockerfile\n</code></pre> <p>The C++ version of the application can then be built by launching this container and using the provided <code>run</code> script.</p> <pre><code>./dev_container launch\n./run build cvcuda_basic\n</code></pre>"},{"location":"applications/cvcuda_basic/python/#running-the-application","title":"Running the Application","text":"<p>This application uses the endoscopy dataset as an example. The <code>run build</code> command above will automatically download it. This application is then run inside the container.</p> <pre><code>./dev_container launch\n</code></pre> <p>The Python version of the simple CV-CUDA pipeline example can be run via <pre><code>python applications/cvcuda_basic/python/cvcuda_basic.py --data=/workspace/holohub/data/endoscopy\n</code></pre></p> <p>or using the run script</p> <pre><code>./run launch cvcuda_basic python\n</code></pre> <p>The C++ version of the simple CV-CUDA pipeline example can then be run via <pre><code>./build/applications/cvcuda_basic/cpp/cvcuda_basic --data=/workspace/holohub/data/endoscopy\n</code></pre></p> <p>or using the run script</p> <pre><code>./run launch cvcuda_basic cpp\n</code></pre>"},{"location":"applications/cvcuda_basic/python/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./dev_container vscode cvcuda_basic\n</code></pre>"},{"location":"applications/cvcuda_basic/python/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":""},{"location":"applications/cvcuda_basic/python/#c","title":"C++","text":"<p>Use the <code>**(gdb) cvcuda_basic/cpp**</code> launch profile configured for this application to debug the application.</p>"},{"location":"applications/cvcuda_basic/python/#python","title":"Python","text":"<p>There are two launch profiles configured for this Python application:</p> <ol> <li>(debugpy) cvcuda_basic/python: Launch cvcuda_basic using a launch profile that enables debugging of Python code.</li> <li>(pythoncpp) cvcuda_basic/python: Launch cvcuda_basic using a launch profile that enables debugging of Python and C++ code.</li> </ol>"},{"location":"applications/dds/dds_video/","title":"DDS Video Application","text":"<p> Authors: Ian Stewart (NVIDIA) Supported platforms: amd64, arm64 Last modified: March 3, 2025 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>The DDS Video application demonstrates how video frames can be written to or read from a DDS databus in order to provide flexible integration between Holoscan applications and other applications (using Holoscan or not) via DDS.</p> <p>The application can be run as either a publisher or as a subscriber. In either case, it will use the VideoFrame data topic registered by the <code>DDSVideoPublisherOp</code> or <code>DDSVideoSubscriberOp</code> operators in order to write or read the video frame data to/from the DDS databus, respectively.</p> <p>When run as a publisher, the source for the input video frames will come from an attached V4L2-compatible camera via the <code>V4L2VideoCaptureOp</code> operator.</p> <p>When run as a subscriber, the application will use Holoviz to render the received video frames to the display. In addition to the video stream, the subscriber application will also subscribe to the <code>Square</code>, <code>Circle</code>, and <code>Triangle</code> topics as used by the RTI Shapes Demo. Any shapes received by this subscriber will also be overlaid on top of the Holoviz output.</p> <p></p>","tags":["DDS","RTI Connext","Video","Shapes"]},{"location":"applications/dds/dds_video/#prerequisites","title":"Prerequisites","text":"<ul> <li>This application requires RTI Connext be installed and configured with a valid RTI Connext license prior to use. </li> <li>V4L2 capable device</li> </ul> <p>[!NOTE] Instructions below are based on the `.run' installer from RTI Connext. Refer to the Linux installation for details.</p>","tags":["DDS","RTI Connext","Video","Shapes"]},{"location":"applications/dds/dds_video/#quick-start","title":"Quick Start","text":"<pre><code># Start the publisher\n./dev_container build_and_run dds_video --container_args \"-v $HOME/rti_connext_dds-7.3.0:/opt/rti.com/rti_connext_dds-7.3.0/\" --run_args \"-p\"\n\n# Start the subscriber\n./dev_container build_and_run dds_video --container_args \"-v $HOME/rti_connext_dds-7.3.0:/opt/rti.com/rti_connext_dds-7.3.0/\" --run_args \"-s\"\n</code></pre>","tags":["DDS","RTI Connext","Video","Shapes"]},{"location":"applications/dds/dds_video/#building-the-application","title":"Building the Application","text":"<p>To build on an IGX devkit (using the <code>armv8</code> architecture), follow the instructions to build Connext DDS applications for embedded Arm targets up to, and including, step 5 (Installing Java and setting JREHOME).</p> <p>To build the application, the <code>RTI_CONNEXT_DDS_DIR</code> CMake variable must point to the installation path for RTI Connext. This can be done automatically by setting the <code>NDDSHOME</code> environment variable to the RTI Connext installation directory (such as when using the RTI <code>setenv</code> scripts), or manually at build time, e.g.:</p> <pre><code>$ ./run build dds_video --configure-args -DRTI_CONNEXT_DDS_DIR=~/rti/rti_connext_dds-7.3.0\n</code></pre>","tags":["DDS","RTI Connext","Video","Shapes"]},{"location":"applications/dds/dds_video/#building-with-a-container","title":"Building with a Container","text":"<p>Due to the license requirements of RTI Connext it is not currently supported to install RTI Connext into a development container. Instead, Connext should be installed onto the host as above and then the development container can be launched with the RTI Connext folder mounted at runtime. To do so, ensure that the <code>NDDSHOME</code> and <code>CONNEXTDDS_ARCH</code> environment variables are set (which can be done using the RTI <code>setenv</code> script) and use the following:</p> <pre><code># 1. Build the container\n./dev_container build --docker_file applications/dds/dds_video/Dockerfile\n# 2. Launch the container\n./dev_container launch --docker_opts \"-v $HOME/rti_connext_dds-7.3.0:/opt/rti.com/rti_connext_dds-7.3.0/\"\n# 3. Build the application\n./run build dds_video\n# Continue to the next section to run the application with the publisher. \n# Open a new terminal to repeat step #2 and launch a new container for the subscriber.\n</code></pre>","tags":["DDS","RTI Connext","Video","Shapes"]},{"location":"applications/dds/dds_video/#running-the-application","title":"Running the Application","text":"<p>Both a publisher and subscriber process must be launched to see the result of writing to and reading the video stream from DDS, respectively.</p> <p>To run the publisher process, use the <code>-p</code> option:</p> <pre><code>$ ./run launch dds_video --extra_args \"-p\"\n</code></pre> <p>To run the subscriber process, use the <code>-s</code> option:</p> <pre><code>$ ./run launch dds_video --extra_args \"-s\"\n</code></pre> <p>If running the application generates an error about <code>RTI Connext DDS No Source for License information</code>, ensure that the RTI Connext license has either been installed system-wide or the <code>NDDSHOME</code> environment variable has been set to point to your user's RTI Connext installation path.</p> <p>Note that these processes can be run on the same or different systems, so long as they are both discoverable by the other via RTI Connext. If the processes are run on different systems then they will communicate using UDPv4, for which optimizations have been defined in the default <code>qos_profiles.xml</code> file. These optimizations include increasing the buffer size used by RTI Connext for network sockets, and so the systems running the application must also be configured to increase their maximum send and receive socket buffer sizes. This can be done by running the <code>set_socket_buffer_sizes.sh</code> script within this directory:</p> <pre><code>$ ./set_socket_buffer_sizes.sh\n</code></pre> <p>For more details, see the RTI Connext Guide to Improve DDS Network Performance on Linux Systems</p> <p>The QoS profiles used by the application can also be modified by editing the <code>qos_profiles.xml</code> file in the application directory. For more information about modifying the QoS profiles, see the RTI Connext Basic QoS tutorial or the RTI Connext QoS Reference Guide.</p>","tags":["DDS","RTI Connext","Video","Shapes"]},{"location":"applications/dds/dds_video/#publishing-shapes-from-the-rti-shapes-demo","title":"Publishing Shapes from the RTI Shapes Demo","text":"<p>The RTI Shapes Demo can be used to publish shapes which are then read and overlaid onto the video stream by this application. However, the domain participant QoS used by this application is not compatible with the default DDS QoS settings, so the RTI Shapes Demo must be configured to use the QoS settings provided by this application.  To do this, follow these steps:</p> <ol> <li>Launch the RTI Shapes Demo</li> <li>Select <code>Controls</code>, then <code>Configuration</code> from the menu bar</li> <li>Click <code>Stop</code> to disable the default domain participant</li> <li>Click <code>Manage QoS</code></li> <li>Click <code>Add</code> then navigate to and select the <code>qos_profiles.xml</code> file in this    application's directory.</li> <li>Click <code>OK</code> to close the <code>Manage QoS</code> window.</li> <li>In the <code>Choose the profile</code> drop-down, select <code>HoloscanDDSTransport::SHMEM+LAN</code></li> <li>Click <code>Start</code> to join the domain.</li> </ol> <p>Once the Shapes Demo is running and has joined the domain of a running <code>dds_video</code> subscriber, shapes published by the application should be rendered on top of the subscriber's video stream.</p>","tags":["DDS","RTI Connext","Video","Shapes"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/","title":"H.264 Endoscopy Tool Tracking Application with gRPC","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: January 16, 2025 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 2.6.0 Tested Holoscan SDK versions: 2.6.0 Contribution metric: Level 0 - Core Stable</p> <p>This application demonstrates how to offload heavy workloads to a remote Holoscan application using gRPC.</p>","tags":["Endoscopy","Tracking","AJA","Video Decoding","Video Encoding"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#overview","title":"Overview","text":"<p>In this sample application, we divided the h.264 Endoscopy Tool Tracking application into a server and client application where the two communicate via gRPC.</p> <p>The client application reads a pre-recorded h.264 video file and streams the encoded video frames to the server application. The server application handles the heavy workloads of inferencing and post-processing of the video frames. It receives the video frames, processes each frame through the endoscopy tool tracking pipeline, and then streams the results to the client.</p> <p> h.264 Endoscopy Tool Tracking Application with gRPC</p> <p>From the diagram above, we can see that both the App Cloud (the server) and the App Edge (the client) are very similar to the standalone Endoscopy Tool Tracking application. This section will only describe the differences; for details on inference and post-processing, please refer to the link above.</p> <p>On the client side, the differences are the queues and the gRPC client. In the Video Input Fragment, we added the following: - Outgoing Requests operator (<code>GrpcClientRequestOp</code>): It converts the video frames (GXF entities) received from the Video Read Stream operator into <code>EntityRequest</code> protobuf messages and queues each frame in the Request Queue. - gRPC Service &amp; Client (<code>EntityClientService</code> &amp; <code>EntityClient</code>): The gRPC Service is responsible for controlling the life cycle of the gRPC client. The client connects to the remote gRPC server and then sends the requests found in the Request Queue. When it receives a response, it converts it into a GXF entity and queues it in the Response Queue. - Incoming Responses operator (<code>GrpcClientResponseOp</code>): This operator is configured with an <code>AsynchronousCondition</code> condition to check the availability of the Response Queue. When notified of available responses in the queue, it dequeues each item and emits each to the output port.</p> <p> Details of App Cloud</p> <p>The App Cloud (the server) application consists of a gRPC server and a few components for managing Holoscan applications. When the server receives a new remote procedure call in this sample application, it launches a new instance of the Endoscopy Tool Tracking application. This is facilitated by the <code>ApplicationFactory</code> used for application registration.</p> <p>Under the hood, the Endoscopy Tool Tracking application here inherits a custom base class (<code>HoloscanGrpcApplication</code>) which manages the <code>Request Queue</code> and the <code>Response Queue</code> as well as the <code>GrpcServerRequestOp</code> and <code>GrpcServerResponseOp</code> operators for receiving requests and serving results, respectively. When the RPC is complete, the instance of the Endoscopy Tool Tracking application is destroyed and ready to serve the subsequent request.</p>","tags":["Endoscopy","Tracking","AJA","Video Decoding","Video Encoding"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#requirements","title":"Requirements","text":"<p>This application is configured to use H.264 elementary stream from endoscopy sample data as input.</p>","tags":["Endoscopy","Tracking","AJA","Video Decoding","Video Encoding"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded when building the application.</p>","tags":["Endoscopy","Tracking","AJA","Video Decoding","Video Encoding"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#building-and-running-grpc-h264-endoscopy-tool-tracking-application","title":"Building and Running gRPC H.264 Endoscopy Tool Tracking Application","text":"<ul> <li>Building and running the application from the top level Holohub directory:</li> </ul>","tags":["Endoscopy","Tracking","AJA","Video Decoding","Video Encoding"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#c","title":"C++","text":"<pre><code># Start the gRPC Server\n./dev_container build_and_run grpc_h264_endoscopy_tool_tracking --run_args cloud [--language cpp]\n\n# Start the gRPC Client\n./dev_container build_and_run grpc_h264_endoscopy_tool_tracking --run_args edge [--language cpp]\n</code></pre> <p>Important: on aarch64, applications also need tegra folder mounted inside the container and the <code>LD_LIBRARY_PATH</code> environment variable should be updated to include tegra folder path.</p> <p>Open and edit the Dockerfile and uncomment line 66:</p> <pre><code># Uncomment the following line for aarch64 support\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/aarch64-linux-gnu/tegra/\n</code></pre>","tags":["Endoscopy","Tracking","AJA","Video Decoding","Video Encoding"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./dev_container vscode h264\n</code></pre>","tags":["Endoscopy","Tracking","AJA","Video Decoding","Video Encoding"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Endoscopy","Tracking","AJA","Video Decoding","Video Encoding"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#c_1","title":"C++","text":"<p>The following launch profiles are available:</p> <ul> <li>(compound) grpc_h264_endoscopy_tool_tracking/cpp (cloud &amp; edge): Launch both the gRPC server and the client.</li> <li>(gdb) grpc_h264_endoscopy_tool_tracking/cpp (cloud): Launch the gRPC server.</li> <li>(gdb) grpc_h264_endoscopy_tool_tracking/cpp (edge): Launch the gRPC client.</li> </ul>","tags":["Endoscopy","Tracking","AJA","Video Decoding","Video Encoding"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#limitations-known-issues","title":"Limitations &amp; Known Issues","text":"<ul> <li>The connection between the server and the client is controlled by <code>rpc_timeout</code>. If no data is received or sent within the configured time, it assumes the call has been completed and hangs up. The <code>rpc_timeout</code> value can be configured in the endoscopy_tool_tracking.yaml file with a default of 5 seconds. Increasing this value may help on a slow network.</li> <li>The server can serve one request at any given time. Any subsequent call receives a <code>grpc::StatusCode::RESOURCE_EXHAUSTED</code> status.</li> <li>When debugging using the compound profile, the server may not be ready to serve, resulting in errors with the client application. When this happens, open tasks.json, find <code>Build grpc_h264_endoscopy_tool_tracking (delay 3s)</code>, and adjust the <code>command</code> field with a higher sleep value.</li> <li>The client is expected to exit with the following error. It is how the client application terminates when it completes streaming and displays the entire video.   <pre><code>[error] [program.cpp:614] Event notification 2 for entity [video_in__outgoing_requests] with id [33] received in an unexpected state [Origin]\n</code></pre></li> </ul>","tags":["Endoscopy","Tracking","AJA","Video Decoding","Video Encoding"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/cpp/","title":"Endoscopy Tool Tracking Application with gRPC","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: March 7, 2025 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 2.7.0 Tested Holoscan SDK versions: 2.7.0 Contribution metric: Level 0 - Core Stable</p> <p>This application demonstrates how to offload heavy workloads to a remote Holoscan application using gRPC.</p>","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/cpp/#overview","title":"Overview","text":"<p>In this sample application, we divided the Endoscopy Tool Tracking application into a server and client application where the two communicate via gRPC.</p> <p>The client application inputs a video file and streams the video frames to the server application. The server application handles the heavy workloads of inferencing and post-processing of the video frames. It receives the video frames, processes each frame through the endoscopy tool tracking pipeline, and then streams the results to the client.</p> <p> Endoscopy Tool Tracking Application with gRPC</p> <p>From the diagram above, we can see that both the App Cloud (the server) and the App Edge (the client) are very similar to the standalone Endoscopy Tool Tracking application. This section will only describe the differences; for details on inference and post-processing, please refer to the link above.</p> <p>On the client side, we provided two examples, one using a single fragment and another one using two fragments. When comparing the client side to the standalone Endoscopy Tool Tracking application, the differences are the queues and the gRPC client. We added the following: - Outgoing Requests operator (<code>GrpcClientRequestOp</code>): It converts the video frames (GXF entities) received from the Video Stream Replayer operator into <code>EntityRequest</code> protobuf messages and queues each frame in the Request Queue. - gRPC Service &amp; Client (<code>EntityClientService</code> &amp; <code>EntityClient</code>): The gRPC Service is responsible for controlling the life cycle of the gRPC client. The client connects to the remote gRPC server and then sends the requests found in the Request Queue. When it receives a response, it converts it into a GXF entity and queues it in the Response Queue. - Incoming Responses operator (<code>GrpcClientResponseOp</code>): This operator is configured with an <code>AsynchronousCondition</code> condition to check the availability of the Response Queue. When notified of available responses in the queue, it dequeues each item and emits each to the output port.</p> <p>The App Cloud (the server) application consists of a gRPC server and a few components for managing Holoscan applications. When the server receives a new remote procedure call in this sample application, it launches a new instance of the Endoscopy Tool Tracking application. This is facilitated by the <code>ApplicationFactory</code> used for application registration.</p> <p>Under the hood, the Endoscopy Tool Tracking application here inherits a custom base class (<code>HoloscanGrpcApplication</code>) which manages the <code>Request Queue</code> and the <code>Response Queue</code> as well as the <code>GrpcServerRequestOp</code> and <code>GrpcServerResponseOp</code> operators for receiving requests and serving results, respectively. When the RPC is complete, the instance of the Endoscopy Tool Tracking application is destroyed and ready to serve the subsequent request.</p>","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/cpp/#requirements","title":"Requirements","text":"","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/cpp/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded when building the application.</p>","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/cpp/#building-and-running-grpc-endoscopy-tool-tracking-application","title":"Building and Running gRPC Endoscopy Tool Tracking Application","text":"","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/cpp/#c","title":"C++","text":"<pre><code># Start the gRPC Server\n./dev_container build_and_run grpc_endoscopy_tool_tracking --run_args cloud [--language cpp]\n\n# Start the gRPC Client\n./dev_container build_and_run grpc_endoscopy_tool_tracking --run_args edge [--language cpp]\n</code></pre>","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/cpp/#python","title":"Python","text":"<pre><code># Start the gRPC Server\n./dev_container build_and_run grpc_endoscopy_tool_tracking --language python --run_args cloud\n\n# Start the gRPC Client\n./dev_container build_and_run grpc_endoscopy_tool_tracking --language python --run_args edge\n</code></pre>","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/cpp/#configurations","title":"Configurations","text":"<p>The Edge application runs in a single-fragment mode by default. However, it can be configured to run in a multi-fragment mode, as in the picture above.</p> <p>To switch to multi-fragment mode, edit the endoscopy_tool_tracking.yaml YAML file and change <code>multifragment</code> to <code>true</code>:</p> <pre><code>application:\n  multifragment: false\n  benchmarking: false\n</code></pre> <p>[!NOTE] The Python version of this application is only available in single-fragment mode with benchmarking turned on.</p> <p>Data Flow Tracking can also be enabled by editing the endoscopy_tool_tracking.yaml YAML file and changing <code>benchmarking</code> to <code>true</code>. This enables the built-in mechanism to profile the application and analyze the fine-grained timing properties and data flow between operators.</p> <p>For example, on the server side, when a client disconnects, it will output the results for that session:</p> <pre><code>Data Flow Tracking Results:\nTotal paths: 1\n\nPath 1: grpc_request_op,format_converter,lstm_inferer,tool_tracking_postprocessor,grpc_response_op\nNumber of messages: 663\nMin Latency Message No: 249\nMin end-to-end Latency (ms): 1.868\nAvg end-to-end Latency (ms): 2.15161\nMax Latency Message No: 371\nMax end-to-end Latency (ms): 4.19\n\nNumber of source messages [format: source operator-&gt;transmitter name: number of messages]:\ngrpc_request_op-&gt;output: 683\n</code></pre> <p>Similarly, on the client side, when it completes playing the video, it will print the results:</p> <pre><code>Data Flow Tracking Results:\nTotal paths: 3\n\nPath 1: incoming_responses,visualizer_op\nNumber of messages: 663\nMin Latency Message No: 249\nMin end-to-end Latency (ms): 0.214\nAvg end-to-end Latency (ms): 0.374005\nMax Latency Message No: 378\nMax end-to-end Latency (ms): 2.751\n\nPath 2: replayer,outgoing_requests\nNumber of messages: 663\nMin Latency Message No: 379\nMin end-to-end Latency (ms): 24.854\nAvg end-to-end Latency (ms): 27.1886\nMax Latency Message No: 142\nMax end-to-end Latency (ms): 28.003\n\nPath 3: replayer,visualizer_op\nNumber of messages: 663\nMin Latency Message No: 372\nMin end-to-end Latency (ms): 30.966\nAvg end-to-end Latency (ms): 33.325\nMax Latency Message No: 397\nMax end-to-end Latency (ms): 35.479\n\nNumber of source messages [format: source operator-&gt;transmitter name: number of messages]:\nincoming_responses-&gt;output: 683\nreplayer-&gt;output: 683\n</code></pre>","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/cpp/#development-environment","title":"Development Environment","text":"","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/cpp/#dev-container","title":"Dev Container","text":"<p>To start the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./dev_container vscode\n</code></pre>","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/cpp/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/cpp/#c_1","title":"C++","text":"<p>The following launch profiles are available:</p> <ul> <li>(compound) grpc_endoscopy_tool_tracking/cpp (cloud &amp; edge): Launch both the gRPC server and the client.</li> <li>(gdb) grpc_endoscopy_tool_tracking/cpp (cloud): Launch the gRPC server.</li> <li>(gdb) grpc_endoscopy_tool_tracking/cpp (edge): Launch the gRPC client.</li> </ul>","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/cpp/#python_1","title":"Python","text":"<p>The following launch profiles are available:</p> <ul> <li>(compound) grpc_endoscopy_tool_tracking/python (cloud &amp; edge): Launch both the gRPC server and the client.</li> <li>(pythoncpp) grpc_endoscopy_tool_tracking/python (cloud): Launch the gRPC server with <code>pythoncpp</code>.</li> <li>(pythoncpp) grpc_endoscopy_tool_tracking/python (edge): Launch the gRPC client with <code>pythoncpp</code>.</li> <li>(debugpy) grpc_endoscopy_tool_tracking/python (cloud): Launch the gRPC server with <code>debugpy</code>.</li> <li>(debugpy) grpc_endoscopy_tool_tracking/python (edge):Launch the gRPC client with <code>debugpy</code>.</li> </ul> <p>[!NOTE] The <code>compound</code> profile uses the <code>debugpy</code> extension due to a limitation that prevents launching the cloud and the edge apps together using <code>pythoncpp</code>.</p>","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/cpp/#limitations-known-issues","title":"Limitations &amp; Known Issues","text":"","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/cpp/#c_2","title":"C++","text":"<ol> <li>Connection Timeout:</li> <li>The connection between the server and client is controlled by <code>rpc_timeout</code></li> <li>Default timeout is 5 seconds, configurable in endoscopy_tool_tracking.yaml</li> <li> <p>Consider increasing this value on slower networks</p> </li> <li> <p>Server Limitations:</p> </li> <li>Can only serve one request at a time</li> <li> <p>Subsequent calls receive <code>grpc::StatusCode::RESOURCE_EXHAUSTED</code> status</p> </li> <li> <p>Debugging Issues:</p> </li> <li>When using the compound profile, the server may need additional startup time</li> <li> <p>If needed, adjust the sleep value in tasks.json under <code>Build grpc_endoscopy_tool_tracking (delay 3s)</code></p> </li> <li> <p>Expected Exit Behavior:</p> </li> <li>The client will exit with the following expected error when the video completes:      <pre><code>[error] [program.cpp:614] Event notification 2 for entity [video_in__outgoing_requests] with id [33] received in an unexpected state [Origin]\n</code></pre></li> </ol>","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/cpp/#python_2","title":"Python","text":"<ul> <li>The client may require manual termination (CTRL+C) if errors occur during execution</li> </ul>","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/cpp/#containerization","title":"Containerization","text":"<p>To containerize the application:</p> <ol> <li>Install Holoscan CLI</li> <li>Build the application:    <pre><code>./dev_container build_and_install grpc_endoscopy_tool_tracking\n</code></pre></li> <li>Run the appropriate packaging script:</li> <li>C++: cpp/package-app.sh</li> <li>Python: python/package-app.sh</li> <li>Follow the generated output instructions to package and run the application</li> </ol> <p>For more information about packaging Holoscan applications, refer to the Packaging Holoscan Applications section in the Holoscan User Guide.</p>","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/python/","title":"Endoscopy Tool Tracking Application with gRPC","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: March 7, 2025 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 2.7.0 Tested Holoscan SDK versions: 2.7.0 Contribution metric: Level 0 - Core Stable</p> <p>This application demonstrates how to offload heavy workloads to a remote Holoscan application using gRPC.</p>","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/python/#overview","title":"Overview","text":"<p>In this sample application, we divided the Endoscopy Tool Tracking application into a server and client application where the two communicate via gRPC.</p> <p>The client application inputs a video file and streams the video frames to the server application. The server application handles the heavy workloads of inferencing and post-processing of the video frames. It receives the video frames, processes each frame through the endoscopy tool tracking pipeline, and then streams the results to the client.</p> <p> Endoscopy Tool Tracking Application with gRPC</p> <p>From the diagram above, we can see that both the App Cloud (the server) and the App Edge (the client) are very similar to the standalone Endoscopy Tool Tracking application. This section will only describe the differences; for details on inference and post-processing, please refer to the link above.</p> <p>On the client side, we provided two examples, one using a single fragment and another one using two fragments. When comparing the client side to the standalone Endoscopy Tool Tracking application, the differences are the queues and the gRPC client. We added the following: - Outgoing Requests operator (<code>GrpcClientRequestOp</code>): It converts the video frames (GXF entities) received from the Video Stream Replayer operator into <code>EntityRequest</code> protobuf messages and queues each frame in the Request Queue. - gRPC Service &amp; Client (<code>EntityClientService</code> &amp; <code>EntityClient</code>): The gRPC Service is responsible for controlling the life cycle of the gRPC client. The client connects to the remote gRPC server and then sends the requests found in the Request Queue. When it receives a response, it converts it into a GXF entity and queues it in the Response Queue. - Incoming Responses operator (<code>GrpcClientResponseOp</code>): This operator is configured with an <code>AsynchronousCondition</code> condition to check the availability of the Response Queue. When notified of available responses in the queue, it dequeues each item and emits each to the output port.</p> <p>The App Cloud (the server) application consists of a gRPC server and a few components for managing Holoscan applications. When the server receives a new remote procedure call in this sample application, it launches a new instance of the Endoscopy Tool Tracking application. This is facilitated by the <code>ApplicationFactory</code> used for application registration.</p> <p>Under the hood, the Endoscopy Tool Tracking application here inherits a custom base class (<code>HoloscanGrpcApplication</code>) which manages the <code>Request Queue</code> and the <code>Response Queue</code> as well as the <code>GrpcServerRequestOp</code> and <code>GrpcServerResponseOp</code> operators for receiving requests and serving results, respectively. When the RPC is complete, the instance of the Endoscopy Tool Tracking application is destroyed and ready to serve the subsequent request.</p>","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/python/#requirements","title":"Requirements","text":"","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/python/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded when building the application.</p>","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/python/#building-and-running-grpc-endoscopy-tool-tracking-application","title":"Building and Running gRPC Endoscopy Tool Tracking Application","text":"","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/python/#c","title":"C++","text":"<pre><code># Start the gRPC Server\n./dev_container build_and_run grpc_endoscopy_tool_tracking --run_args cloud [--language cpp]\n\n# Start the gRPC Client\n./dev_container build_and_run grpc_endoscopy_tool_tracking --run_args edge [--language cpp]\n</code></pre>","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/python/#python","title":"Python","text":"<pre><code># Start the gRPC Server\n./dev_container build_and_run grpc_endoscopy_tool_tracking --language python --run_args cloud\n\n# Start the gRPC Client\n./dev_container build_and_run grpc_endoscopy_tool_tracking --language python --run_args edge\n</code></pre>","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/python/#configurations","title":"Configurations","text":"<p>The Edge application runs in a single-fragment mode by default. However, it can be configured to run in a multi-fragment mode, as in the picture above.</p> <p>To switch to multi-fragment mode, edit the endoscopy_tool_tracking.yaml YAML file and change <code>multifragment</code> to <code>true</code>:</p> <pre><code>application:\n  multifragment: false\n  benchmarking: false\n</code></pre> <p>[!NOTE] The Python version of this application is only available in single-fragment mode with benchmarking turned on.</p> <p>Data Flow Tracking can also be enabled by editing the endoscopy_tool_tracking.yaml YAML file and changing <code>benchmarking</code> to <code>true</code>. This enables the built-in mechanism to profile the application and analyze the fine-grained timing properties and data flow between operators.</p> <p>For example, on the server side, when a client disconnects, it will output the results for that session:</p> <pre><code>Data Flow Tracking Results:\nTotal paths: 1\n\nPath 1: grpc_request_op,format_converter,lstm_inferer,tool_tracking_postprocessor,grpc_response_op\nNumber of messages: 663\nMin Latency Message No: 249\nMin end-to-end Latency (ms): 1.868\nAvg end-to-end Latency (ms): 2.15161\nMax Latency Message No: 371\nMax end-to-end Latency (ms): 4.19\n\nNumber of source messages [format: source operator-&gt;transmitter name: number of messages]:\ngrpc_request_op-&gt;output: 683\n</code></pre> <p>Similarly, on the client side, when it completes playing the video, it will print the results:</p> <pre><code>Data Flow Tracking Results:\nTotal paths: 3\n\nPath 1: incoming_responses,visualizer_op\nNumber of messages: 663\nMin Latency Message No: 249\nMin end-to-end Latency (ms): 0.214\nAvg end-to-end Latency (ms): 0.374005\nMax Latency Message No: 378\nMax end-to-end Latency (ms): 2.751\n\nPath 2: replayer,outgoing_requests\nNumber of messages: 663\nMin Latency Message No: 379\nMin end-to-end Latency (ms): 24.854\nAvg end-to-end Latency (ms): 27.1886\nMax Latency Message No: 142\nMax end-to-end Latency (ms): 28.003\n\nPath 3: replayer,visualizer_op\nNumber of messages: 663\nMin Latency Message No: 372\nMin end-to-end Latency (ms): 30.966\nAvg end-to-end Latency (ms): 33.325\nMax Latency Message No: 397\nMax end-to-end Latency (ms): 35.479\n\nNumber of source messages [format: source operator-&gt;transmitter name: number of messages]:\nincoming_responses-&gt;output: 683\nreplayer-&gt;output: 683\n</code></pre>","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/python/#development-environment","title":"Development Environment","text":"","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/python/#dev-container","title":"Dev Container","text":"<p>To start the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./dev_container vscode\n</code></pre>","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/python/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/python/#c_1","title":"C++","text":"<p>The following launch profiles are available:</p> <ul> <li>(compound) grpc_endoscopy_tool_tracking/cpp (cloud &amp; edge): Launch both the gRPC server and the client.</li> <li>(gdb) grpc_endoscopy_tool_tracking/cpp (cloud): Launch the gRPC server.</li> <li>(gdb) grpc_endoscopy_tool_tracking/cpp (edge): Launch the gRPC client.</li> </ul>","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/python/#python_1","title":"Python","text":"<p>The following launch profiles are available:</p> <ul> <li>(compound) grpc_endoscopy_tool_tracking/python (cloud &amp; edge): Launch both the gRPC server and the client.</li> <li>(pythoncpp) grpc_endoscopy_tool_tracking/python (cloud): Launch the gRPC server with <code>pythoncpp</code>.</li> <li>(pythoncpp) grpc_endoscopy_tool_tracking/python (edge): Launch the gRPC client with <code>pythoncpp</code>.</li> <li>(debugpy) grpc_endoscopy_tool_tracking/python (cloud): Launch the gRPC server with <code>debugpy</code>.</li> <li>(debugpy) grpc_endoscopy_tool_tracking/python (edge):Launch the gRPC client with <code>debugpy</code>.</li> </ul> <p>[!NOTE] The <code>compound</code> profile uses the <code>debugpy</code> extension due to a limitation that prevents launching the cloud and the edge apps together using <code>pythoncpp</code>.</p>","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/python/#limitations-known-issues","title":"Limitations &amp; Known Issues","text":"","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/python/#c_2","title":"C++","text":"<ol> <li>Connection Timeout:</li> <li>The connection between the server and client is controlled by <code>rpc_timeout</code></li> <li>Default timeout is 5 seconds, configurable in endoscopy_tool_tracking.yaml</li> <li> <p>Consider increasing this value on slower networks</p> </li> <li> <p>Server Limitations:</p> </li> <li>Can only serve one request at a time</li> <li> <p>Subsequent calls receive <code>grpc::StatusCode::RESOURCE_EXHAUSTED</code> status</p> </li> <li> <p>Debugging Issues:</p> </li> <li>When using the compound profile, the server may need additional startup time</li> <li> <p>If needed, adjust the sleep value in tasks.json under <code>Build grpc_endoscopy_tool_tracking (delay 3s)</code></p> </li> <li> <p>Expected Exit Behavior:</p> </li> <li>The client will exit with the following expected error when the video completes:      <pre><code>[error] [program.cpp:614] Event notification 2 for entity [video_in__outgoing_requests] with id [33] received in an unexpected state [Origin]\n</code></pre></li> </ol>","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/python/#python_2","title":"Python","text":"<ul> <li>The client may require manual termination (CTRL+C) if errors occur during execution</li> </ul>","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/python/#containerization","title":"Containerization","text":"<p>To containerize the application:</p> <ol> <li>Install Holoscan CLI</li> <li>Build the application:    <pre><code>./dev_container build_and_install grpc_endoscopy_tool_tracking\n</code></pre></li> <li>Run the appropriate packaging script:</li> <li>C++: cpp/package-app.sh</li> <li>Python: python/package-app.sh</li> <li>Follow the generated output instructions to package and run the application</li> </ol> <p>For more information about packaging Holoscan applications, refer to the Packaging Holoscan Applications section in the Holoscan User Guide.</p>","tags":["Endoscopy","Tracking","gRPC"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/cpp/","title":"Distributed Endoscopy Tool Tracking","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: January 22, 2025 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 0 - Core Stable</p> <p>This application is similar to the Endoscopy Tool Tracking application, but the distributed version divides the application into three fragments:</p> <ol> <li>Video Input: get video input from a pre-recorded video file.</li> <li>Inference: run the inference using LSTM and run the post-processing script.</li> <li>Visualization: display input video and inference results.</li> </ol> <p>Based on an LSTM (long-short term memory) stateful model, these applications demonstrate the use of custom components for tool tracking, including composition and rendering of text, tool position, and mask (as heatmap) combined with the original video stream.</p>","tags":["Endoscopy","Tracking"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/cpp/#requirements","title":"Requirements","text":"<p>The provided applications are configured to use a pre-recorded endoscopy video (replayer).</p>","tags":["Endoscopy","Tracking"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/cpp/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Endoscopy","Tracking"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/cpp/#run-instructions","title":"Run Instructions","text":"<pre><code># Build the Holohub container for the Distributed Endoscopy Tool Tracking application\n./dev_container build --docker_file applications/distributed/ucx/ucx_endoscopy_tool_tracking/Dockerfile --img holohub:ucx_endoscopy_tool_tracking\n\n# Launch the container\n./dev_container launch --img holohub:ucx_endoscopy_tool_tracking\n\n# Build the Distributed Endoscopy Tool Tracking application\n./run build ucx_endoscopy_tool_tracking\n\n# Generate the TRT engine file from onnx\npython3 utilities/generate_trt_engine.py --input data/endoscopy/tool_loc_convlstm.onnx --output data/endoscopy/engines/ --fp16\n\n# Start the application with all three fragments\n./run launch ucx_endoscopy_tool_tracking cpp\n\n# Once you have completed the step to generate the TRT engine file, you may exit the container and\n#  use the following commands to run the application in distributed mode:\n\n# Start the application with the video_in fragment\n./dev_container build_and_run ucx_endoscopy_tool_tracking --language cpp --run_args \"--driver --worker --fragments video_in --address :9999\"\n# Start the application with the inference fragment\n./dev_container build_and_run ucx_endoscopy_tool_tracking --language cpp --run_args \"--worker --fragments inference --address :9999\"\n# Start the application with the visualization fragment\n./dev_container build_and_run ucx_endoscopy_tool_tracking --language cpp --run_args \"--worker --fragments viz --address :9999\"\n</code></pre>","tags":["Endoscopy","Tracking"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/python/","title":"Distributed Endoscopy Tool Tracking","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: November 13, 2024 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application is similar to the Endoscopy Tool Tracking application, but the distributed version divides the application into three fragments:</p> <ol> <li>Video Input: get video input from a pre-recorded video file.</li> <li>Inference: run the inference using LSTM and run the post-processing script.</li> <li>Visualization: display input video and inference results.</li> </ol> <p>Based on an LSTM (long-short term memory) stateful model, these applications demonstrate the use of custom components for tool tracking, including composition and rendering of text, tool position, and mask (as heatmap) combined with the original video stream.</p>","tags":["Endoscopy","Tracking"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/python/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8+</li> <li>The provided applications are configured to use a pre-recorded endoscopy video (replayer). </li> </ul>","tags":["Endoscopy","Tracking"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/python/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Endoscopy","Tracking"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/python/#run-instructions","title":"Run Instructions","text":"<pre><code># Start the application with all three fragments\n./dev_container build_and_run ucx_endoscopy_tool_tracking --language python\n\n# Use the following commands to run the same application three processes:\n# Start the application with the video_in fragment\n./dev_container build_and_run ucx_endoscopy_tool_tracking --language python --run_args \"--driver --worker --fragments video_in --address :10000\"\n# Start the application with the inference fragment\n./dev_container build_and_run ucx_endoscopy_tool_tracking --language python --run_args \"--worker --fragments inference --address :10000\"\n# Start the application with the visualization fragment\n./dev_container build_and_run ucx_endoscopy_tool_tracking --language python --run_args \"--worker --fragments viz --address :10000\"\n</code></pre>","tags":["Endoscopy","Tracking"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/cpp/","title":"Distributed H.264 Endoscopy Tool Tracking Application","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: January 16, 2025 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 2.6.0 Tested Holoscan SDK versions: 2.6.0 Contribution metric: Level 0 - Core Stable</p> <p>This application is similar to the H.264 Endoscopy Tool Tracking application, but this distributed version divides the application into three fragments:</p> <ol> <li>Video Input: get video input from a pre-recorded video file.</li> <li>Inference: run the inference using LSTM and run the post-processing script.</li> <li>Visualization: display input video and inference results.</li> </ol>","tags":["Endoscopy","Tracking","AJA","Video Decoding","Video Encoding"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/cpp/#requirements","title":"Requirements","text":"<p>This application is configured to use H.264 elementary stream from endoscopy sample data as input.</p>","tags":["Endoscopy","Tracking","AJA","Video Decoding","Video Encoding"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/cpp/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded when building the application.</p>","tags":["Endoscopy","Tracking","AJA","Video Decoding","Video Encoding"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/cpp/#building-and-running-h264-endoscopy-tool-tracking-application","title":"Building and Running H.264 Endoscopy Tool Tracking Application","text":"<ul> <li>Building and running the application from the top level Holohub directory:</li> </ul>","tags":["Endoscopy","Tracking","AJA","Video Decoding","Video Encoding"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/cpp/#c","title":"C++","text":"<pre><code># Start the application with all three fragments\n./dev_container build_and_run ucx_h264_endoscopy_tool_tracking --language cpp\n\n# Use the following commands to run the same application three processes:\n# Start the application with the video_in fragment\n./dev_container build_and_run ucx_h264_endoscopy_tool_tracking --language cpp --run_args \"--driver --worker --fragments video_in --address :10000 --worker-address :10001\"\n# Start the application with the inference fragment\n./dev_container build_and_run ucx_h264_endoscopy_tool_tracking --language cpp --run_args \"--worker --fragments inference --address :10000 --worker-address :10002\"\n# Start the application with the visualization fragment\n./dev_container build_and_run ucx_h264_endoscopy_tool_tracking --language cpp --run_args \"--worker --fragments viz --address :10000 --worker-address :10003\"\n</code></pre>","tags":["Endoscopy","Tracking","AJA","Video Decoding","Video Encoding"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/cpp/#python","title":"Python","text":"<pre><code># Start the application with all three fragments\n./dev_container build_and_run ucx_h264_endoscopy_tool_tracking --language python\n\n# Use the following commands to run the same application three processes:\n# Start the application with the video_in fragment\n./dev_container build_and_run ucx_h264_endoscopy_tool_tracking --language python --run_args \"--driver --worker --fragments video_in --address :10000 --worker-address :10001\"\n# Start the application with the inference fragment\n./dev_container build_and_run ucx_h264_endoscopy_tool_tracking --language python --run_args \"--worker --fragments inference --address :10000 --worker-address :10002\"\n# Start the application with the visualization fragment\n./dev_container build_and_run ucx_h264_endoscopy_tool_tracking --language python --run_args \"--worker --fragments viz --address :10000 --worker-address :10003\"\n</code></pre> <p>Important: on aarch64, applications also need tegra folder mounted inside the container and the <code>LD_LIBRARY_PATH</code> environment variable should be updated to include tegra folder path.</p> <p>Open and edit the Dockerfile and uncomment line 66:</p> <pre><code># Uncomment the following line for aarch64 support\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/aarch64-linux-gnu/tegra/\n</code></pre>","tags":["Endoscopy","Tracking","AJA","Video Decoding","Video Encoding"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/cpp/#dev-container","title":"Dev Container","text":"<p>To start the VS Code Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./dev_container vscode h264\n</code></pre>","tags":["Endoscopy","Tracking","AJA","Video Decoding","Video Encoding"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/cpp/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Endoscopy","Tracking","AJA","Video Decoding","Video Encoding"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/cpp/#c_1","title":"C++","text":"<p>Use the (gdb) ucx_h264_endoscopy_tool_tracking/cpp (all fragments) launch profile to run and debug the C++ application.</p>","tags":["Endoscopy","Tracking","AJA","Video Decoding","Video Encoding"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/cpp/#python_1","title":"Python","text":"<p>Use the (pythoncpp) ucx_h264_endoscopy_tool_tracking/python (all fragments) launch profile to run and debug the Python application.</p>","tags":["Endoscopy","Tracking","AJA","Video Decoding","Video Encoding"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/python/","title":"Distributed H.264 Endoscopy Tool Tracking Application","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: November 13, 2024 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 2.6.0 Tested Holoscan SDK versions: 2.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application is similar to the H.264 Endoscopy Tool Tracking application, but this distributed version divides the application into three fragments:</p> <ol> <li>Video Input: get video input from a pre-recorded video file.</li> <li>Inference: run the inference using LSTM and run the post-processing script.</li> <li>Visualization: display input video and inference results.</li> </ol>","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/python/#requirements","title":"Requirements","text":"<p>This application is configured to use H.264 elementary stream from endoscopy sample data as input.</p>","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/python/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded when building the application.</p>","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/python/#building-and-running-h264-endoscopy-tool-tracking-application","title":"Building and Running H.264 Endoscopy Tool Tracking Application","text":"<ul> <li>Building and running the application from the top level Holohub directory:</li> </ul>","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/python/#c","title":"C++","text":"<pre><code># Start the application with all three fragments\n./dev_container build_and_run ucx_h264_endoscopy_tool_tracking --language cpp\n\n# Use the following commands to run the same application three processes:\n# Start the application with the video_in fragment\n./dev_container build_and_run ucx_h264_endoscopy_tool_tracking --language cpp --run_args \"--driver --worker --fragments video_in --address :10000 --worker-address :10001\"\n# Start the application with the inference fragment\n./dev_container build_and_run ucx_h264_endoscopy_tool_tracking --language cpp --run_args \"--worker --fragments inference --address :10000 --worker-address :10002\"\n# Start the application with the visualization fragment\n./dev_container build_and_run ucx_h264_endoscopy_tool_tracking --language cpp --run_args \"--worker --fragments viz --address :10000 --worker-address :10003\"\n</code></pre>","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/python/#python","title":"Python","text":"<pre><code># Start the application with all three fragments\n./dev_container build_and_run ucx_h264_endoscopy_tool_tracking --language python\n\n# Use the following commands to run the same application three processes:\n# Start the application with the video_in fragment\n./dev_container build_and_run ucx_h264_endoscopy_tool_tracking --language python --run_args \"--driver --worker --fragments video_in --address :10000 --worker-address :10001\"\n# Start the application with the inference fragment\n./dev_container build_and_run ucx_h264_endoscopy_tool_tracking --language python --run_args \"--worker --fragments inference --address :10000 --worker-address :10002\"\n# Start the application with the visualization fragment\n./dev_container build_and_run ucx_h264_endoscopy_tool_tracking --language python --run_args \"--worker --fragments viz --address :10000 --worker-address :10003\"\n</code></pre> <p>Important: on aarch64, applications also need tegra folder mounted inside the container and the <code>LD_LIBRARY_PATH</code> environment variable should be updated to include tegra folder path.</p> <p>Open and edit the Dockerfile and uncomment line 66:</p> <pre><code># Uncomment the following line for aarch64 support\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/aarch64-linux-gnu/tegra/\n</code></pre>","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/python/#dev-container","title":"Dev Container","text":"<p>To start the VS Code Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./dev_container vscode h264\n</code></pre>","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/python/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/python/#c_1","title":"C++","text":"<p>Use the (gdb) ucx_h264_endoscopy_tool_tracking/cpp (all fragments) launch profile to run and debug the C++ application.</p>","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/python/#python_1","title":"Python","text":"<p>Use the (pythoncpp) ucx_h264_endoscopy_tool_tracking/python (all fragments) launch profile to run and debug the Python application.</p>","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/ehr_query_llm/fhir/","title":"FHIR Client Application for Retrieving and posting FHIR Resources","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: March 10, 2025 Language: Python Latest version: 1.0.0 Minimum Holoscan SDK version: 2.5.0 Tested Holoscan SDK versions: 2.5.0, 2.7.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This is an application to interface with a FHIR server to retrieve or post FHIR resources.</p> <p>It requires the FHIR Server endpoint URL be provided on the command line as well as client authentication credentials if required. As of now, authentication and authorization is limited to OAuth2.0 server to server workflow. When authorization is requiired by the server, its OAuth2.0 token service URL along with client ID and secret must be provided to the application.</p> <p>This application also uses ZeroMQ to communicate with its own clients, listening on a well known port on local host for messages to retrieve resources of a patient, as well as publishing the retrieved resources on another well known port. For simplicity, the listening port is defined in the code to be <code>5600</code>, and the publishing port <code>5601</code>. Messaging security, at transport or message level, is not implemented in this example.</p> <p>Message schema is simple, with a well known topic string and topic specific content schema in JSON format.</p> <p>The default set of FHIR resource types to retrieve are listed below, which can be overridden by the request message - Observation - ImagingStudy - FamilyMemberHistory - Condition - DiagnosticReport - DocumentReference</p>","tags":["ehr_query_llm","FHIR service client","FHIR"]},{"location":"applications/ehr_query_llm/fhir/#requirements","title":"Requirements","text":"<ul> <li>On a Holohub supported platform</li> <li>Python 3.10+</li> <li>Python packages on Pypi, including holoscan, fhir.resources, holoscan, pyzmq, requests and their dependencies</li> </ul>","tags":["ehr_query_llm","FHIR service client","FHIR"]},{"location":"applications/ehr_query_llm/fhir/#run-instructions","title":"Run Instructions","text":"<p>There are several ways to build and run this application and package it as a Holoscan Application Package, an Open Container Initiative compliant image. The following sections describe each in detail.</p> <p>It is further expected that you have read the HoloHub README, have cloned the HoloHub repository to your local system, and the current working directory is the HoloHub root, <code>holohub</code>.</p> <p>Note: The application listens on request message to start retrieving resources from the server and then publishes the results, so another application is needed to drive this workflow, e.g. the LLM application. To help with simple testing, a Python script is provided as part of this application, and its usage is described below in this section.</p>","tags":["ehr_query_llm","FHIR service client","FHIR"]},{"location":"applications/ehr_query_llm/fhir/#quick-start-using-holohub-container","title":"Quick Start Using HoloHub Container","text":"<p>This is the simplest and fastest way to start the application in a HoloHub dev container and get it ready to listen to request messages.</p> <p>Note: Please use your own FHIR server endpoint, as well as the OAuth2.0 authorization endpoint and client credential as needed.</p> <pre><code>./dev_container build_and_run fhir --run_args \"--fhir_url &lt;f_url&gt; --auth_url &lt;a_url&gt; --uid &lt;id&gt; --secret &lt;token&gt;\"\n</code></pre> <p>Add the additional command line option, <code>--container_args \"-u root\"</code>, to avoid seeing the following error (though no impact on execution)</p> <pre><code>Error processing line 1 of /usr/local/lib/python3.10/dist-packages/holoscan-2.4.0.pth:\n\n  Traceback (most recent call last):\n    File \"/usr/lib/python3.10/site.py\", line 192, in addpackage\n      exec(line)\n    File \"&lt;string&gt;\", line 1, in &lt;module&gt;\n    File \"/workspace/holohub/.local/lib/python3.10/site-packages/wheel_axle/runtime/__init__.py\", line 80, in finalize\n      with FileLock(lock_path):\n    File \"/workspace/holohub/.local/lib/python3.10/site-packages/filelock/_api.py\", line 376, in __enter__\n      self.acquire()\n    File \"/workspace/holohub/.local/lib/python3.10/site-packages/filelock/_api.py\", line 332, in acquire\n      self._acquire()\n    File \"/workspace/holohub/.local/lib/python3.10/site-packages/filelock/_unix.py\", line 42, in _acquire\n      fd = os.open(self.lock_file, open_flags, self._context.mode)\n  PermissionError: [Errno 13] Permission denied: '/usr/local/lib/python3.10/dist-packages/holoscan-2.4.0.dist-info/axle.lck'\n</code></pre>","tags":["ehr_query_llm","FHIR service client","FHIR"]},{"location":"applications/ehr_query_llm/fhir/#run-the-application-in-dev-container","title":"Run the Application in Dev Container","text":"<p>This is a step wise way to run the application in a dev container. <pre><code>./dev_container build --docker_file applications/ehr_query_llm/fhir/Dockerfile --img holoscan:fhir --verbose --no-cache\n</code></pre></p> <p>Optionally check the newly built image <pre><code>$ docker images\nREPOSITORY         TAG               IMAGE ID       CREATED          SIZE\nholoscan           fhir              508140b8d446   3 minutes ago    14.1GB\n</code></pre></p> <p>Launch the container <pre><code>./dev_container launch --img holoscan:fhir --as_root\n</code></pre></p> <p>Now in the container, build and run the application</p> <pre><code>root:~# pwd\n/workspace/holohub\n\nroot:~# ./run clear_cache\nroot:~# ./run build fhir\nroot:~# ./run launch fhir --extra_args \"--fhir_url &lt;f_url&gt; --auth_url &lt;a_url&gt; --uid &lt;id&gt; --secret &lt;token&gt;\"\n</code></pre> <p>Once done, <code>exit</code> the container.</p>","tags":["ehr_query_llm","FHIR service client","FHIR"]},{"location":"applications/ehr_query_llm/fhir/#run-the-application-in-the-host-dev-environment-with-dev_container-script","title":"Run the Application in the Host Dev Environment with dev_container script","text":"<p>First create and activate a Python virtual environment, followed with installing the dependencies</p> <pre><code>python3 -m venv .testenv\nsource .testenv/bin/activate\npip install -r applications/ehr_query_llm/fhir/requirements.txt\n</code></pre> <p>Build and install the application with <code>dev_container</code> <pre><code>./dev_container build_and_install fhir\n</code></pre></p> <p>Now, run the application which is installed in the <code>install</code> folder, with server URLs and credential of your own <pre><code>python install/bin/fhir/python/ --fhir_url &lt;f_url&gt; --auth_url &lt;a_url&gt; --uid &lt;id&gt; --secret &lt;token&gt;\n</code></pre></p>","tags":["ehr_query_llm","FHIR service client","FHIR"]},{"location":"applications/ehr_query_llm/fhir/#test-the-running-application","title":"Test the Running Application","text":"<p>Once the FHIR application has been started with one of the ways, a test application can be used to request and receive FHIR resources, namely <code>applications/ehr_query_llm/fhir/test_fhir_client.py</code>.</p> <p>The test application contains hard coded patient name, patient FHIR resource ID, etc., corresponding to a specific test dataset, though can be easily modified for another dataset.</p> <p>It is strongly recommended to run this test application in a Python virtual environment, which can be the same as in running the FHIR application. The following describes running it in its own environment.</p> <pre><code>echo \"Assuming venv already created with `python3 -m venv .testenv`\"\nsource .testenv/bin/activate\npip install -r applications/ehr_query_llm/domain_specific/fhir/requirements.txt\nexport PYTHONPATH=${PWD}\npython applications/ehr_query_llm/fhir/test_fhir_client.py\n</code></pre> <p>From the menu, pick one of the choices for the resources of interest.</p>","tags":["ehr_query_llm","FHIR service client","FHIR"]},{"location":"applications/ehr_query_llm/fhir/#packaging-the-application-for-distribution-and-deployment","title":"Packaging the Application for Distribution and Deployment","text":"<p>With Holoscan CLI, an applications built with Holoscan SDK can be packaged into a Holoscan Application Package (HAP), which is an Open Container Initiative compliant image. An HAP is well suited to be distributed for deployment on hosting platforms, be it Docker Compose, Kubernetes, or else. Please refer to Packaging Holoscan Applications in the User Guide for more information.</p> <p>This example application provides all the necessary contents for HAP packaging. It is required to perform the packaging in a Python virtual environment, with the application's dependencies installed, before running the following script to reveal specific packaging commands. <pre><code>applications/ehr_query_llm/fhir/packageHAP.sh\n</code></pre></p> <p>Once the HAP is created, it can then be saved and restored on the target deployment host, and run with <code>docker run</code> command, shown below with to be substituted user specific parameters. <pre><code>docker run -it --rm --net host holohub-fhir-x64-workstation-dgpu-linux-amd64:1.0 \\\n--fhir_url &lt;f_url&gt; \\\n--auth_url &lt;a_url&gt; \\\n--uid &lt;id&gt; \\\n--secret &lt;token&gt;\n</code></pre></p>","tags":["ehr_query_llm","FHIR service client","FHIR"]},{"location":"applications/ehr_query_llm/lmm/","title":"EHR Agent Framework","text":"","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#ehr-agent-framework","title":"EHR Agent Framework","text":"<p>The EHR Agent Framework is designed to handle and interact with EHR (Electronic Health Records) and it provides a modular and extensible system for handling various types of queries through specialized agents, with robust error handling and performance optimization features.</p>","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Agent Framework overview</li> <li>Setup Instructions</li> <li>Run Instructions</li> <li>Offline Mode</li> </ul>","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#agent-framework-overview","title":"Agent Framework overview","text":"<p>The <code>AgentFrameworkOp</code> orchestrates multiple specialized agents to handle different types of queries and responses, it maintains a streaming queue for responses and it handles response states through <code>ResponseHandler</code>. It tracks conversation history using ChatHistory class and updates history based on agent responses and ASR transcripts.</p> <p>Agent Lifecycle:</p> <ul> <li>Processes requests asynchronously using threads</li> <li>Controls response streaming and muting during speech</li> </ul>","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#the-base-agent-class","title":"The Base Agent Class","text":"<p>This is an abstract base class implementing common agent functionality:</p> <pre><code>- Uses threading.Lock for LLM and , if needed, LMM access\n- Prevents concurrent requests to language models\n</code></pre> <p>It loads configuration from YAML files and it handles prompt templates, tokens limits, and model URLs. The Base Agent is designed to stream responses from LLM server and it supports both text and image-based prompts while enforcing maximum prompt token limits. Throughout the agent lifecycle it maintains conversation context and it creates conversation strings within token limits.</p>","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#the-selector-agent","title":"The Selector Agent","text":"<p>The Selector Agent routes user queries to the appropriate specialized agent by analyzing user input to determine the appropriate agent and return the selected agent name and corrected input text. For response parsing, it handles JSON response format. If there are parsing failures, it logs them and it returns <code>None</code> for invalid selection.</p>","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#the-ehr-builder-agent","title":"The EHR Builder Agent","text":"<p>The EHR Builder Agent handles EHR database construction on demand and it tracks and reports build time performance in the process. For response generation, it uses custom prompt templates for EHR tasks and it returns structured JSON responses. It also verifies build capability before execution and it reports success/failure status.</p>","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#the-ehr-agent","title":"The EHR Agent","text":"<p>The EHR Agent handles EHR queries and data retrieval. It uses Chroma for document storage while implementing HuggingFaceBgeEmbeddings for embeddings. For the RAG (Retrieval-Augmented Generation) pipeline, it performs MMR (Maximal Marginal Relevance) search with configurable search parameters (<code>k</code>, <code>lambda_mult</code>, <code>fetch_k</code>). For prompt generation, it incorporates retrieved documents into prompts and it supports both standard and RAG-specific prompts. The EHR Agent is using CUDA for embedding computation and optimizes for cosine similarity calculations.</p>","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#common-features-across-agents","title":"Common features across agents","text":"","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#configuration-management","title":"Configuration Management:","text":"<pre><code>    -YAML-based settings\n    -Configurable prompts and rules\n    -Extensible tool support\n</code></pre>","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#response-handling","title":"Response Handling:","text":"<pre><code>    -Streaming response support\n    -Mutable response states\n    -Structured output formatting\n</code></pre>","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#error-management","title":"Error Management:","text":"<pre><code>    -Connection retry logic\n    -Comprehensive error logging\n    -Graceful failure handling\n</code></pre>","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#performance-optimization","title":"Performance Optimization:","text":"<pre><code>    -Thread-safe operations\n    -Token usage optimization\n    -Efficient resource management\n</code></pre>","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#setup-instructions","title":"Setup Instructions","text":"","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#speech-pipeline","title":"Speech pipeline","text":"<p>Note</p> <p>NVIDIA Riva provides speech and translation services for user interaction with the LLM. We recommend running Riva in the bare metal host environment outside of the development container to minimize demands on container resources. During test run, it was observed that Riva could take up around 8 GB of GPU memory, while the rest of the application around 12 GB of GPU memory.</p> <p>NVIDIA Riva Version Compatibility : tested with v2.13.0 / v2.14.0.</p> <p>Please adhere to the \"Data Center\" configuration specifications in the Riva Quick Start guide.</p> <p>To optimize Riva installation footprint: * Locate the <code>config.sh</code> file in the riva_quickstart_vX.XX.X directory. * Modify the <code>service_enabled_*</code> variables as follows:</p> <pre><code>service_enabled_asr=true\nservice_enabled_nlp=false\nservice_enabled_tts=true\nservice_enabled_nmt=false\n</code></pre>","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#model-acquisition","title":"Model acquisition:","text":"<p>It is recommended to create a directory called <code>/models</code> on your machine to download the LLM.</p> <p>Download the quantized Mistral 7B finetuned LLM from HugginFace.co:</p> <pre><code>wget -nc -P &lt;your_model_dir&gt; https://huggingface.co/TheBloke/openchat-3.5-0106-GGUF/resolve/main/openchat-3.5-0106.Q8_0.gguf\n</code></pre> <p>Download the BGE-large finetuned embedding model from NGC:  <code>bash  wget https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/models/bge-large-ehr-finetune</code> </p> <p>Execute the following command from the Holohub root directory:</p> <pre><code>./dev_container build --docker_file applications/ehr_query_llm/lmm/Dockerfile --img ehr_query_llm:llm\n</code></pre>","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#run-instructions","title":"Run Instructions:","text":"","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#step-1-enabling-httpsssl-only-required-once","title":"Step 1: Enabling HTTPS/SSL (only required once)","text":"<p>\u26a0\ufe0f Note: This has only been tested with Chrome and Chromium</p> <p>Browsers require HTTPS to be used in order to access the client's microphone.  Hence, you'll need to create a self-signed SSL certificate and key.</p> <p>This key must be placed in <code>/applications/ehr_query_llm/lmm/ssl</code></p> <p><pre><code>cd &lt;holohub root&gt;/applications/ehr_query_llm/lmm/\nmkdir ssl\ncd ssl\nopenssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -sha256 -days 365 -nodes -subj '/CN=localhost'\n</code></pre> When you first navigate your browser to a page that uses these self-signed certificates, it will issue you a warning since they don't originate from a trusted authority. Ignore this and proceed to the web app:</p> <p></p> <p>ehr_query_llm will use your default speaker and microphone. To change this go to your Ubuntu sound settings and choose the correct devices:</p> <p></p>","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#step-2-ensure-riva-server-is-running","title":"Step 2: Ensure Riva server is running","text":"<p>The Riva server must be running to use the LLM pipeline. If it is already running you can skip this step. <pre><code>cd &lt;riva install dir&gt;\nbash riva_start.sh\n</code></pre></p>","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#step-3-launch-and-run-the-app","title":"Step 3: Launch and Run the App","text":"","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#step-31","title":"Step 3.1","text":"<p>Launch the <code>ehr_query_llm:llm</code> container: <pre><code>sudo ./dev_container launch --img ehr_query_llm:llm --add-volume &lt;your_model_dir&gt;\n</code></pre> * Note, if the parent directory of  is not <code>/models</code> you must update the asr_llm_tts.yaml and ehr.yaml files with the complete path to your model inside the container. You will also need to update the run_lmm.sh so the correct directory is exported in the <code>set_transformer_cache()</code> function. (You can determine these paths by looking in <code>/workspace/volumes</code> inside the launched container) or you can use the following <code>sed</code> commands: <p><code>sed -i -e 's#^model_path:.*#model_path: /workspace/volumes/&lt;your_model_dir&gt;#' asr_llm_tts.yaml</code></p> <p><code>sed -i -e 's#^model_path:.*#model_path: /workspace/volumes/&lt;your_model_dir&gt;#' agents_configs/ehr.yaml</code></p> <p><code>sed -i -e 's#^export TRANSFORMERS_CACHE=.*#export TRANSFORMERS_CACHE=\"/workspace/volumes/&lt;your_model_dir&gt;\"#' run_lmm.sh</code></p>","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#step-32","title":"Step 3.2","text":"<p>Then run the application: <pre><code>./applications/ehr_query_llm/lmm/run_lmm.sh\n</code></pre> This command builds ehr_query_llm/lmm, starts an LLM api server, then launches the ehr_query_llm app. Access the web interface at <code>https://127.0.0.1:8080</code>. Llama.cpp LLM server output is redirected to <code>./applications/ehr_query_llm/lmm/llama_cpp.log/</code>.</p> <p>To interact with ehr_query_llm using voice input:</p> <ul> <li>Press and hold the space bar to activate the voice recognition feature.</li> <li>Speak your query or command clearly while maintaining pressure on the space bar.</li> <li>Release the space bar when you've finished speaking to signal the end of your input.</li> <li>ehr_query_llm will then process your speech and generate a response.</li> </ul> <p>\u26a0\ufe0f Note: When running via VNC, you must have your keyboard focus on the VNC terminal that you are using to run ehr_query_llm in order to use the push-to-talk feature.</p>","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#stopping-instructions","title":"Stopping Instructions","text":"<p>To stop the main app, simply use <code>ctrl+c</code></p> <p>To stop Riva server: <pre><code>bash &lt;Riva_install_dir&gt;riva_stop.sh\n</code></pre></p>","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#asr_to_llm-application-arguments","title":"ASR_To_LLM Application arguments","text":"<p>The <code>asr_llm_tts.py</code> can receive the following optional cli argument:</p> <p><code>--sample-rate-hz</code>: The number of frames per second in audio streamed from the selected microphone.</p>","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#offline-mode","title":"Offline mode:","text":"<p>To enable offline use (no internet connection required): 1. First run the complete application as-is (This ensures all relevant models are downloaded) 2. Uncomment <code>set_offline_flags</code> at line 52 of run_lmm.sh</p>","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#troubleshooting","title":"Troubleshooting","text":"","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#adding-agents","title":"Adding Agents:","text":"<p>An Agent is an LLM (or LMM) with a task specific \"persona\" - such as a EHRAgent, etc., each with their own specific task. They also have a specific prompt tailored to complete that task, pre-fix prompts specific to the model used, grammar to constrain output, as well as context length. </p> <p>The AgentFrameworkOp works by using a SelectorAgent to select which Agent should be called upon based on user input.</p> <p>Adding a new \"agent\" for ehr_query_llm involves creating a new agent .py and YAML file in the <code>agents</code> directory, and in the new .py inheriting the Agent base class <code>agents/base_agent.py</code>.</p> <p>When creating a new agent .py file, you will need to define:</p> <p>Agent name: A class name which will also need to be added to the selector agent YAML, so it knows the agent is available to be called. process_request: A runtime method describing the logic of how an agent should carry out its task and send a response. </p> <p>For the YAML file, the fields needed are:</p> <p>name: This is the name of the agent, as well as what is used as the ZeroMQ topic when the agent  publishes its output. So you must make sure your listener is using this as the topic.</p> <p>user_prefix, bot_prefix, bot_rule_prefix, end_token:: These are dependent on the particular llm or lmm being used, and help to set the correct template for the model to interact with. </p> <p>agent_prompt: This gives the agent its \"persona\" - how it should behave, and for what purpose. It should have as much context as possible.</p> <p>ctx_length: Context length for the model. This determines how much output the agent is capable of generating. Smaller values lead to faster to first token time, but can be at the sacrifice of detail and verbosity.</p> <p>grammar: This is the BNF grammar used to constrain the models output. It can be a bit tricky to write. ChatGPT is great at writing these grammars for you if you give an example JSON of what you want. Also helpful, is the Llama.cpp BNF grammar guide.</p> <p>publish: The only important part of this field is the \"ags\" sub-field. This should be a list of your arg names. This is important as this is used as the list of keys to pull the relevant args from the LMM's response, and thus ensure the relevant fields are complete for a given tool use.</p> <p>For a specific example, please refer to the EHR Agent YAML file below:</p> <pre><code>description: This tool is used to search the patient's EHR in order to answer questions about the patient, or general questions about the patient's medical history.\nuser_prefix: \"&lt;|im_start|&gt;user\"\nbot_prefix: \"&lt;|im_start|&gt;assistant\"\nbot_rule_prefix: \"&lt;|im_start|&gt;system\"\nend_token: \"&lt;|im_end|&gt;\"\nagent_prompt: |\n  You are NVIDIA's EHR Agent,your job is to assist surgeons as they prepare for surgery.\n  You are an expert when it comes to surgery and medical topics - and answer all questions to the best of your abilities.\n  The patient has signed consent for you to access and discuss all of their electronic records.\n  Be as concise as you can be with your answers.\n\n  You NEVER make-up information that isn't grounded in the provided medical documents.\n\n  If applicable, include the relevant date (use sparingly)\n  The following medical documents may be helpful to answer the surgeon's question:\n  {documents}\n  Use your expert knowledge and the above context to answer the surgeon.\n# This is the request that the LLM replies to, where '{text}' indicates where the transcribed\n# text is inserted into the prompt\nrequest: \"{text}\"\n\nctx_length: 256\n\ngrammar: |\n  space ::= \" \"?\n  string ::= \"\\\"\" ([^\"\\\\])* \"\\\"\" space \n  root ::= \"{\" space \"\\\"name\\\"\" space \":\" space \"\\\"EHRAgent\\\"\" space \",\" space \"\\\"response\\\"\" space \":\" space string \"}\" space\n\npublish:\n  ags:\n    - \"response\"\n</code></pre> <p>With a complete YAML file, an agent should be able to use any new tool effectively. The only remaining step is ensure you have a ZeroMQ listener in the primary app with a topic that is the same as the tool's name.</p> <p>The <code>AgentFrameworkOp</code> is based on a ZeroMQ publish/subscribe pattern to send and receive messages from the Message Bus. It uses the <code>MessageReceiver</code> and <code>MessageSender</code> classes implemented in the <code>message_handling.py</code> Python script. The <code>MessageSender</code> creates a ZeroMQ PUB socket that binds to port 5555, accepts connections from any interface, and is used to broadcast messages and commands from the agent framework. It uses <code>send_json()</code> to send JSON-encoded messages with topics. A 0.1-second sleep on initialization prevents the \"slow joiner\" problem where early messages might be lost. The `MessageReceiver`` creates a ZeroMQ SUB socket connecting to port 5560 and uses receive_json() to get messages, with configurable blocking behavior.</p>","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#ehr-rag","title":"EHR RAG","text":"<p>To test new document formats for the database use test_db.py This will start the current Vector database in <code>./rag/ehr/db</code> and allow you to test different queries via the CLI to see what documents are returned.</p> <p>When changing the Vector DB, remove the previous database first: <pre><code>rm -rf ./rag/ehr/db\n</code></pre></p>","tags":["EHR","LLM","RAG"]},{"location":"applications/ehr_query_llm/lmm/#riva-cant-find-speaker-to-use","title":"Riva - Can't find speaker to use:","text":"<p>This usually means that some process is using the speaker you wish to use. This could be a Riva process that didn't exit correctly, or even Outlook loaded in your browser using your speakers to play notification sounds.</p> <p>First see what processes are using your speakers: <pre><code>pactl list sink-inputs | grep -E 'Sink Input|application.name|client|media.name|sink: '\n</code></pre> Sometimes that will give you all the information you need to kill the process responsible. If not, and the process has unfamiliar name such as \"speech-dispatcher-espeak-ng\" then find the responsible process ID: <pre><code>pgrep -l -f &lt;grep expression here (ex: 'speech')&gt;\n</code></pre> Once you know the PID's of the responsible process, kill them :) <pre><code>kill &lt;PID&gt;\n</code></pre></p>","tags":["EHR","LLM","RAG"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/","title":"Endoscopy Out of Body Detection Application","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 13, 2025 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.9.0 Contribution metric: Level 1 - Highly Reliable</p> <p></p>","tags":["Endoscopy","Classification"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#overview","title":"Overview","text":"<p>This application performs real-time detection of whether an endoscope is inside or outside the body during endoscopic procedures. For each input frame, the application:</p> <ul> <li>Classifies the frame as either \"in-body\" or \"out-of-body\"</li> <li>Provides a confidence score for the classification</li> <li>Outputs either to the console or to a CSV file (when analytics is enabled)</li> </ul> <p>Note: This application does not include visualization components.</p>","tags":["Endoscopy","Classification"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#quick-start","title":"Quick Start","text":"<p>Run the following command to build and launch the application on a supported Holoscan platform:</p> <pre><code>./dev_container build_and_run endoscopy_out_of_body_detection --language &lt;cpp/python&gt;\n</code></pre>","tags":["Endoscopy","Classification"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#prerequisites","title":"Prerequisites","text":"<ul> <li>NVIDIA Holoscan SDK (version 0.5 or higher)</li> <li>A supported Holoscan platform or workstation with a CUDA-capable NVIDIA GPU</li> <li>CMake build system</li> <li>FFmpeg (for data conversion)</li> </ul>","tags":["Endoscopy","Classification"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#data-requirements","title":"Data Requirements","text":"","tags":["Endoscopy","Classification"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#model-and-sample-data","title":"Model and Sample Data","text":"<p>The endoscopy detection model and sample datasets are available on NGC. The package includes:</p> <ul> <li>Pre-trained ONNX model for out-of-body detection: <code>out_of_body_detection.onnx</code></li> <li>Sample endoscopy video clips (MP4 format): <code>sample_clip_out_of_body_detection.mp4</code></li> </ul>","tags":["Endoscopy","Classification"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#data-preparation-optional","title":"Data Preparation (optional)","text":"<p>The application requires the input videos to be converted to GXF tensor format. This conversion happens automatically during building, but manual conversion can be done following these steps:</p> <ol> <li>Download and extract the data:</li> </ol> <pre><code>unzip [NGC_DOWNLOAD].zip -d &lt;data_dir&gt;\n</code></pre> <ol> <li>Convert the video to GXF tensor format using the provided script:</li> </ol> <pre><code>ffmpeg -i &lt;INPUT_VIDEO_FILE&gt; -fs 900M -pix_fmt rgb24 -f rawvideo pipe:1 | \\\npython convert_video_to_gxf_entities.py --width 256 --height 256 --channels 3 --framerate 30\n</code></pre> <p>Note: The conversion script (<code>convert_video_to_gxf_entities.py</code>) is available in the Holoscan SDK repository.</p> <ol> <li>Organize the data directory as follows:</li> </ol> <pre><code>data/\n\u2514\u2500\u2500 endoscopy_out_of_body_detection/\n  \u251c\u2500\u2500 LICENSE.md\n  \u251c\u2500\u2500 out_of_body_detection.onnx\n  \u251c\u2500\u2500 sample_clip_out_of_body_detection.gxf_entities\n  \u251c\u2500\u2500 sample_clip_out_of_body_detection.gxf_index\n  \u2514\u2500\u2500 sample_clip_out_of_body_detection.mp4\n</code></pre>","tags":["Endoscopy","Classification"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#configuration","title":"Configuration","text":"<p>The application uses <code>endoscopy_out_of_body_detection.yaml</code> for configuration. Key settings include:</p> <ul> <li>Input video parameters in the <code>replayer</code> section</li> <li>Model parameters in the <code>inference</code> section</li> <li>Analytics settings for data export</li> </ul>","tags":["Endoscopy","Classification"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#building","title":"Building","text":"<pre><code>./dev_container build\n./dev_container launch\n./run build endoscopy_out_of_body_detection &lt;cpp/python&gt;\n./run launch endoscopy_out_of_body_detection &lt;cpp/python&gt;\n</code></pre> <p>For more information, see the Holohub README.md.</p>","tags":["Endoscopy","Classification"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#running-the-application","title":"Running the Application","text":"","tags":["Endoscopy","Classification"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#basic-usage","title":"Basic Usage","text":"<p>For C++: From your build directory:</p> <pre><code>applications/endoscopy_out_of_body_detection/endoscopy_out_of_body_detection \\\n  --config endoscopy_out_of_body_detection.yaml \\\n  --data ../data/endoscopy_out_of_body_detection\n</code></pre> <p>For Python:</p> <pre><code>applications/endoscopy_out_of_body_detection/endoscopy_out_of_body_detection.py \\\n  --config endoscopy_out_of_body_detection.yaml \\\n  --data ../data/endoscopy_out_of_body_detection\n</code></pre>","tags":["Endoscopy","Classification"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#analytics-mode","title":"Analytics Mode","text":"<p>To enable analytics and export results to CSV:</p> <ol> <li>Set <code>enable_analytics: true</code> in the configuration file:</li> </ol> <pre><code># endoscopy_out_of_body_detection.yaml\nenable_analytics: true\n</code></pre> <ol> <li>Configure analytics output (optional):</li> </ol> <pre><code># Set output directory (default: current directory)\nexport HOLOSCAN_ANALYTICS_DATA_DIRECTORY=\"/path/to/output\"\n\n# Set output filename (default: data.csv)\nexport HOLOSCAN_ANALYTICS_DATA_FILE_NAME=\"results.csv\"\n</code></pre> <p>The application will create:</p> <ul> <li>A directory named after the application</li> <li>Subdirectories with timestamps for each run</li> <li>CSV files containing frame-by-frame classification results</li> </ul>","tags":["Endoscopy","Classification"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#output-format","title":"Output Format","text":"<ul> <li> <p>Console Mode: Displays \"Likely in-body\" or \"Likely out-of-body\" along with confidence scores for each frame.</p> </li> <li> <p>Analytics Mode: Outputs a CSV file with frame-by-frame classification results in the following format:</p> </li> </ul> <pre><code>In-body,Out-of-body,Confidence Score\n1,0,0.972432\n1,0,0.902066\n</code></pre>","tags":["Endoscopy","Classification"]},{"location":"applications/endoscopy_out_of_body_detection/python/","title":"Endoscopy Out of Body Detection Application","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 18, 2025 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.9.0 Contribution metric: Level 1 - Highly Reliable</p> <p></p>","tags":["Endoscopy","Classification"]},{"location":"applications/endoscopy_out_of_body_detection/python/#overview","title":"Overview","text":"<p>This application performs real-time detection of whether an endoscope is inside or outside the body during endoscopic procedures. For each input frame, the application:</p> <ul> <li>Classifies the frame as either \"in-body\" or \"out-of-body\"</li> <li>Provides a confidence score for the classification</li> <li>Outputs either to the console or to a CSV file (when analytics is enabled)</li> </ul> <p>Note: This application does not include visualization components.</p>","tags":["Endoscopy","Classification"]},{"location":"applications/endoscopy_out_of_body_detection/python/#quick-start","title":"Quick Start","text":"<p>Run the following command to build and launch the application on a supported Holoscan platform:</p> <pre><code>./dev_container build_and_run endoscopy_out_of_body_detection --language &lt;cpp/python&gt;\n</code></pre>","tags":["Endoscopy","Classification"]},{"location":"applications/endoscopy_out_of_body_detection/python/#prerequisites","title":"Prerequisites","text":"<ul> <li>NVIDIA Holoscan SDK (version 0.5 or higher)</li> <li>A supported Holoscan platform or workstation with a CUDA-capable NVIDIA GPU</li> <li>CMake build system</li> <li>FFmpeg (for data conversion)</li> </ul>","tags":["Endoscopy","Classification"]},{"location":"applications/endoscopy_out_of_body_detection/python/#data-requirements","title":"Data Requirements","text":"","tags":["Endoscopy","Classification"]},{"location":"applications/endoscopy_out_of_body_detection/python/#model-and-sample-data","title":"Model and Sample Data","text":"<p>The endoscopy detection model and sample datasets are available on NGC. The package includes:</p> <ul> <li>Pre-trained ONNX model for out-of-body detection: <code>out_of_body_detection.onnx</code></li> <li>Sample endoscopy video clips (MP4 format): <code>sample_clip_out_of_body_detection.mp4</code></li> </ul>","tags":["Endoscopy","Classification"]},{"location":"applications/endoscopy_out_of_body_detection/python/#data-preparation-optional","title":"Data Preparation (optional)","text":"<p>The application requires the input videos to be converted to GXF tensor format. This conversion happens automatically during building, but manual conversion can be done following these steps:</p> <ol> <li>Download and extract the data:</li> </ol> <pre><code>unzip [NGC_DOWNLOAD].zip -d &lt;data_dir&gt;\n</code></pre> <ol> <li>Convert the video to GXF tensor format using the provided script:</li> </ol> <pre><code>ffmpeg -i &lt;INPUT_VIDEO_FILE&gt; -fs 900M -pix_fmt rgb24 -f rawvideo pipe:1 | \\\npython convert_video_to_gxf_entities.py --width 256 --height 256 --channels 3 --framerate 30\n</code></pre> <p>Note: The conversion script (<code>convert_video_to_gxf_entities.py</code>) is available in the Holoscan SDK repository.</p> <ol> <li>Organize the data directory as follows:</li> </ol> <pre><code>data/\n\u2514\u2500\u2500 endoscopy_out_of_body_detection/\n  \u251c\u2500\u2500 LICENSE.md\n  \u251c\u2500\u2500 out_of_body_detection.onnx\n  \u251c\u2500\u2500 sample_clip_out_of_body_detection.gxf_entities\n  \u251c\u2500\u2500 sample_clip_out_of_body_detection.gxf_index\n  \u2514\u2500\u2500 sample_clip_out_of_body_detection.mp4\n</code></pre>","tags":["Endoscopy","Classification"]},{"location":"applications/endoscopy_out_of_body_detection/python/#configuration","title":"Configuration","text":"<p>The application uses <code>endoscopy_out_of_body_detection.yaml</code> for configuration. Key settings include:</p> <ul> <li>Input video parameters in the <code>replayer</code> section</li> <li>Model parameters in the <code>inference</code> section</li> <li>Analytics settings for data export</li> </ul>","tags":["Endoscopy","Classification"]},{"location":"applications/endoscopy_out_of_body_detection/python/#building","title":"Building","text":"<pre><code>./dev_container build\n./dev_container launch\n./run build endoscopy_out_of_body_detection &lt;cpp/python&gt;\n./run launch endoscopy_out_of_body_detection &lt;cpp/python&gt;\n</code></pre> <p>For more information, see the Holohub README.md.</p>","tags":["Endoscopy","Classification"]},{"location":"applications/endoscopy_out_of_body_detection/python/#running-the-application","title":"Running the Application","text":"","tags":["Endoscopy","Classification"]},{"location":"applications/endoscopy_out_of_body_detection/python/#basic-usage","title":"Basic Usage","text":"<p>For C++: From your build directory:</p> <pre><code>applications/endoscopy_out_of_body_detection/endoscopy_out_of_body_detection \\\n  --config endoscopy_out_of_body_detection.yaml \\\n  --data ../data/endoscopy_out_of_body_detection\n</code></pre> <p>For Python:</p> <pre><code>applications/endoscopy_out_of_body_detection/endoscopy_out_of_body_detection.py \\\n  --config endoscopy_out_of_body_detection.yaml \\\n  --data ../data/endoscopy_out_of_body_detection\n</code></pre>","tags":["Endoscopy","Classification"]},{"location":"applications/endoscopy_out_of_body_detection/python/#analytics-mode","title":"Analytics Mode","text":"<p>To enable analytics and export results to CSV:</p> <ol> <li>Set <code>enable_analytics: true</code> in the configuration file:</li> </ol> <pre><code># endoscopy_out_of_body_detection.yaml\nenable_analytics: true\n</code></pre> <ol> <li>Configure analytics output (optional):</li> </ol> <pre><code># Set output directory (default: current directory)\nexport HOLOSCAN_ANALYTICS_DATA_DIRECTORY=\"/path/to/output\"\n\n# Set output filename (default: data.csv)\nexport HOLOSCAN_ANALYTICS_DATA_FILE_NAME=\"results.csv\"\n</code></pre> <p>The application will create:</p> <ul> <li>A directory named after the application</li> <li>Subdirectories with timestamps for each run</li> <li>CSV files containing frame-by-frame classification results</li> </ul>","tags":["Endoscopy","Classification"]},{"location":"applications/endoscopy_out_of_body_detection/python/#output-format","title":"Output Format","text":"<ul> <li> <p>Console Mode: Displays \"Likely in-body\" or \"Likely out-of-body\" along with confidence scores for each frame.</p> </li> <li> <p>Analytics Mode: Outputs a CSV file with frame-by-frame classification results in the following format:</p> </li> </ul> <pre><code>In-body,Out-of-body,Confidence Score\n1,0,0.972432\n1,0,0.902066\n</code></pre>","tags":["Endoscopy","Classification"]},{"location":"applications/endoscopy_tool_tracking/cpp/","title":"Endoscopy Tool Tracking","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 21, 2025 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3, 2.0.0, 2.1.0, 2.2.0 Contribution metric: Level 0 - Core Stable</p> <p>Based on a LSTM (long-short term memory) stateful model, these applications demonstrate the use of custom components for tool tracking, including composition and rendering of text, tool position, and mask (as heatmap) combined with the original video stream.</p>","tags":["Endoscopy","Tracking","AJA"]},{"location":"applications/endoscopy_tool_tracking/cpp/#requirements","title":"Requirements","text":"<p>The provided applications are configured to either use capture cards for input stream, or a pre-recorded endoscopy video (replayer).</p> <p>Follow the setup instructions from the user guide to use the AJA capture card.</p> <p>Refer to the Deltacast documentation to use the Deltacast VideoMaster capture card.</p> <p>Refer to the Yuan documentation to use the Yuan QCap capture card.</p>","tags":["Endoscopy","Tracking","AJA"]},{"location":"applications/endoscopy_tool_tracking/cpp/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Endoscopy","Tracking","AJA"]},{"location":"applications/endoscopy_tool_tracking/cpp/#build-instructions","title":"Build Instructions","text":"<p>Please refer to the top level Holohub README.md file for information on how to build this application. In order to build with the Deltacast VideoMaster operator use <code>./run build --with deltacast_videomaster</code></p>","tags":["Endoscopy","Tracking","AJA"]},{"location":"applications/endoscopy_tool_tracking/cpp/#run-instructions","title":"Run Instructions","text":"<p>In your <code>build</code> directory, run the commands of your choice:</p> <ul> <li> <p>Using a pre-recorded video     <pre><code>sed -i -e 's#^source:.*#source: replayer#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\napplications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking --data &lt;data_dir&gt;/endoscopy\n</code></pre></p> </li> <li> <p>Using a vtk_renderer instead of holoviz     <pre><code>sed -i -e 's#^visualizer:.*#visualizer: \"vtk\"#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\napplications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking --data &lt;data_dir&gt;/endoscopy\n</code></pre></p> </li> <li> <p>Using a holoviz instead of vtk_renderer     <pre><code>sed -i -e 's#^visualizer:.*#visualizer: \"holoviz\"#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\napplications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking --data &lt;data_dir&gt;/endoscopy\n</code></pre></p> </li> <li> <p>Using an AJA card     <pre><code>sed -i -e 's#^source:.*#source: aja#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\napplications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking\n</code></pre></p> </li> <li> <p>Using a Deltacast card     <pre><code>sed -i -e '/^#.*deltacast_videomaster/s/^#//' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\nsed -i -e 's#^source:.*#source: deltacast#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\napplications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking\n</code></pre></p> </li> <li>Using a Yuan card     <pre><code>sed -i -e '/^#.*yuan_qcap/s/^#//' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\nsed -i -e 's#^source:.*#source: yuan#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\napplications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking\n</code></pre></li> </ul>","tags":["Endoscopy","Tracking","AJA"]},{"location":"applications/endoscopy_tool_tracking/python/","title":"Endoscopy Tool Tracking","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 21, 2025 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3, 2.0.0, 2.1.0, 2.2.0 Contribution metric: Level 1 - Highly Reliable</p> <p>Based on a LSTM (long-short term memory) stateful model, these applications demonstrate the use of custom components for tool tracking, including composition and rendering of text, tool position, and mask (as heatmap) combined with the original video stream.</p>","tags":["Endoscopy","Tracking","AJA"]},{"location":"applications/endoscopy_tool_tracking/python/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8+</li> <li>The provided applications are configured to either use the AJA or Yuan capture cards for input stream, or a pre-recorded endoscopy video (replayer).  Follow the setup instructions from the user guide to use the AJA capture card.</li> </ul>","tags":["Endoscopy","Tracking","AJA"]},{"location":"applications/endoscopy_tool_tracking/python/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Endoscopy","Tracking","AJA"]},{"location":"applications/endoscopy_tool_tracking/python/#run-instructions","title":"Run Instructions","text":"<p>To run this application, you'll need to configure your PYTHONPATH environment variable to locate the necessary python libraries based on your Holoscan SDK installation type.</p> <p>You should refer to the glossary for the terms defining specific locations within HoloHub.</p> <p>If your Holoscan SDK installation type is:</p> <ul> <li>python wheels:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <ul> <li>otherwise:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOSCAN_INSTALL_DIR&gt;/python/lib:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <p>Next, run the commands of your choice:</p> <p>This application should be run in the build directory of Holohub in order to load the GXF extensions. Alternatively, the relative path of the extensions in the corresponding yaml file can be modified to match path of the working directory.</p> <ul> <li> <p>Using a pre-recorded video     <pre><code>cd &lt;HOLOHUB_BUILD_DIR&gt;\npython3 &lt;HOLOHUB_SOURCE_DIR&gt;/applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking.py --source=replayer --data=&lt;DATA_DIR&gt;/endoscopy\n</code></pre></p> </li> <li> <p>Using an AJA card     <pre><code>cd &lt;HOLOHUB_BUILD_DIR&gt;\npython3  &lt;HOLOHUB_SOURCE_DIR&gt;/applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking.py --source=aja\n</code></pre></p> </li> <li> <p>Using a YUAN card     <pre><code>cd &lt;HOLOHUB_BUILD_DIR&gt;\npython3  &lt;HOLOHUB_SOURCE_DIR&gt;/applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking.py --source=yuan\n</code></pre></p> </li> </ul>","tags":["Endoscopy","Tracking","AJA"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/cpp/","title":"H.264 Endoscopy Tool Tracking Application","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: January 16, 2025 Language: C++ Latest version: 2.1 Minimum Holoscan SDK version: 2.6.0 Tested Holoscan SDK versions: 2.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The application showcases how to use H.264 video source as input to and output from the Holoscan pipeline. This application is a modified version of Endoscopy Tool Tracking reference application in Holoscan SDK that supports H.264 elementary streams as the input and output.</p> <p>The H.264 video decode operators do not adjust framerate as it reads the elementary stream input. As a result the video stream can be displayed as quickly as the decoding can be performed. This application uses <code>PeriodicCondition</code> to play video at the same speed as the source video.</p>","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/cpp/#requirements","title":"Requirements","text":"<p>This application is configured to use H.264 elementary stream from endoscopy sample data as input. The recording of the output can be enabled by setting <code>record_output</code> flag in the config file to <code>true</code>. If the <code>record_output</code> flag in the config file is set to <code>true</code>, the output of the pipeline is again recorded to a H.264 elementary stream on the disk, file name / path for this can be specified in the 'h264_endoscopy_tool_tracking.yaml' file.</p>","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/cpp/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded when building the application.</p>","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/cpp/#building-and-running-h264-endoscopy-tool-tracking-application","title":"Building and Running H.264 Endoscopy Tool Tracking Application","text":"<ul> <li>Building and running the application from the top level Holohub directory:</li> </ul> <pre><code># C++ version\n./dev_container build_and_run h264_endoscopy_tool_tracking --language cpp\n\n# Python version\n./dev_container build_and_run h264_endoscopy_tool_tracking --language python\n</code></pre> <p>Important: on aarch64, applications also need tegra folder mounted inside the container and the <code>LD_LIBRARY_PATH</code> environment variable should be updated to include tegra folder path.</p> <p>Open and edit the Dockerfile and uncomment line 66:</p> <pre><code># Uncomment the following line for aarch64 support\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/aarch64-linux-gnu/tegra/\n</code></pre>","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/cpp/#enable-recording-of-the-output","title":"Enable recording of the output","text":"<p>The recording of the output can be enabled by setting <code>record_output</code> flag in the config file <code>&lt;build_dir&gt;/applications/h264/endoscopy_tool_tracking/h264_endoscopy_tool_tracking.yaml</code> to <code>true</code>.</p>","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/cpp/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./dev_container vscode h264\n</code></pre>","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/cpp/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/cpp/#c","title":"C++","text":"<p>Use the (gdb) h264_endoscopy_tool_tracking/cpp launch profile to run and debug the C++ application.</p>","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/cpp/#python","title":"Python","text":"<p>There are a couple of launch profiles configured for this application:</p> <ol> <li>(debugpy) h264_endoscopy_tool_tracking/python: Launch the h.264 Endoscopy Tool Tracking application with the ability to debug Python code.</li> <li>(pythoncpp) h264_endoscopy_tool_tracking/python: Launch the h.264 Endoscopy Tool Tracking application with the ability to debug both Python and C++ code.</li> </ol>","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/python/","title":"H.264 Endoscopy Tool Tracking Application","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: October 31, 2024 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 2.6.0 Tested Holoscan SDK versions: 2.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The application showcases how to use H.264 video source as input to and output from the Holoscan pipeline. This application is a modified version of Endoscopy Tool Tracking reference application in Holoscan SDK that supports H.264 elementary streams as the input and output.</p> <p>The H.264 video decode operators do not adjust framerate as it reads the elementary stream input. As a result the video stream can be displayed as quickly as the decoding can be performed. This application uses <code>PeriodicCondition</code> to play video at the same speed as the source video.</p>","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/python/#requirements","title":"Requirements","text":"<p>This application is configured to use H.264 elementary stream from endoscopy sample data as input. The recording of the output can be enabled by setting <code>record_output</code> flag in the config file to <code>true</code>. If the <code>record_output</code> flag in the config file is set to <code>true</code>, the output of the pipeline is again recorded to a H.264 elementary stream on the disk, file name / path for this can be specified in the 'h264_endoscopy_tool_tracking.yaml' file.</p>","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/python/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded when building the application.</p>","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/python/#building-and-running-h264-endoscopy-tool-tracking-application","title":"Building and Running H.264 Endoscopy Tool Tracking Application","text":"<ul> <li>Building and running the application from the top level Holohub directory:</li> </ul> <pre><code># C++ version\n./dev_container build_and_run h264_endoscopy_tool_tracking --language cpp\n\n# Python version\n./dev_container build_and_run h264_endoscopy_tool_tracking --language python\n</code></pre> <p>Important: on aarch64, applications also need tegra folder mounted inside the container and the <code>LD_LIBRARY_PATH</code> environment variable should be updated to include tegra folder path.</p> <p>Open and edit the Dockerfile and uncomment line 66:</p> <pre><code># Uncomment the following line for aarch64 support\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/aarch64-linux-gnu/tegra/\n</code></pre>","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/python/#enable-recording-of-the-output","title":"Enable recording of the output","text":"<p>The recording of the output can be enabled by setting <code>record_output</code> flag in the config file <code>&lt;build_dir&gt;/applications/h264/endoscopy_tool_tracking/h264_endoscopy_tool_tracking.yaml</code> to <code>true</code>.</p>","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/python/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./dev_container vscode h264\n</code></pre>","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/python/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/python/#c","title":"C++","text":"<p>Use the (gdb) h264_endoscopy_tool_tracking/cpp launch profile to run and debug the C++ application.</p>","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/python/#python","title":"Python","text":"<p>There are a couple of launch profiles configured for this application:</p> <ol> <li>(debugpy) h264_endoscopy_tool_tracking/python: Launch the h.264 Endoscopy Tool Tracking application with the ability to debug Python code.</li> <li>(pythoncpp) h264_endoscopy_tool_tracking/python: Launch the h.264 Endoscopy Tool Tracking application with the ability to debug both Python and C++ code.</li> </ol>","tags":["Endoscopy","Video Decoding","Video Encoding"]},{"location":"applications/h264/h264_video_decode/cpp/","title":"H.264 Video Decode Reference Application","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: January 16, 2025 Language: C++ Latest version: 2.0 Minimum Holoscan SDK version: 2.6.0 Tested Holoscan SDK versions: 2.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This is a minimal reference application demonstrating usage of H.264 video decode operators. This application makes use of H.264 elementary stream reader operator for reading H.264 elementary stream input and uses Holoviz operator for rendering decoded data to the native window.</p> <p>The H.264 video decode operators do not adjust framerate as it reads the elementary stream input. As a result the video stream can be displayed as quickly as the decoding can be performed. This application uses <code>PeriodicCondition</code> to play video at the same speed as the source video.</p>","tags":["H.264","Video Decoding"]},{"location":"applications/h264/h264_video_decode/cpp/#requirements","title":"Requirements","text":"<p>This application is configured to use H.264 elementary stream from endoscopy sample data as input. To use any other stream, the filename / path for the input file can be specified in the 'h264_video_decode.yaml' file.</p>","tags":["H.264","Video Decoding"]},{"location":"applications/h264/h264_video_decode/cpp/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded when building the application.</p>","tags":["H.264","Video Decoding"]},{"location":"applications/h264/h264_video_decode/cpp/#building-and-running-h264-endoscopy-tool-tracking-application","title":"Building and Running H.264 Endoscopy Tool Tracking Application","text":"<ul> <li>Building and running the application from the top level Holohub directory:</li> </ul> <pre><code># C++ version\n./dev_container build_and_run h264_video_decode --docker_file applications/h264/Dockerfile --language cpp\n\n# Python version\n./dev_container build_and_run h264_video_decode --docker_file applications/h264/Dockerfile --language python\n</code></pre> <p>Important: on aarch64, applications also need tegra folder mounted inside the container and the <code>LD_LIBRARY_PATH</code> environment variable should be updated to include tegra folder path.</p> <p>Open and edit the Dockerfile and uncomment line 66:</p> <pre><code># Uncomment the following line for aarch64 support\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/aarch64-linux-gnu/tegra/\n</code></pre>","tags":["H.264","Video Decoding"]},{"location":"applications/h264/h264_video_decode/cpp/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./dev_container vscode h264\n</code></pre>","tags":["H.264","Video Decoding"]},{"location":"applications/h264/h264_video_decode/cpp/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["H.264","Video Decoding"]},{"location":"applications/h264/h264_video_decode/cpp/#c","title":"C++","text":"<p>Use the (gdb) h264_video_decode/cpp launch profile to run and debug the C++ application.</p>","tags":["H.264","Video Decoding"]},{"location":"applications/h264/h264_video_decode/cpp/#python","title":"Python","text":"<p>There are a couple of launch profiles configured for this application:</p> <ol> <li>(debugpy) h264_video_decode/python: Launch the h.264 Video Decode application with the ability to debug Python code.</li> <li>(pythoncpp) h264_video_decode/python: Launch the h.264 Video Decode application with the ability to debug both Python and C++ code.</li> </ol>","tags":["H.264","Video Decoding"]},{"location":"applications/h264/h264_video_decode/python/","title":"H.264 Video Decode Reference Application","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: October 31, 2024 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 2.6.0 Tested Holoscan SDK versions: 2.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This is a minimal reference application demonstrating usage of H.264 video decode operators. This application makes use of H.264 elementary stream reader operator for reading H.264 elementary stream input and uses Holoviz operator for rendering decoded data to the native window.</p> <p>The H.264 video decode operators do not adjust framerate as it reads the elementary stream input. As a result the video stream can be displayed as quickly as the decoding can be performed. This application uses <code>PeriodicCondition</code> to play video at the same speed as the source video.</p>","tags":["H264","Video Decoding"]},{"location":"applications/h264/h264_video_decode/python/#requirements","title":"Requirements","text":"<p>This application is configured to use H.264 elementary stream from endoscopy sample data as input. To use any other stream, the filename / path for the input file can be specified in the 'h264_video_decode.yaml' file.</p>","tags":["H264","Video Decoding"]},{"location":"applications/h264/h264_video_decode/python/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded when building the application.</p>","tags":["H264","Video Decoding"]},{"location":"applications/h264/h264_video_decode/python/#building-and-running-h264-endoscopy-tool-tracking-application","title":"Building and Running H.264 Endoscopy Tool Tracking Application","text":"<ul> <li>Building and running the application from the top level Holohub directory:</li> </ul> <pre><code># C++ version\n./dev_container build_and_run h264_video_decode --docker_file applications/h264/Dockerfile --language cpp\n\n# Python version\n./dev_container build_and_run h264_video_decode --docker_file applications/h264/Dockerfile --language python\n</code></pre> <p>Important: on aarch64, applications also need tegra folder mounted inside the container and the <code>LD_LIBRARY_PATH</code> environment variable should be updated to include tegra folder path.</p> <p>Open and edit the Dockerfile and uncomment line 66:</p> <pre><code># Uncomment the following line for aarch64 support\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/aarch64-linux-gnu/tegra/\n</code></pre>","tags":["H264","Video Decoding"]},{"location":"applications/h264/h264_video_decode/python/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./dev_container vscode h264\n</code></pre>","tags":["H264","Video Decoding"]},{"location":"applications/h264/h264_video_decode/python/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["H264","Video Decoding"]},{"location":"applications/h264/h264_video_decode/python/#c","title":"C++","text":"<p>Use the (gdb) h264_video_decode/cpp launch profile to run and debug the C++ application.</p>","tags":["H264","Video Decoding"]},{"location":"applications/h264/h264_video_decode/python/#python","title":"Python","text":"<p>There are a couple of launch profiles configured for this application:</p> <ol> <li>(debugpy) h264_video_decode/python: Launch the h.264 Video Decode application with the ability to debug Python code.</li> <li>(pythoncpp) h264_video_decode/python: Launch the h.264 Video Decode application with the ability to debug both Python and C++ code.</li> </ol>","tags":["H264","Video Decoding"]},{"location":"applications/high_speed_endoscopy/cpp/","title":"High-Speed Endoscopy","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: December 6, 2024 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The application showcases how high resolution cameras can be used to capture the scene, post-processed on GPU and displayed at high frame rate.</p>","tags":["Camera","Emergent"]},{"location":"applications/high_speed_endoscopy/cpp/#requirements","title":"Requirements","text":"<p>This application requires: 1. an Emergent Vision Technologies camera (see setup instructionsy</p>","tags":["Camera","Emergent"]},{"location":"applications/high_speed_endoscopy/cpp/#build-instructions","title":"Build Instructions","text":"<p>Please refer to the top level Holohub README.md file for information on how to build this application.</p> <p>\u26a0\ufe0f At this time, camera controls are hardcoded within the <code>gxf_emergent_source</code> extension. To update them at the application level, the GXF extension, and the application need to be rebuilt. For more information on the controls, refer to the EVT Camera Attributes Manual</p>","tags":["Camera","Emergent"]},{"location":"applications/high_speed_endoscopy/cpp/#run-instructions","title":"Run Instructions","text":"<p>First, go in your <code>build</code> or <code>install</code> directory. Then, run the commands of your choice:</p> <ul> <li> <p>RDMA disabled     <pre><code># C++\nsed -i -e 's#rdma:.*#rdma: false#' ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy.yaml \\\n    &amp;&amp; sudo ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy\n</code></pre></p> </li> <li> <p>RDMA enabled     <pre><code># C++\nsed -i -e 's#rdma:.*#rdma: true#' ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy.yaml \\\n    &amp;&amp; sudo MELLANOX_RINGBUFF_FACTOR=14 ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy\n</code></pre></p> </li> </ul> <p>\u2139\ufe0f The <code>MELLANOX_RINGBUFF_FACTOR</code> is used by the EVT driver to decide how much BAR1 size memory would be used on the dGPU. It can be changed to different number based on different use cases.</p>","tags":["Camera","Emergent"]},{"location":"applications/high_speed_endoscopy/python/","title":"High-Speed Endoscopy","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: October 8, 2024 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The application showcases how high resolution cameras can be used to capture the scene, post-processed on GPU and displayed at high frame rate.</p>","tags":["Camera","Emergent"]},{"location":"applications/high_speed_endoscopy/python/#requirements","title":"Requirements","text":"<p>This application requires: 1. an Emergent Vision Technologies camera (see setup instructionsy</p>","tags":["Camera","Emergent"]},{"location":"applications/high_speed_endoscopy/python/#run-instructions","title":"Run Instructions","text":"<p>TODO</p>","tags":["Camera","Emergent"]},{"location":"applications/holoviz/holoviz_hdr/","title":"Holoviz HDR","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: November 4, 2024 Language: C++ Latest version: 1.0.0 Minimum Holoscan SDK version: 2.5 Tested Holoscan SDK versions: 2.5 Contribution metric: Level 1 - Highly Reliable</p> <p> This application demonstrates displaying HDR images using the Holoviz operator. The application creates image data in HDR10 (BT2020 color space) with SMPTE ST2084 Perceptual Quantizer (PQ) EOTF and displays the image on the screen.</p> <p>Note that the screenshot above does not show the real HDR image on the display since it's not possible to take screenshots of HDR images.</p> <p>The Holoviz operator parameter <code>display_color_space</code> is used to set the color space. This allows HDR output on Linux distributions and displays supporting that feature. See https://docs.nvidia.com/holoscan/sdk-user-guide/visualization.html#hdr for more information.</p> <pre><code>    auto holoviz = make_operator&lt;ops::HolovizOp&gt;(\n        \"holoviz\",\n        // select the HDR10 ST2084 display color space\n        Arg(\"display_color_space\", ops::HolovizOp::ColorSpace::HDR10_ST2084));\n</code></pre>","tags":["Holoviz HDR","BT.2020","ST.2084","EOTF"]},{"location":"applications/holoviz/holoviz_hdr/#run-instructions","title":"Run Instructions","text":"<p>To build and start the application:</p> <pre><code>./dev_container build_and_run holoviz_hdr\n</code></pre>","tags":["Holoviz HDR","BT.2020","ST.2084","EOTF"]},{"location":"applications/holoviz/holoviz_srgb/","title":"Holoviz sRGB","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: October 18, 2024 Language: C++ Latest version: 1.0.0 Minimum Holoscan SDK version: 2.3 Tested Holoscan SDK versions: 2.3 Contribution metric: Level 1 - Highly Reliable</p> <p> This application demonstrates the handling of the sRGB color space supported by the Holoviz operator.</p> <p>The Holoviz operator can convert sRGB input images to linear color space before rendering and also can convert from linear color space to sRGB before writing to the frame buffer.</p> <p>sRGB color space can be enabled for input images and for the frame buffer independently. By default, the sRGB color space is disabled for both.</p> <p>By default, the Holoviz operator is auto detecting the input image format. Auto detection always assumes linear color space for input images. To change this to sRGB color space explicitly set the <code>image_format_</code> member of the input spec for that input image to a format ending with <code>SRGB</code>:</p> <pre><code>    // By default the image format is auto detected. Auto detection assumes linear color space,\n    // but we provide an sRGB encoded image. Create an input spec and change the image format to\n    // sRGB.\n    ops::HolovizOp::InputSpec input_spec(\"image\", ops::HolovizOp::InputType::COLOR);\n    input_spec.image_format_ = ops::HolovizOp::ImageFormat::R8G8B8_SRGB;\n\n    auto holoviz = make_operator&lt;ops::HolovizOp&gt;(\n        \"holoviz\",\n        Arg(\"tensors\", std::vector&lt;ops::HolovizOp::InputSpec&gt;{input_spec}));\n</code></pre> <p>By default, the frame buffer is using linear color space. To use the sRGB color space, set the <code>framebuffer_srbg</code> argument of the Holoviz operator to <code>true</code>:</p> <pre><code>    auto holoviz = make_operator&lt;ops::HolovizOp&gt;(\n        \"holoviz\",\n        // enable the sRGB frame buffer\n        Arg(\"framebuffer_srbg\", true));\n</code></pre>","tags":["Holoviz sRGB"]},{"location":"applications/holoviz/holoviz_srgb/#run-instructions","title":"Run Instructions","text":"<p>To build and start the application:</p> <pre><code>./dev_container build_and_run holoviz_srgb\n</code></pre>","tags":["Holoviz sRGB"]},{"location":"applications/holoviz/holoviz_ui/","title":"Holoviz UI","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: October 18, 2024 Language: C++ Latest version: 1.0.0 Minimum Holoscan SDK version: 2.5 Tested Holoscan SDK versions: 2.5 Contribution metric: Level 1 - Highly Reliable</p> <p> This application uses the layer callback provided by the Holoviz operator and leverages the Holoviz module API to add an UI layer with <code>Dear ImGui</code> elements and a geometry layer dynamically changing based on user input.</p>","tags":["Holoviz UI"]},{"location":"applications/holoviz/holoviz_ui/#run-instructions","title":"Run Instructions","text":"<p>To build and start the application:</p> <pre><code>./dev_container build_and_run holoviz_ui\n</code></pre>","tags":["Holoviz UI"]},{"location":"applications/holoviz/holoviz_vsync/","title":"Holoviz vsync","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: October 18, 2024 Language: C++ Latest version: 1.0.0 Minimum Holoscan SDK version: 2.3 Tested Holoscan SDK versions: 2.3 Contribution metric: Level 1 - Highly Reliable</p> <p> This application demonstrates the capability of the Holoviz operator to wait for the vertical blank of the display before updating the current image. It prints the displayed frames per second to the console, if sync to vertical blank is enabled the frames per second are capped to the display refresh rate.</p> <p>To enable syncing to vertical blank set the <code>vsync</code> parameter of the Holoviz operator to <code>true</code>:</p> <pre><code>    auto holoviz = make_operator&lt;ops::HolovizOp&gt;(\n        \"holoviz\",\n        // enable synchronization to vertical blank\n        Arg(\"vsync\", true));\n</code></pre> <p>By default, the Holoviz operator is not syncing to the vertical blank of the display.</p>","tags":["Holoviz vsync"]},{"location":"applications/holoviz/holoviz_vsync/#run-instructions","title":"Run Instructions","text":"<p>To build and start the application:</p> <pre><code>./dev_container build_and_run holoviz_vsync\n</code></pre>","tags":["Holoviz vsync"]},{"location":"applications/holoviz/holoviz_yuv/","title":"Holoviz YUV","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: October 18, 2024 Language: C++ Latest version: 1.0.0 Minimum Holoscan SDK version: 2.4 Tested Holoscan SDK versions: 2.4 Contribution metric: Level 1 - Highly Reliable</p> <p> This application demonstrates the capability of the Holoviz operator to display images in YUV (aka YCbCr) format.</p> <p>Holoviz supports multiple YUV formats including 420 and 422, 8 and 16 bit, single plane and multi plane. It supports BT.601, BT.709 and BT.2020 color conversions, narrow and full range and cosited even and midpoint chroma downsample positions.</p> <p>The application creates a GXF video buffer containing YUV 420 BT.601 extended range data.</p> <p>The YUV image properties are specified using a input spec structure:</p> <pre><code>    ops::HolovizOp::InputSpec input_spec(\"image\", ops::HolovizOp::InputType::COLOR);\n\n    // Set the YUV image format, model conversion and range for the input tensor.\n    input_spec.image_format_ = ops::HolovizOp::ImageFormat::Y8_U8V8_2PLANE_420_UNORM;\n    input_spec.yuv_model_conversion_ = ops::HolovizOp::YuvModelConversion::YUV_601;\n    input_spec.yuv_range_ = ops::HolovizOp::YuvRange::ITU_FULL;\n\n    auto holoviz = make_operator&lt;ops::HolovizOp&gt;(\n        \"holoviz\",\n        Arg(\"tensors\", std::vector&lt;ops::HolovizOp::InputSpec&gt;{input_spec}));\n</code></pre>","tags":["Holoviz YUV","YCbCr"]},{"location":"applications/holoviz/holoviz_yuv/#run-instructions","title":"Run Instructions","text":"<p>To build and start the application:</p> <pre><code>./dev_container build_and_run holoviz_yuv\n</code></pre>","tags":["Holoviz YUV","YCbCr"]},{"location":"applications/laser_detection_latency/evt_cam_calibration/","title":"Laser Detection Latency","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: arm64 Last modified: December 6, 2024 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.2.0 Contribution metric: Level 1 - Highly Reliable</p>"},{"location":"applications/laser_detection_latency/evt_cam_calibration/#introduction","title":"Introduction","text":"<p>The laser detection latency application demonstrates the latency differences between two different cameras visually.</p> <p> </p> <p>This folder contains three applications, please refer to the respective application folder for application and configuration file.</p> <ol> <li>USB Camera Calibration app:     This app is designed to do monitor registration by the USB camera. It uses a gray image which has April tags on all the four corners and is as shown below.</li> </ol> <p> </p> <p>The detection of these four April tags are done using <code>ApriltagDetectionOp</code>. For proper and correct monitor registration, the camera should be able to completely see all the four corners of the monitor. If it does not, the app will not output correct corners for the monitor. It is also important to make sure that the scene is well lit with no light sources in the back of the monitor.</p> <p>This app is designed using Logitech 4k Pro Webcam. If a different camera is being used, please change the camera settings in the python app or yaml configuration file.</p> <ol> <li>EVT Camera Calibration app:     This app is designed to do monitor registration by the Emergent Vision Technologies (EVT) camera. It uses the same gray image which has April tags on all the four corners as shown above. </li> </ol> <p>In this app the detection of these four April tags are done using <code>ApriltagDetectionOp</code> as well. For proper and correct monitor registration, the camera should be able to completely see all the four corners of the monitor. If it does not, the app will not output correct corners for the monitor. It is also important to make sure that the scene is well lit with no light sources in the back of the monitor.</p> <p>This app is designed using EVT HB-9000-G 25GE. If a different camera is being used, please change the camera settings in the python app or yaml configuration file.</p> <ol> <li>Laser Detection app:     The laser detection is the app that is run after the calibration apps detected the monitor successfully. This app uses two camera sources: one is EVT camera and other is USB camera. The video feed from both camera is used to detect laser pointed at the monitor. There are two icons that will be shown on the display. The white icon represents USB camera and the green icon represents the EVT camera. When the laser is detected the respective icons move to the coordinates. The laser detection algorithm is same for both camera sources.</li> </ol>"},{"location":"applications/laser_detection_latency/evt_cam_calibration/#hardware-requirements","title":"Hardware requirements","text":""},{"location":"applications/laser_detection_latency/evt_cam_calibration/#1-usb-camera","title":"1. USB camera","text":"<p>The app is designed using Logitech 4k Pro Webcam. A different webcam can also be used, but if resolution settings are different the application code and yaml file will need to be updated accordingly.</p>"},{"location":"applications/laser_detection_latency/evt_cam_calibration/#2-evt-camera","title":"2. EVT camera","text":"<p>Visit Holoscan SDK user guide to check the hardware requirements for EVT camera.</p>"},{"location":"applications/laser_detection_latency/evt_cam_calibration/#3-monitor","title":"3. Monitor","text":"<p>Any matte screen monitor with a refresh rate of at least 120fps is required to see the performance differences between the two camera sources.</p>"},{"location":"applications/laser_detection_latency/evt_cam_calibration/#4-laser-pointer","title":"4. Laser pointer","text":"<p>Please make sure to only buy laser pointers that are safe to use for viewing purposes.</p>"},{"location":"applications/laser_detection_latency/evt_cam_calibration/#setting-up-igx-development-kit","title":"Setting up IGX Development Kit","text":""},{"location":"applications/laser_detection_latency/evt_cam_calibration/#1-setup-the-evt-camera","title":"1. Setup the EVT camera","text":"<p>To setup the EVT camera, refer to Holoscan SDK user guide.</p>"},{"location":"applications/laser_detection_latency/evt_cam_calibration/#2-build-holohub-app-natively","title":"2. Build Holohub app natively","text":"<p>Currently EVT camera based apps are only available for native environment. To build the setup natively, download the Holohub repo and run following.</p> <pre><code>sudo ./run setup\n</code></pre>"},{"location":"applications/laser_detection_latency/evt_cam_calibration/#3-install-python3-requirements","title":"3. Install Python3 requirements","text":"<pre><code>sudo pip3 install -r applications/laser_detection_app/requirements.txt\n</code></pre>"},{"location":"applications/laser_detection_latency/evt_cam_calibration/#4-install-cvcuda","title":"4. Install CVCUDA","text":"<p>Download and install the latest CVCUDA package. Replace the latest version in the below commands. <pre><code>wget -O cvcuda-lib-0.10.1_beta-cuda12-aarch64-linux.deb https://github.com/CVCUDA/CV-CUDA/releases/download/v0.10.1-beta/cvcuda-lib-0.10.1_beta-cuda12-aarch64-linux.deb\nwget -O cvcuda-python3.10-0.10.1_beta-cuda12-aarch64-linux.deb https://github.com/CVCUDA/CV-CUDA/releases/download/v0.10.1-beta/cvcuda-python3.10-0.10.1_beta-cuda12-aarch64-linux.deb\nsudo dpkg -i cvcuda-lib-0.10.1_beta-cuda12-aarch64-linux.deb\nsudo dpkg -i cvcuda-python3.10-0.10.1_beta-cuda12-aarch64-linux.deb\n</code></pre></p>"},{"location":"applications/laser_detection_latency/evt_cam_calibration/#5-setup-cuapriltags","title":"5. Setup cuApriltags","text":"<p>Download the latest cuApriltags library and header file, and set it up on IGX Orin as follows: <pre><code>sudo mkdir /opt/nvidia/cu-april-tags\nsudo mkdir /opt/nvidia/cu-april-tags/include\nsudo mkdir /opt/nvidia/cu-april-tags/lib\nsudo cp libcuapriltags.a /opt/nvidia/cu-april-tags/lib/.\nsudo cp cuAprilTags.h /opt/nvidia/cu-april-tags/include/.\n</code></pre></p>"},{"location":"applications/laser_detection_latency/evt_cam_calibration/#running-the-sample-apps","title":"Running the sample apps","text":""},{"location":"applications/laser_detection_latency/evt_cam_calibration/#1-build-and-run-usb_cam_calibration","title":"1. Build and run <code>usb_cam_calibration</code>","text":"<p>Before running the app, make sure that the USB camera can see all the corners of the monitor. The <code>v4l2_camera</code> app can be used to verify it visually.</p> <p><pre><code>./run build usb_cam_calibration\nLD_PRELOAD=/usr/lib/aarch64-linux-gnu/nvidia/libnvjpeg.so ./run launch usb_cam_calibration\n</code></pre> This will output a file <code>usb-cali.npy</code> in the build directory.</p>"},{"location":"applications/laser_detection_latency/evt_cam_calibration/#2-build-and-run-evt_cam_calibration","title":"2. Build and run <code>evt_cam_calibration</code>","text":"<p>Before running the app, make sure that the EVT camera can see all the corners of the monitor. The <code>high_speed_endoscopy</code> app can be used to verify it visually. <pre><code>./run build evt_cam_calibration\nsudo ./run launch evt_cam_calibration\n</code></pre> This will output a file <code>evt-cali.npy</code> in the build directory.</p>"},{"location":"applications/laser_detection_latency/evt_cam_calibration/#note","title":"note:","text":"<p>Use <code>sudo</code> when running the application with EVT camera.</p>"},{"location":"applications/laser_detection_latency/evt_cam_calibration/#3-build-and-run-laser_detection","title":"3. Build and run <code>laser_detection</code>","text":"<p><pre><code>./run build laser_detection\nsudo LD_PRELOAD=/usr/lib/aarch64-linux-gnu/nvidia/libnvjpeg.so ./run launch laser_detection\n</code></pre> Now you can use the laser pointer and point it to the monitor. If the icons are not detecting the laser or they are moving in random fashion, then redo the calibration steps.</p>"},{"location":"applications/laser_detection_latency/evt_cam_calibration/#note_1","title":"note:","text":"<ul> <li>Please make sure that the monitor that is being used has matte screen. The glossy screen can create specular reflections and could damage the eye of the person looking at the monitor at certain angles.</li> <li>Please make sure to only buy laser pointers that are safe to use for viewing purposes.</li> </ul>"},{"location":"applications/laser_detection_latency/laser_detection/","title":"Laser Detection Latency","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: arm64 Last modified: December 6, 2024 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.2.0 Contribution metric: Level 1 - Highly Reliable</p>"},{"location":"applications/laser_detection_latency/laser_detection/#introduction","title":"Introduction","text":"<p>The laser detection latency application demonstrates the latency differences between two different cameras visually.</p> <p> </p> <p>This folder contains three applications, please refer to the respective application folder for application and configuration file.</p> <ol> <li>USB Camera Calibration app:     This app is designed to do monitor registration by the USB camera. It uses a gray image which has April tags on all the four corners and is as shown below.</li> </ol> <p> </p> <p>The detection of these four April tags are done using <code>ApriltagDetectionOp</code>. For proper and correct monitor registration, the camera should be able to completely see all the four corners of the monitor. If it does not, the app will not output correct corners for the monitor. It is also important to make sure that the scene is well lit with no light sources in the back of the monitor.</p> <p>This app is designed using Logitech 4k Pro Webcam. If a different camera is being used, please change the camera settings in the python app or yaml configuration file.</p> <ol> <li>EVT Camera Calibration app:     This app is designed to do monitor registration by the Emergent Vision Technologies (EVT) camera. It uses the same gray image which has April tags on all the four corners as shown above. </li> </ol> <p>In this app the detection of these four April tags are done using <code>ApriltagDetectionOp</code> as well. For proper and correct monitor registration, the camera should be able to completely see all the four corners of the monitor. If it does not, the app will not output correct corners for the monitor. It is also important to make sure that the scene is well lit with no light sources in the back of the monitor.</p> <p>This app is designed using EVT HB-9000-G 25GE. If a different camera is being used, please change the camera settings in the python app or yaml configuration file.</p> <ol> <li>Laser Detection app:     The laser detection is the app that is run after the calibration apps detected the monitor successfully. This app uses two camera sources: one is EVT camera and other is USB camera. The video feed from both camera is used to detect laser pointed at the monitor. There are two icons that will be shown on the display. The white icon represents USB camera and the green icon represents the EVT camera. When the laser is detected the respective icons move to the coordinates. The laser detection algorithm is same for both camera sources.</li> </ol>"},{"location":"applications/laser_detection_latency/laser_detection/#hardware-requirements","title":"Hardware requirements","text":""},{"location":"applications/laser_detection_latency/laser_detection/#1-usb-camera","title":"1. USB camera","text":"<p>The app is designed using Logitech 4k Pro Webcam. A different webcam can also be used, but if resolution settings are different the application code and yaml file will need to be updated accordingly.</p>"},{"location":"applications/laser_detection_latency/laser_detection/#2-evt-camera","title":"2. EVT camera","text":"<p>Visit Holoscan SDK user guide to check the hardware requirements for EVT camera.</p>"},{"location":"applications/laser_detection_latency/laser_detection/#3-monitor","title":"3. Monitor","text":"<p>Any matte screen monitor with a refresh rate of at least 120fps is required to see the performance differences between the two camera sources.</p>"},{"location":"applications/laser_detection_latency/laser_detection/#4-laser-pointer","title":"4. Laser pointer","text":"<p>Please make sure to only buy laser pointers that are safe to use for viewing purposes.</p>"},{"location":"applications/laser_detection_latency/laser_detection/#setting-up-igx-development-kit","title":"Setting up IGX Development Kit","text":""},{"location":"applications/laser_detection_latency/laser_detection/#1-setup-the-evt-camera","title":"1. Setup the EVT camera","text":"<p>To setup the EVT camera, refer to Holoscan SDK user guide.</p>"},{"location":"applications/laser_detection_latency/laser_detection/#2-build-holohub-app-natively","title":"2. Build Holohub app natively","text":"<p>Currently EVT camera based apps are only available for native environment. To build the setup natively, download the Holohub repo and run following.</p> <pre><code>sudo ./run setup\n</code></pre>"},{"location":"applications/laser_detection_latency/laser_detection/#3-install-python3-requirements","title":"3. Install Python3 requirements","text":"<pre><code>sudo pip3 install -r applications/laser_detection_app/requirements.txt\n</code></pre>"},{"location":"applications/laser_detection_latency/laser_detection/#4-install-cvcuda","title":"4. Install CVCUDA","text":"<p>Download and install the latest CVCUDA package. Replace the latest version in the below commands. <pre><code>wget -O cvcuda-lib-0.10.1_beta-cuda12-aarch64-linux.deb https://github.com/CVCUDA/CV-CUDA/releases/download/v0.10.1-beta/cvcuda-lib-0.10.1_beta-cuda12-aarch64-linux.deb\nwget -O cvcuda-python3.10-0.10.1_beta-cuda12-aarch64-linux.deb https://github.com/CVCUDA/CV-CUDA/releases/download/v0.10.1-beta/cvcuda-python3.10-0.10.1_beta-cuda12-aarch64-linux.deb\nsudo dpkg -i cvcuda-lib-0.10.1_beta-cuda12-aarch64-linux.deb\nsudo dpkg -i cvcuda-python3.10-0.10.1_beta-cuda12-aarch64-linux.deb\n</code></pre></p>"},{"location":"applications/laser_detection_latency/laser_detection/#5-setup-cuapriltags","title":"5. Setup cuApriltags","text":"<p>Download the latest cuApriltags library and header file, and set it up on IGX Orin as follows: <pre><code>sudo mkdir /opt/nvidia/cu-april-tags\nsudo mkdir /opt/nvidia/cu-april-tags/include\nsudo mkdir /opt/nvidia/cu-april-tags/lib\nsudo cp libcuapriltags.a /opt/nvidia/cu-april-tags/lib/.\nsudo cp cuAprilTags.h /opt/nvidia/cu-april-tags/include/.\n</code></pre></p>"},{"location":"applications/laser_detection_latency/laser_detection/#running-the-sample-apps","title":"Running the sample apps","text":""},{"location":"applications/laser_detection_latency/laser_detection/#1-build-and-run-usb_cam_calibration","title":"1. Build and run <code>usb_cam_calibration</code>","text":"<p>Before running the app, make sure that the USB camera can see all the corners of the monitor. The <code>v4l2_camera</code> app can be used to verify it visually.</p> <p><pre><code>./run build usb_cam_calibration\nLD_PRELOAD=/usr/lib/aarch64-linux-gnu/nvidia/libnvjpeg.so ./run launch usb_cam_calibration\n</code></pre> This will output a file <code>usb-cali.npy</code> in the build directory.</p>"},{"location":"applications/laser_detection_latency/laser_detection/#2-build-and-run-evt_cam_calibration","title":"2. Build and run <code>evt_cam_calibration</code>","text":"<p>Before running the app, make sure that the EVT camera can see all the corners of the monitor. The <code>high_speed_endoscopy</code> app can be used to verify it visually. <pre><code>./run build evt_cam_calibration\nsudo ./run launch evt_cam_calibration\n</code></pre> This will output a file <code>evt-cali.npy</code> in the build directory.</p>"},{"location":"applications/laser_detection_latency/laser_detection/#note","title":"note:","text":"<p>Use <code>sudo</code> when running the application with EVT camera.</p>"},{"location":"applications/laser_detection_latency/laser_detection/#3-build-and-run-laser_detection","title":"3. Build and run <code>laser_detection</code>","text":"<p><pre><code>./run build laser_detection\nsudo LD_PRELOAD=/usr/lib/aarch64-linux-gnu/nvidia/libnvjpeg.so ./run launch laser_detection\n</code></pre> Now you can use the laser pointer and point it to the monitor. If the icons are not detecting the laser or they are moving in random fashion, then redo the calibration steps.</p>"},{"location":"applications/laser_detection_latency/laser_detection/#note_1","title":"note:","text":"<ul> <li>Please make sure that the monitor that is being used has matte screen. The glossy screen can create specular reflections and could damage the eye of the person looking at the monitor at certain angles.</li> <li>Please make sure to only buy laser pointers that are safe to use for viewing purposes.</li> </ul>"},{"location":"applications/laser_detection_latency/usb_cam_calibration/","title":"Laser Detection Latency","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: arm64 Last modified: December 6, 2024 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.2.0 Contribution metric: Level 1 - Highly Reliable</p>"},{"location":"applications/laser_detection_latency/usb_cam_calibration/#introduction","title":"Introduction","text":"<p>The laser detection latency application demonstrates the latency differences between two different cameras visually.</p> <p> </p> <p>This folder contains three applications, please refer to the respective application folder for application and configuration file.</p> <ol> <li>USB Camera Calibration app:     This app is designed to do monitor registration by the USB camera. It uses a gray image which has April tags on all the four corners and is as shown below.</li> </ol> <p> </p> <p>The detection of these four April tags are done using <code>ApriltagDetectionOp</code>. For proper and correct monitor registration, the camera should be able to completely see all the four corners of the monitor. If it does not, the app will not output correct corners for the monitor. It is also important to make sure that the scene is well lit with no light sources in the back of the monitor.</p> <p>This app is designed using Logitech 4k Pro Webcam. If a different camera is being used, please change the camera settings in the python app or yaml configuration file.</p> <ol> <li>EVT Camera Calibration app:     This app is designed to do monitor registration by the Emergent Vision Technologies (EVT) camera. It uses the same gray image which has April tags on all the four corners as shown above. </li> </ol> <p>In this app the detection of these four April tags are done using <code>ApriltagDetectionOp</code> as well. For proper and correct monitor registration, the camera should be able to completely see all the four corners of the monitor. If it does not, the app will not output correct corners for the monitor. It is also important to make sure that the scene is well lit with no light sources in the back of the monitor.</p> <p>This app is designed using EVT HB-9000-G 25GE. If a different camera is being used, please change the camera settings in the python app or yaml configuration file.</p> <ol> <li>Laser Detection app:     The laser detection is the app that is run after the calibration apps detected the monitor successfully. This app uses two camera sources: one is EVT camera and other is USB camera. The video feed from both camera is used to detect laser pointed at the monitor. There are two icons that will be shown on the display. The white icon represents USB camera and the green icon represents the EVT camera. When the laser is detected the respective icons move to the coordinates. The laser detection algorithm is same for both camera sources.</li> </ol>"},{"location":"applications/laser_detection_latency/usb_cam_calibration/#hardware-requirements","title":"Hardware requirements","text":""},{"location":"applications/laser_detection_latency/usb_cam_calibration/#1-usb-camera","title":"1. USB camera","text":"<p>The app is designed using Logitech 4k Pro Webcam. A different webcam can also be used, but if resolution settings are different the application code and yaml file will need to be updated accordingly.</p>"},{"location":"applications/laser_detection_latency/usb_cam_calibration/#2-evt-camera","title":"2. EVT camera","text":"<p>Visit Holoscan SDK user guide to check the hardware requirements for EVT camera.</p>"},{"location":"applications/laser_detection_latency/usb_cam_calibration/#3-monitor","title":"3. Monitor","text":"<p>Any matte screen monitor with a refresh rate of at least 120fps is required to see the performance differences between the two camera sources.</p>"},{"location":"applications/laser_detection_latency/usb_cam_calibration/#4-laser-pointer","title":"4. Laser pointer","text":"<p>Please make sure to only buy laser pointers that are safe to use for viewing purposes.</p>"},{"location":"applications/laser_detection_latency/usb_cam_calibration/#setting-up-igx-development-kit","title":"Setting up IGX Development Kit","text":""},{"location":"applications/laser_detection_latency/usb_cam_calibration/#1-setup-the-evt-camera","title":"1. Setup the EVT camera","text":"<p>To setup the EVT camera, refer to Holoscan SDK user guide.</p>"},{"location":"applications/laser_detection_latency/usb_cam_calibration/#2-build-holohub-app-natively","title":"2. Build Holohub app natively","text":"<p>Currently EVT camera based apps are only available for native environment. To build the setup natively, download the Holohub repo and run following.</p> <pre><code>sudo ./run setup\n</code></pre>"},{"location":"applications/laser_detection_latency/usb_cam_calibration/#3-install-python3-requirements","title":"3. Install Python3 requirements","text":"<pre><code>sudo pip3 install -r applications/laser_detection_app/requirements.txt\n</code></pre>"},{"location":"applications/laser_detection_latency/usb_cam_calibration/#4-install-cvcuda","title":"4. Install CVCUDA","text":"<p>Download and install the latest CVCUDA package. Replace the latest version in the below commands. <pre><code>wget -O cvcuda-lib-0.10.1_beta-cuda12-aarch64-linux.deb https://github.com/CVCUDA/CV-CUDA/releases/download/v0.10.1-beta/cvcuda-lib-0.10.1_beta-cuda12-aarch64-linux.deb\nwget -O cvcuda-python3.10-0.10.1_beta-cuda12-aarch64-linux.deb https://github.com/CVCUDA/CV-CUDA/releases/download/v0.10.1-beta/cvcuda-python3.10-0.10.1_beta-cuda12-aarch64-linux.deb\nsudo dpkg -i cvcuda-lib-0.10.1_beta-cuda12-aarch64-linux.deb\nsudo dpkg -i cvcuda-python3.10-0.10.1_beta-cuda12-aarch64-linux.deb\n</code></pre></p>"},{"location":"applications/laser_detection_latency/usb_cam_calibration/#5-setup-cuapriltags","title":"5. Setup cuApriltags","text":"<p>Download the latest cuApriltags library and header file, and set it up on IGX Orin as follows: <pre><code>sudo mkdir /opt/nvidia/cu-april-tags\nsudo mkdir /opt/nvidia/cu-april-tags/include\nsudo mkdir /opt/nvidia/cu-april-tags/lib\nsudo cp libcuapriltags.a /opt/nvidia/cu-april-tags/lib/.\nsudo cp cuAprilTags.h /opt/nvidia/cu-april-tags/include/.\n</code></pre></p>"},{"location":"applications/laser_detection_latency/usb_cam_calibration/#running-the-sample-apps","title":"Running the sample apps","text":""},{"location":"applications/laser_detection_latency/usb_cam_calibration/#1-build-and-run-usb_cam_calibration","title":"1. Build and run <code>usb_cam_calibration</code>","text":"<p>Before running the app, make sure that the USB camera can see all the corners of the monitor. The <code>v4l2_camera</code> app can be used to verify it visually.</p> <p><pre><code>./run build usb_cam_calibration\nLD_PRELOAD=/usr/lib/aarch64-linux-gnu/nvidia/libnvjpeg.so ./run launch usb_cam_calibration\n</code></pre> This will output a file <code>usb-cali.npy</code> in the build directory.</p>"},{"location":"applications/laser_detection_latency/usb_cam_calibration/#2-build-and-run-evt_cam_calibration","title":"2. Build and run <code>evt_cam_calibration</code>","text":"<p>Before running the app, make sure that the EVT camera can see all the corners of the monitor. The <code>high_speed_endoscopy</code> app can be used to verify it visually. <pre><code>./run build evt_cam_calibration\nsudo ./run launch evt_cam_calibration\n</code></pre> This will output a file <code>evt-cali.npy</code> in the build directory.</p>"},{"location":"applications/laser_detection_latency/usb_cam_calibration/#note","title":"note:","text":"<p>Use <code>sudo</code> when running the application with EVT camera.</p>"},{"location":"applications/laser_detection_latency/usb_cam_calibration/#3-build-and-run-laser_detection","title":"3. Build and run <code>laser_detection</code>","text":"<p><pre><code>./run build laser_detection\nsudo LD_PRELOAD=/usr/lib/aarch64-linux-gnu/nvidia/libnvjpeg.so ./run launch laser_detection\n</code></pre> Now you can use the laser pointer and point it to the monitor. If the icons are not detecting the laser or they are moving in random fashion, then redo the calibration steps.</p>"},{"location":"applications/laser_detection_latency/usb_cam_calibration/#note_1","title":"note:","text":"<ul> <li>Please make sure that the monitor that is being used has matte screen. The glossy screen can create specular reflections and could damage the eye of the person looking at the monitor at certain angles.</li> <li>Please make sure to only buy laser pointers that are safe to use for viewing purposes.</li> </ul>"},{"location":"applications/matlab_gpu_coder/matlab_beamform/","title":"Ultrasound Beamforming with MATLAB GPU Coder","text":"<p> Authors: Holoscan Team (NVIDIA), MathWorks Team (MathWorks) Supported platforms: amd64, arm64 Last modified: June 4, 2024 Language: C++ Latest version: 1.0.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 4 - Experimental</p> <p>This application does real-time ultrasound beamforming of simulated data. The beamforming algorithm is implemented in MATLAB and converted to CUDA using MATLAB GPU Coder. When the application is run, Holoviz will display the beamformed data in real time.</p>","tags":["MATLAB","Ultrasound","Beamforming","CUDA"]},{"location":"applications/matlab_gpu_coder/matlab_beamform/#folder-structure","title":"Folder Structure","text":"<pre><code>matlab_beamform\n\u251c\u2500\u2500 data  # Data is generated with generate_data.mlx\n\u2502   \u2514\u2500\u2500 ultrasound_beamforming.bin  # Simulated ultrasound data\n\u251c\u2500\u2500 matlab  # MATLAB files\n\u2502   \u251c\u2500\u2500 generate_beamform_jetson.m  # MATLAB script to generate CUDA DLLs on Jetson\n\u2502   \u251c\u2500\u2500 generate_beamform_x86.m  # MATLAB script to generate CUDA DLLs on x86\n\u2502   \u251c\u2500\u2500 generate_data.mlx  # MATLAB script to generate simulated data\n\u2502   \u2514\u2500\u2500 matlab_beamform.m  # MATLAB function that CUDA code is generated from\n\u251c\u2500\u2500 CMakeLists.txt  # CMake build file\n\u251c\u2500\u2500 main.cpp  # Ultrasound beamforming app\n\u2514\u2500\u2500 matlab_beamform.yaml  # Ultrasound beamforming config\n</code></pre>","tags":["MATLAB","Ultrasound","Beamforming","CUDA"]},{"location":"applications/matlab_gpu_coder/matlab_beamform/#generate-simulated-data","title":"Generate Simulated Data","text":"<p>The required MATLAB Toolboxes are: * Phased Array System Toolbox * Communications Toolbox</p> <p>Simply run the script <code>matlab/generate_data.mlx</code> from MATLAB and a binary file <code>ultrasound_beamforming.bin</code> will be written to a top-level <code>data</code> folder. The binary file contains the simulated ultrasound data, prior to beamforming.</p>","tags":["MATLAB","Ultrasound","Beamforming","CUDA"]},{"location":"applications/matlab_gpu_coder/matlab_beamform/#generate-cuda-code-with-matlab-gpu-coder","title":"Generate CUDA Code with MATLAB GPU Coder","text":"","tags":["MATLAB","Ultrasound","Beamforming","CUDA"]},{"location":"applications/matlab_gpu_coder/matlab_beamform/#x86-ubuntu","title":"x86: Ubuntu","text":"<p>In order to generate the CUDA Code, start MATLAB and <code>cd</code> to the <code>matlab</code> folder and open the <code>generate_beamform_x86.m</code> script. Run the script and a folder <code>codegen/dll/matlab_beamform</code> will be generated in the <code>matlab_beamform</code> folder.</p>","tags":["MATLAB","Ultrasound","Beamforming","CUDA"]},{"location":"applications/matlab_gpu_coder/matlab_beamform/#arm64-jetson","title":"arm64: Jetson","text":"<p>On an x86 computer with MATLAB installed, <code>cd</code> to the <code>matlab</code> folder and open the <code>generate_beamform_jetson.m</code> script. Having an <code>ssh</code> connection to the Jetson device you want to build the CUDA DLLs on, specify the parameters of that connection in the <code>hwobj</code> on line 7, also replace <code>&lt;ABSOLUTE_PATH&gt;</code> of <code>cfg.Hardware.BuildDir</code> on line 39, as the absolute path (on the Jetson device) to <code>holohub</code> folder. Run the script and a folder <code>MATLAB_ws</code> will be created in the <code>matlab_beamform</code> folder.</p>","tags":["MATLAB","Ultrasound","Beamforming","CUDA"]},{"location":"applications/matlab_gpu_coder/matlab_beamform/#configure-holoscan-for-matlab","title":"Configure Holoscan for MATLAB","text":"<p>If you have not already, start by building HoloHub: <pre><code>./dev_container build\n</code></pre></p>","tags":["MATLAB","Ultrasound","Beamforming","CUDA"]},{"location":"applications/matlab_gpu_coder/matlab_beamform/#x86-ubuntu_1","title":"x86: Ubuntu","text":"<p>Define the environment variable: <pre><code>export MATLAB_ROOT=\"/usr/local/MATLAB\"\nexport MATLAB_VERSION=\"R2023b\"\n</code></pre> where you, if need be, replace <code>MATLAB_ROOT</code> with the location of your MATLAB install and <code>MATLAB_VERSION</code> with the correct version.</p> <p>Next, run the HoloHub Docker container: <pre><code>./dev_container launch \\\n    --add-volume ${MATLAB_ROOT}/${MATLAB_VERSION} \\\n    --docker_opts \"-e MATLAB_ROOT=/workspace/volumes/${MATLAB_VERSION}\"\n</code></pre></p>","tags":["MATLAB","Ultrasound","Beamforming","CUDA"]},{"location":"applications/matlab_gpu_coder/matlab_beamform/#arm64-jetson_1","title":"arm64: Jetson","text":"<p>The folder <code>MATLAB_ws</code>, created by MATLAB, mirrors the folder structure of the host machine and is therefore different from one user to another; hence, we need to specify the path to the <code>codegen</code> folder in the <code>CMakeLists.txt</code>, in order for the build to find the required libraries. Set the variable <code>REL_PTH_MATLAB_CODEGEN</code> to the relative path where the <code>codegen</code> folder is located in the <code>MATLAB_ws</code> folder. For example, if GPU Coder created the following folder structure on the Jetson device: <pre><code>matlab_beamform\n\u2514\u2500\u2500 MATLAB_ws\n    \u2514\u2500\u2500 R2023b\n        \u2514\u2500\u2500 C\n            \u2514\u2500\u2500 Users\n                \u2514\u2500\u2500 Jensen\n                    \u2514\u2500\u2500 holohub\n                        \u2514\u2500\u2500 applications\n                            \u2514\u2500\u2500 matlab_gpu_coder\n                                \u2514\u2500\u2500 matlab_beamform\n                                    \u2514\u2500\u2500 matlab\n                                        \u2514\u2500\u2500 codegen\n</code></pre> the variable should be set as: <pre><code>REL_PTH_MATLAB_CODEGEN=MATLAB_ws/R2023b/C/Users/Jensen/holohub/applications/matlab_gpu_coder/matlab_beamform/matlab/codegen\n</code></pre></p> <p>Next, run the HoloHub Docker container: <pre><code>./dev_container launch\n</code></pre></p>","tags":["MATLAB","Ultrasound","Beamforming","CUDA"]},{"location":"applications/matlab_gpu_coder/matlab_image_processing/","title":"Image Processing with MATLAB GPU Coder","text":"<p> Authors: Holoscan Team (NVIDIA), MathWorks Team (MathWorks) Supported platforms: amd64, arm64 Last modified: October 29, 2024 Language: C++ Latest version: 1.0.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 4 - Experimental</p> <p>This application does real-time image processing of Holoscan sample data. The image processing is implemented in MATLAB and converted to CUDA using GPU Coder. When the application is run, Holoviz will display the processed data in real time.</p> <p></p>","tags":["MATLAB","Image Processing","Signal Processing","Computer Vision","CUDA"]},{"location":"applications/matlab_gpu_coder/matlab_image_processing/#folder-structure","title":"Folder Structure","text":"<pre><code>matlab_image_processing\n\u251c\u2500\u2500 matlab  # MATLAB files\n\u2502   \u251c\u2500\u2500 generate_image_processing_jetson.m  # MATLAB script to generate CUDA DLLs on Jetson\n\u2502   \u251c\u2500\u2500 generate_image_processing_x86.m  # MATLAB script to generate CUDA DLLs on x86\n\u2502   \u251c\u2500\u2500 matlab_image_processing.m  # MATLAB function that CUDA code is generated from\n\u2502   \u2514\u2500\u2500 test_image_processing.m  # MATLAB script to test MATLAB function\n\u251c\u2500\u2500 CMakeLists.txt  # CMake build file\n\u251c\u2500\u2500 main.cpp  # Ultrasound beamforming app\n\u2514\u2500\u2500 matlab_image_processing.yaml  # Ultrasound beamforming config\n</code></pre>","tags":["MATLAB","Image Processing","Signal Processing","Computer Vision","CUDA"]},{"location":"applications/matlab_gpu_coder/matlab_image_processing/#generate-cuda-code-with-matlab-gpu-coder","title":"Generate CUDA Code with MATLAB GPU Coder","text":"","tags":["MATLAB","Image Processing","Signal Processing","Computer Vision","CUDA"]},{"location":"applications/matlab_gpu_coder/matlab_image_processing/#x86-ubuntu","title":"x86: Ubuntu","text":"<p>In order to generate the CUDA Code, start MATLAB and <code>cd</code> to the <code>matlab</code> folder and open the <code>generate_image_processing_x86.m</code> script. Run the script and a folder <code>codegen/dll/matlab_image_processing</code> will be generated in the <code>matlab_image_processing</code> folder.</p>","tags":["MATLAB","Image Processing","Signal Processing","Computer Vision","CUDA"]},{"location":"applications/matlab_gpu_coder/matlab_image_processing/#arm64-jetson","title":"arm64: Jetson","text":"<p>On an x86 computer with MATLAB installed, <code>cd</code> to the <code>matlab</code> folder and open the <code>generate_image_processing_jetson.m</code> script. Having an <code>ssh</code> connection to the Jetson device you want to build the CUDA DLLs on, specify the parameters of that connection in the <code>hwobj</code> on line 7, also replace <code>&lt;ABSOLUTE_PATH&gt;</code> of <code>cfg.Hardware.BuildDir</code> on line 39, as the absolute path (on the Jetson device) to <code>holohub</code> folder. Run the script and a folder <code>MATLAB_ws</code> will be created in the <code>matlab_image_processing</code> folder.</p>","tags":["MATLAB","Image Processing","Signal Processing","Computer Vision","CUDA"]},{"location":"applications/matlab_gpu_coder/matlab_image_processing/#configure-holoscan-for-matlab","title":"Configure Holoscan for MATLAB","text":"<p>If you have not already, start by building HoloHub: <pre><code>./dev_container build\n</code></pre></p>","tags":["MATLAB","Image Processing","Signal Processing","Computer Vision","CUDA"]},{"location":"applications/matlab_gpu_coder/matlab_image_processing/#x86-ubuntu_1","title":"x86: Ubuntu","text":"<p>Define the environment variable: <pre><code>export MATLAB_ROOT=\"/usr/local/MATLAB\"\nexport MATLAB_VERSION=\"R2023b\"\n</code></pre> where you, if need be, replace <code>MATLAB_ROOT</code> with the location of your MATLAB install and <code>MATLAB_VERSION</code> with the correct version.</p> <p>Next, run the HoloHub Docker container: <pre><code>./dev_container launch \\\n    --add-volume ${MATLAB_ROOT}/${MATLAB_VERSION} \\\n    --docker_opts \"-e MATLAB_ROOT=/workspace/volumes/${MATLAB_VERSION}\"\n</code></pre> and build the endoscopy tool tracking application to download the necessary data: <pre><code>./run build endoscopy_tool_tracking\n</code></pre></p>","tags":["MATLAB","Image Processing","Signal Processing","Computer Vision","CUDA"]},{"location":"applications/matlab_gpu_coder/matlab_image_processing/#arm64-jetson_1","title":"arm64: Jetson","text":"<p>The folder <code>MATLAB_ws</code>, created by MATLAB, mirrors the folder structure of the host machine and is therefore different from one user to another; hence, we need to specify the path to the <code>codegen</code> folder in the <code>CMakeLists.txt</code>, in order for the build to find the required libraries. Set the variable <code>REL_PTH_MATLAB_CODEGEN</code> to the relative path where the <code>codegen</code> folder is located in the <code>MATLAB_ws</code> folder. For example, if GPU Coder created the following folder structure on the Jetson device: <pre><code>matlab_gpu_coder\n\u2514\u2500\u2500 MATLAB_ws\n    \u2514\u2500\u2500 R2023b\n        \u2514\u2500\u2500 C\n            \u2514\u2500\u2500 Users\n                \u2514\u2500\u2500 Jensen\n                    \u2514\u2500\u2500 holohub\n                        \u2514\u2500\u2500 applications\n                            \u2514\u2500\u2500 matlab_gpu_coder\n                                \u2514\u2500\u2500 matlab_image_processing\n                                    \u2514\u2500\u2500 matlab\n                                        \u2514\u2500\u2500 codegen\n</code></pre> the variable should be set as: <pre><code>REL_PTH_MATLAB_CODEGEN=MATLAB_ws/R2023b/C/Users/Jensen/holohub/applications/matlab_gpu_coder/matlab_image_processing/matlab/codegen\n</code></pre> Next, run the HoloHub Docker container: <pre><code>./dev_container launch\n</code></pre> and build the endoscopy tool tracking application to download the necessary data: <pre><code>./run build endoscopy_tool_tracking\n</code></pre></p>","tags":["MATLAB","Image Processing","Signal Processing","Computer Vision","CUDA"]},{"location":"applications/multiai_endoscopy/cpp/","title":"Multi AI Application with SSD Detection and MONAI Endoscopic Tool Segmentation","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 5, 2025 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 2 - Trusted In this application, we show how to build a Multi AI application with detection and segmentation models, write postprocessing operators using CuPy and NumPy in Python tensor interop and MatX library (An efficient C++17 GPU numerical computing library with Python-like syntax) in C++ tensor interop, and pass multiple tensors from postprocessing to Holoviz.</p> <p> Fig. 1 Endoscopy (laparoscopy) image from a cholecystectomy (gallbladder removal surgery) showing tool detection and segmentation results from two concurrently executed AI models. Image courtesy of Research Group Camma, IHU Strasbourg and the University of Strasbourg (NGC Resource)</p> <p>Please refer to the README under ./app_dev_process to see the process of developing the applications.</p> <p>The application graph looks like: </p>","tags":["Multiai","SSD","bounding box","Detection","MONAI","Segmentation"]},{"location":"applications/multiai_endoscopy/cpp/#model","title":"Model","text":"<p>We combine two models from the single model applications SSD Tool Detection and MONAI Endoscopic Tool Segmentation:</p> <ul> <li>SSD model from NGC with additional NMS op: <code>epoch24_nms.onnx</code></li> <li>MONAI tool segmentation model from NGC: <code>model_endoscopic_tool_seg_sanitized_nhwc_in_nchw_out.onnx</code></li> </ul>","tags":["Multiai","SSD","bounding box","Detection","MONAI","Segmentation"]},{"location":"applications/multiai_endoscopy/cpp/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p>","tags":["Multiai","SSD","bounding box","Detection","MONAI","Segmentation"]},{"location":"applications/multiai_endoscopy/cpp/#requirements","title":"Requirements","text":"<p>Ensure you have installed the Holoscan SDK via one of the methods specified in the SDK user guide.</p> <p>The directory specified by <code>--data</code> at app runtime is assumed to contain three subdirectories, corresponding to the three NGC resources specified in Model and Data: <code>endoscopy</code>, <code>monai_tool_seg_model</code> and <code>ssd_model</code>.  These resources will be automatically downloaded to the holohub data directory when building the application.</p>","tags":["Multiai","SSD","bounding box","Detection","MONAI","Segmentation"]},{"location":"applications/multiai_endoscopy/cpp/#building-the-application","title":"Building the application","text":"<p>The repo level build command  <pre><code>./run build multiai_endoscopy\n</code></pre> will build one of the cpp apps <code>post-proc-cpu</code>. </p>","tags":["Multiai","SSD","bounding box","Detection","MONAI","Segmentation"]},{"location":"applications/multiai_endoscopy/cpp/#running-the-application","title":"Running the application","text":"","tags":["Multiai","SSD","bounding box","Detection","MONAI","Segmentation"]},{"location":"applications/multiai_endoscopy/cpp/#python-apps","title":"Python Apps","text":"<p>To run the Python application, you can make use of the run script <pre><code>./run launch multiai_endoscopy python\n</code></pre> Alternatively, to run this application, you'll need to configure your PYTHONPATH environment variable to locate the necessary python libraries based on your Holoscan SDK installation type.</p> <p>You should refer to the glossary for the terms defining specific locations within HoloHub.</p> <p>If your Holoscan SDK installation type is:</p> <ul> <li>python wheels:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <ul> <li>otherwise:</li> </ul> <p><pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOSCAN_INSTALL_DIR&gt;/python/lib:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> Next, run the application: <pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;/applications/multiai_endoscopy/python\npython3 multi_ai.py --data &lt;DATA_DIR&gt;\n</code></pre></p>","tags":["Multiai","SSD","bounding box","Detection","MONAI","Segmentation"]},{"location":"applications/multiai_endoscopy/cpp/#c-apps","title":"C++ Apps","text":"<p>There are three versions of C++ apps, with the only difference being that they implement the inference post-processing operator <code>DetectionPostprocessorOp</code> in different ways:</p> <ul> <li><code>post-proc-cpu</code>: Multi-AI app running the inference post-processing operator on the CPU using <code>std</code> features only.</li> <li><code>post-proc-matx-cpu</code>: Multi-AI app running the inference post-processing operator on the CPU using the MatX library.</li> <li><code>post-proc-matx-gpu</code>: Multi-AI app running  the inference post-processing operator on the GPU using MatX (CUDA).</li> </ul> <p>To run <code>post-proc-cpu</code>, since it already gets built with <code>./run build multiai_endoscopy</code>: <pre><code>./run launch multiai_endoscopy cpp\n</code></pre></p> <p>For the other two C++ applications, you'll need to build these without the run script as follows.</p> <p>To run <code>post-proc-matx-cpu</code> or <code>post-proc-matx-gpu</code>, first navigate to the app directory.</p> <pre><code>cd cpp/post-proc-matx-cpu\n</code></pre> <p>Next we need to configure and build the app.</p>","tags":["Multiai","SSD","bounding box","Detection","MONAI","Segmentation"]},{"location":"applications/multiai_endoscopy/cpp/#configuring","title":"Configuring","text":"<p>First, create a build folder:</p> <pre><code>mkdir -p build\n</code></pre> <p>then run CMake configure with:</p> <pre><code>cmake -S . -B build\n</code></pre> <p>Unless you make changes to <code>CMakeLists.txt</code>, this step only needs to be done once.</p>","tags":["Multiai","SSD","bounding box","Detection","MONAI","Segmentation"]},{"location":"applications/multiai_endoscopy/cpp/#building","title":"Building","text":"<p>The app can be built with:</p> <pre><code>cmake --build build\n</code></pre> <p>or equally:</p> <pre><code>cd build\nmake\n</code></pre>","tags":["Multiai","SSD","bounding box","Detection","MONAI","Segmentation"]},{"location":"applications/multiai_endoscopy/cpp/#running","title":"Running","text":"<p>You can run the app with:</p> <pre><code>./build/multi_ai --data &lt;DATA_DIR&gt;\n</code></pre>","tags":["Multiai","SSD","bounding box","Detection","MONAI","Segmentation"]},{"location":"applications/multiai_endoscopy/python/","title":"Multi AI Application with SSD Detection and MONAI Endoscopic Tool Segmentation","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 5, 2025 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 2 - Trusted In this application, we show how to build a Multi AI application with detection and segmentation models, write postprocessing operators using CuPy and NumPy in Python tensor interop and MatX library (An efficient C++17 GPU numerical computing library with Python-like syntax) in C++ tensor interop, and pass multiple tensors from postprocessing to Holoviz.</p> <p> Fig. 1 Endoscopy (laparoscopy) image from a cholecystectomy (gallbladder removal surgery) showing tool detection and segmentation results from two concurrently executed AI models. Image courtesy of Research Group Camma, IHU Strasbourg and the University of Strasbourg (NGC Resource)</p> <p>Please refer to the README under ./app_dev_process to see the process of developing the applications.</p> <p>The application graph looks like: </p>","tags":["Multiai","SSD","bounding box","Detection","MONAI","Segmentation"]},{"location":"applications/multiai_endoscopy/python/#model","title":"Model","text":"<p>We combine two models from the single model applications SSD Tool Detection and MONAI Endoscopic Tool Segmentation:</p> <ul> <li>SSD model from NGC with additional NMS op: <code>epoch24_nms.onnx</code></li> <li>MONAI tool segmentation model from NGC: <code>model_endoscopic_tool_seg_sanitized_nhwc_in_nchw_out.onnx</code></li> </ul>","tags":["Multiai","SSD","bounding box","Detection","MONAI","Segmentation"]},{"location":"applications/multiai_endoscopy/python/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p>","tags":["Multiai","SSD","bounding box","Detection","MONAI","Segmentation"]},{"location":"applications/multiai_endoscopy/python/#requirements","title":"Requirements","text":"<p>Ensure you have installed the Holoscan SDK via one of the methods specified in the SDK user guide.</p> <p>The directory specified by <code>--data</code> at app runtime is assumed to contain three subdirectories, corresponding to the three NGC resources specified in Model and Data: <code>endoscopy</code>, <code>monai_tool_seg_model</code> and <code>ssd_model</code>.  These resources will be automatically downloaded to the holohub data directory when building the application.</p>","tags":["Multiai","SSD","bounding box","Detection","MONAI","Segmentation"]},{"location":"applications/multiai_endoscopy/python/#building-the-application","title":"Building the application","text":"<p>The repo level build command  <pre><code>./run build multiai_endoscopy\n</code></pre> will build one of the cpp apps <code>post-proc-cpu</code>. </p>","tags":["Multiai","SSD","bounding box","Detection","MONAI","Segmentation"]},{"location":"applications/multiai_endoscopy/python/#running-the-application","title":"Running the application","text":"","tags":["Multiai","SSD","bounding box","Detection","MONAI","Segmentation"]},{"location":"applications/multiai_endoscopy/python/#python-apps","title":"Python Apps","text":"<p>To run the Python application, you can make use of the run script <pre><code>./run launch multiai_endoscopy python\n</code></pre> Alternatively, to run this application, you'll need to configure your PYTHONPATH environment variable to locate the necessary python libraries based on your Holoscan SDK installation type.</p> <p>You should refer to the glossary for the terms defining specific locations within HoloHub.</p> <p>If your Holoscan SDK installation type is:</p> <ul> <li>python wheels:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <ul> <li>otherwise:</li> </ul> <p><pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOSCAN_INSTALL_DIR&gt;/python/lib:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> Next, run the application: <pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;/applications/multiai_endoscopy/python\npython3 multi_ai.py --data &lt;DATA_DIR&gt;\n</code></pre></p>","tags":["Multiai","SSD","bounding box","Detection","MONAI","Segmentation"]},{"location":"applications/multiai_endoscopy/python/#c-apps","title":"C++ Apps","text":"<p>There are three versions of C++ apps, with the only difference being that they implement the inference post-processing operator <code>DetectionPostprocessorOp</code> in different ways:</p> <ul> <li><code>post-proc-cpu</code>: Multi-AI app running the inference post-processing operator on the CPU using <code>std</code> features only.</li> <li><code>post-proc-matx-cpu</code>: Multi-AI app running the inference post-processing operator on the CPU using the MatX library.</li> <li><code>post-proc-matx-gpu</code>: Multi-AI app running  the inference post-processing operator on the GPU using MatX (CUDA).</li> </ul> <p>To run <code>post-proc-cpu</code>, since it already gets built with <code>./run build multiai_endoscopy</code>: <pre><code>./run launch multiai_endoscopy cpp\n</code></pre></p> <p>For the other two C++ applications, you'll need to build these without the run script as follows.</p> <p>To run <code>post-proc-matx-cpu</code> or <code>post-proc-matx-gpu</code>, first navigate to the app directory.</p> <pre><code>cd cpp/post-proc-matx-cpu\n</code></pre> <p>Next we need to configure and build the app.</p>","tags":["Multiai","SSD","bounding box","Detection","MONAI","Segmentation"]},{"location":"applications/multiai_endoscopy/python/#configuring","title":"Configuring","text":"<p>First, create a build folder:</p> <pre><code>mkdir -p build\n</code></pre> <p>then run CMake configure with:</p> <pre><code>cmake -S . -B build\n</code></pre> <p>Unless you make changes to <code>CMakeLists.txt</code>, this step only needs to be done once.</p>","tags":["Multiai","SSD","bounding box","Detection","MONAI","Segmentation"]},{"location":"applications/multiai_endoscopy/python/#building","title":"Building","text":"<p>The app can be built with:</p> <pre><code>cmake --build build\n</code></pre> <p>or equally:</p> <pre><code>cd build\nmake\n</code></pre>","tags":["Multiai","SSD","bounding box","Detection","MONAI","Segmentation"]},{"location":"applications/multiai_endoscopy/python/#running","title":"Running","text":"<p>You can run the app with:</p> <pre><code>./build/multi_ai --data &lt;DATA_DIR&gt;\n</code></pre>","tags":["Multiai","SSD","bounding box","Detection","MONAI","Segmentation"]},{"location":"applications/multiai_ultrasound/cpp/","title":"Multi-AI Ultrasound","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 5, 2025 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates how to run multiple inference pipelines in a single application by leveraging the Holoscan Inference module, a framework that facilitates designing and executing inference applications in the Holoscan SDK.</p> <p>The Inference and the Processing operators use APIs from the Holoscan Inference module to extract data, initialize and execute the inference workflow, process, and transmit data for visualization.</p> <p>The applications uses models and echocardiogram data from iCardio.ai. The models include: - a Plax chamber model, that identifies four critical linear measurements of the heart - a Viewpoint Classifier model, that determines confidence of each frame to known 28 cardiac anatomical view as defined by the guidelines of the American Society of Echocardiography - an Aortic Stenosis Classification model, that provides a score which determines likeability for the presence of aortic stenosis</p> <p>The default configuration (<code>multiai_ultrasound.yaml</code>) runs on default GPU (GPU-0). Multi-AI Ultrasound application can be executed on multiple GPUs with the Holoscan SDK version 0.6 onwards. A sample configuration file for multi GPU configuration for multi-AI ultrasound application (<code>mgpu_multiai_ultrasound.yaml</code>) is present in both <code>cpp</code> and <code>python</code> applications. The multi-GPU configuration file is designed for a system with at least 2 GPUs connected to the same PCIE network.</p>","tags":["Ultrasound","MultiAI"]},{"location":"applications/multiai_ultrasound/cpp/#requirements","title":"Requirements","text":"<p>The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the echocardiogram (replayer). Follow the setup instructions from the user guide to use the AJA capture card.</p>","tags":["Ultrasound","MultiAI"]},{"location":"applications/multiai_ultrasound/cpp/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for Multi-AI Ultrasound Pipeline</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Ultrasound","MultiAI"]},{"location":"applications/multiai_ultrasound/cpp/#build-instructions","title":"Build Instructions","text":"<p>Please refer to the top level Holohub README.md file for information on how to build this application.</p>","tags":["Ultrasound","MultiAI"]},{"location":"applications/multiai_ultrasound/cpp/#run-instructions","title":"Run Instructions","text":"<p>In your <code>build</code> directory, run the commands of your choice:</p> <ul> <li> <p>Using a pre-recorded video     <pre><code>sed -i -e 's#^source:.*#source: replayer#' applications/multiai_ultrasound/cpp/multiai_ultrasound.yaml\napplications/multiai_ultrasound/cpp/multiai_ultrasound --data &lt;DATA_DIR&gt;/multiai_ultrasound\n</code></pre></p> </li> <li> <p>Using a pre-recorded video on multi-GPU system     <pre><code>sed -i -e 's#^source:.*#source: replayer#' applications/multiai_ultrasound/cpp/mgpu_multiai_ultrasound.yaml\napplications/multiai_ultrasound/cpp/multiai_ultrasound applications/multiai_ultrasound/cpp/mgpu_multiai_ultrasound.yaml --data &lt;DATA_DIR&gt;/multiai_ultrasound\n</code></pre></p> </li> <li> <p>Using an AJA card     <pre><code>sed -i -e 's#^source:.*#source: aja#' applications/multiai_ultrasound/cpp/multiai_ultrasound.yaml\napplications/multiai_ultrasound/cpp/multiai_ultrasound\n</code></pre></p> </li> </ul>","tags":["Ultrasound","MultiAI"]},{"location":"applications/multiai_ultrasound/python/","title":"Multi-AI Ultrasound","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 5, 2025 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates how to run multiple inference pipelines in a single application by leveraging the Holoscan Inference module, a framework that facilitates designing and executing inference applications in the Holoscan SDK.</p> <p>The Multi AI operators (inference and postprocessor) use APIs from the Holoscan Inference module to extract data, initialize and execute the inference workflow, process, and transmit data for visualization.</p> <p>The applications uses models and echocardiogram data from iCardio.ai. The models include: - a Plax chamber model, that identifies four critical linear measurements of the heart - a Viewpoint Classifier model, that determines confidence of each frame to known 28 cardiac anatomical view as defined by the guidelines of the American Society of Echocardiography - an Aortic Stenosis Classification model, that provides a score which determines likeability for the presence of aortic stenosis</p> <p>The default configuration (<code>multiai_ultrasound.yaml</code>) runs on default GPU (GPU-0). Multi-AI Ultrasound application can be executed on multiple GPUs with the Holoscan SDK version 0.6 onwards. A sample configuration file for multi GPU configuration for multi-AI ultrasound application (<code>mgpu_multiai_ultrasound.yaml</code>) is present in both <code>cpp</code> and <code>python</code> applications. The multi-GPU configuration file is designed for a system with at least 2 GPUs connected to the same PCIE network.</p>","tags":["Ultrasound","MultiAI"]},{"location":"applications/multiai_ultrasound/python/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8+</li> <li>The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the echocardiogram (replayer). Follow the setup instructions from the user guide to use the AJA capture card.</li> </ul>","tags":["Ultrasound","MultiAI"]},{"location":"applications/multiai_ultrasound/python/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for Multi-AI Ultrasound Pipeline</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Ultrasound","MultiAI"]},{"location":"applications/multiai_ultrasound/python/#run-instructions","title":"Run Instructions","text":"<p>To run this application, you'll need to configure your PYTHONPATH environment variable to locate the necessary python libraries based on your Holoscan SDK installation type.</p> <p>You should refer to the glossary for the terms defining specific locations within HoloHub.</p> <p>If your Holoscan SDK installation type is:</p> <ul> <li>python wheels:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <ul> <li>otherwise:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOSCAN_INSTALL_DIR&gt;/python/lib:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <p>Next, run the commands of your choice:</p> <ul> <li> <p>Using a pre-recorded video     <pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;/applications/multiai_ultrasound/python\npython3 multiai_ultrasound.py --source=replayer --data &lt;DATA_DIR&gt;/multiai_ultrasound\n</code></pre></p> </li> <li> <p>Using a pre-recorded video on multi-GPU system     <pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;/applications/multiai_ultrasound/python\npython3 multiai_ultrasound.py --config mgpu_multiai_ultrasound.yaml --source=replayer --data &lt;DATA_DIR&gt;/multiai_ultrasound\n</code></pre></p> </li> <li> <p>Using an AJA card     <pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;/applications/multiai_ultrasound/python\npython3 multiai_ultrasound.py --source=aja\n</code></pre></p> </li> </ul>","tags":["Ultrasound","MultiAI"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/","title":"Chat with NVIDIA NIM","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: November 13, 2024 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3, 2.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This is a sample application that shows how to use the OpenAI SDK with NVIDIA Inference Microservice (NIM). Whether you are using a NIM from build.nvidia.com/ or a self-hosted NIM, this sample application will work for both.</p>","tags":["OpenAI API","NIM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#quick-start","title":"Quick Start","text":"<ol> <li>Add API key in <code>nvidia_nim.yaml</code></li> <li><code>./dev_container build_and_run nvidia_nim_chat</code></li> </ol>","tags":["OpenAI API","NIM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#configuring-the-sample-application","title":"Configuring the sample application","text":"<p>Use the <code>nvidia_nim.yaml</code> configuration file to configure the sample application:</p>","tags":["OpenAI API","NIM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#connection-information","title":"Connection Information","text":"<pre><code>nim:\n  base_url: https://integrate.api.nvidia.com/v1\n  api_key:\n</code></pre> <p><code>base_url</code>: The URL of your NIM instance. Defaults to NVIDIA hosted NIMs. <code>api_key</code>: Your API key to access NVIDIA hosted NIMs.</p>","tags":["OpenAI API","NIM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#model-information","title":"Model Information","text":"<p>The <code>models</code> section in the YAML file is configured with multiple NVIDIA hosted models by default. This allows you to switch between different models easily within the application by sending the prompt <code>/m</code> to the application.</p> <p>Model parameters may be added or adjusted in the <code>models</code> section as well per model.</p>","tags":["OpenAI API","NIM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#run-the-sample-application","title":"Run the sample application","text":"<p>There are a couple of options to run the sample application:</p>","tags":["OpenAI API","NIM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#run-using-docker","title":"Run using Docker","text":"<p>To run the sample application with Docker, you must first build a Docker image that includes the sample application and its dependencies:</p> <pre><code># Build the Docker images from the root directory of Holohub\n./dev_container build --docker_file applications/nvidia_nim/Dockerfile\n</code></pre> <p>Then, run the Docker image:</p> <pre><code>./dev_container  launch\n</code></pre> <p>Continue to the Start the Application section once inside the Docker container.</p>","tags":["OpenAI API","NIM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#run-the-application-without-docker","title":"Run the Application without Docker","text":"<p>Install all dependencies from the <code>requirements.txt</code> file:</p> <pre><code># optionally create a virtual environment and activate it\npython3 -m venv .venv\nsource .venv/bin/activate\n\n# install the required packages\npip install -r applications/nvidia_nim/chat/requirements.txt\n</code></pre>","tags":["OpenAI API","NIM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#start-the-application","title":"Start the Application","text":"<p>To use the NIMs on build.nvidia.com/, configure your API key in the <code>nvidia_nim.yaml</code> configuration file and run the sample app as follows:</p> <p>note: you may also configure your api key using an environment variable. E.g., <code>export API_KEY=...</code></p> <pre><code># To use NVIDIA hosted NIMs available on build.nvidia.com, export your API key first\nexport API_KEY=[enter your api key here]\n\n./run launch nvidia_nim_chat\n</code></pre> <p>Have fun!</p>","tags":["OpenAI API","NIM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#connecting-with-locally-hosted-nims","title":"Connecting with Locally Hosted NIMs","text":"<p>To use a locally hosted NIM, first download and start the NIM. Then configure the <code>base_url</code> parameter in the <code>nvidia_nim.yaml</code> configuration file to point to your local NIM instance.</p> <p>The following example shows a NIM running locally and serving its APIs and the <code>meta-llama3-8b-instruct</code> model from <code>http://0.0.0.0:8000/v1</code>.</p> <pre><code>nim:\n  base_url: http://0.0.0.0:8000/v1/\n\nmodels:\n  llama3-8b-instruct:\n    model: meta-llama3-8b-instruct # name of the model serving by the NIM\n    # add/update/remove the following key/value pairs to configure the parameters for the model\n    top_p: 1\n    n: 1\n    max_tokens: 1024\n    frequency_penalty: 1.0\n</code></pre>","tags":["OpenAI API","NIM"]},{"location":"applications/nvidia_nim/nvidia_nim_imaging/","title":"NVIDIA NIM Imaging with Vista-3D","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: January 16, 2025 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 1.0.0 Tested Holoscan SDK versions: 1.0.0, 2.0.2 Contribution metric: Level 1 - Highly Reliable</p> <p>Vista-3D is a specialized interactive foundation model for segmenting and annotating human anatomies. This sample application demonstrates using the Vista-3D NVIDIA Inference Microservice (NIM) in a Holoscan pipeline.</p> <p>The application instructs the Vista-3D NIM API to process the given dataset and downloads and extracts the results of a segmentation NRRD file onto a local directory.</p> <p>Visit build.nvidia.com to learn more about Vista-3D and generate an API key to use with this application.</p>","tags":["OpenAI API","NIM"]},{"location":"applications/nvidia_nim/nvidia_nim_imaging/#quick-start","title":"Quick Start","text":"<ol> <li>Add API key in <code>nvidia_nim.yaml</code></li> <li><code>./dev_container build_and_run nvidia_nim_imaging</code></li> </ol>","tags":["OpenAI API","NIM"]},{"location":"applications/nvidia_nim/nvidia_nim_imaging/#configuring-the-sample-application","title":"Configuring the sample application","text":"<p>Use the <code>nvidia_nim.yaml</code> configuration file to configure the sample application:</p>","tags":["OpenAI API","NIM"]},{"location":"applications/nvidia_nim/nvidia_nim_imaging/#connection-information","title":"Connection Information","text":"<pre><code>nim:\n base_url: https://integrate.api.nvidia.com/v1\n api_key:\n</code></pre> <ul> <li><code>base_url</code>: The URL of your NIM instance. Defaults to NVIDIA-hosted NIMs.</li> <li><code>api_key</code>: Your API key to access NVIDIA-hosted NIMs.</li> </ul>","tags":["OpenAI API","NIM"]},{"location":"applications/nvidia_nim/nvidia_nim_imaging/#build-and-run-the-sample-application","title":"Build and Run the sample application","text":"<pre><code># Build the Docker images from the root directory of Holohub\n./dev_container build_and_run nvidia_nim_imaging\n</code></pre>","tags":["OpenAI API","NIM"]},{"location":"applications/nvidia_nim/nvidia_nim_imaging/#display-the-results","title":"Display the Results","text":"<p>In this section, we will show how to view the sample data and segmentation results returned from Vista-3D.</p> <ol> <li>Download 3D Slicer: https://download.slicer.org/</li> <li>Decompress and launch 3D Slicer    <pre><code>tar -xvzf Slicer-5.6.2-linux-amd64.tar.gz\n</code></pre></li> <li>Locate the sample data volume and the segmentation results in <code>build/nvidia_nim_imaging/applications/nvidia_nim/nvidia_nim_imaging</code> <pre><code>drwxr-xr-x 3 user domain-users \u00a0 \u00a0 4096 Jul \u00a03 11:41 ./\ndrwxr-xr-x 4 user domain-users \u00a0 \u00a0 4096 Jul \u00a03 11:40 ../\n-rw-r--r-- 1 user user         27263336 Jul 23 14:22 example-1_seg.nrrd\n-rw-r--r-- 1 user user         33037057 Jul 23 14:21 sample.nii.gz\n</code></pre></li> <li>In 3D Slicer, click File, Add Data and click Choose File(s) to Add.    From the Add Data into the scene dialog, find and add the <code>sample.nii.gz</code> file and the <code>example-1_seg.nrrd</code> file.    For the <code>sample.nrrd</code> file, select Segmentation and click Ok.    </li> <li>3D Slicer shall display the volume and the segmentation results as shown below:    </li> </ol>","tags":["OpenAI API","NIM"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/","title":"NVIDIA NV-CLIP","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: November 13, 2024 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3, 2.1.0, 2.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>NV-CLIP is a multimodal embeddings model for image and text, and this is a sample application that shows how to use the OpenAI SDK with NVIDIA Inference Microservice (NIM). Whether you are using a NIM from build.nvidia.com/ or a self-hosted NIM, this sample application will work for both.</p>","tags":["OpenAI API","NIM"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/#quick-start","title":"Quick Start","text":"<p>Get your API Key and start the sample application.</p> <ol> <li>Enter your API key in <code>nvidia_nim.yaml</code></li> <li><code>./dev_container build_and_run nvidia_nim_nvclip</code></li> </ol>","tags":["OpenAI API","NIM"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/#advanced","title":"Advanced","text":"","tags":["OpenAI API","NIM"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/#configuring-the-sample-application","title":"Configuring the sample application","text":"<p>Use the <code>nvidia_nim.yaml</code> configuration file to configure the sample application:</p>","tags":["OpenAI API","NIM"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/#nvidia-hosted-nv-clip-nim","title":"NVIDIA-Hosted NV-CLIP NIM","text":"<p>By default, the application is configured to use NVIDIA-hosted NV-CLIP NIM.</p> <pre><code>nim:\n base_url: https://integrate.api.nvidia.com/v1\n api_key:\n</code></pre> <p><code>base_url</code>: The URL of your NIM instance. Defaults to NVIDIA-hosted NIMs. <code>api_key</code>: Your API key to access NVIDIA-hosted NIMs.</p> <p>Note: you may also configure your API key using an environment variable. E.g., <code>export API_KEY=...</code></p> <pre><code># To use NVIDIA hosted NIMs available on build.nvidia.com, export your API key first\nexport API_KEY=[enter your API key here]\n</code></pre>","tags":["OpenAI API","NIM"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/#self-hosted-nims","title":"Self-Hosted NIMs","text":"<p>To use a self-hosted NIM, refer to the NV-CLIP NIM documentation to configure and start the NIM.</p> <p>Then, comment out the NVIDIA-hosted section and uncomment the self-hosted configuration section in the <code>nvidia_nim.yaml</code> file.</p> <pre><code>nim:\n  base_url: http://0.0.0.0:8000/v1/\n  encoding_format: float\n  api_key: NA\n  model: nvidia/nvclip-vit-h-14\n</code></pre>","tags":["OpenAI API","NIM"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/#build-the-application","title":"Build The Application","text":"<p>To run the sample application, you must first build a Docker image that includes the sample application and its dependencies:</p> <pre><code># Build the Docker images from the root directory of Holohub\n./dev_container build --docker_file applications/nvidia_nim/nvidia_nim_nvclip/Dockerfile\n</code></pre> <p>Then, run the Docker image:</p> <pre><code>./dev_container launch\n</code></pre>","tags":["OpenAI API","NIM"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/#run-the-application","title":"Run the Application","text":"<p>To use the NIMs on build.nvidia.com/, configure your API key in the <code>nvidia_nim.yaml</code> configuration file and run the sample app as follows:</p> <pre><code>./run launch nvidia_nim_nvclip\n</code></pre>","tags":["OpenAI API","NIM"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/#using-the-application","title":"Using the Application","text":"<p>Once the application is ready, it will prompt you to input URLs to the images you want to perform inference.</p> <pre><code>Enter a URL to an image: https://domain.to/my/image-cat.jpg\nDownloading image...\n\nEnter a URL to another image or hit ENTER to continue: https://domain.to/my/image-rabbit.jpg\nDownloading image...\n\nEnter a URL to another image or hit ENTER to continue: https://domain.to/my/image-dog.jpg\nDownloading image...\n</code></pre> <p>If there are no more images that you want to use, hit ENTER to continue and then enter a prompt:</p> <pre><code>Enter a URL to another image or hit ENTER to continue:\n\nEnter a prompt: Which image contains a rabbit?\n</code></pre> <p>The application will connect to the NIM to generate an answer and then calculate the cosine similarity between the images and the prompt:</p> <pre><code>\u2827 Generating...\nPrompt: Which image contains a rabbit?\nOutput:\nImage 1: 3.0%\nImage 2: 52.0%\nImage 3: 46.0%\n</code></pre>","tags":["OpenAI API","NIM"]},{"location":"applications/openigtlink_3dslicer/cpp/","title":"Holoscan SDK as an Inference Backend for 3D Slicer","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: January 16, 2025 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 2 - Trusted</p> <p>This application demonstrates how to interface Holoscan SDK with 3D Slicer, using the OpenIGTLink protocol. The application is shown in the application graph below.</p> <p></p> <p>In summary, the <code>openigtlink</code> transmit and receive operators are used in conjunction with an AI segmentation pipeline to:</p> <ol> <li>Send Holoscan sample video data from a node running Holoscan SDK, using <code>OpenIGTLinkTxOp</code>, to 3D Slicer running on a different node (simulating a video source connected to 3D Slicer):<ul> <li>For <code>cpp</code> application, the ultrasound sample data is sent.</li> <li>For <code>python</code> application, the colonoscopy sample data is sent.</li> </ul> </li> <li>Transmit the video data back to Holoscan SDK using OpenIGTLinkIF Module, and receive the data with the <code>OpenIGTLinkRxOp</code> operator.</li> <li>Perform an AI segmentation pipeline in Holoscan:<ul> <li>For <code>cpp</code> application, the ultrasound segmentation model is deployed.</li> <li>For <code>python</code> application, the colonoscopy segmentation model is deployed.</li> </ul> </li> <li>Use Holoviz in <code>headless</code> mode to render image and segmentation and then send the data back to 3D Slicer using the <code>OpenIGTLinkTxOp</code> operator.</li> </ol> <p>This workflow allows for sending image data from 3D Slicer over network to Holoscan SDK (running on either <code>x86</code> or <code>arm</code>), do some compute task (e.g., AI inference), and send the results back to 3D Slicer for visualization. Nodes can run distributed; for example, Holoscan SDK can run on an IGX Orin (Node A) sending the video data, 3D Slicer on a Windows laptop (Node B) and the AI inference pipeline on yet another machine (Node C). Also, note that the <code>openigtlink</code> operators can connect to any software/library that supports the OpenIGTLink protocol; here, 3D Slicer is used as it is a popular open source software package for image analysis and scientific visualization.</p> <p>For the <code>cpp</code> application, which does ultrasound segmentations the results look like</p> <p></p> <p>and for the <code>python</code> application, which does colonoscopy segmentation, the results look like</p> <p></p> <p>where the image data before Holoscan processing is shown in the left slice view, and the image data with segmentation overlay (after Holoscan processing) is shown in the right slice view.</p>","tags":["Streaming","Ethernet","3DSlicer","Segmentation"]},{"location":"applications/openigtlink_3dslicer/cpp/#run-instructions","title":"Run Instructions","text":"","tags":["Streaming","Ethernet","3DSlicer","Segmentation"]},{"location":"applications/openigtlink_3dslicer/cpp/#machine-running-3d-slicer","title":"Machine running 3D Slicer","text":"<p>On the machine running 3D Slicer do: 1. In 3D Slicer, open the Extensions Manager and install the <code>SlicerOpenIGTLink</code> extension. 2. Next, load the scene <code>openigtlink_3dslicer/scene/openigtlink_3dslicer.mrb</code> into 3D Slicer. 3. Go to the <code>OpenIGTLinkIF</code> module and make sure that the <code>SendToHoloscan</code> connector has the IP address of the machine running Holoscan SDK in the Hostname input box (under Properties). 4. Then activate the two connectors <code>ReceiveFromHoloscan</code> and <code>SendToHoloscan</code> (click Active check box under Properties).</p>","tags":["Streaming","Ethernet","3DSlicer","Segmentation"]},{"location":"applications/openigtlink_3dslicer/cpp/#machine-running-holoscan-sdk","title":"Machine running Holoscan SDK","text":"<p>On the machine running Holoscan SDK do the below steps.</p> <p>First, ensure that the <code>host_name</code> parameters of the two <code>OpenIGTLinkRxOp</code> operators (<code>openigtlink_tx_slicer_img</code> and <code>openigtlink_tx_slicer_holoscan</code>) have the IP address of the machine running 3D Slicer.</p> <p>Next, the application requires OpenIGTLink. For simplicity a DockerFile is available. To generate the container run: <pre><code>./dev_container build --docker_file ./applications/openigtlink_3dslicer/Dockerfile --img holohub:openigtlink\n</code></pre></p> <p>The application can then be built by launching this container and using the provided <code>run</code> script: <pre><code>./dev_container launch --img holohub:openigtlink\n./run build openigtlink_3dslicer\n</code></pre></p> <p>Then, to run the <code>python</code> application do: <pre><code>./run launch openigtlink_3dslicer python\n</code></pre> and to run the <code>cpp</code> application do: <pre><code>./run launch openigtlink_3dslicer cpp\n</code></pre></p>","tags":["Streaming","Ethernet","3DSlicer","Segmentation"]},{"location":"applications/openigtlink_3dslicer/python/","title":"Holoscan SDK as an Inference Backend for 3D Slicer","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: January 16, 2025 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 2 - Trusted</p> <p>This application demonstrates how to interface Holoscan SDK with 3D Slicer, using the OpenIGTLink protocol. The application is shown in the application graph below.</p> <p></p> <p>In summary, the <code>openigtlink</code> transmit and receive operators are used in conjunction with an AI segmentation pipeline to:</p> <ol> <li>Send Holoscan sample video data from a node running Holoscan SDK, using <code>OpenIGTLinkTxOp</code>, to 3D Slicer running on a different node (simulating a video source connected to 3D Slicer):<ul> <li>For <code>cpp</code> application, the ultrasound sample data is sent.</li> <li>For <code>python</code> application, the colonoscopy sample data is sent.</li> </ul> </li> <li>Transmit the video data back to Holoscan SDK using OpenIGTLinkIF Module, and receive the data with the <code>OpenIGTLinkRxOp</code> operator.</li> <li>Perform an AI segmentation pipeline in Holoscan:<ul> <li>For <code>cpp</code> application, the ultrasound segmentation model is deployed.</li> <li>For <code>python</code> application, the colonoscopy segmentation model is deployed.</li> </ul> </li> <li>Use Holoviz in <code>headless</code> mode to render image and segmentation and then send the data back to 3D Slicer using the <code>OpenIGTLinkTxOp</code> operator.</li> </ol> <p>This workflow allows for sending image data from 3D Slicer over network to Holoscan SDK (running on either <code>x86</code> or <code>arm</code>), do some compute task (e.g., AI inference), and send the results back to 3D Slicer for visualization. Nodes can run distributed; for example, Holoscan SDK can run on an IGX Orin (Node A) sending the video data, 3D Slicer on a Windows laptop (Node B) and the AI inference pipeline on yet another machine (Node C). Also, note that the <code>openigtlink</code> operators can connect to any software/library that supports the OpenIGTLink protocol; here, 3D Slicer is used as it is a popular open source software package for image analysis and scientific visualization.</p> <p>For the <code>cpp</code> application, which does ultrasound segmentations the results look like</p> <p></p> <p>and for the <code>python</code> application, which does colonoscopy segmentation, the results look like</p> <p></p> <p>where the image data before Holoscan processing is shown in the left slice view, and the image data with segmentation overlay (after Holoscan processing) is shown in the right slice view.</p>","tags":["Streaming","Ethernet","3DSlicer","Segmentation"]},{"location":"applications/openigtlink_3dslicer/python/#run-instructions","title":"Run Instructions","text":"","tags":["Streaming","Ethernet","3DSlicer","Segmentation"]},{"location":"applications/openigtlink_3dslicer/python/#machine-running-3d-slicer","title":"Machine running 3D Slicer","text":"<p>On the machine running 3D Slicer do: 1. In 3D Slicer, open the Extensions Manager and install the <code>SlicerOpenIGTLink</code> extension. 2. Next, load the scene <code>openigtlink_3dslicer/scene/openigtlink_3dslicer.mrb</code> into 3D Slicer. 3. Go to the <code>OpenIGTLinkIF</code> module and make sure that the <code>SendToHoloscan</code> connector has the IP address of the machine running Holoscan SDK in the Hostname input box (under Properties). 4. Then activate the two connectors <code>ReceiveFromHoloscan</code> and <code>SendToHoloscan</code> (click Active check box under Properties).</p>","tags":["Streaming","Ethernet","3DSlicer","Segmentation"]},{"location":"applications/openigtlink_3dslicer/python/#machine-running-holoscan-sdk","title":"Machine running Holoscan SDK","text":"<p>On the machine running Holoscan SDK do the below steps.</p> <p>First, ensure that the <code>host_name</code> parameters of the two <code>OpenIGTLinkRxOp</code> operators (<code>openigtlink_tx_slicer_img</code> and <code>openigtlink_tx_slicer_holoscan</code>) have the IP address of the machine running 3D Slicer.</p> <p>Next, the application requires OpenIGTLink. For simplicity a DockerFile is available. To generate the container run: <pre><code>./dev_container build --docker_file ./applications/openigtlink_3dslicer/Dockerfile --img holohub:openigtlink\n</code></pre></p> <p>The application can then be built by launching this container and using the provided <code>run</code> script: <pre><code>./dev_container launch --img holohub:openigtlink\n./run build openigtlink_3dslicer\n</code></pre></p> <p>Then, to run the <code>python</code> application do: <pre><code>./run launch openigtlink_3dslicer python\n</code></pre> and to run the <code>cpp</code> application do: <pre><code>./run launch openigtlink_3dslicer cpp\n</code></pre></p>","tags":["Streaming","Ethernet","3DSlicer","Segmentation"]},{"location":"applications/orsi/orsi_in_out_body/cpp/","title":"Orsi In - Out - Body Detection sample app","text":"<p> Authors: Jasper Hofman (Orsi Academy) Supported platforms: amd64, arm64 Last modified: October 8, 2024 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 3 - Developmental</p> <p>  Fig. 1: Example of anonymized result after inference </p>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_in_out_body/cpp/#introduction","title":"Introduction","text":"<p>In robotic surgery, anonymization of video is necessary to ensure privacy and protect patient data. During e.g. swapping of robotic tools or lens cleaning the endoscope is removed from the body, possibly capturing faces, recognizable tattoos or written patient data. Therefore, going out of body is a suiting definition for an anonymization boundary.</p>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_in_out_body/cpp/#pipeline","title":"Pipeline","text":"<p>  Fig. 2: Schematic overview of Holoscan application </p> <p>Towards realtime anonymization, a binary out-of-body classifier was trained and deployed using Holoscan platform. Figure 2 shows a schematic overview of the application. After capturing the frame, the alpha channel is dropped by the Format Converter operator. Additionally, the black padding borders added by the robotic system are removed, the tensor is resized to 512x512 pixels and the tensor type is converted from int [0, 255] to float [0, 1]. In the Anonymization Preprocessor operator the tensor pixel values of every collor channel are normalized using the corresponding means and standard deviations of the anonymization dataset. After model inference with the Multi-AI inference operator, the result frame is anonymized in the Orsi Visualizer operator according to the model output. The blurring is applied using a glsl program.</p>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_in_out_body/cpp/#controls","title":"Controls","text":"Action Control Enable anonymization B","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_in_out_body/cpp/#build-app","title":"Build app","text":"<pre><code>./run build orsi_in_out_body\n</code></pre>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_in_out_body/cpp/#launch-app","title":"Launch app","text":"<p>C++ </p> <pre><code>./run launch orsi_in_out_body cpp\n</code></pre> <p>Python</p> <pre><code>./run launch orsi_in_out_body python\n</code></pre> <p> </p>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_in_out_body/python/","title":"Orsi In - Out - Body Detection sample app","text":"<p> Authors: Jasper Hofman (Orsi Academy) Supported platforms: amd64, arm64 Last modified: June 26, 2024 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 3 - Developmental</p> <p>  Fig. 1: Example of anonymized result after inference </p>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_in_out_body/python/#introduction","title":"Introduction","text":"<p>In robotic surgery, anonymization of video is necessary to ensure privacy and protect patient data. During e.g. swapping of robotic tools or lens cleaning the endoscope is removed from the body, possibly capturing faces, recognizable tattoos or written patient data. Therefore, going out of body is a suiting definition for an anonymization boundary.</p>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_in_out_body/python/#pipeline","title":"Pipeline","text":"<p>  Fig. 2: Schematic overview of Holoscan application </p> <p>Towards realtime anonymization, a binary out-of-body classifier was trained and deployed using Holoscan platform. Figure 2 shows a schematic overview of the application. After capturing the frame, the alpha channel is dropped by the Format Converter operator. Additionally, the black padding borders added by the robotic system are removed, the tensor is resized to 512x512 pixels and the tensor type is converted from int [0, 255] to float [0, 1]. In the Anonymization Preprocessor operator the tensor pixel values of every collor channel are normalized using the corresponding means and standard deviations of the anonymization dataset. After model inference with the Multi-AI inference operator, the result frame is anonymized in the Orsi Visualizer operator according to the model output. The blurring is applied using a glsl program.</p>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_in_out_body/python/#controls","title":"Controls","text":"Action Control Enable anonymization B","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_in_out_body/python/#build-app","title":"Build app","text":"<pre><code>./run build orsi_in_out_body\n</code></pre>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_in_out_body/python/#launch-app","title":"Launch app","text":"<p>C++ </p> <pre><code>./run launch orsi_in_out_body cpp\n</code></pre> <p>Python</p> <pre><code>./run launch orsi_in_out_body python\n</code></pre> <p> </p>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_multi_ai_ar/cpp/","title":"Orsi Multi AI and AR sample app","text":"<p> Authors: Jasper Hofman (Orsi Academy) Supported platforms: amd64, arm64 Last modified: October 8, 2024 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 3 - Developmental</p> <p>  Fig. 1: Application screenshots  </p>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_multi_ai_ar/cpp/#introduction","title":"Introduction","text":"<p>3D models are used in surgery to improve patient outcomes. They provide information on patient specific anatomies that are not visible in the present surgical scene. Especially in robotic surgery these 3D models give great insights because they can be projected and aligned directly onto the endoscopic video. This augmented reality supports navigation for the surgeon in the console. The downside of the 3D model projection is that it occludes the surgical instruments, creating a possible hazardous situation for the patient. This application uses a deep learning segmentation model to identify non-organic items such as tools, clips and wires and projects them on top of the 3D model. This solves the occlusion problem and adds a sense of depth to the AR application. Figure 1 shows in-app screenshots where the 3D model is used to support navigation. The large tumor in green is visible as anatomical landmark.  In addition, the application features an out-of-body detector. Anonymization of video is necessary to ensure privacy and protect patient data. During e.g. swapping of robotic tools or lens cleaning the endoscope is removed from the body, possibly capturing faces, recognizable tattoos or written patient data. Therefore, going out of body is a suiting definition for an anonymization boundary.</p> <p>  Fig. 2: 3D model of kidney tumor case </p> <p>The application was successfully used to remove a kidney tumor during a partial nephrectomy. Figure 2 shows the kidney 3D model where the tumor is colored in yellow.</p>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_multi_ai_ar/cpp/#pipeline","title":"Pipeline","text":"<p>  Fig. 3: Schematic overview of Holoscan application </p> <p>Towards realtime segmentation, AR overlay and anonymization, a binary segmentation model for non-organic items and a binary out-of-body detector was trained and deployed using Holoscan platform. Figure 3 shows a schematic overview of the application. After capturing the frame, the alpha channel is dropped by the Format Converter operator. Additionally, the black padding borders added by the robotic system are removed, the tensor is resized to 512x512 pixels and the tensor type is converted from int [0, 255] to float [0, 1]. In the Segmentation Preprocessor and Anonymization Preprocessor operators the tensor pixel values of every color channel are normalized using the corresponding means and standard deviations of the segmentation and anonymization dataset respectively. After model inference with the Multi-AI inference operator, a sigmoid layer is applied to the model predictions by the Segmentation Postprocessor resulting in a binary segmentation mask. Additionally, the resizing and cropping operations are inverted by first resizing and then adding black padding borders as to not compromise resolution. In the Orsi Visualizer operator the 3D model is rendered using the VTK library and composited onto the endoscopic image using OpenGL. The pixels corresponding to non-organic items are passed through the 3D model layer using a glsl shader program. Finally, the frame is anonymized according to the out-of-body detector output.</p>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_multi_ai_ar/cpp/#controls","title":"Controls","text":"Action Control Enable/Disable anonymization B Enable/Disable manipulations T Load 3D model orientation preset CTRL + L Save current 3D model orientation as preset (will overwrite default preset) CTRL + S Rotate 3D model (3 degrees of freedom) Left Click + Drag Rotate 3D model (1 degree of freedom) CTRL + Left Click + Drag Zoom 3D model Right Click + Drag Translate 3D  model SHIFT + Left Click + Drag Enable/Disable 3D model E Enable/Disable segmentation overlay O Increase opacity 3D model + Decrease opacity 3D model - Toggle arterial tree 1 Toggle venous tree 2 Toggle ureter 4 Toggle parenchyma 5 Toggle tumor 6","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_multi_ai_ar/cpp/#build-app","title":"Build app","text":"<pre><code>./run build orsi_multi_ai_ar\n</code></pre>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_multi_ai_ar/cpp/#launch-app","title":"Launch app","text":"<pre><code>./run launch orsi_multi_ai_ar cpp\n</code></pre> <p>or</p> <pre><code>./run launch orsi_multi_ai_ar python\n</code></pre> <p> </p>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_multi_ai_ar/python/","title":"Orsi Multi AI and AR sample app","text":"<p> Authors: Jasper Hofman (Orsi Academy) Supported platforms: amd64, arm64 Last modified: June 26, 2024 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 3 - Developmental</p> <p>  Fig. 1: Application screenshots  </p>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_multi_ai_ar/python/#introduction","title":"Introduction","text":"<p>3D models are used in surgery to improve patient outcomes. They provide information on patient specific anatomies that are not visible in the present surgical scene. Especially in robotic surgery these 3D models give great insights because they can be projected and aligned directly onto the endoscopic video. This augmented reality supports navigation for the surgeon in the console. The downside of the 3D model projection is that it occludes the surgical instruments, creating a possible hazardous situation for the patient. This application uses a deep learning segmentation model to identify non-organic items such as tools, clips and wires and projects them on top of the 3D model. This solves the occlusion problem and adds a sense of depth to the AR application. Figure 1 shows in-app screenshots where the 3D model is used to support navigation. The large tumor in green is visible as anatomical landmark.  In addition, the application features an out-of-body detector. Anonymization of video is necessary to ensure privacy and protect patient data. During e.g. swapping of robotic tools or lens cleaning the endoscope is removed from the body, possibly capturing faces, recognizable tattoos or written patient data. Therefore, going out of body is a suiting definition for an anonymization boundary.</p> <p>  Fig. 2: 3D model of kidney tumor case </p> <p>The application was successfully used to remove a kidney tumor during a partial nephrectomy. Figure 2 shows the kidney 3D model where the tumor is colored in yellow.</p>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_multi_ai_ar/python/#pipeline","title":"Pipeline","text":"<p>  Fig. 3: Schematic overview of Holoscan application </p> <p>Towards realtime segmentation, AR overlay and anonymization, a binary segmentation model for non-organic items and a binary out-of-body detector was trained and deployed using Holoscan platform. Figure 3 shows a schematic overview of the application. After capturing the frame, the alpha channel is dropped by the Format Converter operator. Additionally, the black padding borders added by the robotic system are removed, the tensor is resized to 512x512 pixels and the tensor type is converted from int [0, 255] to float [0, 1]. In the Segmentation Preprocessor and Anonymization Preprocessor operators the tensor pixel values of every color channel are normalized using the corresponding means and standard deviations of the segmentation and anonymization dataset respectively. After model inference with the Multi-AI inference operator, a sigmoid layer is applied to the model predictions by the Segmentation Postprocessor resulting in a binary segmentation mask. Additionally, the resizing and cropping operations are inverted by first resizing and then adding black padding borders as to not compromise resolution. In the Orsi Visualizer operator the 3D model is rendered using the VTK library and composited onto the endoscopic image using OpenGL. The pixels corresponding to non-organic items are passed through the 3D model layer using a glsl shader program. Finally, the frame is anonymized according to the out-of-body detector output.</p>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_multi_ai_ar/python/#controls","title":"Controls","text":"Action Control Enable/Disable anonymization B Enable/Disable manipulations T Load 3D model orientation preset CTRL + L Save current 3D model orientation as preset (will overwrite default preset) CTRL + S Rotate 3D model (3 degrees of freedom) Left Click + Drag Rotate 3D model (1 degree of freedom) CTRL + Left Click + Drag Zoom 3D model Right Click + Drag Translate 3D  model SHIFT + Left Click + Drag Enable/Disable 3D model E Enable/Disable segmentation overlay O Increase opacity 3D model + Decrease opacity 3D model - Toggle arterial tree 1 Toggle venous tree 2 Toggle ureter 4 Toggle parenchyma 5 Toggle tumor 6","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_multi_ai_ar/python/#build-app","title":"Build app","text":"<pre><code>./run build orsi_multi_ai_ar\n</code></pre>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_multi_ai_ar/python/#launch-app","title":"Launch app","text":"<pre><code>./run launch orsi_multi_ai_ar cpp\n</code></pre> <p>or</p> <pre><code>./run launch orsi_multi_ai_ar python\n</code></pre> <p> </p>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_segmentation_ar/cpp/","title":"Orsi Non Organic Structure Segmentation and AR sample app","text":"<p> Authors: Jasper Hofman (Orsi Academy) Supported platforms: amd64, arm64 Last modified: October 8, 2024 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 3 - Developmental</p> <p>  Fig. 1: Application screenshot  </p>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_segmentation_ar/cpp/#introduction","title":"Introduction","text":"<p>3D models are used in surgery to improve patient outcomes. They provide information on patient specific anatomies that are not visible in the present surgical scene. Especially in robotic surgery these 3D models give great insights because they can be projected and aligned directly onto the endoscopic video. This augmented reality supports navigation for the surgeon in the console. The downside of the 3D model projection is that it occludes the surgical instruments, creating a possible hazardous situation for the patient. This application uses a deep learning segmentation model to identify non-organic items such as tools, clips and wires and projects them on top of the 3D model. This solves the occlusion problem and adds a sense of depth to the AR application. Figure 1 shows an in-app screenshot where the 3D model is aligned with the anatomy and the tools are segmented on top.</p> <p>  Fig. 2: 3D model of nutcracker case </p> <p>The application was successfully used to verify stent location during a nutcracker syndrome stent removal. Nutcracker syndrome is a rare vein compression disorder where the left renal vein is squeezed between the superior mesenteric artery and abdominal aorta, obstructing blood drainage and even risking blood to flow backwards, causing pain and blood in the urine. Typically, blood flow is restored through endovascular stenting. Although effective, over time the stent had migrated beyond the compression site, causing the symptoms to return and in addition posing an incidental obstruction for blood flow. The stent ofcourse had to be removed. Figure 2 shows a 3D model depicting the current situation.</p>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_segmentation_ar/cpp/#pipeline","title":"Pipeline","text":"<p>  Fig. 3: Schematic overview of Holoscan application </p> <p>Towards realtime segmentation and AR overlay, a binary segmentation model for non-organic items was trained and deployed using Holoscan platform. Figure 3 shows a schematic overview of the application. After capturing the frame, the alpha channel is dropped by the Format Converter operator. Additionally, the black padding borders added by the robotic system are removed, the tensor is resized to 512x512 pixels and the tensor type is converted from int [0, 255] to float [0, 1]. In the Segmentation Preprocessor operator the tensor pixel values of every color channel are normalized using the corresponding means and standard deviations of the segmentation dataset. After model inference with the Multi-AI inference operator, a sigmoid layer is applied to the model predictions by the Segmentation Postprocessor resulting in a binary segmentation mask. Additionally, the resizing and cropping operations are inverted by first resizing and then adding black padding borders as to not compromise resolution. In the Orsi Visualizer operator the 3D model is rendered using the VTK library and composited onto the endoscopic image using OpenGL. The pixels corresponding to non-organic items are passed through the 3D model layer using a glsl shader program.</p>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_segmentation_ar/cpp/#controls","title":"Controls","text":"Action Control Enable/Disable anonymization B Enable/Disable manipulations T Load 3D model orientation preset CTRL + L Save current 3D model orientation as preset (will overwrite default preset) CTRL + S Rotate 3D model (3 degrees of freedom) Left Click + Drag Rotate 3D model (1 degree of freedom) CTRL + Left Click + Drag Zoom 3D model Right Click + Drag Translate 3D  model SHIFT + Left Click + Drag Enable/Disable 3D model E Enable/Disable segmentation overlay O Increase opacity 3D model + Decrease opacity 3D model - Toggle venous tree 0 Toggle venous stent zone 1 Toggle stent 2","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_segmentation_ar/cpp/#build-app","title":"Build app","text":"<pre><code>./run build orsi_segmentation_ar\n</code></pre>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_segmentation_ar/cpp/#launch-app","title":"Launch app","text":"<pre><code>./run launch orsi_segmentation_ar cpp\n</code></pre> <p>or</p> <pre><code>./run launch orsi_segmentation_ar python\n</code></pre> <p> </p>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_segmentation_ar/python/","title":"Orsi Non Organic Structure Segmentation and AR sample app","text":"<p> Authors: Jasper Hofman (Orsi Academy) Supported platforms: amd64, arm64 Last modified: June 26, 2024 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 3 - Developmental</p> <p>  Fig. 1: Application screenshot  </p>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_segmentation_ar/python/#introduction","title":"Introduction","text":"<p>3D models are used in surgery to improve patient outcomes. They provide information on patient specific anatomies that are not visible in the present surgical scene. Especially in robotic surgery these 3D models give great insights because they can be projected and aligned directly onto the endoscopic video. This augmented reality supports navigation for the surgeon in the console. The downside of the 3D model projection is that it occludes the surgical instruments, creating a possible hazardous situation for the patient. This application uses a deep learning segmentation model to identify non-organic items such as tools, clips and wires and projects them on top of the 3D model. This solves the occlusion problem and adds a sense of depth to the AR application. Figure 1 shows an in-app screenshot where the 3D model is aligned with the anatomy and the tools are segmented on top.</p> <p>  Fig. 2: 3D model of nutcracker case </p> <p>The application was successfully used to verify stent location during a nutcracker syndrome stent removal. Nutcracker syndrome is a rare vein compression disorder where the left renal vein is squeezed between the superior mesenteric artery and abdominal aorta, obstructing blood drainage and even risking blood to flow backwards, causing pain and blood in the urine. Typically, blood flow is restored through endovascular stenting. Although effective, over time the stent had migrated beyond the compression site, causing the symptoms to return and in addition posing an incidental obstruction for blood flow. The stent ofcourse had to be removed. Figure 2 shows a 3D model depicting the current situation.</p>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_segmentation_ar/python/#pipeline","title":"Pipeline","text":"<p>  Fig. 3: Schematic overview of Holoscan application </p> <p>Towards realtime segmentation and AR overlay, a binary segmentation model for non-organic items was trained and deployed using Holoscan platform. Figure 3 shows a schematic overview of the application. After capturing the frame, the alpha channel is dropped by the Format Converter operator. Additionally, the black padding borders added by the robotic system are removed, the tensor is resized to 512x512 pixels and the tensor type is converted from int [0, 255] to float [0, 1]. In the Segmentation Preprocessor operator the tensor pixel values of every color channel are normalized using the corresponding means and standard deviations of the segmentation dataset. After model inference with the Multi-AI inference operator, a sigmoid layer is applied to the model predictions by the Segmentation Postprocessor resulting in a binary segmentation mask. Additionally, the resizing and cropping operations are inverted by first resizing and then adding black padding borders as to not compromise resolution. In the Orsi Visualizer operator the 3D model is rendered using the VTK library and composited onto the endoscopic image using OpenGL. The pixels corresponding to non-organic items are passed through the 3D model layer using a glsl shader program.</p>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_segmentation_ar/python/#controls","title":"Controls","text":"Action Control Enable/Disable anonymization B Enable/Disable manipulations T Load 3D model orientation preset CTRL + L Save current 3D model orientation as preset (will overwrite default preset) CTRL + S Rotate 3D model (3 degrees of freedom) Left Click + Drag Rotate 3D model (1 degree of freedom) CTRL + Left Click + Drag Zoom 3D model Right Click + Drag Translate 3D  model SHIFT + Left Click + Drag Enable/Disable 3D model E Enable/Disable segmentation overlay O Increase opacity 3D model + Decrease opacity 3D model - Toggle venous tree 0 Toggle venous stent zone 1 Toggle stent 2","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_segmentation_ar/python/#build-app","title":"Build app","text":"<pre><code>./run build orsi_segmentation_ar\n</code></pre>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/orsi/orsi_segmentation_ar/python/#launch-app","title":"Launch app","text":"<pre><code>./run launch orsi_segmentation_ar cpp\n</code></pre> <p>or</p> <pre><code>./run launch orsi_segmentation_ar python\n</code></pre> <p> </p>","tags":["DeltaCast","VideoMaster","Endoscopy","Segmentation","Augmented Reality","VTK"]},{"location":"applications/prohawk_video_replayer/cpp/","title":"Prohawk video replayer","text":"<p> Authors: Tim Wooldridge (Prohawk Technology Group) Supported platforms: arm64 Last modified: January 16, 2025 Language: C++ Latest version: 1.0.0 Minimum Holoscan SDK version: 0.5.1 Tested Holoscan SDK versions: 0.5.1, 0.6.0 Contribution metric: Level 4 - Experimental</p> <p>This application utilizes the ProHawk restoration operator along with Holoscan's Video Replayer and Holoviz operators to enhance and restore medical imagery in real-time, offering superior image quality. The user-friendly interface of the application provides a range of filter options, enabling users to dynamically select the most suitable filter for optimal results.</p> <p></p>","tags":["Video Processing","Prohawk"]},{"location":"applications/prohawk_video_replayer/cpp/#prohawk-vision-restoration-operator","title":"ProHawk Vision Restoration Operator","text":"<p>The ProHawk Vision Operator is a groundbreaking solution that is transforming both healthcare and manufacturing industries by revolutionizing computer vision technology with its patented restoration capabilities. It seamlessly integrates into the NVIDIA Holoscan full-stack infrastructure, fundamentally altering the way healthcare professionals diagnose and treat patients, while also optimizing manufacturing processes. In healthcare, the ProHawk Vision Operator Plugin automatically interprets medical imaging frames, identifies real-world conditions, and employs precise control over ProHawk Vision Restoration algorithms, all driven by an objective mathematical model using quantitative measurements to enhance accuracy in diagnoses and treatments by restoring degraded frames. In manufacturing, the ProHawk Vision Operator Plugin algorithms reveal manufacturing line defects ensuring product quality.</p>","tags":["Video Processing","Prohawk"]},{"location":"applications/prohawk_video_replayer/cpp/#application-controls","title":"Application Controls","text":"<p>The operator can be controlled with keyboard shortcuts:</p> <ul> <li>AFS (0) - Automatic Filter Selection (AFS) \u2013 Automatically and continuously adjust the image pixel values to maximize image detail and visibility.</li> <li>LowLight (1) - Lowlight preset filter that corrects lighting compromised imagery.</li> <li>Vascular Detail (2) - Vasculature Detail Preset Filter that reveals the fine details of vasculature structures without the need for dye or contrast agents.</li> <li>Vapor (3) - Vapor Preset Filter that removes vapor, smoke, and stream from the video.</li> <li>Disable Restoration (d) - Disable ProHawk Vision computer vision restoration.</li> <li>Side-by-Side View (v) - Display Side-by-Side (restored/non-restores) Video.</li> <li>Display Menu Items (m) - Display menus control items.</li> <li>Quit (q) - Exit the application</li> </ul>","tags":["Video Processing","Prohawk"]},{"location":"applications/prohawk_video_replayer/cpp/#data","title":"Data","text":"<p>The following dataset is used by this application: \ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking.</p>","tags":["Video Processing","Prohawk"]},{"location":"applications/prohawk_video_replayer/cpp/#building-the-application","title":"Building the application","text":"<p>The easiest way to build this application is to use the provided Docker file.</p> <p>From the Holohub main directory run the following command:</p> <pre><code>./dev_container build --docker_file applications/prohawk_video_replayer/Dockerfile --img holohub:prohawk\n</code></pre> <p>Then launch the container to build the application:</p> <pre><code>./dev_container launch --img holohub:prohawk\n</code></pre> <p>Inside the container build the application:</p> <pre><code>./run build prohawk_video_replayer\n</code></pre> <p>Inside the container run the application:</p> <ul> <li>C++:     <pre><code>./run launch prohawk_video_replayer cpp\n</code></pre></li> <li>Python:     <pre><code>export PYTHONPATH=$PYTHONPATH:/workspace/holohub/build/python/lib/\npython &lt;prohawk_app_dir&gt;/python/prohawk_video_replayer.py\n</code></pre></li> </ul> <p>For more information about this application and operator please visit https://prohawk.ai/prohawk-vision-operator/#learn For technical support or other assistance, please don't hesitate to visit us at https://prohawk.ai/contact</p>","tags":["Video Processing","Prohawk"]},{"location":"applications/prohawk_video_replayer/python/","title":"Prohawk video replayer","text":"<p> Authors: Nigel Nelson (NVIDIA) Supported platforms: arm64 Last modified: November 9, 2023 Language: Python Latest version: 1.0.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 4 - Experimental</p> <p>This application utilizes the ProHawk restoration operator along with Holoscan's Video Replayer and Holoviz operators to enhance and restore medical imagery in real-time, offering superior image quality. The user-friendly interface of the application provides a range of filter options, enabling users to dynamically select the most suitable filter for optimal results.</p> <p></p>","tags":["Video Processing","Prohawk"]},{"location":"applications/prohawk_video_replayer/python/#prohawk-vision-restoration-operator","title":"ProHawk Vision Restoration Operator","text":"<p>The ProHawk Vision Operator is a groundbreaking solution that is transforming both healthcare and manufacturing industries by revolutionizing computer vision technology with its patented restoration capabilities. It seamlessly integrates into the NVIDIA Holoscan full-stack infrastructure, fundamentally altering the way healthcare professionals diagnose and treat patients, while also optimizing manufacturing processes. In healthcare, the ProHawk Vision Operator Plugin automatically interprets medical imaging frames, identifies real-world conditions, and employs precise control over ProHawk Vision Restoration algorithms, all driven by an objective mathematical model using quantitative measurements to enhance accuracy in diagnoses and treatments by restoring degraded frames. In manufacturing, the ProHawk Vision Operator Plugin algorithms reveal manufacturing line defects ensuring product quality.</p>","tags":["Video Processing","Prohawk"]},{"location":"applications/prohawk_video_replayer/python/#application-controls","title":"Application Controls","text":"<p>The operator can be controlled with keyboard shortcuts:</p> <ul> <li>AFS (0) - Automatic Filter Selection (AFS) \u2013 Automatically and continuously adjust the image pixel values to maximize image detail and visibility.</li> <li>LowLight (1) - Lowlight preset filter that corrects lighting compromised imagery.</li> <li>Vascular Detail (2) - Vasculature Detail Preset Filter that reveals the fine details of vasculature structures without the need for dye or contrast agents.</li> <li>Vapor (3) - Vapor Preset Filter that removes vapor, smoke, and stream from the video.</li> <li>Disable Restoration (d) - Disable ProHawk Vision computer vision restoration.</li> <li>Side-by-Side View (v) - Display Side-by-Side (restored/non-restores) Video.</li> <li>Display Menu Items (m) - Display menus control items.</li> <li>Quit (q) - Exit the application</li> </ul>","tags":["Video Processing","Prohawk"]},{"location":"applications/prohawk_video_replayer/python/#data","title":"Data","text":"<p>The following dataset is used by this application: \ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking.</p>","tags":["Video Processing","Prohawk"]},{"location":"applications/prohawk_video_replayer/python/#building-the-application","title":"Building the application","text":"<p>The easiest way to build this application is to use the provided Docker file.</p> <p>From the Holohub main directory run the following command:</p> <pre><code>./dev_container build --docker_file applications/prohawk_video_replayer/Dockerfile --img holohub:prohawk\n</code></pre> <p>Then launch the container to build the application:</p> <pre><code>./dev_container launch --img holohub:prohawk\n</code></pre> <p>Inside the container build the application:</p> <pre><code>./run build prohawk_video_replayer\n</code></pre> <p>Inside the container run the application:</p> <ul> <li>C++:     <pre><code>./run launch prohawk_video_replayer cpp\n</code></pre></li> <li>Python:     <pre><code>export PYTHONPATH=$PYTHONPATH:/workspace/holohub/build/python/lib/\npython &lt;prohawk_app_dir&gt;/python/prohawk_video_replayer.py\n</code></pre></li> </ul> <p>For more information about this application and operator please visit https://prohawk.ai/prohawk-vision-operator/#learn For technical support or other assistance, please don't hesitate to visit us at https://prohawk.ai/contact</p>","tags":["Video Processing","Prohawk"]},{"location":"applications/simple_radar_pipeline/cpp/","title":"Simple Radar Pipeline Application","text":"<p> Authors: Cliff Burdick (NVIDIA) Supported platforms: amd64, arm64 Last modified: November 19, 2024 Language: C++ Latest version: 1.1 Minimum Holoscan SDK version: 2.6.0 Tested Holoscan SDK versions: 2.6.0 Contribution metric: Level 2 - Trusted</p> <p>This demonstration walks the developer through building a simple radar signal processing pipeline, targeted towards detecting objects, with Holoscan. In this example, we generate random radar and waveform data, passing both through: 1. Pulse Compression 2. Moving Target Indication (MTI) Filtering 3. Range-Doppler Map 4. Constant False Alarm Rate (CFAR) Analysis</p> <p>While this example generates 'offline' complex-valued data, it could be extended to accept streaming data from a phased array system or simulation via modification of the <code>SignalGeneratorOperator</code>.</p> <p>The output of this demonstration is a measure of the number of pulses per second processed on GPU.</p> <p>The main objectives of this demonstration are to: - Highlight developer productivity in building an end-to-end streaming application with Holoscan and existing GPU-Accelerated Python libraries - Demonstrate how to construct and connect isolated units of work via Holoscan operators, particularly with handling multiple inputs and outputs into an Operator - Emphasize that operators created for this application can be re-used in other ones doing similar tasks</p>","tags":["Signal Processing","RADAR"]},{"location":"applications/simple_radar_pipeline/cpp/#building-the-application","title":"Building the application","text":"<p>Make sure CMake (https://www.cmake.org) is installed on your system (minimum version 3.20)</p> <ul> <li> <p>Holoscan Debian Package - Follow the instructions in the link to install the latest version of Holoscan Debian package from NGC.</p> </li> <li> <p>Create a build directory:   <pre><code>mkdir -p &lt;build_dir&gt; &amp;&amp; cd &lt;build_dir&gt;\n</code></pre></p> </li> <li>Configure with CMake:</li> </ul> <p>Make sure CMake can find your installation of the Holoscan SDK. For example, setting <code>holoscan_ROOT</code> to its install directory during configuration:</p> <pre><code>cmake -S &lt;source_dir&gt; -B &lt;build_dir&gt; -DAPP_simple_radar_pipeline=1 \n</code></pre> <p>Notes: If the error <code>No CMAKE_CUDA_COMPILER could be found</code> is encountered, make sure that the :code:<code>nvcc</code> executable can be found by adding the CUDA runtime location to your <code>PATH</code> variable:</p> <pre><code>export PATH=$PATH:/usr/local/cuda/bin\n</code></pre> <ul> <li>Build:</li> </ul> <pre><code>cmake --build &lt;build_dir&gt;\n</code></pre>","tags":["Signal Processing","RADAR"]},{"location":"applications/simple_radar_pipeline/cpp/#running-the-application","title":"Running the application","text":"<pre><code>&lt;build_dir&gt;/simple_radar_pipeline\n</code></pre>","tags":["Signal Processing","RADAR"]},{"location":"applications/simple_radar_pipeline/python/","title":"Simple Radar Pipeline Application","text":"<p> Authors: Cliff Burdick (NVIDIA) Supported platforms: amd64 Last modified: July 28, 2023 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 0.4.0 Tested Holoscan SDK versions: 0.4.0 Contribution metric: Level 2 - Trusted</p> <p>This demonstration walks the developer through building a simple radar signal processing pipeline, targeted towards detecting objects, with Holoscan. In this example, we generate random radar and waveform data, passing both through: 1. Pulse Compression 2. Moving Target Indication (MTI) Filtering 3. Range-Doppler Map 4. Constant False Alarm Rate (CFAR) Analysis</p> <p>While this example generates 'offline' complex-valued data, it could be extended to accept streaming data from a phased array system or simulation via modification of the <code>SignalGeneratorOperator</code>.</p> <p>The output of this demonstration is a measure of the number of pulses per second processed on GPU.</p> <p>The main objectives of this demonstration are to: - Highlight developer productivity in building an end-to-end streaming application with Holoscan and existing GPU-Accelerated Python libraries - Demonstrate how to construct and connect isolated units of work via Holoscan operators, particularly with handling multiple inputs and outputs into an Operator - Emphasize that operators created for this application can be re-used in other ones doing similar tasks</p>","tags":["Aerospace, Defense, Communications"]},{"location":"applications/simple_radar_pipeline/python/#running-the-application","title":"Running the Application","text":"<p>Prior to running the application, the user needs to install the necessary dependencies. This is most easily done in an Anaconda environment.</p> <pre><code>conda create --name holoscan-sdr-demo python=3.8\nconda activate holoscan-sdr-demo\nconda install -c conda-forge -c rapidsai -c nvidia cusignal\npip install holoscan\n</code></pre> <p>The simple radar signal processing pipeline example can then be run via <pre><code>python applications/simple_radar_pipeline/simple_radar_pipeline.py\n</code></pre></p>","tags":["Aerospace, Defense, Communications"]},{"location":"applications/ultrasound_segmentation/cpp/","title":"Ultrasound Bone Scoliosis Segmentation","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 5, 2025 Language: C++ Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>Full workflow including a generic visualization of segmentation results from a spinal scoliosis segmentation model of ultrasound videos. The model used is stateless, so this workflow could be configured to adapt to any vanilla DNN model. </p>","tags":["Ultrasound","Segmentation"]},{"location":"applications/ultrasound_segmentation/cpp/#requirements","title":"Requirements","text":"<p>The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the ultrasound data (replayer). Follow the setup instructions from the user guide to use the AJA capture card.</p>","tags":["Ultrasound","Segmentation"]},{"location":"applications/ultrasound_segmentation/cpp/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Bone Scoliosis Segmentation</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Ultrasound","Segmentation"]},{"location":"applications/ultrasound_segmentation/cpp/#build-instructions","title":"Build Instructions","text":"<p>Please refer to the top level Holohub README.md file for information on how to build this application.</p>","tags":["Ultrasound","Segmentation"]},{"location":"applications/ultrasound_segmentation/cpp/#run-instructions","title":"Run Instructions","text":"<p>In your <code>build</code> directory, run the commands of your choice:</p> <ul> <li> <p>Using a pre-recorded video     <pre><code>sed -i -e 's#^source:.*#source: replayer#' applications/ultrasound_segmentation/cpp/ultrasound_segmentation.yaml\napplications/ultrasound_segmentation/cpp/ultrasound_segmentation --data &lt;data_dir&gt;/ultrasound_segmentation\n</code></pre></p> </li> <li> <p>Using an AJA card     <pre><code>sed -i -e 's#^source:.*#source: aja#' applications/ultrasound_segmentation/cpp/ultrasound_segmentation.yaml\napplications/ultrasound_segmentation/cpp/ultrasound_segmentation\n</code></pre></p> </li> </ul>","tags":["Ultrasound","Segmentation"]},{"location":"applications/ultrasound_segmentation/python/","title":"Ultrasound Bone Scoliosis Segmentation","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 5, 2025 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>Full workflow including a generic visualization of segmentation results from a spinal scoliosis segmentation model of ultrasound videos. The model used is stateless, so this workflow could be configured to adapt to any vanilla DNN model. </p>","tags":["Ultrasound","Segmentation"]},{"location":"applications/ultrasound_segmentation/python/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8+</li> <li>The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the ultrasound data (replayer). Follow the setup instructions from the user guide to use the AJA capture card.</li> </ul>","tags":["Ultrasound","Segmentation"]},{"location":"applications/ultrasound_segmentation/python/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Bone Scoliosis Segmentation</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Ultrasound","Segmentation"]},{"location":"applications/ultrasound_segmentation/python/#run-instructions","title":"Run Instructions","text":"<p>To run this application, you'll need to configure your PYTHONPATH environment variable to locate the necessary python libraries based on your Holoscan SDK installation type.</p> <p>You should refer to the glossary for the terms defining specific locations within HoloHub.</p> <p>If your Holoscan SDK installation type is:</p> <ul> <li>python wheels:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <ul> <li>otherwise:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOSCAN_INSTALL_DIR&gt;/python/lib:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <p>Next, run the commands of your choice:</p> <ul> <li> <p>Using a pre-recorded video     <pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;/applications/ultrasound_segmentation/python\npython3 ultrasound_segmentation.py --source=replayer --data &lt;DATA_DIR&gt;/ultrasound_segmentation\n</code></pre></p> </li> <li> <p>Using an AJA card     <pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;/applications/ultrasound_segmentation/python\npython3 ultrasound_segmentation.py --source=aja\n</code></pre></p> </li> </ul>","tags":["Ultrasound","Segmentation"]},{"location":"applications/volume_rendering/cpp/","title":"Volume rendering using ClaraViz","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 18, 2025 Language: C++ Latest version: 1.0.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3, 2.0.0, 2.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p></p> <p>This application loads a medical CT scan and renders it in real time at interactive frame rates using ClaraViz (https://github.com/NVIDIA/clara-viz).</p> <p>The application uses the <code>VolumeLoaderOp</code> operator to load the medical volume data, the <code>VolumeRendererOp</code> operator to render the volume and the <code>HolovizOp</code> operator to display the result and handle the camera movement.</p>","tags":["Volume","Render","ClaraViz"]},{"location":"applications/volume_rendering/cpp/#data","title":"Data","text":"<p>You can find CT scan datasets for use with this application from embodi3d.</p> <p>Datasets are bundled with a default ClaraViz JSON configuration file for volume rendering. See <code>VolumeRendererOp</code> documentation for details on configuration schema.</p> <p>See <code>VolumeLoaderOp</code> documentation for supported volume formats.</p>","tags":["Volume","Render","ClaraViz"]},{"location":"applications/volume_rendering/cpp/#build-and-run-instructions","title":"Build and Run Instructions","text":"<p>To build and run this application, use the <code>dev_container</code> script:</p> <pre><code># C++\n ./dev_container build_and_run volume_rendering --language cpp\n\n # Python\n  ./dev_container build_and_run volume_rendering --language python\n</code></pre> <p>The path of the volume configuration file, volume density file and volume mask file can be passed to the application.</p> <p>You can use the following command to get more information on command line parameters for this application:</p> <pre><code>./dev_container build_and_run volume_rendering --language [cpp|python] --run_args --usages\n</code></pre>","tags":["Volume","Render","ClaraViz"]},{"location":"applications/volume_rendering/cpp/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./dev_container vscode\n</code></pre>","tags":["Volume","Render","ClaraViz"]},{"location":"applications/volume_rendering/cpp/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Volume","Render","ClaraViz"]},{"location":"applications/volume_rendering/cpp/#c","title":"C++","text":"<p>Use the (gdb) volume_rendering/cpp launch profile to run and debug the C++ application.</p>","tags":["Volume","Render","ClaraViz"]},{"location":"applications/volume_rendering/cpp/#python","title":"Python","text":"<p>There are a couple of launch profiles configured for this application:</p> <ol> <li>(debugpy) python_rendering/python: Launch the Volume Rendering application with the ability to debug Python code.</li> <li>(pythoncpp) python_rendering/python: Launch the Volume Rendering application with the ability to debug both Python and C++ code.</li> </ol>","tags":["Volume","Render","ClaraViz"]},{"location":"applications/volume_rendering/cpp/#command-line-arguments","title":"Command Line Arguments","text":"<pre><code>Holoscan ClaraViz volume renderer.\nUsage: ./applications/volume_rendering/volume_rendering [options]\nOptions:\n  -h,-u, --help, --usages               Display this information\n  -c &lt;FILENAME&gt;, --config &lt;FILENAME&gt;    Name of the renderer JSON configuration file to load (default '../../../data/volume_rendering/config.json')\n  -p &lt;FILENAME&gt;, --preset &lt;FILENAME&gt;    Name of the renderer JSON preset file to load. This will be merged into the settings loaded from the configuration file. Multiple presets can be specified.\n  -w &lt;FILENAME&gt;, --write_config &lt;FILENAME&gt; Name of the renderer JSON configuration file to write to (default '')\n  -d &lt;FILENAME&gt;, --density &lt;FILENAME&gt;   Name of density volume file to load (default '../../../data/volume_rendering/highResCT.mhd')\n  -i &lt;MIN&gt;, --density_min &lt;MIN&gt;         Set the minimum of the density element values. If not set this is calculated from the volume data. In practice CT volumes have a minimum value of -1024 which corresponds to the lower value of the Hounsfield scale range usually used.\n  -a &lt;MAX&gt;, --density_max &lt;MAX&gt;         Set the maximum of the density element values. If not set this is calculated from the volume data. In practice CT volumes have a maximum value of 3071 which corresponds to the upper value of the Hounsfield scale range usually used.\n  -m &lt;FILENAME&gt;, --mask &lt;FILENAME&gt;      Name of mask volume file to load (default '../../../data/volume_rendering/smoothmasks.seg.mhd')\n  -n &lt;COUNT&gt;, --count &lt;COUNT&gt;           Duration to run application (default '-1' for unlimited duration)\n  ```\n\n### Importing CT datasets\n\nThis section describes the steps to user CT datasets additionally to the dataset provided by the volume rendering application.\n\nFirst get the data in a supported format. Supported formats are:\n* [MHD](https://itk.org/Wiki/ITK/MetaIO/Documentation)\n* [NIFTI](https://nifti.nimh.nih.gov/)\n* [NRRD](https://teem.sourceforge.net/nrrd/format.html)\n\nCT Data for the example dataset is downloaded to the `data/volume_rendering` folder when the application builds.\n\nAdditionally information on lighting, transfer functions and other settings is needed for the renderer to create an image. These settings are loaded from JSON files. The JSON files for the included example dataset is here `data/volume_rendering/config.json`.\n\nThere are two options to create a config file for a new dataset. First, use the example config as a reference to create a new config and modify parameters. Or let the renderer create a config file with settings deduced from the dataset.\n\nAssuming the volume file is is named `new_volume.nrrd`. Specify the new volume file (`-d new_volume.nrrd`), set the config file option to an empty string (`-c \"\"`) to force the renderer to deduce settings and specify the name of the config file to write (`-w new_config.json`):\n\n```bash\n  ./applications/volume_rendering/cpp/volume_rendering -d new_volume.nrrd -c \"\" -w new_config.json\n</code></pre> <p>This will create a file <code>new_config.json</code>. If there is a segmentation volume present add it with <code>-m new_seg_volume.nrrd</code>.</p> <p>By default the configuration is set up for rendering still images. For interactive rendering change the <code>timeSlot</code> setting in <code>RenderSettings</code> to the desired frame time in milliseconds, e.g. <code>33.0</code> for 30 fps.</p> <p>Also by default all lights and the background are shown in the scene. To avoid this change all <code>\"show\": true,</code> values to <code>\"show\": false,</code>.</p> <p>Modify the configuration file to your needs. To display the volume with the new configuration file add the configuration with the <code>-c new_config.json</code> argument:</p> <pre><code>  ./applications/volume_rendering/cpp/volume_rendering -d new_volume.nrrd -c new_config.json\n</code></pre> <p>It's possible to load preset JSON configuration files by using the <code>--preset preset.json</code> command line option. Presets are merged into the settings loaded from the configuration file. Multiple presets can be specified.</p> <p>A preset for bones is included. To load that preset use this command:</p> <pre><code>  ./applications/volume_rendering/cpp/volume_rendering -d new_volume.nrrd -c new_config.json -p presets/bones.json\n</code></pre>","tags":["Volume","Render","ClaraViz"]},{"location":"applications/volume_rendering/cpp/#transfer-functions","title":"Transfer functions","text":"<p>Usually CT datasets are stored in Hounsfield scale. The renderer maps these values in Hounsfield scale to opacity in order to display the volume. These mappings are called transfer functions. Multiple transfer functions for different input value regions can be defined. Transfer functions also include material properties like diffuse, specular and emissive color. The range of input values the transfer function is applied to is in normalized input range <code>[0, 1]</code>.</p>","tags":["Volume","Render","ClaraViz"]},{"location":"applications/volume_rendering/cpp/#segmentation-volume","title":"Segmentation volume","text":"<p>Different organs often have very similar Hounsfield values, therefore additionally an segmentation volume is supported. The segmentation volume contains an integer index for each element of the volume. Transfer functions can be restricted on specific segmentation indices. The segmentation volume can, for example, be generated using TotalSegmentator.</p>","tags":["Volume","Render","ClaraViz"]},{"location":"applications/volume_rendering/python/","title":"Volume rendering using ClaraViz","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 18, 2025 Language: Python Latest version: 1.0.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3, 2.0.0, 2.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p></p> <p>This application loads a medical CT scan and renders it in real time at interactive frame rates using ClaraViz (https://github.com/NVIDIA/clara-viz).</p> <p>The application uses the <code>VolumeLoaderOp</code> operator to load the medical volume data, the <code>VolumeRendererOp</code> operator to render the volume and the <code>HolovizOp</code> operator to display the result and handle the camera movement.</p>","tags":["Volume","Render","ClaraViz"]},{"location":"applications/volume_rendering/python/#data","title":"Data","text":"<p>You can find CT scan datasets for use with this application from embodi3d.</p> <p>Datasets are bundled with a default ClaraViz JSON configuration file for volume rendering. See <code>VolumeRendererOp</code> documentation for details on configuration schema.</p> <p>See <code>VolumeLoaderOp</code> documentation for supported volume formats.</p>","tags":["Volume","Render","ClaraViz"]},{"location":"applications/volume_rendering/python/#build-and-run-instructions","title":"Build and Run Instructions","text":"<p>To build and run this application, use the <code>dev_container</code> script:</p> <pre><code># C++\n ./dev_container build_and_run volume_rendering --language cpp\n\n # Python\n  ./dev_container build_and_run volume_rendering --language python\n</code></pre> <p>The path of the volume configuration file, volume density file and volume mask file can be passed to the application.</p> <p>You can use the following command to get more information on command line parameters for this application:</p> <pre><code>./dev_container build_and_run volume_rendering --language [cpp|python] --run_args --usages\n</code></pre>","tags":["Volume","Render","ClaraViz"]},{"location":"applications/volume_rendering/python/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./dev_container vscode\n</code></pre>","tags":["Volume","Render","ClaraViz"]},{"location":"applications/volume_rendering/python/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Volume","Render","ClaraViz"]},{"location":"applications/volume_rendering/python/#c","title":"C++","text":"<p>Use the (gdb) volume_rendering/cpp launch profile to run and debug the C++ application.</p>","tags":["Volume","Render","ClaraViz"]},{"location":"applications/volume_rendering/python/#python","title":"Python","text":"<p>There are a couple of launch profiles configured for this application:</p> <ol> <li>(debugpy) python_rendering/python: Launch the Volume Rendering application with the ability to debug Python code.</li> <li>(pythoncpp) python_rendering/python: Launch the Volume Rendering application with the ability to debug both Python and C++ code.</li> </ol>","tags":["Volume","Render","ClaraViz"]},{"location":"applications/volume_rendering/python/#command-line-arguments","title":"Command Line Arguments","text":"<pre><code>Holoscan ClaraViz volume renderer.\nUsage: ./applications/volume_rendering/volume_rendering [options]\nOptions:\n  -h,-u, --help, --usages               Display this information\n  -c &lt;FILENAME&gt;, --config &lt;FILENAME&gt;    Name of the renderer JSON configuration file to load (default '../../../data/volume_rendering/config.json')\n  -p &lt;FILENAME&gt;, --preset &lt;FILENAME&gt;    Name of the renderer JSON preset file to load. This will be merged into the settings loaded from the configuration file. Multiple presets can be specified.\n  -w &lt;FILENAME&gt;, --write_config &lt;FILENAME&gt; Name of the renderer JSON configuration file to write to (default '')\n  -d &lt;FILENAME&gt;, --density &lt;FILENAME&gt;   Name of density volume file to load (default '../../../data/volume_rendering/highResCT.mhd')\n  -i &lt;MIN&gt;, --density_min &lt;MIN&gt;         Set the minimum of the density element values. If not set this is calculated from the volume data. In practice CT volumes have a minimum value of -1024 which corresponds to the lower value of the Hounsfield scale range usually used.\n  -a &lt;MAX&gt;, --density_max &lt;MAX&gt;         Set the maximum of the density element values. If not set this is calculated from the volume data. In practice CT volumes have a maximum value of 3071 which corresponds to the upper value of the Hounsfield scale range usually used.\n  -m &lt;FILENAME&gt;, --mask &lt;FILENAME&gt;      Name of mask volume file to load (default '../../../data/volume_rendering/smoothmasks.seg.mhd')\n  -n &lt;COUNT&gt;, --count &lt;COUNT&gt;           Duration to run application (default '-1' for unlimited duration)\n  ```\n\n### Importing CT datasets\n\nThis section describes the steps to user CT datasets additionally to the dataset provided by the volume rendering application.\n\nFirst get the data in a supported format. Supported formats are:\n* [MHD](https://itk.org/Wiki/ITK/MetaIO/Documentation)\n* [NIFTI](https://nifti.nimh.nih.gov/)\n* [NRRD](https://teem.sourceforge.net/nrrd/format.html)\n\nCT Data for the example dataset is downloaded to the `data/volume_rendering` folder when the application builds.\n\nAdditionally information on lighting, transfer functions and other settings is needed for the renderer to create an image. These settings are loaded from JSON files. The JSON files for the included example dataset is here `data/volume_rendering/config.json`.\n\nThere are two options to create a config file for a new dataset. First, use the example config as a reference to create a new config and modify parameters. Or let the renderer create a config file with settings deduced from the dataset.\n\nAssuming the volume file is is named `new_volume.nrrd`. Specify the new volume file (`-d new_volume.nrrd`), set the config file option to an empty string (`-c \"\"`) to force the renderer to deduce settings and specify the name of the config file to write (`-w new_config.json`):\n\n```bash\n  ./applications/volume_rendering/cpp/volume_rendering -d new_volume.nrrd -c \"\" -w new_config.json\n</code></pre> <p>This will create a file <code>new_config.json</code>. If there is a segmentation volume present add it with <code>-m new_seg_volume.nrrd</code>.</p> <p>By default the configuration is set up for rendering still images. For interactive rendering change the <code>timeSlot</code> setting in <code>RenderSettings</code> to the desired frame time in milliseconds, e.g. <code>33.0</code> for 30 fps.</p> <p>Also by default all lights and the background are shown in the scene. To avoid this change all <code>\"show\": true,</code> values to <code>\"show\": false,</code>.</p> <p>Modify the configuration file to your needs. To display the volume with the new configuration file add the configuration with the <code>-c new_config.json</code> argument:</p> <pre><code>  ./applications/volume_rendering/cpp/volume_rendering -d new_volume.nrrd -c new_config.json\n</code></pre> <p>It's possible to load preset JSON configuration files by using the <code>--preset preset.json</code> command line option. Presets are merged into the settings loaded from the configuration file. Multiple presets can be specified.</p> <p>A preset for bones is included. To load that preset use this command:</p> <pre><code>  ./applications/volume_rendering/cpp/volume_rendering -d new_volume.nrrd -c new_config.json -p presets/bones.json\n</code></pre>","tags":["Volume","Render","ClaraViz"]},{"location":"applications/volume_rendering/python/#transfer-functions","title":"Transfer functions","text":"<p>Usually CT datasets are stored in Hounsfield scale. The renderer maps these values in Hounsfield scale to opacity in order to display the volume. These mappings are called transfer functions. Multiple transfer functions for different input value regions can be defined. Transfer functions also include material properties like diffuse, specular and emissive color. The range of input values the transfer function is applied to is in normalized input range <code>[0, 1]</code>.</p>","tags":["Volume","Render","ClaraViz"]},{"location":"applications/volume_rendering/python/#segmentation-volume","title":"Segmentation volume","text":"<p>Different organs often have very similar Hounsfield values, therefore additionally an segmentation volume is supported. The segmentation volume contains an integer index for each element of the volume. Transfer functions can be restricted on specific segmentation indices. The segmentation volume can, for example, be generated using TotalSegmentator.</p>","tags":["Volume","Render","ClaraViz"]},{"location":"benchmarks/","title":"Benchmarks","text":"<p>The HoloHub benchmark resources are a critical resource for developers aiming to optimize and validate the performance of their AI sensor processing applications built with the Holoscan SDK. </p> <p>Holohub provides a collection of benchmarking tools and reference implementations designed to measure and compare the efficiency, speed, and scalability of various Holoscan workflows. By offering detailed performance metrics and best practices, these benchmarks help developers identify bottlenecks and optimize their applications for high performance and low latency. Whether you are focusing on real-time data processing, model inference, or end-to-end workflow performance, the benchmarks on this page provide valuable insights and guidelines to ensure your applications meet the highest standards.</p>"},{"location":"benchmarks/cpp/","title":"Benchmark Model","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: November 14, 2024 Language: cpp Latest version: 1.0 Minimum Holoscan SDK version: 1.0.2 Tested Holoscan SDK versions: 1.0.2 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates an easy, quick and straightforward way to benchmark the scalaibility of inferencing with a model against a single video data stream in a typical Holoscan application. The video stream is played via a V4L2 loopback device. Then, the stream is preprocessed and fed to the model for inferencing. Then, the results are visualized after postprocessing.</p>","tags":["Benchmarking"]},{"location":"benchmarks/cpp/#usage","title":"Usage","text":"<p>As this application is, by default, set to use the  ultrasound segmentation example, you can build and run the ultrasound segmentation example first, and then try running this benchmarking application.</p> <p>Build and run the ultrasound segmentation application: <pre><code>./run build ultrasound_segmentation &amp;&amp; ./run launch ultrasound_segmentation cpp\n</code></pre></p> <p>Now, this benchmarking application can be built and run. However, before doing so, the v4l2loopback must be run first. Check out the notes and prerequisites here to play a video via a V4L2 loopback device. Assuming, everything is set up correctly, the ultrasound segmentation example video could be run with the following command:</p> <p>Note: we are playing the video to <code>/dev/video6</code> after running <code>sudo modprobe v4l2loopback video_nr=6 max_buffers=4</code> <pre><code>$ ffmpeg -stream_loop -1 -re -i ./data/ultrasound_segmentation/ultrasound_256x256.avi -pix_fmt yuyv422 -f v4l2 /dev/video6\nffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/\n  ...\n  libavutil      56. 31.100 / 56. 31.100\n  libavcodec     58. 54.100 / 58. 54.100\n  ...\n[mov,mp4,m4a,3gp,3g2,mj2 @ 0x560a570b0740] st: 0 edit list: 1 Missing key frame while searching for timestamp: 0\n...\nInput #0, mov,mp4,m4a,3gp,3g2,mj2, from './data/ultrasound_segmentation/ultrasound_256x256.avi':\n...\n</code></pre></p> <p>Now, the benchmarking application can be built and run: <pre><code>./run build model_benchmarking\n./run launch model_benchmarking &lt;cpp/python&gt;\n</code></pre></p> <p>To use a different video, the video can be played via the above <code>ffmpeg</code> command.</p> <p>To use a different model, you can specify the data path in the  <code>./run launch model_benchmarking &lt;cpp/python&gt;</code> command with the <code>-d</code> option, and the model name, residing in the data path directory, with the <code>-m</code> option.</p> <pre><code>./run launch model_benchmarking &lt;cpp/python&gt; --extra_args \"-d &lt;data_path&gt; -m &lt;model_name&gt;\"\n</code></pre> <p>To check the full list of options, run: <pre><code>./run launch model_benchmarking &lt;cpp/python&gt; --extra_args \"-h\"\n</code></pre></p>","tags":["Benchmarking"]},{"location":"benchmarks/cpp/#capabilities","title":"Capabilities","text":"<p>This benchmarking application can be used to measure performance of parallel inferences for the same model on a single video stream. The <code>-l</code> option can be used to specify the number of parallel inferences to run. Then, the same model will be loaded to the GPU multiple times defined by the <code>-l</code> parameter. </p> <p>The schematic diagram of this benchmarking application is in Figure 1. The visualization and (visualization + postprocessing) steps are marked as grey, as they can optionally be turned off with, respectively, <code>-p</code> and <code>-i</code> options. The figure shows a single video data stream is used in the application. Multiple ML/AI models are ingested by the Holoscan Inference operator to perform inference on a single data stream. The same ML model is replicated to be loaded multiple times to the GPU in this application.</p> <p></p> <p>Figure 1. The schematic diagram of the benchmarking application</p>","tags":["Benchmarking"]},{"location":"benchmarks/exclusive_display/","title":"Exclusive Display Benchmark","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: arm64 Last modified: September 5, 2024 Latest version: 0.1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This document investigates the performance of the exclusive display feature in <code>holoviz</code> operator of Holoscan SDK.</p>","tags":["Benchmarking","Exclusive Display"]},{"location":"benchmarks/exclusive_display/#introduction","title":"Introduction","text":"<p>By default, Holoscan SDK application uses the <code>holoviz</code> operator for display, and it uses a windowing system like X11. X11 is a compositor which combines the display requests from different applications and renders them on the screen. In <code>exclusive_display</code> mode, <code>holoviz</code> turns off the default display manager and directly renders the output in full-screen mode. This mode is obviously more performant as a single application exclusively uses the monitor to display its output.</p> <p>In this document, we provide the performance measurements of the <code>exclusive_display</code> mode and compare it with the default mode that uses <code>X11</code>. We execute the endoscopy tool tracking in these two display modes and measure its maximum and average end-to-end latency. In addition, we also run a number of \"headless\" applications simultaneously. These headless applications are executing both AI workloads and graphics processing but do not utilize the screen to display any output. They are representative of background workloads. Usually, these background workloads run alongside a primary display application which, in this case, is the endoscopy tool tracking application using display in either <code>exclusive</code> or <code>default</code> mode.</p>","tags":["Benchmarking","Exclusive Display"]},{"location":"benchmarks/exclusive_display/#platform","title":"Platform","text":"<p>The experiments are conducted with Holoscan v2.1 container on IGX Orin with RTX A6000 GPU flashed with IGX SW 1.0.</p>","tags":["Benchmarking","Exclusive Display"]},{"location":"benchmarks/exclusive_display/#results","title":"Results","text":"<p>For the experiment, we use the endoscopy tool tracking application which is using the display monitor for outputs in two modes, as said above. This application is executed with <code>realtime: false</code> for the video stream replayer source, so that the source feeds the frames as fast as possible, without an external frame-rate limitation.</p> <p>For the headless applications, we run the same endoscopy tool application in different process instances but in <code>headless: true</code> mode.</p> <p>In the graphs below, Y-axis shows the end-to-end latency of the endoscopy tool tracking with display. In the X-axis, we vary the number of headless applications from 0 to 11. <code>0</code> means only the endoscopy application with display is running. We do not show any numbers when the latency is more than 200ms.</p>","tags":["Benchmarking","Exclusive Display"]},{"location":"benchmarks/exclusive_display/#maximum-end-to-end-latency","title":"Maximum End-to-end Latency","text":"<p>The maximum end-to-end latency results are given below:</p> <p></p> <p>In the above graph, the maximum end-to-end latency for the default mode increases from 15 ms to 23 ms when the number of background headless applications rises from 0 to 3. For more than 3 background headless applications, the maximum end-to-end latency in default mode is more than 200 ms.</p> <p>The exclusive display mode performs much better than the default mode because of no overhead of the compositor. The maximum end-to-end latency is 20 to 30% lower in presence of up to 3 headless applications. The benefits are more pronounced when the number of background headless applications is more than 3.</p> <p>Despite better performance with exclusive display, the maximum end-to-end latency increases to 51 ms when the number of background headless applications is 11. Therefore, exclusive display mode alone cannot guarantee an upper bound on the latency if the number of applications using the GPU increases.</p>","tags":["Benchmarking","Exclusive Display"]},{"location":"benchmarks/exclusive_display/#average-end-to-end-latency","title":"Average End-to-end Latency","text":"<p>The average end-to-end latency results are given below:</p> <p></p> <p>In the above graph, the average end-to-end latency for the default mode increases from 8 ms to 136 ms when the number of headless applications increases from 0 to 9. For more than 9 headless applications, the average end-to-end latency in default mode is more than 200 ms.</p> <p>The exclusive display mode performs much better than the default mode in average latency as well. Average end-to-end latency is up to 80% lower in exclusive display mode compared to the default mode, for up to 9 simultaneous headless applications. The average latency increases from 8 ms to 29 ms in exclusive mode when the number of headless applications increases from 0 to 11.</p>","tags":["Benchmarking","Exclusive Display"]},{"location":"benchmarks/exclusive_display/#conclusions","title":"Conclusions","text":"<ul> <li>The exclusive display mode provides better average latency and deterministic performance   (maximum latency) than the default mode.</li> <li>Headless applications using the GPU which are running in the background, impact the performance both in default and exclusive display   modes. However, exclusive display mode is more resilient than the default mode to the background   applications using the GPU.</li> <li>Even in the exclusive display mode, the maximum latency, capturing the performance predictability,   increases 4-5x while the background headless applications increase from 0 to 11. Therefore, the exclusive mode does not provide a guarantee on the upper bound of the latency in presence of other GPU workloads.</li> </ul>","tags":["Benchmarking","Exclusive Display"]},{"location":"benchmarks/holoscan_flow_benchmarking/","title":"Holoscan Flow Benchmarking for HoloHub","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: March 10, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 3 - Developmental</p> <p>This is a benchmarking tool to evaluate the performance of HoloHub and other Holoscan applications. Following is a high-level overview of Holoscan Flow Benchmarking. For more details on its possible use-cases, please follow Holoscan Flow Benchmarking Tutorial  (up-to-date) or  Holoscan Flow Benchmarking whitepaper.</p> <p>The tool supports benchmarking of any Holoscan application. Holoscan Python applications are supported since Holoscan v1.0.</p>","tags":["Benchmarking","Flow"]},{"location":"benchmarks/holoscan_flow_benchmarking/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Pre-requisites</li> <li>Steps for Holoscan Flow Benchmarking</li> <li>Generate Application Graph with Latency Numbers</li> </ul>","tags":["Benchmarking","Flow"]},{"location":"benchmarks/holoscan_flow_benchmarking/#pre-requisites","title":"Pre-requisites","text":"<p>The following Python libraries need to be installed to run the benchmarking scripts (<code>pip install -r requirements.txt</code> can be used):</p> <pre><code>numpy matplotlib nvitop argparse pydot xdot\n</code></pre> <p>The preferred way to use Flow Benchmarking is to use the provided Holohub docker image for automatic management of dependencies. Otherwise, bare-metal installation of the required libraries is needed. Extra dependencies might be needed to be installed to satisfy the requirements. For example, <code>xdot</code> has following dependencies.</p>","tags":["Benchmarking","Flow"]},{"location":"benchmarks/holoscan_flow_benchmarking/#steps-for-holoscan-flow-benchmarking","title":"Steps for Holoscan Flow Benchmarking","text":"<ol> <li>Patch the application for benchmarking</li> </ol> <pre><code>$ ./benchmarks/holoscan_flow_benchmarking/patch_application.sh &lt;application directory&gt;\n</code></pre> <p>For example, to patch the endoscopy tool tracking application, you would run:</p> <p><pre><code>$ ./benchmarks/holoscan_flow_benchmarking/patch_application.sh applications/endoscopy_tool_tracking\n</code></pre> This script saves the original <code>cpp</code> files in a <code>*.cpp.bak</code> file.</p> <ol> <li>Build the application</li> </ol> <pre><code>$ ./run build &lt;application name&gt; &lt;other options&gt; --benchmark\n</code></pre> <p>Please make sure to test that the application runs correctly after building it, and before going to next steps of performance evaluation. You may also make sure that all the necessary TensorRT engine files are generated by running the application at least once, for example, for the endoscopy tool tracking application.</p> <ol> <li>Run the performance evaluation</li> </ol> <pre><code>$ python benchmarks/holoscan_flow_benchmarking/benchmark.py -a &lt;application name&gt; &lt;other options&gt;\n</code></pre> <p>The above command will run an application which is executed normally by  <code>./run launch &lt;application name&gt; cpp</code>. If an application is executed differently, then use the <code>--run-command</code> argument to specify the command to run an application.</p> <p><code>python benchmarks/holoscan_flow_benchmarking/benchmark.py -h</code> shows all the possible benchmarking options.</p> <p>All the log filenames are printed out at the end of the evaluation. The format of the filename for the data flow tracking log files is: <code>logger_&lt;scheduler&gt;_&lt;run_number&gt;_&lt;instance-id&gt;.log</code>. The format of the filename for the GPU utilization log files is: <code>gpu_utilization_&lt;scheduler&gt;_&lt;run_number&gt;.csv</code>.</p> <p>Example: When the endoscopy tool tracking application is evaluated for the greedy scheduler for 3 runs with 3 instances each for 200 number of data frames, the following output is printed: <pre><code>$ python benchmarks/holoscan_flow_benchmarking/benchmark.py -a endoscopy_tool_tracking -r 3 -i 3 -m 200 --sched greedy -d myoutputs\nLog directory is not found. Creating a new directory at /home/ubuntu/holoscan-sdk/holohub-internal/myoutputs\nRun 1 completed for greedy scheduler.\nRun 2 completed for greedy scheduler.\nRun 3 completed for greedy scheduler.\n\nEvaluation completed.\nLog file directory:  /home/ubuntu/holoscan-sdk/holohub/myoutputs\nAll the data flow tracking log files are: logger_greedy_1_1.log, logger_greedy_1_2.log, logger_greedy_1_3.log, logger_greedy_2_1.log, logger_greedy_2_2.log, logger_greedy_2_3.log, logger_greedy_3_1.log, logger_greedy_3_2.log, logger_greedy_3_3.log\n</code></pre></p> <ol> <li>Get performance results and insights</li> </ol> <p><pre><code>$ python benchmarks/holoscan_flow_benchmarking/analyze.py -g &lt;group of log files&gt; &lt;options&gt;\n</code></pre> <code>python benchmarks/holoscan_flow_benchmarking/analyze.py -h</code> shows all the possible options.</p> <p>Example: For the above example experiment with the <code>benchmark.py</code> script, we can analyze worst-case and average end-to-end latency by the following script:</p> <p><pre><code>python benchmarks/holoscan_flow_benchmarking/analyze.py -m -a -g myoutputs/logger_greedy_* MyCustomGroup\n</code></pre> The above command will produce an output like below:</p> <p></p> <p>We can also produce CDF curve of the observed latencies for a single path by the following commands:</p> <pre><code>$ python benchmarks/holoscan_flow_benchmarking/analyze.py --draw-cdf https://github.com/nvidia-holoscan/holohub/blob/main/benchmarks/holoscan_flow_benchmarking/single_path_cdf.png?raw=true -g myoutputs/logger_greedy_* MyCustomGroup --no-display-graphs\nSaved the CDF curve graph of the first path of each group in: https://github.com/nvidia-holoscan/holohub/blob/main/benchmarks/holoscan_flow_benchmarking/single_path_cdf.png?raw=true\n</code></pre> <p>The <code>https://github.com/nvidia-holoscan/holohub/blob/main/benchmarks/holoscan_flow_benchmarking/single_path_cdf.png?raw=true</code> looks like below:</p> <p></p> <p>A few auxiliary scripts are also provided to help plotting datewise results. For example, the following script plots the average end-to-end latency along with standard deviation for three consecutive dates:</p> <pre><code>python bar_plot_avg_datewise.py avg_values_2023-10-19.csv avg_values_2023-10-20.csv avg_values_2023-10-21.csv stddev_values_2023-10-19.csv stddev_values_2023-10-20.csv stddev_values_2023-10-21.csv\n</code></pre> <p></p> <ol> <li>Restore the application</li> </ol> <p>If benchmarking is not necessary anymore, an application can be restored by the following command:</p> <pre><code>$ ./benchmarks/holoscan_flow_benchmarking/restore_application.sh &lt;application directory&gt;\n</code></pre>","tags":["Benchmarking","Flow"]},{"location":"benchmarks/holoscan_flow_benchmarking/#generate-application-graph-with-latency-numbers","title":"Generate Application Graph with Latency Numbers","text":"<p>The <code>app_perf_graph.py</code> script can be used to generate a graph of a Holoscan application with latency data from benchmarking embedded into the graph. The graph looks like the figure below, where graph nodes are operators along with their average and maximum execution times, and edges represent connection between operators along with the average and maximum data transfer latencies.</p> <p></p> <p>It is also possible to generate such a graph and dynamically update it, while running the <code>benchmarking.py</code> script to benchmark an application. For example, the following three commands can be run in three different terminals to monitor the live performance of an endoscopy_tool_tracking application.</p> <pre><code># the following command initiates a benchmarking job and generates performance log files\n$ python3 benchmarks/holoscan_flow_benchmarking/benchmark.py -a endoscopy_tool_tracking -i 1 -d endoscopy_results --sched=greedy -r 3 -m 1000\n\n# the following command keeps updating an application graph with the latest performance numbers\n# -l means live mode\n$ python3 benchmarks/holoscan_flow_benchmarking/app_perf_graph.py -o live_app_graph.dot -l endoscopy_results\n\n# use another terminal to visualize the graph with xdot. the graph will be updated as app_perf_graph.py updates the graph\n$ xdot live_app_graph.dot\n</code></pre>","tags":["Benchmarking","Flow"]},{"location":"benchmarks/python/","title":"Benchmark Model","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: April 10, 2024 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 1.0.2 Tested Holoscan SDK versions: 1.0.2 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates an easy, quick and straightforward way to benchmark the scalaibility of inferencing with a model against a single video data stream in a typical Holoscan application. The video stream is played via a V4L2 loopback device. Then, the stream is preprocessed and fed to the model for inferencing. Then, the results are visualized after postprocessing.</p>","tags":["Benchmarking"]},{"location":"benchmarks/python/#usage","title":"Usage","text":"<p>As this application is, by default, set to use the  ultrasound segmentation example, you can build and run the ultrasound segmentation example first, and then try running this benchmarking application.</p> <p>Build and run the ultrasound segmentation application: <pre><code>./run build ultrasound_segmentation &amp;&amp; ./run launch ultrasound_segmentation cpp\n</code></pre></p> <p>Now, this benchmarking application can be built and run. However, before doing so, the v4l2loopback must be run first. Check out the notes and prerequisites here to play a video via a V4L2 loopback device. Assuming, everything is set up correctly, the ultrasound segmentation example video could be run with the following command:</p> <p>Note: we are playing the video to <code>/dev/video6</code> after running <code>sudo modprobe v4l2loopback video_nr=6 max_buffers=4</code> <pre><code>$ ffmpeg -stream_loop -1 -re -i ./data/ultrasound_segmentation/ultrasound_256x256.avi -pix_fmt yuyv422 -f v4l2 /dev/video6\nffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/\n  ...\n  libavutil      56. 31.100 / 56. 31.100\n  libavcodec     58. 54.100 / 58. 54.100\n  ...\n[mov,mp4,m4a,3gp,3g2,mj2 @ 0x560a570b0740] st: 0 edit list: 1 Missing key frame while searching for timestamp: 0\n...\nInput #0, mov,mp4,m4a,3gp,3g2,mj2, from './data/ultrasound_segmentation/ultrasound_256x256.avi':\n...\n</code></pre></p> <p>Now, the benchmarking application can be built and run: <pre><code>./run build model_benchmarking\n./run launch model_benchmarking &lt;cpp/python&gt;\n</code></pre></p> <p>To use a different video, the video can be played via the above <code>ffmpeg</code> command.</p> <p>To use a different model, you can specify the data path in the  <code>./run launch model_benchmarking &lt;cpp/python&gt;</code> command with the <code>-d</code> option, and the model name, residing in the data path directory, with the <code>-m</code> option.</p> <pre><code>./run launch model_benchmarking &lt;cpp/python&gt; --extra_args \"-d &lt;data_path&gt; -m &lt;model_name&gt;\"\n</code></pre> <p>To check the full list of options, run: <pre><code>./run launch model_benchmarking &lt;cpp/python&gt; --extra_args \"-h\"\n</code></pre></p>","tags":["Benchmarking"]},{"location":"benchmarks/python/#capabilities","title":"Capabilities","text":"<p>This benchmarking application can be used to measure performance of parallel inferences for the same model on a single video stream. The <code>-l</code> option can be used to specify the number of parallel inferences to run. Then, the same model will be loaded to the GPU multiple times defined by the <code>-l</code> parameter. </p> <p>The schematic diagram of this benchmarking application is in Figure 1. The visualization and (visualization + postprocessing) steps are marked as grey, as they can optionally be turned off with, respectively, <code>-p</code> and <code>-i</code> options. The figure shows a single video data stream is used in the application. Multiple ML/AI models are ingested by the Holoscan Inference operator to perform inference on a single data stream. The same ML model is replicated to be loaded multiple times to the GPU in this application.</p> <p></p> <p>Figure 1. The schematic diagram of the benchmarking application</p>","tags":["Benchmarking"]},{"location":"benchmarks/release_benchmarking/","title":"Release Benchmarking Guide","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: November 12, 2024 Latest version: 0.1.0 Minimum Holoscan SDK version: 2.3.0 Tested Holoscan SDK versions: 2.3.0 Contribution metric: Level 3 - Developmental</p> <p>This tutorial provides a reproducible workflow for developers to accurately measure the latency of curated HoloHub applications across various SDK releases and different deployment scenarios, from single-application to multi-model use cases.</p> <p>Developers can use the Holoscan Flow Benchmarking tools referenced within this guide to systematically analyze performance bottlenecks, optimize execution times, and fine-tune their own applications for real-time, low-latency processing.</p>","tags":["Benchmarking","Performance","Reporting","Release"]},{"location":"benchmarks/release_benchmarking/#contents","title":"Contents","text":"<ul> <li>Background</li> <li>Previous Holoscan Release Benchmark Reports</li> <li>Running the Tutorial</li> <li>Running Benchmarks</li> <li>Summarizing Data</li> <li>Presenting Data</li> <li>Troubleshooting</li> <li>Developer References</li> </ul>","tags":["Benchmarking","Performance","Reporting","Release"]},{"location":"benchmarks/release_benchmarking/#background","title":"Background","text":"<p>Holoscan SDK emphasizes low end-to-end latency in application pipelines. In addition to other benchmarks, we can use HoloHub applications to evaluate Holoscan SDK performance over releases.</p> <p>In this tutorial we provide a reproducible workflow to evaluate end-to-end latency performance on the Endoscopy Tool Tracking and Multi-AI Ultrasound HoloHub projects. These projects are generally maintained by the NVIDIA Holoscan team and demonstrate baseline Holoscan SDK inference pipelines with video replay and Holoviz rendering output.</p> <p>Benchmark scenarios include: - Running multiple Holoscan SDK pipelines concurrently on a single machine - Running video replay input at real-time speeds or as fast as possible - Running Holoviz output with either visual rendering or in headless mode</p> <p>We plan to release HoloHub benchmarks in the release subfolder following Holoscan SDK general releases. You can follow the tutorial below to similarly evaluate performance on your own machine.</p> <p>Refer to related documents for more information: - the results report template file provides additional information on definitions and background - versioned releases are available for review in the release subfolder</p>","tags":["Benchmarking","Performance","Reporting","Release"]},{"location":"benchmarks/release_benchmarking/#previous-holoscan-release-benchmark-reports","title":"Previous Holoscan Release Benchmark Reports","text":"<ul> <li>Holoscan SDK v2.3.0</li> </ul>","tags":["Benchmarking","Performance","Reporting","Release"]},{"location":"benchmarks/release_benchmarking/#running-benchmarks-getting-started","title":"Running Benchmarks: Getting Started","text":"<p>Data collection can be run in the HoloHub base container for both the Endoscopy Tool Tracking and the Multi-AI Ultrasound applications. We've provided a custom Dockerfile with tools to process collected data into a benchmark report.</p> <pre><code># Build the container\n./dev_container build \\\n    --img holohub:release_benchmarking \\\n    --docker_file benchmarks/release_benchmarking/Dockerfile \\\n    --base_img nvcr.io/nvidia/clara-holoscan/holoscan:&lt;holoscan-sdk-version&gt;-$(./dev_container get_host_gpu)\n\n# Launch the dev environment\n./dev_container launch --img holohub:release_benchmarking\n\n# Inside the container, build the applications in benchmarking mode\n./run build endoscopy_tool_tracking --benchmark\n./run build multiai_ultrasound --benchmark\n\n./run build release_benchmarking\n</code></pre> <p>Run the benchmarking script with no arguments to collect performance logs in the <code>./output</code> directory. <pre><code>./run launch release_benchmarking\n</code></pre></p>","tags":["Benchmarking","Performance","Reporting","Release"]},{"location":"benchmarks/release_benchmarking/#summarizing-data","title":"Summarizing Data","text":"<p>After running benchmarks, inside the dev environment, use <code>run launch</code> to process data statistics and create bar plot PNGs:  <pre><code>./dev_container launch --img holohub:release_benchmarking\n./run launch release_benchmarking --extra_args \"--process benchmarks/release_benchmarking\"\n</code></pre></p> <p>Alternatively, collect results across platforms. On each machine: 1. Run benchmarks: <pre><code>./run launch release_benchmarking\n</code></pre> 2. Add platform configuration information: <pre><code>./run launch release_benchmarking --extra_args \"--print\" &gt; benchmarks/release_benchmarking/output/platform.txt\n</code></pre> 3. Transfer output contents from each platform to a single machine: <pre><code># Compress information for transfer\npushd benchmarks/release_benchmarking\ntar cvf benchmarks-&lt;platform-name&gt;.tar.gz output/*\n\n# Migrate the results archive with your transfer tool of choice, such as SCP\n\n# Extract results to a subfolder on the target machine\nmkdir -p output/&lt;release&gt;/&lt;platform-name&gt;/\npushd output/&lt;release&gt;/&lt;platform-name&gt;\ntar xvf benchmarks-&lt;platform-name&gt;\n</code></pre> 4. Use multiple <code>--process</code> flags to generate a batch of bar plots for multiple platform results: <pre><code>./run launch release_benchmarking --extra_args \"\\\n    --process benchmarks/release_benchmarking/2.4/x86_64 \\\n    --process benchmarks/release_benchmarking/2.4/IGX_iGPU \\\n    --process benchmarks/release/benchmarking/2.4/IGX_dGPU\"\n</code></pre></p>","tags":["Benchmarking","Performance","Reporting","Release"]},{"location":"benchmarks/release_benchmarking/#presenting-data","title":"Presenting Data","text":"<p>You can use the template markdown file in the <code>template</code> folder to generate a markdown or PDF report with benchmark data with <code>pandoc</code> and <code>Jinja2</code>.</p> <ol> <li>Copy and edit <code>template/release.json</code> with information about the benchmarking configuration, including the release version, platform configurations, and local paths to processed data. Run <code>./run launch</code> to print JSON-formatted platform details to the console about the current system: <pre><code>./dev_container launch --img holohub:release_benchmarking\n./run launch release_benchmarking --extra_args \"--print\"\n</code></pre></li> <li>Render the document with the Jinja CLI tool: <pre><code>pushd benchmarks/release_benchmarking\njinja2 template/results.md.tmpl template/&lt;release-version&gt;.json --format=json &gt; output/&lt;release-version&gt;.md\n</code></pre></li> </ol>","tags":["Benchmarking","Performance","Reporting","Release"]},{"location":"benchmarks/release_benchmarking/#optional-generating-a-pdf-report-document","title":"(Optional) Generating a PDF report document","text":"<p>You can convert the report to PDF format as an easy way to share your report as a single file with embedded plots.</p> <ol> <li>In your copy of <code>template/release.json</code>, update the <code>\"format\"</code> string to <code>\"pdf\"</code>.</li> <li>Follow the instructions above to generate your markdown report with Jinja2.</li> <li>Use <code>pandoc</code> to convert the markdown file to PDF: <pre><code>pushd output\npandoc &lt;release-version&gt;.md -o &lt;release-version&gt;.pdf --toc\n</code></pre></li> </ol>","tags":["Benchmarking","Performance","Reporting","Release"]},{"location":"benchmarks/release_benchmarking/#optional-submitting-results-to-holohub","title":"(Optional) Submitting Results to HoloHub","text":"<p>The Holoscan SDK team may submit release benchmarking reports to HoloHub git history for general visibility. We use Markdown formatting to make plot diagrams accessible for direct download.</p> <ol> <li>Move <code>&lt;release-version&gt;.md</code> and accompanying plots to a new <code>release/&lt;version&gt;</code> folder.</li> <li>Update image paths in <code>&lt;release-version.md&gt;</code> and verify locally with a markdown renderer such as VS Code.</li> <li>Commit changes, push to GitHub, and open a Pull Request.</li> </ol>","tags":["Benchmarking","Performance","Reporting","Release"]},{"location":"benchmarks/release_benchmarking/#cleanup","title":"Cleanup","text":"<p>Benchmarking changes to application YAML files can be discarded after benchmarks complete. <pre><code>git checkout applications/*.yaml\n</code></pre></p>","tags":["Benchmarking","Performance","Reporting","Release"]},{"location":"benchmarks/release_benchmarking/#troubleshooting","title":"Troubleshooting","text":"<p>Why am I seeing high end-to-end latency spikes as outliers in my data?</p> <p>Latency spikes may occur in display-driven benchmarking if the display goes to sleep. Please configure your display settings to prevent the display from going to sleep before running benchmarks.</p> <p>We have also infrequently observed latency spikes in cases where display drivers and CUDA Toolkit versions are not matched, and due to suboptimal GPU task preemption policies. We are still investigating these issues.</p> <p>Benchmark applications are failing silently without writing log files.</p> <p>Silent failures may indicate an issue with the underlying applications undergoing benchmarking. Try running the applications directly and verify execution is as expected: - <code>./run launch endoscopy_tool_tracking cpp</code> - <code>./run launch multiai_ultrasound cpp</code></p> <p>In some cases you may need to clear your HoloHub build or data folders to address errors: - <code>./run clear_cache</code> - <code>rm -rf ./data</code></p>","tags":["Benchmarking","Performance","Reporting","Release"]},{"location":"benchmarks/release_benchmarking/#developer-references","title":"Developer References","text":"<p>While this tutorial is tailored to curated configurations of the Endoscopy Tool Tracking and Multi-AI Ultrasound HoloHub applications, developers utilize underlying Holoscan data frame flow tracking tools to similarly measure and analyze performance in custom Holoscan applications.</p> <ul> <li>Refer to the Holoscan Flow Benchmarking project for general Holoscan performance profiling tools for both C++ and Python applications.</li> <li>Refer to the Holoscan Flow Benchmarking whitepaper and tutorial for a comprehensive overview of pipeline profiling tools.</li> <li>Refer to <code>run_benchmarks.sh</code> for additional examples demonstrating performance data collection and reporting with Holoscan Flow Tracking scripts.</li> </ul>","tags":["Benchmarking","Performance","Reporting","Release"]},{"location":"operators/","title":"Operators","text":"<p>Operators are fundamental components that extend the functionality of the Holoscan SDK, enabling developers to build custom AI sensor processing workflows.  These operators are designed to handle specific tasks such as data ingestion, preprocessing, model inference, and postprocessing, and can be seamlessly integrated into Holoscan applications.  The repository provides a collection of pre-built operators that serve as reference implementations, demonstrating best practices for efficient and scalable AI processing.</p>"},{"location":"operators/XrFrameOp/","title":"XrFrameOp","text":"","tags":["XR","XRFrame"]},{"location":"operators/XrFrameOp/#xrframe-operator","title":"XrFrame Operator","text":"<p>The <code>XrFrameOp</code> directory contains the <code>XrBeginFrameOp</code> and the <code>XrEndFrameOp</code>. <code>XrBeginFrameOp</code> operator initiates the rendering of a single OpenXR frame. It converts render buffers and events generated by the OpenXR runtime into Holoscan specific events to be consumed by downstream operators. Application specific operators are then expected to render left and right camera views into the given color and depth buffers. Once complete, the buffers must be passed to <code>XrEndFrameOp</code> in order to deliver the frame back to the OpenXR runtime. Note that a single connection xr_frame from <code>XrBeginFrameOp</code> to <code>XrEndFrameOp</code> is required to synchronize the OpenXR calls issued by the two operators.</p>","tags":["XR","XRFrame"]},{"location":"operators/XrTransformControlOp/","title":"XrTransformControlOp","text":"","tags":["XR","XRFrame"]},{"location":"operators/XrTransformControlOp/#user-interface-control-operator","title":"User interface Control Operator","text":"<p>The <code>XrTransformControlOp</code> maintains the state of the mixed reality user interface for the volumetric rendering application. It consumes controller events and produces user interface state structures as well as parameters for the volume rendering operator. The operator is application specific and will grow over time to include additional user interface widgets.</p>","tags":["XR","XRFrame"]},{"location":"operators/XrTransformControlOp/#holoscanopenxrxrtransformcontrolop","title":"<code>holoscan::openxr::XrTransformControlOp</code>","text":"","tags":["XR","XRFrame"]},{"location":"operators/XrTransformControlOp/#inputs","title":"Inputs","text":"<p>Controller state - <code>trigger_click</code>: trigger button state   - type: <code>bool</code> - <code>shoulder_click</code>: shoulder button state   - type: <code>bool</code> - <code>trackpad_touch</code>: trackpad state   - type: <code>bool</code> - <code>trackpad</code>: trackpad values [x,y]   - type: <code>std::array&lt;float, 2&gt;</code> - <code>aim_pose</code>: world space pose of the controller tip   - type: <code>nvidia::gxf::Pose3D</code></p> <p>Device state - <code>head_pose</code>: world space head pose of the device   - type: <code>nvidia::gxf::Pose3D</code></p> <p>Volume state - <code>extent</code>: size of bounding box containing volume   - type: <code>std::array&lt;float, 3&gt;</code></p>","tags":["XR","XRFrame"]},{"location":"operators/XrTransformControlOp/#outputs","title":"Outputs","text":"<p>User interface widget state structures - <code>ux_box</code>: bounding box state structure   - type: <code>UxBoundingBox</code> - <code>ux_cursor</code>: cursor state structure   - type: <code>UxCursor</code></p> <p>Volume rendering parameters - <code>volume_pose</code>: world pose of dataset    - type: <code>nvidia::gxf::Pose3D</code> - <code>crop_box</code>: axis aligned cropping planes in local coordinates   - type: <code>std::array&lt;nvidia::gxf::Vector2f, 3&gt;</code></p>","tags":["XR","XRFrame"]},{"location":"operators/XrTransformOp/","title":"XrTransformOp","text":"","tags":["XR","XRFrame"]},{"location":"operators/XrTransformOp/#user-interface-control-operator","title":"User interface Control Operator","text":"<p>The <code>XrTransformControlOp</code> maintains the state of the mixed reality user interface for the volumetric rendering application. It consumes controller events and produces user interface state structures as well as parameters for the volume rendering operator. The operator is application specific and will grow over time to include additional user interface widgets.</p>","tags":["XR","XRFrame"]},{"location":"operators/XrTransformOp/#holoscanopenxrxrtransformcontrolop","title":"<code>holoscan::openxr::XrTransformControlOp</code>","text":"","tags":["XR","XRFrame"]},{"location":"operators/XrTransformOp/#inputs","title":"Inputs","text":"<p>Controller state - <code>trigger_click</code>: trigger button state   - type: <code>bool</code> - <code>shoulder_click</code>: shoulder button state   - type: <code>bool</code> - <code>trackpad_touch</code>: trackpad state   - type: <code>bool</code> - <code>trackpad</code>: trackpad values [x,y]   - type: <code>std::array&lt;float, 2&gt;</code> - <code>aim_pose</code>: world space pose of the controller tip   - type: <code>nvidia::gxf::Pose3D</code></p> <p>Device state - <code>head_pose</code>: world space head pose of the device   - type: <code>nvidia::gxf::Pose3D</code></p> <p>Volume state - <code>extent</code>: size of bounding box containing volume   - type: <code>std::array&lt;float, 3&gt;</code></p>","tags":["XR","XRFrame"]},{"location":"operators/XrTransformOp/#outputs","title":"Outputs","text":"<p>User interface widget state structures - <code>ux_box</code>: bounding box state structure   - type: <code>UxBoundingBox</code> - <code>ux_cursor</code>: cursor state structure   - type: <code>UxCursor</code></p> <p>Volume rendering parameters - <code>volume_pose</code>: world pose of dataset    - type: <code>nvidia::gxf::Pose3D</code> - <code>crop_box</code>: axis aligned cropping planes in local coordinates   - type: <code>std::array&lt;nvidia::gxf::Vector2f, 3&gt;</code></p>","tags":["XR","XRFrame"]},{"location":"operators/XrTransformRenderOp/","title":"XrTransformRenderOp","text":"","tags":["XR","XRFrame"]},{"location":"operators/XrTransformRenderOp/#user-interface-render-operator","title":"User interface Render Operator","text":"<p>The <code>XrTransformRenderOp</code> renders the mixed reality user interface of the volumetric rendering application. It consumes interface widget state structures as well as render buffers into which to overlay the interface widgets. The operator is application specific and will grow over time to include additional user interface widgets.</p>","tags":["XR","XRFrame"]},{"location":"operators/XrTransformRenderOp/#holoscanopenxrxrtransformrenderop","title":"<code>holoscan::openxr::XrTransformRenderOp</code>","text":"","tags":["XR","XRFrame"]},{"location":"operators/XrTransformRenderOp/#parameters","title":"Parameters","text":"<ul> <li><code>display_width</code>: pixel height of display</li> <li>type: <code>int</code></li> <li><code>display_height</code>: pixel width of display</li> <li>type: <code>int</code></li> </ul>","tags":["XR","XRFrame"]},{"location":"operators/XrTransformRenderOp/#inputs","title":"Inputs","text":"<p>Camera state for stereo view - <code>left_camera_pose</code>: world space pose of the left eye   - type: <code>nvidia::gxf::Pose3D</code> - <code>right_camera_pose</code>: world space pose of the right eye   - type: <code>nvidia::gxf::Pose3D</code> - <code>left_camera_model</code>: camera model for the left eye   - type: <code>nvidia::gxf::CameraModel</code> - <code>right_camera_model</code>: camera model for the right eye   - type: <code>nvidia::gxf::CameraModel</code> - <code>depth_range</code>: depth range</p> <p>User interface widget state structures - <code>ux_box</code>: bounding box state structure   - type: <code>UxBoundingBox</code> - <code>ux_cursor</code>: cursor state structure   - type: <code>UxCursor</code></p> <p>Render buffers to be populated - <code>Collor buffer_in</code>: color buffer   - type: <code>holoscan::gxf::VideoBuffer</code> - <code>Depth buffer_in</code>: depth buffer   - type: <code>holoscan::gxf::VideoBuffer</code></p>","tags":["XR","XRFrame"]},{"location":"operators/XrTransformRenderOp/#outputs","title":"Outputs","text":"<p>Render buffers including interface widgets - <code>color_buffer_out</code>: color buffer   - type: <code>holoscan::gxf::VideoBuffer</code> - <code>depth_buffer_out</code>: depth buffer   - type: <code>holoscan::gxf::VideoBuffer</code></p>","tags":["XR","XRFrame"]},{"location":"operators/advanced_network/","title":"advanced_network","text":"","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#advanced-network-operator","title":"Advanced Network Operator","text":"<p>The Advanced Network Operator provides a way for users to achieve the highest throughput and lowest latency for transmitting and receiving Ethernet frames out of and into their operators. Direct access to the NIC hardware is available in userspace using this operator, thus bypassing the kernel's networking stack entirely. With a properly tuned system the advanced network operator can achieve hundreds of Gbps with latencies in the low microseconds. Performance is highly dependent on system tuning, packet sizes, batch sizes, and other factors. The data may optionally be sent to the GPU using GPUDirect to prevent extra copies to and from the CPU.</p> <p>Since the kernel's networking stack is bypassed, the user is responsible for defining the protocols used over the network. In most cases Ethernet, IP, and UDP are ideal for this type of processing because of their simplicity, but any type of protocol can be implemented or used. The advanced network operator gives the option to use several primitives to remove the need for filling out these headers for basic packet types, but raw headers can also be constructed.</p>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#requirements","title":"Requirements","text":"<ul> <li>Linux</li> <li>An NVIDIA NIC with a ConnectX-6 or later chip</li> <li>System tuning as described below</li> <li>DPDK 22.11</li> <li>MOFED 5.8-1.0.1.1 or later</li> <li>DOCA 2.7 or later</li> </ul>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#features","title":"Features","text":"<ul> <li>High Throughput: Hundreds of gigabits per second is possible with the proper hardware</li> <li>Low Latency: With direct access to the NIC's ring buffers, most latency incurred is only PCIe latency</li> <li>GPUDirect: Optionally send data directly from the NIC to GPU, or directly from the GPU to NIC. GPUDirect has two modes:</li> <li>Header-data split: Split the header portion of the packet to the CPU and the rest (payload) to the GPU. The split point is     configurable by the user. This option should be the preferred method in most cases since it's easy to use and still     gives near peak performance.</li> <li>Batched GPU: Receive batches of whole packets directly into the GPU memory. This option requires the GPU kernel to inspect     and determine how to handle packets. While performance may increase slightly over header-data split, this method     requires more effort and should only be used for advanced users.</li> <li>GPUComms: Optionally control the send or receive communications from the GPU through the GPUDirect Async Kernel-Initiated network technology (enabled with the DOCA GPUNetIO transport layer only).</li> <li>Flow Configuration: Configure the NIC's hardware flow engine for configurable patterns. Currently only UDP source     and destination are supported.</li> </ul>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#limitations","title":"Limitations","text":"<p>The limitations below will be removed in a future release.</p> <ul> <li>Only UDP fill mode is supported</li> </ul>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#implementation","title":"Implementation","text":"<p>Internally the advanced network operator can be implemented by different transport layers, each offering different features. The network transport layer must be specified at the beginning of the application using the ANO API.</p>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#dpdk","title":"DPDK","text":"<p>DPDK is an open-source userspace packet processing library supported across platforms and vendors. While the DPDK interface is abstracted away from users of the advanced network operator, the method in which DPDK integrates with Holoscan is important for understanding how to achieve the highest performance and for debugging.</p> <p>When the advanced network operator is compiled/linked against a Holoscan application, an instance of the DPDK manager is created, waiting to accept configuration. When either an RX or TX advanced network operator is defined in a Holoscan application, their configuration is sent to the DPDK manager. Once all advanced network operators have initialized, the DPDK manager is told to initialize DPDK. At this point the NIC is configured using all parameters given by the operators. This step allocates all packet buffers, initializes the queues on the NIC, and starts the appropriate number of internal threads. The job of the internal threads is to take packets off or put packets onto the NIC as fast as possible. They act as a proxy between the advanced network operators and DPDK by handling packets faster than the operators may be able to.</p> <p>To achieve zero copy throughout the whole pipeline only pointers are passed between each entity above. When the user receives the packets from the network operator it's using the same buffers that the NIC wrote to either CPU or GPU memory. This architecture also implies that the user must explicitly decide when to free any buffers it's owning. Failure to free buffers will result in errors in the advanced network operators not being able to allocate buffers.</p>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#doca","title":"DOCA","text":"<p>NVIDIA DOCA brings together a wide range of powerful APIs, libraries, and frameworks for programming and accelerating modern data center infrastructures\u200b. The DOCA SDK composed by a variety of C/C++ API for different purposes\u200b, exposing all the features supported by NVIDIA hardware and platforms. DOCA GPUNetIO is one of the libraries included in the SDK and it enables the GPU to control, from a CUDA kernel, network communications directly interacting with the network card and completely removing the CPU from the critical data path.</p> <p>If the application wants to enable GPU communications, it must chose DOCA as transport layer. The behaviour of the DOCA transport layer is similar to the DPDK one except that the receive and send are executed by CUDA kernels. Specifically: - Receive: a persistent CUDA kernel is running on a dedicated stream and keeps receiving packets, providing packets' info to the application level. Due to the nature of the operator, the CUDA receiver kernel now is responsible only to receive packets but in a real-world application, it can be extended to receive and process in real-time network packets (DPI, filtering, decrypting, byte modification, etc..) before forwarding packets to the application. - Send: every time the application wants to send packets it launches one or more CUDA kernels to prepare data and create Ethernet packets and then (without the need of synchronizing) forward the send request to the operator. The operator then launches another CUDA kernel that in turn sends the packets (still no need to synchronize with the CPU). The whole pipeline is executed on the GPU. Due to the nature of the operator, the packets' creation and packets' send must be split in two CUDA kernels but in a real-word application, they can be merged into a single CUDA kernel responsible for both packet processing and packet sending.</p> <p>Please refer to the DOCA GPUNetIO programming guide to correctly configure your system before using this transport layer.</p> <p>DOCA transport layer doesn't support the <code>split-boundary</code> option.</p> <p>To build and run the ANO Dockerfile with DOCA support, please follow the steps below:</p> <pre><code># To build Docker image\n./dev_container build --docker_file operators/advanced_network/Dockerfile --img holohub-doca:doca-28-ubuntu2204 --no-cache\n\n# Launch DOCA container\n./operators/advanced_network/run_doca.sh\n\n# To build operator + app from main dir\n./run build adv_networking_bench --configure-args \"-DANO_MGR=gpunetio\"\n\n# Run app\n./build/adv_networking_bench/applications/adv_networking_bench/cpp/adv_networking_bench adv_networking_bench_gpunetio_tx_rx.yaml\n</code></pre> <p>Receiver side, CUDA Persistent kernel note To get the best performance on the receive side, the Advanced Network Operator must be built with with <code>RX_PERSISTENT_ENABLED</code> set to 1 which enables the CUDA receiver kernel to run persistently for the whole execution. For Holoscan internal reasons (not related to the DOCA library), a persistent CUDA kernel may cause issues on some applications on the receive side. This issue is still under investigation. If this happens, there are two options: - build the Advanced Network Operator with <code>RX_PERSISTENT_ENABLED</code> set to 0 - keep the <code>RX_PERSISTENT_ENABLED</code> set to 1 and enable also MPS setting <code>MPS_ENABLED</code> to 1. Then, MPS should be enabled on the system: <pre><code>export CUDA_MPS_PIPE_DIRECTORY=/var\nexport CUDA_MPS_LOG_DIRECTORY=/var\nsudo -E nvidia-cuda-mps-control -d\nsudo -E echo start_server -uid 0 | sudo -E nvidia-cuda-mps-control\n</code></pre></p> <p>This should solve all problems. Both <code>RX_PERSISTENT_ENABLED</code> and <code>MPS_ENABLED</code> are defined in <code>operators/advanced_network/managers/doca/adv_network_doca_mgr.h</code>.</p>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#rivermax","title":"RIVERMAX","text":"<p>NVIDIA Rivermax SDK Optimized networking SDK for media and data streaming applications. NVIDIA\u00ae Rivermax\u00ae offers a unique IP-based solution for any media and data streaming use case. Rivermax together with NVIDIA GPU accelerated computing technologies unlocks innovation for a wide range of applications in Media and Entertainment (M&amp;E), Broadcast, Healthcare, Smart Cities and more. Rivermax leverages NVIDIA ConnectX\u00ae and BlueField\u00ae DPU hardware-streaming acceleration technology that enables direct data transfers to and from the GPU, delivering best-in-class throughput and latency with minimal CPU utilization for streaming workloads. Rivermax is the only fully-virtualized streaming solution that complies with the stringent timing and traffic flow requirements of the SMPTE ST 2110-21 specification. Rivermax enables the future of cloud-based software-defined broadcasting. Product release highlights, documentation, platform support, installation and usage guides can be found in the Rivermax SDK Page. Frequently asked questions, customers product highlights, Video link and more are available on the Rivermax Product Page. While the Rivermax interface is abstracted away from users of the advanced network operator, the method in which Rivermax integrates with Holoscan is important for understanding how to achieve the highest performance and for debugging.</p> <p>When the advanced network operator is compiled/linked against a Holoscan application, an instance of the Rivermax manager is created, waiting to accept configuration. When either an RX or TX advanced network operator is defined in aHoloscan application, their configuration is sent to the Rivermax manager. Once all advanced network operators have initialized, the Rivermax manager is told to initialize Rivermax. At this point the NIC is configured using all parameters given by the operators. This step allocates all packet buffers, initializes the queues on the NIC, and starts the appropriate number of internal threads. The job of the internal threads is to take packets off or put packets onto the NIC as fast as possible. They act as a proxy between the advanced network operators and Rivermax by handling packets faster than the operators may be able to.</p> <p>To achieve zero copy throughout the whole pipeline only pointers are passed between each entity above. When the user receives the packets from the network operator it's using the same buffers that the NIC wrote to either CPU or GPU memory. This architecture also implies that the user must explicitly decide when to free any buffers it's owning. Failure to free buffers will result in errors in the advanced network operators not being able to allocate buffers. Rivermax manager supports receiving the same stream from multiple redundant paths. Each path is a combination of a source IP address, a destination IPaddress, a destination port, and a local IP address of the receiver device. Single path receive supports packet reordering within the NIC, multi-pathreceive also adds recovery of missing packets from other streams.</p> <p>To build and run the ANO Dockerfile with <code>Rivermax</code> support, follow these steps:</p> <ul> <li>Visit the Rivermax SDK Page to download the Rivermax Release SDK.</li> <li>Obtain a Rivermax developer license from the same page. This is necessary for using the SDK.</li> <li>Copy the downloaded SDK tar file (e.g., <code>rivermax_ubuntu2204_1.60.1.tar.gz</code>) into your current working directory.</li> <li>You can adjust the path using the <code>RIVERMAX_SDK_ZIP_PATH</code> build argument if needed.</li> <li>Modify the version using the <code>RIVERMAX_VERSION</code> build argument if you're using a different SDK version.</li> <li>Place the obtained Rivermax developer license file (<code>rivermax.lic</code>) into the <code>/opt/mellanox/rivermax/</code> directory. You can change this path in the run_rivermax.sh script if necessary</li> <li>Build the Docker image:</li> </ul> <pre><code>./dev_container build --docker_file operators/advanced_network/Dockerfile --img holohub:rivermax --build-args \"--target rivermax\"\n</code></pre> <ul> <li>Launch Rivermax container</li> </ul> <pre><code># Launch Rivermax container\n./operators/advanced_network/run_rivermax.sh\n\n# To build operator + app from main dir\n./run build adv_networking_bench --configure-args \"-DANO_MGR=rivermax\"\n\n# Run app\n./build/adv_networking_bench/applications/adv_networking_bench/cpp/adv_networking_bench  adv_networking_bench_rmax_rx.yaml\n</code></pre>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#system-tuning","title":"System Tuning","text":"<p>From a high level, tuning the system for a low latency workload prevents latency spikes large enough to cause anomalies in the application. This section details how to perform the basic tuning steps needed on both a Clara AGX and Orin IGX systems.</p>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#create-hugepages","title":"Create Hugepages","text":"<p>Hugepages give the kernel access to a larger page size than the default (usually 4K) which reduces the number of memory translations that have to be actively maintained in MMUs. 1GB hugepages are ideal, but 2MB may be used as well if 1GB is not available. To configure 1GB hugepages:</p> <pre><code>sudo mkdir /mnt/huge\nsudo mount -t hugetlbfs nodev /mnt/huge\nsudo sh -c \"echo nodev /mnt/huge hugetlbfs pagesize=1GB 0 0 &gt;&gt; /etc/fstab\"\n</code></pre>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#linux-boot-command-line","title":"Linux Boot Command Line","text":"<p>The Linux boot command line allows configuration to be injected into Linux before booting. Some configuration options are only available at the boot command since they must be provided before the kernel has started. On the Orin IGX editing the boot command can be done with the following configuration:</p> <pre><code>sudo vim /etc/default/grub\n# Find the line starting with APPEND and add the following\n\nisolcpus=6-11 nohz_full=6-11 irqaffinity=0-5 rcu_nocbs=6-11 rcu_nocb_poll tsc=reliable audit=0 nosoftlockup default_hugepagesz=1G hugepagesz=1G hugepages=2\n</code></pre> <p>The settings above isolate CPU cores 6-11 on the Orin and 4-7 on the Clara, and turn 1GB hugepages on.</p> <p>For non-IGX or AGX systems please look at the documentation for your system to change the boot command.</p>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#setting-the-cpu-governor","title":"Setting the CPU governor","text":"<p>The CPU governor reduces power consumption by decreasing the clock frequency of the CPU when cores are idle. While this is useful in most environments, increasing the clocks from an idle period can cause long latency stalls. To disable frequency scaling:</p> <pre><code>sudo apt install cpufrequtils\nsudo sed -i 's/^GOVERNOR=.*/GOVERNOR=\"performance\"/' /etc/init.d/cpufrequtils\n</code></pre> <p>Reboot the system after these changes.</p>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#permissions","title":"Permissions","text":"<p>DPDK typically requires running as a root user. If you wish to run as a non-root user, you may follow the directions here: http://doc.dpdk.org/guides/linux_gsg/enable_func.html</p> <p>If running in a container, you will need to run in privileged container, and mount your hugepages mount point from above into the container. This can be done as part of the <code>docker run</code> command by adding the following flags:</p> <pre><code>-v /mnt/huge:/mnt/huge \\\n--privileged \\\n</code></pre>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#configuration-parameters","title":"Configuration Parameters","text":"<p>The advanced network operator contains a separate operator for both transmit and receive. This allows applications to choose whether they need to handle bidirectional traffic or only unidirectional. Transmit and receive are configured separately in a YAML file, and a common configuration contains items used by both directions. Each configuration section is described below.</p>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#common-configuration","title":"Common Configuration","text":"<p>The common configuration container parameters are used by both TX and RX:</p> <ul> <li><code>version</code>: Version of the config. Only 1 is valid currently.</li> <li>type: <code>integer</code></li> <li><code>master_core</code>: Master core used to fork and join network threads. This core is not used for packet processing and can be bound to a non-isolated core. Should differ from isolated cores in queues below.</li> <li>type: <code>integer</code></li> <li><code>manager</code>: Backend networking library. default: <code>dpdk</code>. Other: <code>doca</code> (GPUNet IO), <code>rivermax</code></li> <li>type: <code>string</code></li> <li><code>log_level</code>: Backend log level. default: <code>warn</code>. Other: <code>trace</code> , <code>debug</code>, <code>info</code>, <code>error</code>, <code>critical</code>, <code>off</code></li> <li>type: <code>string</code></li> </ul>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#memory-regions","title":"Memory regions","text":"<p><code>memory_regions:</code> List of regions where buffers are stored.</p> <ul> <li><code>name</code>: Memory Region name</li> <li>type: <code>string</code></li> <li><code>kind</code>: Location. Best options are <code>device</code> (GPU), or <code>huge</code> (pages - CPU). Not recommended: <code>host</code> (CPU), <code>host_pinned</code> (CPU).</li> <li>type: <code>string</code></li> <li><code>affinity</code>: GPU ID for GPU memory, NUMA Node ID for CPU memory</li> <li>type: <code>integer</code></li> <li><code>access</code>: Permissions to the rdma memory region ( <code>local</code> or <code>rmda_read</code> or <code>rdma_write</code>)</li> <li>type: <code>string</code></li> <li><code>num_bufs</code>: Higher value means more time to process, but less space on GPU BAR1. Too low means risk of dropped packets from NIC having nowhere to write (Rx) or higher latency from buffering (Tx). Good rule of \ud83d\udc4d : 3x batch_size</li> <li>type: <code>integer</code></li> <li><code>buf_size</code>: Size of buffer, equal to packet size or less if breaking down packets (ex: header data split)</li> <li>type: <code>integer</code></li> </ul>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#interfaces","title":"Interfaces","text":"<ul> <li><code>interfaces</code>:  List and configure ethernet interfaces     full path: <code>cfg\\interfaces\\</code><ul> <li><code>name</code>: Name of the interfaca</li> <li>type: <code>string</code></li> <li><code>address</code>: PCIe BDF address (lspci) or linux link name (ip link)</li> <li>type: <code>string</code></li> <li><code>rx|tx</code> category of queues below full path: <code>cfg\\interfaces\\[rx|tx]</code></li> </ul> </li> </ul>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#receive-configuration-rx","title":"Receive Configuration (rx)","text":"<ul> <li> <p><code>queues</code>: List of queues on NIC     type: <code>list</code>     full path: <code>cfg\\interfaces\\rx\\queues</code></p> <ul> <li><code>name</code>: Name of queue<ul> <li>type: <code>string</code></li> </ul> </li> <li><code>id</code>: Integer ID used for flow connection or lookup in operator compute method<ul> <li>type: <code>integer</code></li> </ul> </li> <li><code>cpu_core</code>: CPU core ID. Should be isolated when CPU polls the NIC for best performance.. Not in use for Doca GPUNetIO     Rivermax manager can accept coma separated list of CPU IDs<ul> <li>type: <code>string</code></li> </ul> </li> <li><code>batch_size</code>: Number of packets in a batch that is passed between the advanced network operator and the user's operator. A larger number increases throughput and latency by requiring fewer messages between operators, but takes longer to populate a single buffer. A smaller number reduces latency and bandwidth by passing more messages.<ul> <li>type: <code>integer</code></li> </ul> </li> <li><code>split_boundary</code>: HDS (Header Data Split) Split point in bytes between header and payload. If set to 0 HDS is disabled<ul> <li>type: <code>integer</code></li> </ul> </li> <li><code>output_port</code>:  Name of the ANO Rx operator output port for aggregator operators to connect to<ul> <li>type: <code>string</code></li> </ul> </li> <li><code>memory_regions</code>: List of memory regions where buffers are stored. memory regions names are configured in the Memory Regions section     type: <code>list</code></li> <li><code>timeout_us</code>: Timeout value that a batch will be sent on even if not enough packets to fill a batch were received<ul> <li>type: <code>integer</code></li> </ul> </li> </ul> </li> <li> <p><code>flows</code>: List of flows - rules to apply to packets, mostly to divert to the right queue. (Not in use for Rivermax manager)   type: <code>list</code>   full path: <code>cfg\\interfaces\\[rx|tx]\\flows</code></p> <ul> <li><code>name</code>: Name of the flow</li> <li>type: <code>string</code></li> <li><code>id</code>: ID of the flow</li> <li>type: <code>integer</code></li> <li><code>action</code>: Action section of flow (what happens. Currently only supports steering to a given queue)</li> <li>type: <code>sequence</code><ul> <li><code>type</code>: Type of action. Only <code>queue</code> is supported currently.</li> <li>type: <code>string</code></li> <li><code>id</code>: ID of queue to steer to<ul> <li>type: <code>integer</code></li> </ul> </li> </ul> </li> <li><code>match</code>: Match section of flow</li> <li>type: <code>sequence</code><ul> <li><code>udp_src</code>: UDP source port</li> <li>type: <code>integer</code></li> <li><code>udp_dst</code>: UDP destination port</li> <li>type: <code>integer</code></li> <li><code>ipv4_len</code>: IPv4 payload length</li> <li>type: <code>integer</code></li> </ul> </li> </ul> </li> </ul>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#extended-receive-configuration-for-rivermax-manager","title":"Extended Receive Configuration for Rivermax manager","text":"<ul> <li> <p><code>rmax_rx_settings</code>: Extended RX settings for Rivermax Manager. Rivermax Manager supports receiving the same stream from multiple redundant paths (IPO - Inline Packet Ordering).     Each path is a combination of a source IP address, a destination IP address, a destination port, and a local IP address of the receiver device.   type: <code>list</code>   full path: <code>cfg\\interfaces\\rx\\queues\\rmax_rx_settings</code></p> <ul> <li><code>memory_registration</code>: Flag, when enabled, reduces the number of memory keys in use by registering all the memory in a single pass on the application side.     Can be used only together with HDS enabled<ul> <li>type: <code>boolean</code></li> <li>default:<code>false</code></li> </ul> </li> <li><code>max_path_diff_us</code>: Sets the maximum number of microseconds that receiver waits for the same packet to arrive from a different stream (if IPO is enabled)<ul> <li>type: <code>integer</code></li> <li>default:<code>0</code></li> </ul> </li> <li><code>ext_seq_num</code>: The RTP sequence number is used by the hardware to determine the location of arriving packets in the receive buffer.     The application supports two sequence number parsing modes: 16-bit RTP sequence number (default) and 32-bit extended sequence number,     consisting of 16 low order RTP sequence number bits and 16 high order bits from the start of RTP payload. When set to <code>true</code> 32-bit ext. sequence number will be used<ul> <li>type: <code>boolean</code></li> <li>default:<code>true</code></li> </ul> </li> <li><code>sleep_between_operations_us</code>: Specifies the duration, in microseconds, that the receiver will pause or sleep between two consecutive receive (RX) operations.<ul> <li>type: <code>integer</code></li> <li>default:<code>0</code></li> </ul> </li> <li><code>local_ip_addresses</code>: List of Local NIC IP Addresses (one address per receiving path)<ul> <li>type: <code>sequence</code></li> </ul> </li> <li><code>source_ip_addresses</code>: List of Sender IP Addresses (one address per receiving path)<ul> <li>type: <code>sequence</code></li> </ul> </li> <li><code>destination_ip_addresses</code>: List of Destination IP Addresses (one address per receiving path), can be multicast<ul> <li>type: <code>sequence</code></li> </ul> </li> <li><code>destination_ports</code>: List of Destination IP ports (one port per receiving path)<ul> <li>type: <code>sequence</code></li> </ul> </li> <li><code>rx_stats_period_report_ms</code>: Specifies the duration, in milliseconds, that the receiver will display statistics in the log. Set <code>0</code> to disable statistics logging feature<ul> <li>type: <code>integer</code></li> <li>default:<code>0</code></li> </ul> </li> <li><code>send_packet_ext_info</code>: Enables the transmission of extended metadata for each received packet<ul> <li>type: <code>boolean</code></li> <li>default:<code>true</code></li> </ul> </li> </ul> </li> <li> <p>Example of the Rivermax queue configuration for redundant stream using HDS and GPU   This example demonstrates receiving a redundant stream sent from a sender with source addresses 192.168.100.4 and 192.168.100.3.   The stream is received via NIC which have local IP (same) 192.168.100.5 (listed twice, once per stream).   The multicast addresses and UDP ports on which the stream is being received are 224.1.1.1:5001 and 224.1.1.2:5001  The incoming packets are of size 1152 bytes. The initial 20 bytes are stripped from the payload as an  application header and placed in buffers allocated in RAM.  The remaining 1132 bytes are placed in dedicated payload buffers.  In this case, the payload buffers are allocated in GPU 0 memory. <pre><code>    memory_regions:\n    - name: \"Data_RX_CPU\"\n      kind: \"huge\"\n      affinity: 0\n      access:\n        - local\n      num_bufs: 43200\n      buf_size: 20\n    - name: \"Data_RX_GPU\"\n      kind: \"device\"\n      affinity: 0\n      access:\n        - local\n      num_bufs: 43200\n      buf_size: 1132\n    interfaces:\n    - address: 0005:03:00.0\n      name: data1\n      rx:\n        queues:\n        - name: Data1\n          id: 1\n          cpu_core: '11'\n          batch_size: 4320\n          output_port: bench_rx_out_1\n          rmax_rx_settings:\n            memory_registration: true\n            max_path_diff_us: 100\n            ext_seq_num: true\n            sleep_between_operations_us: 100\n            memory_regions:\n            - \"Data_RX_CPU\"\n            - \"Data_RX_GPU\"\n            local_ip_addresses:\n            - 192.168.100.5\n            - 192.168.100.5\n            source_ip_addresses:\n            - 192.168.100.4\n            - 192.168.100.4\n            destination_ip_addresses:\n            - 224.1.1.1\n            - 224.1.1.2\n            destination_ports:\n            - 50001\n            - 50001\n            rx_stats_period_report_ms: 3000\n            send_packet_ext_info: true\n</code></pre></p> </li> </ul>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#transmit-configuration-tx","title":"Transmit Configuration (tx)","text":"<p>(Current version of Rivermax manager doesn't support TX)</p> <ul> <li><code>queues</code>: List of queues on NIC     type: <code>list</code>     full path: <code>cfg\\interfaces\\tx\\queues</code><ul> <li><code>name</code>: Name of queue<ul> <li>type: <code>string</code></li> </ul> </li> <li><code>id</code>: Integer ID used for flow connection or lookup in operator compute method<ul> <li>type: <code>integer</code></li> </ul> </li> <li><code>cpu_core</code>: CPU core ID. Should be isolated when CPU polls the NIC for best performance.. Not in use for Doca GPUNet IORivermax manager     Rivermax manager can accept coma separated list of CPU IDs<ul> <li>type: <code>string</code></li> </ul> </li> <li><code>batch_size</code>: Number of packets in a batch that is passed between the advanced network operator and the user's operator. A larger number increases throughput and latency by requiring fewer messages between operators, but takes longer to populate a single buffer. A smaller number reduces latency and bandwidth by passing more messages.<ul> <li>type: <code>integer</code></li> </ul> </li> <li><code>split_boundary</code>: HDS (Header Data Split) Split point in bytes between header and payload. If set to 0 HDS is disabled<ul> <li>type: <code>integer</code></li> </ul> </li> <li><code>memory_regions</code>: List of memory regions where buffers are stored. memory regions names are configured in the Memory Regions section     type: <code>list</code></li> </ul> </li> </ul>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#api-structures","title":"API Structures","text":"<p>Both the transmit and receive operators use a common structure named <code>BurstParams</code> to pass data to/from other operators. <code>BurstParams</code> provides pointers to packet memory locations (e.g., CPU or GPU) and contains metadata needed by the operator to track allocations. Since the advanced network operator utilizes a generic interface that does not expose the underlying low-level network card library, interacting with <code>BurstParams</code> is mostly done with the helper functions described below. A user should never modify any members of <code>BurstParams</code> directly, as this may break in future versions. The <code>BurstParams</code> is described below:</p> <pre><code>struct BurstParams {\n  BurstHeader hdr;\n\n  std::array&lt;void**, MAX_NUM_SEGS&gt; pkts;\n  std::array&lt;uint32_t*, MAX_NUM_SEGS&gt; pkt_lens;\n  void** pkt_extra_info;\n  cudaEvent_t event;\n};\n</code></pre> <p>Starting from the top, the <code>hdr</code> field contains metadata about the batch of packets. The <code>pkts</code> array stores opaque pointers to packet memory locations (e.g., CPU or GPU) across multiple segments, and <code>pkt_lens</code> stores the lengths of these packets. <code>pkt_extra_info</code> contains additional metadata about each packet, and <code>event</code> is a CUDA event used for synchronization.</p> <p>As mentioned above, the <code>pkts</code> and <code>pkt_lens</code> fields are opaque and should not be accessed directly. Instead, refer to the helper functions in the next section for interacting with these fields to ensure compatibility with future versions.</p>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#example-api-usage","title":"Example API Usage","text":"<p>For an entire list of API functions, please see the <code>advanced_network/common.h</code> header file.</p>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#receive","title":"Receive","text":"<p>The section below describes a workflowusing GPUDirect to receive packets using header-data split. The job of the user's operator(s) is to process and free the buffers as quickly as possible. This might be copying to interim buffers or freeing before the entire pipeline is done processing. This allows the networking piece to use relatively few buffers while still achieving very high rates.</p> <p>The first step in receiving from the advanced network operator is to tie your operator's input port to the output port of the RX network operator's <code>burst_out</code> port.</p> <pre><code>auto adv_net_rx    = make_operator&lt;ops::AdvNetworkOpRx&gt;(\"adv_network_rx\", from_config(\"adv_network_common\"), from_config(\"adv_network_rx\"), make_condition&lt;BooleanCondition&gt;(\"is_alive\", true));\nauto my_receiver   = make_operator&lt;ops::MyReceiver&gt;(\"my_receiver\", from_config(\"my_receiver\"));\nadd_flow(adv_net_rx, my_receiver, ('burst_out', 'burst_in'));\n</code></pre> <p>Once the ports are connected, inside the <code>compute()</code> function of your operator you will receive a <code>BurstParams</code> structure when a batch is complete:</p> <pre><code>auto burst = op_input.receive&lt;std::shared_ptr&lt;BurstParams&gt;&gt;(\"burst_in\").value();\n</code></pre> <p>The packets arrive in scattered packet buffers. Depending on the application, you may need to iterate through the packets to aggregate them into a single buffer. Alternatively the operator handling the packet data can operate on a list of packet pointers rather than a contiguous buffer. Below is an example of aggregating separate GPU packet buffers into a single GPU buffer:</p> <pre><code>  for (int p = 0; p &lt; get_num_packets(burst); p++) {\n    h_dev_ptrs_[aggr_pkts_recv_ + p]   = get_cpu_packet_ptr(burst, p);\n    ttl_bytes_in_cur_batch_           += get_gpu_packet_length(burst, p) + sizeof(UDPPkt);\n  }\n\n  simple_packet_reorder(buffer, h_dev_ptrs, packet_len, burst-&gt;hdr.num_pkts);\n</code></pre> <p>For this example we are tossing the header portion (CPU), so we don't need to examine the packets. Since we launched a reorder kernel to aggregate the packets in GPU memory, we are also done with the GPU pointers. All buffers may be freed to the advanced network operator at this point:</p> <pre><code>free_all_burst_packets_and_burst(burst_bufs_[b]);\n</code></pre>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#transmit","title":"Transmit","text":"<p>Transmitting packets works similar to the receive side, except the user is tasked with filling out the packets as much as it needs to. As mentioned above, helper functions are available to fill in most boilerplate header information if that doesn't change often.</p> <p>Similar to the receive, the transmit operator needs to connect to <code>burst_in</code> on the advanced network operator transmitter:</p> <pre><code>auto my_transmitter  = make_operator&lt;ops::MyTransmitter&gt;(\"my_transmitter\", from_config(\"my_transmitter\"), make_condition&lt;BooleanCondition&gt;(\"is_alive\", true));\nauto adv_net_tx       = make_operator&lt;ops::AdvNetworkOpTx&gt;(\"adv_network_tx\", from_config(\"adv_network_common\"), from_config(\"adv_network_tx\"));\nadd_flow(my_transmitter, adv_net_tx, ('burst_out', 'burst_in'));\n</code></pre> <p>Before sending packets, the user's transmit operator must request a buffer from the advanced network operator pool:</p> <pre><code>auto msg = std::make_shared&lt;BurstParams&gt;();\nmsg-&gt;hdr.num_pkts = num_pkts;\nif ((ret = get_tx_packet_burst(msg.get())) != Status::SUCCESS) {\n  HOLOSCAN_LOG_ERROR(\"Error returned from get_tx_packet_burst: {}\", static_cast&lt;int&gt;(ret));\n  return;\n}\n</code></pre> <p>The code above creates a shared <code>BurstParams</code> that will be passed to the advanced network operator, and uses <code>get_tx_packet_burst</code> to populate the burst buffers with valid packet buffers. On success, the buffers inside the burst structure will be allocate and are ready to be filled in. Each packet must be filled in by the user. In this example we loop through each packet and populate a buffer:</p> <pre><code>for (int num_pkt = 0; num_pkt &lt; msg-&gt;hdr.num_pkts; num_pkt++) {\n  void *payload_src = data_buf + num_pkt * nom_pkt_size;\n  if (set_udp_payload(msg-&gt;cpu_packets[num_pkt], payload_src, nom_pkt_size) != Status::SUCCESS) {\n    HOLOSCAN_LOG_ERROR(\"Failed to create packet {}\", num_pkt);\n  }\n}\n</code></pre> <p>The code iterates over <code>msg-&gt;hdr.num_pkts</code> (defined by the user) and passes a pointer to the payload and the packet size to <code>set_udp_payload</code>. In this example our configuration is using <code>fill_mode</code> \"udp\" on the transmitter, so <code>set_udp_payload</code> will populate the Ethernet, IP, and UDP headers. The payload pointer passed by the user is also copied into the buffer. Alternatively a user could use the packet buffers directly as output from a previous stage to avoid this extra copy.</p> <p>With the <code>BurstParams</code> populated, the burst can be sent off to the advanced network operator for transmission:</p> <pre><code>op_output.emit(msg, \"burst_out\");\n</code></pre>","tags":["Network","Networking","DPDK","UDP","Ethernet","IP","GPUDirect","RDMA"]},{"location":"operators/aja_source/","title":"AJA Source Operator","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 20, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The AJA Source operator provides functionality to capture high-quality video streams from AJA capture cards and devices. It offers comprehensive support for both SDI (Serial Digital Interface) and HDMI (High-Definition Multimedia Interface) input sources, allowing for professional video capture in various formats and resolutions. The operator is designed to work seamlessly with AJA's hardware capabilities, including features like frame synchronization and format detection. Additionally, it provides an optional overlay channel capability that enables real-time mixing and compositing of multiple video streams, making it suitable for applications requiring picture-in-picture, graphics overlay, or other video mixing scenarios.</p>","tags":["Camera","AJA"]},{"location":"operators/aja_source/#requirements","title":"Requirements","text":"<ul> <li>AJA capture card (e.g., KONA HDMI)</li> <li>CUDA-capable GPU</li> <li>Holoscan SDK 1.0.3 or later</li> </ul>","tags":["Camera","AJA"]},{"location":"operators/aja_source/#parameters","title":"Parameters","text":"<p>The following parameters can be configured for this operator:</p> Parameter Type Description Default <code>device</code> string Device specifier (e.g., \"0\" for device 0) \"0\" <code>channel</code> NTV2Channel Camera channel to use for input NTV2_CHANNEL1 <code>width</code> uint32_t Width of the video stream 1920 <code>height</code> uint32_t Height of the video stream 1080 <code>framerate</code> uint32_t Frame rate of the video stream 60 <code>interlaced</code> bool Whether the video is interlaced false <code>rdma</code> bool Enable RDMA for video input false <code>enable_overlay</code> bool Enable overlay channel false <code>overlay_channel</code> NTV2Channel Camera channel to use for overlay NTV2_CHANNEL2 <code>overlay_rdma</code> bool Enable RDMA for overlay false","tags":["Camera","AJA"]},{"location":"operators/aja_source/#supported-video-formats","title":"Supported Video Formats","text":"<p>The operator supports various video formats based on resolution, frame rate, and scan type:</p> <ul> <li>720p (1280x720) at 50/59.94/60 fps</li> <li>1080i (1920x1080) at 50/59.94/60 fps</li> <li>1080p (1920x1080) at 23.98/24/25/29.97/30/50/59.94/60 fps</li> <li>UHD (3840x2160) at 23.98/24/25/29.97/30/50/59.94/60 fps</li> <li>4K (4096x2160) at 23.98/24/25/29.97/30/50/59.94/60 fps</li> </ul>","tags":["Camera","AJA"]},{"location":"operators/aja_source/#input-ports","title":"Input Ports","text":"<ul> <li>overlay_buffer_input (optional): Video buffer for overlay mixing when <code>enable_overlay</code> is true</li> </ul>","tags":["Camera","AJA"]},{"location":"operators/aja_source/#output-ports","title":"Output Ports","text":"<ul> <li>video_buffer_output: Video buffer containing the captured frame</li> <li>overlay_buffer_output (optional): Empty video buffer for overlay when <code>enable_overlay</code> is true</li> </ul>","tags":["Camera","AJA"]},{"location":"operators/apriltag_detector/","title":"HoloHub Operators","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: December 6, 2024 Latest version: 1.0 Minimum Holoscan SDK version: 2.2.0 Tested Holoscan SDK versions: 2.2.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This directory contains operators for the Holoscan Platform.</p>","tags":["Camera"]},{"location":"operators/apriltag_detector/#contributing-to-holohub-operators","title":"Contributing to HoloHub Operators","text":"<p>Please review the CONTRIBUTING.md file guidelines to contribute operators.</p>","tags":["Camera"]},{"location":"operators/base/","title":"dds_base","text":"","tags":["DDS","RTI Connext","Shapes"]},{"location":"operators/base/#dds-base-operator","title":"DDS Base Operator","text":"<p>The DDS Base Operator provides a base class which can be inherited by any operator class which requires access to a DDS domain.</p> <p>This operator requires an installation of RTI Connext to provide access to the DDS domain, as specified by the OMG Data-Distribution Service</p>","tags":["DDS","RTI Connext","Shapes"]},{"location":"operators/base/#holoscanopsddsoperatorbase","title":"<code>holoscan::ops::DDSOperatorBase</code>","text":"<p>Base class which provides the parameters and members required to access a DDS domain.</p> <p>For more documentation about how these parameters (and other similar inheriting-class parameters) are used, see the RTI Connext Documentation.</p>","tags":["DDS","RTI Connext","Shapes"]},{"location":"operators/base/#parameters","title":"Parameters","text":"<ul> <li><code>qos_provider</code>: URI for the DDS QoS Provider</li> <li>type: <code>std::string</code></li> <li><code>participant_qos</code>: Name of the QoS profile to use for the DDS DomainParticipant</li> <li>type: <code>std::string</code></li> <li><code>domain_id</code>: The ID of the DDS domain to use</li> <li>type: <code>uint32_t</code></li> </ul>","tags":["DDS","RTI Connext","Shapes"]},{"location":"operators/basic_network/","title":"basic_network","text":"","tags":["Network","Networking","UDP","Ethernet","IP","TCP"]},{"location":"operators/basic_network/#basic-networking-operator","title":"Basic networking operator","text":"<p>The <code>basic_network_operator</code> operator provides a way to send and receive data over Linux sockets. The destination can be on the same machine or over a network. The basic network operator contains separate operators for transmit and receive. Users may choose one or the other, or use both in applications  requiring bidirectional traffic.</p> <p>For TCP sockets the basic network operator only supports a single stream currently. Future versions may expand this to launch multiple threads to listen on different streams.</p> <p>The basic networking operators use class names: <code>BasicNetworkOpTx</code> and <code>BasicNetworkOpRx</code></p>","tags":["Network","Networking","UDP","Ethernet","IP","TCP"]},{"location":"operators/basic_network/#nvidiaholoscanbasic_network_operator","title":"<code>nvidia::holoscan::basic_network_operator</code>","text":"<p>Basic networking operator</p>","tags":["Network","Networking","UDP","Ethernet","IP","TCP"]},{"location":"operators/basic_network/#receiver-configuration-parameters","title":"Receiver Configuration Parameters","text":"<ul> <li><code>batch_size</code>: Bytes in batch</li> <li>type: <code>integer</code></li> <li><code>max_payload_size</code>: Maximum payload size for a single packet</li> <li>type: <code>integer</code></li> <li><code>udp_dst_port</code>: UDP destination port for packets</li> <li>type: <code>integer</code></li> <li><code>l4_proto</code>: Layer 4 protocol</li> <li>type: <code>string</code> (<code>udp</code>/<code>tcp</code>)</li> <li><code>ip_addr</code>: Destination IP address</li> <li>type: <code>string</code> </li> </ul>","tags":["Network","Networking","UDP","Ethernet","IP","TCP"]},{"location":"operators/basic_network/#transmitter-configuration-parameters","title":"Transmitter Configuration Parameters","text":"<ul> <li><code>max_payload_size</code>: Maximum payload size for a single packet</li> <li>type: <code>integer</code></li> <li><code>udp_dst_port</code>: UDP destination port for packets</li> <li>type: <code>integer</code></li> <li><code>l4_proto</code>: Layer 4 protocol</li> <li>type: <code>string</code> (<code>udp</code>/<code>tcp</code>)</li> <li><code>ip_addr</code>: Destination IP address</li> <li>type: <code>string</code> </li> <li><code>min_ipg_ns</code>: Minimum inter-packet gap in nanoseconds</li> <li>type: <code>integer</code> </li> </ul>","tags":["Network","Networking","UDP","Ethernet","IP","TCP"]},{"location":"operators/basic_network/#transmitter-and-receiver-operator-parameters","title":"Transmitter and Receiver Operator Parameters","text":"<p>The transmitter and receiver operator both use the <code>NetworkOpBurstParams</code> structure as input and output to their ports, respectively. <code>NetworkOpBurstParams</code> contains the following fields:</p> <ul> <li><code>data</code>: Pointer to batch of packet data</li> <li>type: <code>uint8_t *</code></li> <li><code>len</code>: Length of total buffer in bytes</li> <li>type: <code>integer</code></li> <li><code>num_pkts</code>: Number of packets in batch</li> <li>type: <code>integer</code></li> </ul> <p>To receive messages from the Receive operator use the output port <code>burst_out</code>. To send messages to the Transmit operator use the input port <code>burst_in</code>.</p>","tags":["Network","Networking","UDP","Ethernet","IP","TCP"]},{"location":"operators/begin_frame/","title":"XrBeginFrameOp","text":"","tags":["XR","XRFrame"]},{"location":"operators/begin_frame/#xrbeginframe-operator","title":"XRBeginFrame Operator","text":"<p>The <code>XrBeginFrameOp</code> operator initiates the rendering of a single OpenXR frame. It converts render buffers and events generated by the OpenXR runtime into Holoscan specific events to be consumed by downstream operators. Application specific operators are then expected to render left and right camera views into the given color and depth buffers. Once complete, the buffers must be passed to <code>XrEndFrameOp</code> in order to deliver the frame back to the OpenXR runtime. Note that a single arc xr_frame from <code>XrBeginFrameOp</code> to <code>XrEndFrameOp</code> is required to synchronize the OpenXR calls issued by the two operators.</p>","tags":["XR","XRFrame"]},{"location":"operators/begin_frame/#holoscanopenxrxrbeginframeop","title":"<code>holoscan::openxr::XrBeginFrameOp</code>","text":"","tags":["XR","XRFrame"]},{"location":"operators/begin_frame/#outputs","title":"Outputs","text":"<p>Output for camera state  - <code>left_camera_pose</code>: camera pose for the left eye   - type: <code>nvidia::gxf::Pose3D</code> - <code>right_camera_pose</code>: camera pose for the right eye   - type: <code>nvidia::gxf::Pose3D</code> - <code>left_camera_model</code>: camera model for the left eye   - type: <code>nvidia::gxf::CameraModel</code> - <code>right_camera_model</code>: camera model for the right eye   - type: <code>nvidia::gxf::CameraModel</code> - <code>depth_range</code>: depth range   - type: <code>nvidia::gxf::Vector2f</code></p> <p>Output for input state  - <code>trigger_click</code>: trigger click , values true/false   - type: <code>bool</code> - <code>shoulder_click</code>: shoulder click , values true/false   - type: <code>bool</code> - <code>trackpad_touch</code>: trackpad touch , values true/false   - type: <code>bool</code> - <code>trackpad</code>: trackpad values [x.y]   - type: <code>std::array&lt;float, 2&gt;</code> - <code>aim_pose</code>: aim pose for the controller specific for the right hand   - type: <code>nvidia::gxf::Pose3D</code> - <code>head_pose</code>: head pose    - type: <code>nvidia::gxf::Pose3D</code> - <code>color_buffer</code>: color buffer   - type: <code>holoscan::gxf::Entity</code> - <code>depth_buffer</code>: depth buffer   - type: <code>holoscan::gxf::Entity</code></p>","tags":["XR","XRFrame"]},{"location":"operators/begin_frame/#parameters","title":"Parameters","text":"<ul> <li><code>XrSession</code>: A class that encapsulates a single OpenXR session</li> <li>type: <code>holoscan::openxr::XrSession</code></li> </ul> <p>Note:</p> <ul> <li><code>XrCudaInteropSwapchain</code>: A class that encapsulates the Vulkan buffers of the OpenXR runtime and compatible CUDA buffer to provide interoperability between ClaraViz and OpenXR</li> </ul>","tags":["XR","XRFrame"]},{"location":"operators/convert_depth/","title":"Convert Depth to Screen Space","text":"","tags":["Convert","Depth","Screen"]},{"location":"operators/convert_depth/#convert-depth-to-screen-space-operator","title":"Convert Depth To Screen Space Operator","text":"<p>The <code>ConvertDepthToScreenSpaceOp</code> operator remaps the depth buffer from Clara Viz to an OpenXR specific range. The depth buffer is converted in place.</p>","tags":["Convert","Depth","Screen"]},{"location":"operators/convert_depth/#holoscanopenxrconvertdepthtoscreenspaceop","title":"<code>holoscan::openxr::ConvertDepthToScreenSpaceOp</code>","text":"<p>Converts a depth buffer from linear world units to screen space ([0,1])</p>","tags":["Convert","Depth","Screen"]},{"location":"operators/convert_depth/#inputs","title":"Inputs","text":"<ul> <li><code>depth_buffer_in</code>: input depth buffer to be remapped</li> <li>type: <code>holoscan::gxf::VideoBuffer</code></li> <li><code>depth_range</code>: Allocator used to allocate the volume data</li> <li>type: <code>nvidia::gxf::Vector2f</code></li> </ul>","tags":["Convert","Depth","Screen"]},{"location":"operators/convert_depth/#outputs","title":"Outputs","text":"<ul> <li><code>depth_buffer_out</code>: output depth buffer </li> <li>type: <code>holoscan::gxf::Entity</code></li> </ul>","tags":["Convert","Depth","Screen"]},{"location":"operators/cvcuda_holoscan_interop/","title":"cvcuda_holoscan_interop","text":"","tags":["CV-CUDA","Computer Vision","CV"]},{"location":"operators/cvcuda_holoscan_interop/#cvcuda-holoscan-interoperability-operators","title":"CVCUDA Holoscan Interoperability Operators","text":"<p>This directory contains two operators to enable interoperability between the CVCUDA and Holoscan tensors: <code>holoscan::ops::CvCudaToHoloscan</code> and <code>holoscan::ops::HoloscanToCvCuda</code>.</p>","tags":["CV-CUDA","Computer Vision","CV"]},{"location":"operators/cvcuda_holoscan_interop/#holoscanopscvcudatoholoscan","title":"<code>holoscan::ops::CvCudaToHoloscan</code>","text":"<p>Operator class to convert a <code>nvcv::Tensor</code> to a <code>holoscan::Tensor</code>.</p>","tags":["CV-CUDA","Computer Vision","CV"]},{"location":"operators/cvcuda_holoscan_interop/#inputs","title":"Inputs","text":"<ul> <li><code>input</code>: a CV-CUDA tensor</li> <li>type: <code>nvcv::Tensor</code></li> </ul>","tags":["CV-CUDA","Computer Vision","CV"]},{"location":"operators/cvcuda_holoscan_interop/#outputs","title":"Outputs","text":"<ul> <li><code>output</code>: a Holoscan tensor as <code>holoscan::Tensor</code> in <code>holoscan::TensorMap</code></li> <li>type: <code>holoscan::TensorMap</code></li> </ul>","tags":["CV-CUDA","Computer Vision","CV"]},{"location":"operators/cvcuda_holoscan_interop/#holoscanopsholoscantocvcuda","title":"<code>holoscan::ops::HoloscanToCvCuda</code>","text":"","tags":["CV-CUDA","Computer Vision","CV"]},{"location":"operators/cvcuda_holoscan_interop/#inputs_1","title":"Inputs","text":"<ul> <li><code>input</code>: a <code>gxf::Entity</code> containing a Holoscan tensor as <code>holoscan::Tensor</code></li> <li>type: <code>gxf::Entity</code></li> </ul>","tags":["CV-CUDA","Computer Vision","CV"]},{"location":"operators/cvcuda_holoscan_interop/#outputs_1","title":"Outputs","text":"<ul> <li><code>output</code>: a CV-CUDA tensor</li> <li>type: <code>nvcv::Tensor</code></li> </ul>","tags":["CV-CUDA","Computer Vision","CV"]},{"location":"operators/data_writer/","title":"data_writer","text":"","tags":["SDR","DSP"]},{"location":"operators/data_writer/#data-writer-operator","title":"Data Writer Operator","text":"","tags":["SDR","DSP"]},{"location":"operators/data_writer/#overview","title":"Overview","text":"<p>Writes binary data from its input to an output file. This operator is intened for use as a debugging aid.</p>","tags":["SDR","DSP"]},{"location":"operators/data_writer/#description","title":"Description","text":"<p>The data writer operator takes in a <code>std::tuple&lt;tensor_t&lt;complex, 2&gt;, cuda_stream_t&gt;</code>, copies the data to a host tensor, then writes the data out to a binary file.</p> <p>The file path is determined based on input metadata with the following keys:</p> <ol> <li><code>channel_number</code> (default <code>0</code>)</li> <li><code>bandwidth_hz</code> (default <code>0.0</code>)</li> <li><code>rf_ref_freq_hz</code> (default <code>0.0</code>)</li> </ol> <p>With this, it creates: <code>data_writer_out_ch{channel_number}_bw{bandwidth_hz}_freq{rf_ref_freq_hz}.dat</code>.</p>","tags":["SDR","DSP"]},{"location":"operators/data_writer/#requirements","title":"Requirements","text":"<ul> <li>MatX</li> </ul>","tags":["SDR","DSP"]},{"location":"operators/data_writer/#configuration","title":"Configuration","text":"<p>The data writer operator takes in a few parameters:</p> <pre><code>data_writer:\n  burst_size: 1280\n  num_bursts: 625\n</code></pre> <ul> <li><code>burst_size</code>: Number of samples contained in each burst</li> <li><code>num_bursts</code>: Number of bursts to process at once</li> </ul>","tags":["SDR","DSP"]},{"location":"operators/data_writer/#example-usage","title":"Example Usage","text":"<p>For an example of how to use this operator, see the <code>psd_pipeline</code> application.</p> <p>Usually, you'd just want to write one burst of data to a file. To do that, you could use a <code>CountCondition</code> to limit the number of times this operator runs:</p> <pre><code>auto dataWriterOp = make_operator&lt;ops::DataWriter&gt;(\n    \"dataWriterOp\",\n    make_condition&lt;CountCondition&gt;(1));\n</code></pre>","tags":["SDR","DSP"]},{"location":"operators/dds_shapes_subscriber/","title":"dds_shapes","text":"","tags":["DDS","RTI Connext","Shapes"]},{"location":"operators/dds_shapes_subscriber/#dds-shape-subscriber-operator","title":"DDS Shape Subscriber Operator","text":"<p>The DDS Shape Subscriber Operator subscribes to and reads from the <code>Square</code>, <code>Circle</code>, and <code>Triangle</code> shape topics as used by the RTI Shapes Demo. It will then translate the received shape data to an internal <code>Shape</code> datatype for output to downstream operators.</p> <p>This operator requires an installation of RTI Connext to provide access to the DDS domain, as specified by the OMG Data-Distribution Service</p>","tags":["DDS","RTI Connext","Shapes"]},{"location":"operators/dds_shapes_subscriber/#holoscanopsddsshapessubscriberop","title":"<code>holoscan::ops::DDSShapesSubscriberOp</code>","text":"<p>Operator class for the DDS Shapes Subscriber.</p> <p>This operator also inherits the parameters from DDSOperatorBase.</p>","tags":["DDS","RTI Connext","Shapes"]},{"location":"operators/dds_shapes_subscriber/#parameters","title":"Parameters","text":"<ul> <li><code>reader_qos</code>: The name of the QoS profile to use for the DDS DataReader</li> <li>type: <code>std::string</code></li> </ul>","tags":["DDS","RTI Connext","Shapes"]},{"location":"operators/dds_shapes_subscriber/#outputs","title":"Outputs","text":"<ul> <li><code>output</code>: Output shapes, translated from those read from DDS</li> <li>type: <code>holoscan::ops::DDSShapesSubscriberOp::Shape</code></li> </ul>","tags":["DDS","RTI Connext","Shapes"]},{"location":"operators/dds_video_publisher/","title":"dds_video_publisher","text":"","tags":["DDS","RTI Connext","Video"]},{"location":"operators/dds_video_publisher/#dds-video-operators","title":"DDS Video Operators","text":"<p>The DDS Video Operators allow applications to read or write video buffers to a DDS databus, enabling communication with other applications via the VideoFrame DDS topic.</p> <p>This operator requires an installation of RTI Connext to provide access to the DDS domain, as specified by the OMG Data-Distribution Service</p>","tags":["DDS","RTI Connext","Video"]},{"location":"operators/dds_video_publisher/#holoscanopsddsvideopublisherop","title":"<code>holoscan::ops::DDSVideoPublisherOp</code>","text":"<p>Operator class for the DDS video publisher. This operator accepts <code>VideoBuffer</code> objects as input and publishes each buffer to DDS as a VideoFrame.</p> <p>This operator also inherits the parameters from DDSOperatorBase.</p>","tags":["DDS","RTI Connext","Video"]},{"location":"operators/dds_video_publisher/#parameters","title":"Parameters","text":"<ul> <li><code>writer_qos</code>: The name of the QoS profile to use for the DDS DataWriter</li> <li>type: <code>std::string</code></li> <li><code>stream_id</code>: The ID to use for the video stream</li> <li>type: <code>uint32_t</code></li> </ul>","tags":["DDS","RTI Connext","Video"]},{"location":"operators/dds_video_publisher/#inputs","title":"Inputs","text":"<ul> <li><code>input</code>: Input video buffer</li> <li>type: <code>nvidia::gxf::VideoBuffer</code></li> </ul>","tags":["DDS","RTI Connext","Video"]},{"location":"operators/dds_video_publisher/#holoscanopsddsvideosubscriberop","title":"<code>holoscan::ops::DDSVideoSubscriberOp</code>","text":"<p>Operator class for the DDS video subscriber. This operator reads from the VideoFrame DDS topic and outputs each received frame as <code>VideoBuffer</code> objects.</p> <p>This operator also inherits the parameters from DDSOperatorBase.</p>","tags":["DDS","RTI Connext","Video"]},{"location":"operators/dds_video_publisher/#parameters_1","title":"Parameters","text":"<ul> <li><code>reader_qos</code>: The name of the QoS profile to use for the DDS DataReader</li> <li>type: <code>std::string</code></li> <li><code>stream_id</code>: The ID of the video stream to filter for</li> <li>type: <code>uint32_t</code></li> <li><code>allocator</code>: Allocator used to allocate the output data</li> <li>type: <code>std::shared_ptr&lt;Allocator&gt;</code></li> </ul>","tags":["DDS","RTI Connext","Video"]},{"location":"operators/dds_video_publisher/#outputs","title":"Outputs","text":"<ul> <li><code>output</code>: Output video buffer</li> <li>type: <code>nvidia::gxf::VideoBuffer</code></li> </ul>","tags":["DDS","RTI Connext","Video"]},{"location":"operators/dds_video_subscriber/","title":"dds_video_subscriber","text":"","tags":["DDS","RTI Connext","Video"]},{"location":"operators/dds_video_subscriber/#dds-video-operators","title":"DDS Video Operators","text":"<p>The DDS Video Operators allow applications to read or write video buffers to a DDS databus, enabling communication with other applications via the VideoFrame DDS topic.</p> <p>This operator requires an installation of RTI Connext to provide access to the DDS domain, as specified by the OMG Data-Distribution Service</p>","tags":["DDS","RTI Connext","Video"]},{"location":"operators/dds_video_subscriber/#holoscanopsddsvideopublisherop","title":"<code>holoscan::ops::DDSVideoPublisherOp</code>","text":"<p>Operator class for the DDS video publisher. This operator accepts <code>VideoBuffer</code> objects as input and publishes each buffer to DDS as a VideoFrame.</p> <p>This operator also inherits the parameters from DDSOperatorBase.</p>","tags":["DDS","RTI Connext","Video"]},{"location":"operators/dds_video_subscriber/#parameters","title":"Parameters","text":"<ul> <li><code>writer_qos</code>: The name of the QoS profile to use for the DDS DataWriter</li> <li>type: <code>std::string</code></li> <li><code>stream_id</code>: The ID to use for the video stream</li> <li>type: <code>uint32_t</code></li> </ul>","tags":["DDS","RTI Connext","Video"]},{"location":"operators/dds_video_subscriber/#inputs","title":"Inputs","text":"<ul> <li><code>input</code>: Input video buffer</li> <li>type: <code>nvidia::gxf::VideoBuffer</code></li> </ul>","tags":["DDS","RTI Connext","Video"]},{"location":"operators/dds_video_subscriber/#holoscanopsddsvideosubscriberop","title":"<code>holoscan::ops::DDSVideoSubscriberOp</code>","text":"<p>Operator class for the DDS video subscriber. This operator reads from the VideoFrame DDS topic and outputs each received frame as <code>VideoBuffer</code> objects.</p> <p>This operator also inherits the parameters from DDSOperatorBase.</p>","tags":["DDS","RTI Connext","Video"]},{"location":"operators/dds_video_subscriber/#parameters_1","title":"Parameters","text":"<ul> <li><code>reader_qos</code>: The name of the QoS profile to use for the DDS DataReader</li> <li>type: <code>std::string</code></li> <li><code>stream_id</code>: The ID of the video stream to filter for</li> <li>type: <code>uint32_t</code></li> <li><code>allocator</code>: Allocator used to allocate the output data</li> <li>type: <code>std::shared_ptr&lt;Allocator&gt;</code></li> </ul>","tags":["DDS","RTI Connext","Video"]},{"location":"operators/dds_video_subscriber/#outputs","title":"Outputs","text":"<ul> <li><code>output</code>: Output video buffer</li> <li>type: <code>nvidia::gxf::VideoBuffer</code></li> </ul>","tags":["DDS","RTI Connext","Video"]},{"location":"operators/deltacast_videomaster/","title":"VideoMaster GXF Operator","text":"<p> Authors: Laurent Radoux (Deltacast) Supported platforms: amd64, arm64 Last modified: November 2, 2023 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 2 - Trusted</p> <p>This library contains two operators: - videomaster_source: get signal from capture card - videomaster_transmitter: generate signal</p> <p>These operators wrap the GXF extension to provide support for VideoMaster SDK.</p>","tags":["Camera","Deltacast"]},{"location":"operators/deltacast_videomaster/#requirements","title":"Requirements","text":"<p>This operator requires the VideoMaster SDK from Deltacast.</p>","tags":["Camera","Deltacast"]},{"location":"operators/deltacast_videomaster/#building-the-operator","title":"Building the operator","text":"<p>As part of Holohub, running CMake on Holohub and point to Holoscan SDK install tree.</p> <p>The path to the VideoMaster SDK is also mandatory and can be given through the VideoMaster_SDK_DIR parameter.</p>","tags":["Camera","Deltacast"]},{"location":"operators/ehr_query_llm/","title":"HoloHub Operators","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: January 22, 2025 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This directory contains operators for the Holoscan Platform.</p>","tags":["ehr_query_llm","FHIR service client","FHIR"]},{"location":"operators/ehr_query_llm/#contributing-to-holohub-operators","title":"Contributing to HoloHub Operators","text":"<p>Please review the CONTRIBUTING.md file guidelines to contribute operators.</p>","tags":["ehr_query_llm","FHIR service client","FHIR"]},{"location":"operators/emergent_source/","title":"HoloHub Operators","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: December 6, 2024 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This directory contains operators for the Holoscan Platform.</p>","tags":["Camera","Emergent"]},{"location":"operators/emergent_source/#contributing-to-holohub-operators","title":"Contributing to HoloHub Operators","text":"<p>Please review the CONTRIBUTING.md file guidelines to contribute operators.</p>","tags":["Camera","Emergent"]},{"location":"operators/end_frame/","title":"XrEndFrameOp","text":"","tags":["XR","XRFrame"]},{"location":"operators/end_frame/#xrendframe-operator","title":"XrEndFrame Operator","text":"<p>The <code>XrEndFrameOp</code> operator completes the rendering of a single OpenXR frame by passing populated color and depth buffer for the left and right eye to the OpenXR device. Note that a single connection <code>xr_frame</code> from <code>XrBeginFrameOp</code> to <code>XrEndFrameOp</code> is required to synchronize the OpenXR calls issued by the two operators.</p>","tags":["XR","XRFrame"]},{"location":"operators/end_frame/#holoscanopenxrxrendframeop","title":"<code>holoscan::openxr::XrEndFrameOp</code>","text":"","tags":["XR","XRFrame"]},{"location":"operators/end_frame/#parameters","title":"Parameters","text":"<ul> <li><code>XrSession</code>: A class that encapsulates a single OpenXR session</li> <li>type: <code>holoscan::openxr::XrSession</code></li> </ul>","tags":["XR","XRFrame"]},{"location":"operators/end_frame/#inputs","title":"Inputs","text":"<p>Render buffers populated by application - <code>color_buffer</code>: color buffer   - type: <code>holoscan::gxf::VideoBuffer</code> - <code>depth_buffer</code>: depth buffer   - type: <code>holoscan::gxf::VideoBuffer</code></p> <p>OpenXR synchronization - <code>XrFrame</code>: connection to synchronize <code>XrBeginFrameOp</code> and <code>XrEndFrameOp</code>   - type: <code>XrFrame</code></p> <p>Note:</p> <ul> <li><code>XrCudaInteropSwapchain</code>: A class that encapsulates the Vulkan buffers of the OpenXR runtime and compatible CUDA buffer to provide interoperability between ClaraViz and OpenXR</li> </ul>","tags":["XR","XRFrame"]},{"location":"operators/fft/","title":"fft","text":"","tags":["SDR","DSP"]},{"location":"operators/fft/#fft-operator","title":"FFT Operator","text":"","tags":["SDR","DSP"]},{"location":"operators/fft/#overview","title":"Overview","text":"<p>A thin wrapper over the MatX <code>fft()</code> executor.</p>","tags":["SDR","DSP"]},{"location":"operators/fft/#description","title":"Description","text":"<p>The FFT operator takes in a tensor of complex float data, performs an FFT, and emits the resultant tensor.</p>","tags":["SDR","DSP"]},{"location":"operators/fft/#requirements","title":"Requirements","text":"<ul> <li>MatX (dependency)</li> </ul>","tags":["SDR","DSP"]},{"location":"operators/fft/#example-usage","title":"Example Usage","text":"<p>For an example of how to use this operator, see the <code>psd_pipeline</code> application.</p>","tags":["SDR","DSP"]},{"location":"operators/fft/#multiple-channels","title":"Multiple Channels","text":"<p>The zero-indexed <code>channel_number</code> key will be looked up in <code>metadata()</code> on each <code>compute()</code> run. If no value is found, the default channel number is <code>0</code>.</p>","tags":["SDR","DSP"]},{"location":"operators/fft/#configuration","title":"Configuration","text":"<p>The FFT operator takes in a few parameters:</p> <pre><code>fft:\n  burst_size: 1280\n  num_bursts: 625\n  num_channels: 1\n  spectrum_type: 1\n  averaging_type: 1\n  window_time: 0\n  window_type: 0\n  transform_points: 1280\n  window_points: 1280\n  resolution: 6250\n  span: 8000000\n  weighting_factor: 0\n  f1_index: -640\n  f2_index: 639\n  window_time_delta: 0\n</code></pre> <p>The only parameters that actually impacts FFT computation at this point are the <code>burst_size</code> and <code>num_bursts</code> params. The rest of the parameters are simply passed along in the metadata.</p> <ul> <li><code>burst_size</code>: Number of samples to process in each burst</li> <li><code>num_bursts</code>: Number of bursts to process at once</li> <li><code>num_channels</code>: Number of channels for which to allocate memory</li> <li><code>spectrum_type</code>: VITA 49.2 spectrum type to pass along in metadata</li> <li><code>spectrum_type</code>: VITA 49.2 spectrum type to pass along in metadata</li> <li><code>averaging_type</code>: VITA 49.2 averaging type to pass along in metadata</li> <li><code>window_time</code>: VITA 49.2 window time to pass along in metadata</li> <li><code>window_type</code>: VITA 49.2 window type to pass along in metadata</li> <li><code>transform_points</code>: Number of FFT points to take and VITA 49.2 transform points to pass along in metadata</li> <li><code>window_points</code>: VITA 49.2 window points to pass along in metadata</li> <li><code>resolution</code>: VITA 49.2 resolution to pass along in metadata</li> <li><code>span</code>: VITA 49.2 span to pass along in metadata</li> <li><code>weighting_factor</code>: VITA 49.2 weighting factor to pass along in metadata</li> <li><code>f1_index</code>: VITA 49.2 F1 index to pass along in metadata</li> <li><code>f2_index</code>: VITA 49.2 F2 index to pass along in metadata</li> <li><code>window_time_delta</code>: VITA 49.2 window time delta to pass along in metadata</li> </ul>","tags":["SDR","DSP"]},{"location":"operators/grpc_operators/","title":"Holohub gRPC Plugins for Holoscan SDK","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: March 7, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.6.0 Tested Holoscan SDK versions: 2.6.0 Contribution metric: Level 1 - Highly Reliable</p>","tags":["grpc","visualization","tool tracking"]},{"location":"operators/grpc_operators/#overview","title":"Overview","text":"<p>This directory contains the Holohub gRPC plugins for Holoscan SDK, including:</p> <ul> <li><code>client</code>: gRPC client and Holoscan Operators for sending, receiving, and streaming data to a gRPC server.</li> <li><code>server</code>: gRPC server and Holoscan Operators for handling requests from a gRPC client and transmitting data back to the client.</li> <li><code>protos</code>: Protocol buffers definitions of Holoscan SDK.</li> <li><code>common</code>: Tensor &lt;-&gt; protobuf converters and Holoscan Resources to handle incoming and outgoing data.</li> </ul> <p>Please refer to the gRPC h.264 Endoscopy Tool Tracking application for additional details.</p>","tags":["grpc","visualization","tool tracking"]},{"location":"operators/high_rate_psd/","title":"high_rate_psd","text":"","tags":["SDR","DSP"]},{"location":"operators/high_rate_psd/#high-rate-psd-operator","title":"High Rate PSD Operator","text":"","tags":["SDR","DSP"]},{"location":"operators/high_rate_psd/#overview","title":"Overview","text":"<p>A thin wrapper over the MatX <code>abs2()</code> executor.</p>","tags":["SDR","DSP"]},{"location":"operators/high_rate_psd/#description","title":"Description","text":"<p>The high rate PSD operator... - takes in a tensor of complex float data, - performs a squared absolute value operation on the tensor: real(t)^2 + imag(t)^2, - divides by the number of input elements - emits the resultant tensor</p>","tags":["SDR","DSP"]},{"location":"operators/high_rate_psd/#requirements","title":"Requirements","text":"<ul> <li>MatX (dependency)</li> </ul>","tags":["SDR","DSP"]},{"location":"operators/high_rate_psd/#example-usage","title":"Example Usage","text":"<p>For an example of how to use this operator, see the <code>psd_pipeline</code> application.</p>","tags":["SDR","DSP"]},{"location":"operators/high_rate_psd/#multiple-channels","title":"Multiple Channels","text":"<p>The zero-indexed <code>channel_number</code> key will be looked up in <code>metadata()</code> on each <code>compute()</code> run. If no value is found, the default channel number is <code>0</code>.</p>","tags":["SDR","DSP"]},{"location":"operators/high_rate_psd/#configuration","title":"Configuration","text":"<p>The operator only takes the following parameters:</p> <pre><code>high_rate_psd:\n  burst_size: 1280\n  num_bursts: 625\n  num_channels: 1\n</code></pre> <ul> <li><code>burst_size</code>: Number of samples to process in each burst</li> <li><code>num_bursts</code>: Number of bursts to process at once</li> <li><code>num_channels</code>: Number of channels for which to allocate memory</li> </ul>","tags":["SDR","DSP"]},{"location":"operators/low_rate_psd/","title":"low_rate_psd","text":"","tags":["SDR","DSP"]},{"location":"operators/low_rate_psd/#low-rate-psd-operator","title":"Low Rate PSD Operator","text":"","tags":["SDR","DSP"]},{"location":"operators/low_rate_psd/#overview","title":"Overview","text":"<p>PSD accumulator/averager.</p>","tags":["SDR","DSP"]},{"location":"operators/low_rate_psd/#description","title":"Description","text":"<p>The low rate PSD operator... - takes in <code>num_averages</code> tensors of float data, - takes an average of all the accumulated tensors, - performs a 10 * log10() operation on the average, - clamps data to 8-bit integer boundaries, - casts to signed 8-bit integers, - emits the resultant tensor</p>","tags":["SDR","DSP"]},{"location":"operators/low_rate_psd/#requirements","title":"Requirements","text":"<ul> <li>MatX (dependency)</li> </ul>","tags":["SDR","DSP"]},{"location":"operators/low_rate_psd/#example-usage","title":"Example Usage","text":"<p>For an example of how to use this operator, see the <code>psd_pipeline</code> application.</p>","tags":["SDR","DSP"]},{"location":"operators/low_rate_psd/#multiple-channels","title":"Multiple Channels","text":"<p>The zero-indexed <code>channel_number</code> key will be looked up in <code>metadata()</code> on each <code>compute()</code> run. If no value is found, the default channel number is <code>0</code>.</p>","tags":["SDR","DSP"]},{"location":"operators/low_rate_psd/#configuration","title":"Configuration","text":"<p>The low rate PSD operator takes two parameters:</p> <pre><code>low_rate_psd:\n  burst_size: 1280\n  num_averages: 625\n  num_channels: 1\n</code></pre> <ul> <li><code>burst_size</code>: Number of samples to process on each invocation of <code>compute()</code></li> <li><code>num_channels</code>: Number of channels for which to allocate memory</li> <li><code>num_averages</code>: How many PSDs to accumulate before averaging and emitting.</li> </ul>","tags":["SDR","DSP"]},{"location":"operators/lstm_tensor_rt_inference/","title":"lstm_tensor_rt_inference","text":"","tags":["LSTM","TensorRT"]},{"location":"operators/lstm_tensor_rt_inference/#custom-lstm-inference","title":"Custom LSTM Inference","text":"<p>The <code>lstm_tensor_rt_inference</code> extension provides LSTM (Long-Short Term Memory) stateful inference module using TensorRT.</p>","tags":["LSTM","TensorRT"]},{"location":"operators/lstm_tensor_rt_inference/#nvidiaholoscanlstm_tensor_rt_inferencetensorrtinference","title":"<code>nvidia::holoscan::lstm_tensor_rt_inference::TensorRtInference</code>","text":"<p>Codelet, taking input tensors and feeding them into TensorRT for LSTM inference.</p> <p>This implementation is based on <code>nvidia::gxf::TensorRtInference</code>. <code>input_state_tensor_names</code> and <code>output_state_tensor_names</code> parameters are added to specify tensor names for states in LSTM model.</p>","tags":["LSTM","TensorRT"]},{"location":"operators/lstm_tensor_rt_inference/#parameters","title":"Parameters","text":"<ul> <li><code>model_file_path</code>: Path to ONNX model to be loaded</li> <li>type: <code>std::string</code></li> <li><code>engine_cache_dir</code>: Path to a directory containing cached generated engines to be serialized and loaded from</li> <li>type: <code>std::string</code></li> <li><code>plugins_lib_namespace</code>: Namespace used to register all the plugins in this library (default: <code>\"\"</code>)</li> <li>type: <code>std::string</code></li> <li><code>force_engine_update</code>: Always update engine regard less of existing engine file. Such conversion may take minutes (default: <code>false</code>)</li> <li>type: <code>bool</code></li> <li><code>input_tensor_names</code>: Names of input tensors in the order to be fed into the model</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>input_state_tensor_names</code>: Names of input state tensors that are used internally by TensorRT</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>input_binding_names</code>: Names of input bindings as in the model in the same order of what is provided in input_tensor_names</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>output_tensor_names</code>: Names of output tensors in the order to be retrieved from the model</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>input_state_tensor_names</code>: Names of output state tensors that are used internally by TensorRT</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>output_binding_names</code>: Names of output bindings in the model in the same order of of what is provided in output_tensor_names</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>pool</code>: Allocator instance for output tensors</li> <li>type: <code>gxf::Handle&lt;gxf::Allocator&gt;</code></li> <li><code>cuda_stream_pool</code>: Instance of gxf::CudaStreamPool to allocate CUDA stream</li> <li>type: <code>gxf::Handle&lt;gxf::CudaStreamPool&gt;</code></li> <li><code>max_workspace_size</code>: Size of working space in bytes (default: <code>67108864l</code> (64MB))</li> <li>type: <code>int64_t</code></li> <li><code>dla_core</code>: DLA Core to use. Fallback to GPU is always enabled. Default to use GPU only (<code>optional</code>)</li> <li>type: <code>int32_t</code></li> <li><code>max_batch_size</code>: Maximum possible batch size in case the first dimension is dynamic and used as batch size (default: <code>1</code>)</li> <li>type: <code>int32_t</code></li> <li><code>enable_fp16_</code>: Enable inference with FP16 and FP32 fallback (default: <code>false</code>)</li> <li>type: <code>bool</code></li> <li><code>verbose</code>: Enable verbose logging on console (default: <code>false</code>)</li> <li>type: <code>bool</code></li> <li><code>relaxed_dimension_check</code>: Ignore dimensions of 1 for input tensor dimension check (default: <code>true</code>)</li> <li>type: <code>bool</code></li> <li><code>rx</code>: List of receivers to take input tensors</li> <li>type: <code>std::vector&lt;gxf::Handle&lt;gxf::Receiver&gt;&gt;</code></li> <li><code>tx</code>: Transmitter to publish output tensors</li> <li>type: <code>gxf::Handle&lt;gxf::Transmitter&gt;</code></li> </ul>","tags":["LSTM","TensorRT"]},{"location":"operators/medical_imaging/","title":"medical_imaging","text":"","tags":["Medeical Imaging","DICOM","AI","Inference","MONAI","STL"]},{"location":"operators/medical_imaging/#medical-imaging-processing-and-inference-operators","title":"Medical Imaging Processing and Inference Operators","text":"<p>This set of operators accelerate the development of medical imaging AI inference application with DICOM imaging network integration by providing the following,</p> <ul> <li>application classes to automate the inference with MONAI Bundle as well as normal TorchScript models</li> <li>classes to load supported AI model from files to detected devices, GPU or CPU</li> <li>classes to parse runtime options and well-known environment variables</li> <li>DICOM study parsing and selection classes, as well as DICOM instance to volume image conversion</li> <li>DICOM instance writers to encapsulate AI inference results in these DICOM OID,</li> <li>DICOM Segmentation</li> <li>DICOM Basic Text Structured Report</li> <li>DICOM Encapsulated PDF</li> <li>Surface mesh generation and storage in STL format</li> <li>Visualization with Clara-Viz integration, as needed</li> </ul>","tags":["Medeical Imaging","DICOM","AI","Inference","MONAI","STL"]},{"location":"operators/medical_imaging/#requirements","title":"Requirements","text":"<p>This set of operators depends on Holoscan SDK Python package, as well as directly on the following, - highdicom - monai - nibabel - numpy - numpy-stl - Pillow - pydicom - PyPDF2 - scikit-image - SimpleITK - torch - trimesh - typeguard</p>","tags":["Medeical Imaging","DICOM","AI","Inference","MONAI","STL"]},{"location":"operators/medical_imaging/#notices","title":"Notices","text":"<p>Many of this set of operators are <code>Derivative Works</code> of MONAI Deploy App SDK under its Apache-2.0 license, and Nvidia employees have been the main contributors to MONAI Deploy App SDK.</p> <p>The dependency packages' licences can be viewed at their respective links as shown in the above section.</p>","tags":["Medeical Imaging","DICOM","AI","Inference","MONAI","STL"]},{"location":"operators/mesh_to_usd/","title":"HoloHub Operators","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 29, 2024 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 2 - Trusted</p> <p>This directory contains operators for the Holoscan Platform.</p>","tags":["mesh","OpenUSD","STL"]},{"location":"operators/mesh_to_usd/#contributing-to-holohub-operators","title":"Contributing to HoloHub Operators","text":"<p>Please review the CONTRIBUTING.md file guidelines to contribute operators.</p>","tags":["mesh","OpenUSD","STL"]},{"location":"operators/npp_filter/","title":"npp_filter","text":"","tags":["Filter","NPP","Gauss","Sobel"]},{"location":"operators/npp_filter/#npp-filter","title":"NPP Filter","text":"<p>The <code>npp_filter</code> operator uses NPP to apply a filters to a Tensor or VideBuffer.</p>","tags":["Filter","NPP","Gauss","Sobel"]},{"location":"operators/npp_filter/#holoscanopsnppfilter","title":"<code>holoscan::ops::NppFilter</code>","text":"<p>Operator class to apply a filter of the NPP library to a Tensor or VideBuffer.</p>","tags":["Filter","NPP","Gauss","Sobel"]},{"location":"operators/npp_filter/#parameters","title":"Parameters","text":"<ul> <li><code>filter</code>: Name of the filter to apply (supported Gauss, SobelHoriz, SobelVert)</li> <li>type: <code>std::string</code></li> <li><code>mask_size</code>: Filter mask size (supported values 3, 5, 7, 9, 11, 13)</li> <li>type: <code>uint32_t</code></li> <li><code>allocator</code>: Allocator used to allocate the output data</li> <li>type: <code>std::shared_ptr&lt;Allocator&gt;</code></li> </ul>","tags":["Filter","NPP","Gauss","Sobel"]},{"location":"operators/npp_filter/#inputs","title":"Inputs","text":"<ul> <li><code>input</code>: Input frame data</li> <li>type: <code>nvidia::gxf::Tensor</code> or <code>nvidia::gxf::VideoBuffer</code></li> </ul>","tags":["Filter","NPP","Gauss","Sobel"]},{"location":"operators/npp_filter/#outputs","title":"Outputs","text":"<ul> <li><code>input</code>: Output frame data</li> <li>type: <code>nvidia::gxf::Tensor</code> or <code>nvidia::gxf::VideoBuffer</code></li> </ul>","tags":["Filter","NPP","Gauss","Sobel"]},{"location":"operators/openigtlink/","title":"openigtlink","text":"","tags":["Streaming","Ethernet","3DSlicer"]},{"location":"operators/openigtlink/#openigtlink-operator","title":"OpenIGTLink operator","text":"<p>The <code>openigtlink</code> operator provides a way to send and receive imaging data using the OpenIGTLink library. The <code>openigtlink</code> operator contains separate operators for transmit and receive. Users may choose one or the other, or use both in applications requiring bidirectional traffic.</p> <p>The <code>openigtlink</code> operators use class names: <code>OpenIGTLinkTxOp</code> and <code>OpenIGTLinkRxOp</code></p>","tags":["Streaming","Ethernet","3DSlicer"]},{"location":"operators/openigtlink/#nvidiaholoscanopenigtlink","title":"<code>nvidia::holoscan::openigtlink</code>","text":"<p>Operator class to send and transmit data using the OpenIGTLink protocol.</p>","tags":["Streaming","Ethernet","3DSlicer"]},{"location":"operators/openigtlink/#receiver-configuration-parameters","title":"Receiver Configuration Parameters","text":"<ul> <li><code>port</code>: Port number of server</li> <li>type: <code>integer</code></li> <li><code>out_tensor_name</code>: Name of output tensor</li> <li>type: <code>string</code></li> <li><code>flip_width_height</code>: Flip width and height (necessary for receiving from 3D Slicer)</li> <li>type: <code>bool</code></li> </ul>","tags":["Streaming","Ethernet","3DSlicer"]},{"location":"operators/openigtlink/#transmitter-configuration-parameters","title":"Transmitter Configuration Parameters","text":"<ul> <li><code>device_name</code>: OpenIGTLink device name</li> <li>type: <code>string</code></li> <li><code>input_names</code>: Names of input messages</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>host_name</code>: Host name</li> <li>type: <code>string</code></li> <li><code>port</code>: Port number of server</li> <li>type: <code>integer</code></li> </ul>","tags":["Streaming","Ethernet","3DSlicer"]},{"location":"operators/prohawk_video_processing/","title":"HoloHub Operators","text":"<p> Authors: Tim Wooldridge (Prohawk Technology Group) Supported platforms: arm64 Last modified: April 10, 2024 Language: C++ Latest version: 1.0.0 Minimum Holoscan SDK version: 0.5.1 Tested Holoscan SDK versions: 0.5.1, 0.6.0 Contribution metric: Level 4 - Experimental</p> <p>This directory contains operators for the Holoscan Platform.</p>","tags":["Video processing","Prohawk"]},{"location":"operators/prohawk_video_processing/#contributing-to-holohub-operators","title":"Contributing to HoloHub Operators","text":"<p>Please review the CONTRIBUTING.md file guidelines to contribute operators.</p>","tags":["Video processing","Prohawk"]},{"location":"operators/qt_video/","title":"qt_video","text":"","tags":["Qt","QML","QtQuick","Video","UI","Userinterface","Interactive"]},{"location":"operators/qt_video/#qt-video-operator","title":"Qt Video Operator","text":"<p>The <code>qt_video</code> operator is used to display a video in a QtQuick application.</p> <p>For more information on how to use this operator in an application see Qt video replayer example.</p>","tags":["Qt","QML","QtQuick","Video","UI","Userinterface","Interactive"]},{"location":"operators/qt_video/#holoscanopsqtvideoop","title":"<code>holoscan::ops::QtVideoOp</code>","text":"<p>Operator class.</p>","tags":["Qt","QML","QtQuick","Video","UI","Userinterface","Interactive"]},{"location":"operators/qt_video/#parameters","title":"Parameters","text":"<ul> <li><code>QtHoloscanVideo</code>: Instance of QtHoloscanVideo to be used<ul> <li>type: `QtHoloscanVideo</li> </ul> </li> </ul>","tags":["Qt","QML","QtQuick","Video","UI","Userinterface","Interactive"]},{"location":"operators/qt_video/#inputs","title":"Inputs","text":"<ul> <li><code>input</code>: Input frame data</li> <li>type: <code>nvidia::gxf::Tensor</code> or <code>nvidia::gxf::VideoBuffer</code></li> </ul>","tags":["Qt","QML","QtQuick","Video","UI","Userinterface","Interactive"]},{"location":"operators/realsense_camera/","title":"Intel RealSense Camera Operator","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: June 28, 2024 Language: C++ Latest version: 0.1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 1 - Highly Reliable</p>"},{"location":"operators/realsense_camera/#overview","title":"Overview","text":"<p>Captures frames from an Intel RealSense camera.</p>"},{"location":"operators/tensor_to_video_buffer/","title":"tensor_to_video_buffer","text":"","tags":["Tensor","Video"]},{"location":"operators/tensor_to_video_buffer/#gxf-tensor-to-videobuffer-converter","title":"GXF Tensor to VideoBuffer Converter","text":"<p>The <code>tensor_to_video_buffer</code> converts GXF Tensor to VideoBuffer.</p>","tags":["Tensor","Video"]},{"location":"operators/tensor_to_video_buffer/#holoscanopstensortovideobufferop","title":"<code>holoscan::ops::TensorToVideoBufferOp</code>","text":"<p>Operator class to convert GXF Tensor to VideoBuffer. This operator is required for data transfer  between Holoscan operators that output GXF Tensor and the other Holoscan Wrapper Operators that understand only VideoBuffer. It receives GXF Tensor as input and outputs GXF VideoBuffer created from it.</p>","tags":["Tensor","Video"]},{"location":"operators/tensor_to_video_buffer/#parameters","title":"Parameters","text":"<ul> <li><code>data_in</code>: Data in GXF Tensor format</li> <li>type: <code>holoscan::IOSpec*</code></li> <li><code>data_out</code>: Data in GXF VideoBuffer format</li> <li>type: <code>holoscan::IOSpec*</code></li> <li><code>in_tensor_name</code>: Name of the input tensor</li> <li>type: <code>std::string</code></li> <li><code>video_format</code>: The video format, supported values: \"yuv420\", \"rgb\"</li> <li>type: <code>std::string</code></li> </ul>","tags":["Tensor","Video"]},{"location":"operators/tool_tracking_postprocessor/","title":"tool_tracking_postprocessor","text":"","tags":["visualization","tool tracking"]},{"location":"operators/tool_tracking_postprocessor/#tool-tracking-postprocessor","title":"Tool tracking postprocessor","text":"<p>The <code>tool_tracking_postprocessor</code> extension provides a codelet that converts inference output of <code>lstm_tensor_rt_inference</code> used in the endoscopy tool tracking pipeline to be consumed by the <code>holoviz</code> codelet.</p>","tags":["visualization","tool tracking"]},{"location":"operators/tool_tracking_postprocessor/#nvidiaholoscantool_tracking_postprocessor","title":"<code>nvidia::holoscan::tool_tracking_postprocessor</code>","text":"<p>Tool tracking postprocessor codelet</p>","tags":["visualization","tool tracking"]},{"location":"operators/tool_tracking_postprocessor/#parameters","title":"Parameters","text":"<ul> <li><code>in</code>: Input channel, type <code>gxf::Tensor</code></li> <li>type: <code>gxf::Handle&lt;gxf::Receiver&gt;</code></li> <li><code>out</code>: Output channel, type <code>gxf::Tensor</code></li> <li>type: <code>gxf::Handle&lt;gxf::Transmitter&gt;</code></li> <li><code>min_prob</code>: Minimum probability, (default: 0.5)</li> <li>type: <code>float</code></li> <li><code>overlay_img_colors</code>: Color of the image overlays, a list of RGB values with components between 0 and 1, (default: 12 qualitative classes color scheme from colorbrewer2)</li> <li>type: <code>std::vector&lt;std::vector&lt;float&gt;&gt;</code></li> <li><code>device_allocator</code>: Output Allocator</li> <li>type: <code>gxf::Handle&lt;gxf::Allocator&gt;</code></li> <li><code>cuda_stream_pool</code>: Instance of gxf::CudaStreamPool</li> <li>type: <code>gxf::Handle&lt;gxf::CudaStreamPool&gt;</code></li> </ul>","tags":["visualization","tool tracking"]},{"location":"operators/unzip/","title":"unzip","text":"","tags":["unzip"]},{"location":"operators/unzip/#unzip-operator","title":"Unzip Operator","text":"<p>The <code>unzip</code> operator decompresses a zip compressed file into its original contents.</p>","tags":["unzip"]},{"location":"operators/unzip/#parameters","title":"Parameters","text":"<ul> <li><code>filter</code>: File filter for the decompressed files to be copied to the <code>output_path</code></li> <li>type: <code>str</code></li> <li><code>output_path</code>: The directory where the unzipped files will be stored.</li> <li>type: <code>str</code></li> </ul>","tags":["unzip"]},{"location":"operators/video_encoder_request/","title":"video_encoder_request","text":"","tags":["video","encoder","request"]},{"location":"operators/video_encoder_request/#video-encoder-request","title":"Video Encoder Request","text":"<p>The <code>video_encoder_request</code> handles the input for encoding YUV frames to H264 bit stream.</p>","tags":["video","encoder","request"]},{"location":"operators/video_encoder_request/#holoscanopsvideoencoderop","title":"<code>holoscan::ops::VideoEncoderOp</code>","text":"<p>Operator class to handle the input for encoding YUV frames to H264 bit stream.</p> <p>This implementation is based on <code>nvidia::gxf::VideoEncoderRequest</code>.</p>","tags":["video","encoder","request"]},{"location":"operators/video_encoder_request/#parameters","title":"Parameters","text":"<ul> <li><code>input_frame</code>: Receiver to get the input frame.</li> <li>type: <code>holoscan::IOSpec*</code></li> <li><code>videoencoder_context</code>: Encoder context Handle.</li> <li>type: <code>std::shared_ptr&lt;holoscan::ops::VideoEncoderContext&gt;</code></li> <li><code>inbuf_storage_type</code>: Input Buffer storage type, 0: kHost, 1: kDevice. Default: 1</li> <li>type: <code>uint32_t</code></li> <li><code>codec</code>: Video codec to use,  0: H264, only H264 supported. Default: 0.</li> <li>type: <code>int32_t</code></li> <li><code>input_height</code>: Input frame height.</li> <li>type: <code>uint32_t</code></li> <li><code>input_width</code>: Input image width.</li> <li>type: <code>uint32_t</code></li> <li><code>input_format</code>: Input color format, nv12,nv24,yuv420planar. Default: nv12.</li> <li>type: <code>nvidia::gxf::EncoderInputFormat</code></li> <li><code>profile</code>: Encode profile, 0: Baseline Profile, 1: Main, 2: High. Default: 2.</li> <li>type: <code>int32_t</code></li> <li><code>bitrate</code>: Bitrate of the encoded stream, in bits per second. Default: 20000000.</li> <li>type: <code>int32_t</code></li> <li><code>framerate</code>: Frame Rate, frames per second. Default: 30.</li> <li>type: <code>int32_t</code></li> <li><code>qp</code>: Encoder constant QP value. Default: 20.</li> <li>type: <code>uint32_t</code></li> <li><code>level</code>: Video H264 level. Maximum data rate and resolution, select from 0 to 14. Default: 14.</li> <li>type: <code>int32_t</code></li> <li><code>iframe_interval</code>: I Frame Interval, interval between two I frames. Default: 30.</li> <li>type: <code>int32_t</code></li> <li><code>rate_control_mode</code>: Rate control mode, 0: CQP[RC off], 1: CBR, 2: VBR. Default: 1.</li> <li>type: <code>int32_t</code></li> <li><code>config</code>: Preset of parameters, select from pframe_cqp, iframe_cqp, custom. Default: custom.</li> <li>type: <code>nvidia::gxf::EncoderConfig</code></li> </ul>","tags":["video","encoder","request"]},{"location":"operators/visualizer_icardio/","title":"visualizer_icardio","text":"","tags":["video"]},{"location":"operators/visualizer_icardio/#visualizer-icardio","title":"Visualizer iCardio","text":"<p>The <code>visualizer_icardio</code> extension generates the visualization components from the processed results of the plax chamber model.</p>","tags":["video"]},{"location":"operators/visualizer_icardio/#nvidiaholoscanmultiaivisualizericardio","title":"<code>nvidia::holoscan::multiai::VisualizerICardio</code>","text":"<p>Visualizer iCardio extension ingests the processed results of the plax chamber model and generates the key points, the key areas and the lines that are transmitted to the HoloViz codelet.</p>","tags":["video"]},{"location":"operators/visualizer_icardio/#parameters","title":"Parameters","text":"<ul> <li><code>in_tensor_names_</code>: Input tensor names</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>out_tensor_names_</code>: Output tensor names</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>allocator_</code>: Memory allocator</li> <li>type: <code>gxf::Handle&lt;gxf::Allocator&gt;</code></li> <li><code>receivers_</code>: Vector of input receivers. Multiple receivers supported.</li> <li>type: <code>HoloInfer::GXFReceivers</code></li> <li><code>transmitter_</code>: Output transmitter. Single transmitter supported.</li> <li>type: <code>HoloInfer::GXFTransmitters</code></li> </ul>","tags":["video"]},{"location":"operators/vita49_psd_packetizer/","title":"vita49_psd_packetizer","text":"","tags":["SDR","Radio","DSP"]},{"location":"operators/vita49_psd_packetizer/#vita49-psd-packetizer","title":"VITA49 PSD Packetizer","text":"","tags":["SDR","Radio","DSP"]},{"location":"operators/vita49_psd_packetizer/#overview","title":"Overview","text":"<p>Generate VITA 49.2 spectral data packets from incoming data.</p>","tags":["SDR","Radio","DSP"]},{"location":"operators/vita49_psd_packetizer/#description","title":"Description","text":"<p>This operator will take in PSD data computed by upstream operators and format it into VITA 49.2 Spectral Data packets.</p> <p>After creating the VRT packets, it will send the packets to the configured UDP IP/port.</p> <p>[!NOTE] This operator pulls in projects that contain LGPL-3.0 code.</p>","tags":["SDR","Radio","DSP"]},{"location":"operators/vita49_psd_packetizer/#requirements","title":"Requirements","text":"<ul> <li>MatX (dependency)</li> <li>vrtgen (dependency, LGPL-3.0)</li> <li>vita49-psd (dependency, LGPL-3.0)</li> </ul>","tags":["SDR","Radio","DSP"]},{"location":"operators/vita49_psd_packetizer/#multiple-channels","title":"Multiple Channels","text":"<p>If multiple channels are configured, the packetizer will use the base port in the configuration and add the channel index. So, with <code>base_dest_port: 4991</code>, channel <code>0</code> would send data to <code>4991</code>, but channel <code>1</code> would send data to <code>4992</code>.</p> <p>The zero-indexed <code>channel_number</code> key will be looked up in <code>metadata()</code> on each <code>compute()</code> run. If no value is found, the default channel number is <code>0</code>.</p>","tags":["SDR","Radio","DSP"]},{"location":"operators/vita49_psd_packetizer/#example-usage","title":"Example Usage","text":"<p>For an example of how to use this operator, see the <code>psd_pipeline</code> application.</p>","tags":["SDR","Radio","DSP"]},{"location":"operators/vita49_psd_packetizer/#configuration","title":"Configuration","text":"<p>The packetizer takes the following parameters:</p> <pre><code>vita49_psd_packetizer:\n  burst_size: 1280\n  num_channels: 1\n  dest_host: 127.0.0.1\n  base_dest_port: 4991\n  manufacturer_oui: 0xFF5646\n  device_code: 0x80\n</code></pre> <ul> <li><code>burst_size</code>: Number of samples to process in each burst</li> <li><code>num_channels</code>: Number of channels for which to allocate memory</li> <li><code>dest_host</code>: Destination host</li> <li><code>base_dest_port</code>: Base destination UDP port</li> <li><code>manufacturer_oui</code>: Manufacturer identifier to embed in the context packets</li> <li><code>device_code</code>: Device code to embed in the context packets</li> </ul>","tags":["SDR","Radio","DSP"]},{"location":"operators/volume_loader/","title":"Volume Loader","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 18, 2025 Language: C++ Latest version: 1.0.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0, 1.0.3, 2.0.0, 2.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>volume_loader</code> operator reads 3D volumes from the specified input file.</p>","tags":["Volume","Load","MHD","NIFTI","NRRD"]},{"location":"operators/volume_loader/#supported-formats","title":"Supported Formats","text":"<p>The operator supports these medical volume file formats: * MHD (MetaImage)   * Detached-header format only (<code>.mhd</code> + <code>.raw</code>) * NIFTI * NRRD (Nearly Raw Raster Data)   * Attached-header format (<code>.nrrd</code>)   * Detached-header format (<code>.nhdr</code> + <code>.raw</code>)</p> <p>You must convert your data to one of these formats to load it with <code>VolumeLoaderOp</code>. Some third party open source tools for volume file format conversion include: - Command Line Tools   - the Insight Toolkit (ITK) (PyPI, Image IO Examples)   - SimpleITK (PyPI)   - Utah NRRD Utilities (unu) - GUI Applications   - 3D Slicer   - ImageJ</p>","tags":["Volume","Load","MHD","NIFTI","NRRD"]},{"location":"operators/volume_loader/#api","title":"API","text":"","tags":["Volume","Load","MHD","NIFTI","NRRD"]},{"location":"operators/volume_loader/#holoscanopsvolumeloaderop","title":"<code>holoscan::ops::VolumeLoaderOp</code>","text":"<p>Operator class to read a volume.</p>","tags":["Volume","Load","MHD","NIFTI","NRRD"]},{"location":"operators/volume_loader/#parameters","title":"Parameters","text":"<ul> <li><code>file_name</code>: Volume data file name</li> <li>type: <code>std::string</code></li> <li><code>allocator</code>: Allocator used to allocate the volume data</li> <li>type: <code>std::shared_ptr&lt;Allocator&gt;</code></li> </ul>","tags":["Volume","Load","MHD","NIFTI","NRRD"]},{"location":"operators/volume_loader/#outputs","title":"Outputs","text":"<ul> <li><code>volume</code>: Output volume data</li> <li>type: <code>nvidia::gxf::Tensor</code></li> <li><code>spacing</code>: Physical size of each volume element</li> <li>type: <code>std::array&lt;float, 3&gt;</code></li> <li><code>permute_axis</code>: Volume axis permutation of data space to world space, e.g. if x and y of a volume is swapped this is {1, 0, 2}</li> <li>type: <code>std::array&lt;uint32_t, 3&gt;</code></li> <li><code>flip_axes</code>: Volume axis flipping from data space to world space, e.g. if x is flipped this is {true, false, false}</li> <li>type: <code>std::array&lt;bool, 3&gt;</code></li> <li><code>extent</code>: Physical size of the the volume in world space</li> <li>type: <code>std::array&lt;float, 3&gt;</code></li> </ul>","tags":["Volume","Load","MHD","NIFTI","NRRD"]},{"location":"operators/volume_renderer/","title":"Volume Renderer","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 18, 2025 Language: C++ Latest version: 2.0.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3, 2.0.0, 2.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>volume_renderer</code> operator renders a volume using ClaraViz (https://github.com/NVIDIA/clara-viz).</p>","tags":["Volume","Render","ClaraViz"]},{"location":"operators/volume_renderer/#holoscanopsvolumerenderer","title":"<code>holoscan::ops::VolumeRenderer</code>","text":"<p>Operator class to render a volume.</p>","tags":["Volume","Render","ClaraViz"]},{"location":"operators/volume_renderer/#parameters","title":"Parameters","text":"<ul> <li><code>config_file</code>: Config file path. The content of the file is passed to <code>clara::viz::JsonInterface::SetSettings()</code> at initialization time. See Configuration for details.</li> <li>type: <code>std::string</code></li> <li><code>allocator</code>: Allocator used to allocate render buffer outputs when no pre-allocated color or depth buffer is passed to <code>color_buffer_in</code> or <code>depth_buffer_in</code>. Allocator needs to be capable to allocate device memory.</li> <li>type: <code>std::shared_ptr&lt;Allocator&gt;</code></li> <li><code>alloc_width</code>: Width of the render buffer to allocate when no pre-allocated buffers are provided.</li> <li>type: <code>uint32_t</code></li> <li><code>alloc_height</code>: Height of the render buffer to allocate when no pre-allocated buffers are provided.</li> <li>type: <code>uint32_t</code></li> </ul>","tags":["Volume","Render","ClaraViz"]},{"location":"operators/volume_renderer/#inputs","title":"Inputs","text":"<p>All inputs are optional.</p> <ul> <li><code>volume_pose</code>: Transform the volume.</li> <li>type: <code>nvidia::gxf::Pose3D</code></li> <li><code>crop_box</code>: Volume crop box. Each <code>nvidia::gxf::Vector2f</code> contains the min and max values in range <code>[0, 1]</code> of the x, y and z axes of the volume.</li> <li>type: <code>std::array&lt;nvidia::gxf::Vector2f, 3&gt;</code></li> <li><code>depth_range</code>: The distance to the near and far frustum planes.</li> <li>type: <code>nvidia::gxf::Vector2f</code></li> <li><code>left_camera_pose</code>: Camera pose for the left camera when rendering in stereo mode.</li> <li>type: <code>nvidia::gxf::Pose3D</code></li> <li><code>right_camera_pose</code>: Camera pose for the right camera when rendering in stereo mode.</li> <li>type: <code>nvidia::gxf::Pose3D</code></li> <li><code>left_camera_model</code>: Camera model for the left camera when rendering in stereo mode.</li> <li>type: <code>nvidia::gxf::CameraModel</code></li> <li><code>right_camera_model</code>: Camera model for the right camera when rendering in stereo mode.</li> <li>type: <code>nvidia::gxf::CameraModel</code></li> <li><code>camera_pose</code>: Camera pose when not rendering in stereo mode.</li> <li>type: <code>std::array&lt;float, 16&gt;</code> or <code>nvidia::gxf::Pose3D</code></li> <li><code>color_buffer_in</code>: Buffer to store the rendered color data to, format needs to be 8 bit per component RGBA and buffer needs to be in device memory.</li> <li>type: <code>nvidia::gxf::VideoBuffer</code></li> <li><code>depth_buffer_in</code>: Buffer to store the rendered depth data to, format needs to be 32 bit float single component buffer needs to be in device memory.</li> <li>type: <code>nvidia::gxf::VideoBuffer</code></li> <li><code>density_volume</code>: Density volume data. Needs to be a 3D single component array. Supported data types are signed|unsigned 8|16|32 bit integer and 32 bit floating point.</li> <li>type: <code>nvidia::gxf::Tensor</code></li> <li><code>density_spacing</code>: Physical size of each density volume element.</li> <li>type: <code>std::array&lt;float, 3&gt;</code></li> <li><code>density_permute_axis</code>: Density volume axis permutation of data space to world space, e.g. if x and y of a volume is swapped this is {1, 0, 2}.</li> <li>type: <code>std::array&lt;uint32_t, 3&gt;</code></li> <li><code>density_flip_axes</code>: Density volume axis flipping from data space to world space, e.g. if x is flipped this is {true, false, false}.</li> <li>type: <code>std::array&lt;bool, 3&gt;</code></li> <li><code>mask_volume</code>: Mask volume data. Needs to be a 3D single component array. Supported data types are signed|unsigned 8|16|32 bit integer and 32 bit floating point.</li> <li>type: <code>nvidia::gxf::Tensor</code></li> <li><code>mask_spacing</code>: Physical size of each mask volume element.</li> <li>type: <code>std::array&lt;float, 3&gt;</code></li> <li><code>mask_permute_axis</code>: Mask volume axis permutation of data space to world space, e.g. if x and y of a volume is swapped this is {1, 0, 2}.</li> <li>type: <code>std::array&lt;uint32_t, 3&gt;</code></li> <li><code>mask_flip_axes</code>: Mask volume axis flipping from data space to world space, e.g. if x is flipped this is {true, false, false}.</li> <li>type: <code>std::array&lt;bool, 3&gt;</code></li> </ul>","tags":["Volume","Render","ClaraViz"]},{"location":"operators/volume_renderer/#outputs","title":"Outputs","text":"<ul> <li><code>color_buffer_out</code>: Buffer with rendered color data, format is 8 bit per component RGBA and buffer is in device memory.</li> <li>type: <code>nvidia::gxf::VideoBuffer</code></li> <li><code>depth_buffer_out</code>: Buffer with rendered depth data, format is be 32 bit float single component and buffer is in device memory.</li> <li>type: <code>nvidia::gxf::VideoBuffer</code></li> </ul>","tags":["Volume","Render","ClaraViz"]},{"location":"operators/volume_renderer/#configuration","title":"Configuration","text":"<p>The renderer accepts a ClaraViz JSON configuration file at startup to control rendering settings, including - camera parameters; - transfer functions; - lighting; - and more.</p> <p>The ClaraViz JSON configuration file exists in addition to and independent of a Holoscan SDK <code>.yaml</code> configuration file that may be passed to an application.</p> <p>See the <code>volume_rendering_xr</code> application for a sample configuration file. Visit the ClaraViz <code>render_server.proto</code> gRPC specification for insight into configuration file field values.</p>","tags":["Volume","Render","ClaraViz"]},{"location":"operators/volume_renderer/#transfer-functions","title":"Transfer Functions","text":"<p>Usually CT datasets are stored in Hounsfield scale. The renderer maps these values in Hounsfield scale to opacity in order to display the volume. These mappings are called transfer functions. Multiple transfer functions for different input value regions can be defined. Transfer functions also include material properties like diffuse, specular and emissive color. The range of input values the transfer function is applied to is in normalized input range <code>[0, 1]</code>.</p>","tags":["Volume","Render","ClaraViz"]},{"location":"operators/volume_renderer/#segmentation-mask-volume","title":"Segmentation (Mask) Volume","text":"<p>Different organs often have very similar Hounsfield values, therefore additionally an segmentation volume is supported. The segmentation volume contains an integer index for each element of the volume. Transfer functions can be restricted on specific segmentation indices. The segmentation volume can, for example, be generated using TotalSegmentator.</p>","tags":["Volume","Render","ClaraViz"]},{"location":"operators/volume_renderer/#creating-a-configuration-file","title":"Creating a Configuration File","text":"<p>Configuration files are typically specific to a given dataset or modality, and are tailored to a specific voxel intensity range. It may be necessary to create a new configuration file when working with a new dataset in order to produce a meaningful rendering.</p> <p>There are two options to create a configuration file for a new dataset: - Copy from an existing configuration file as a reference and modify parameters manually. An example configuration file is available in the <code>volume_rendering_xr</code> application config folder. - Use <code>VolumeRendererOp</code> to deduce settings for the input dataset. Follow these steps:   1. Use the HoloHub <code>volume_rendering</code> app or a similar application that will load an input dataset and pass it to <code>VolumeRendererOp</code>.   2. Configure application settings via a Holoscan SDK YAML file or command line settings to run with the following values:     - Set the <code>VolumeRendererOp</code> <code>config_file</code> parameter to an empty string to indicate no default config file is present;     - Set the <code>VolumeRendererOp</code> <code>write_config_file</code> parameter to the desired output JSON configuration filepath.   3. Run the application with the desired input volume. The operator will deduce settings and write out the JSON file to reuse on subsequent runs via the <code>config_file</code> parameter.</p>","tags":["Volume","Render","ClaraViz"]},{"location":"operators/vtk_renderer/","title":"vtk_renderer","text":"","tags":["visualization","tool tracking"]},{"location":"operators/vtk_renderer/#vtk_renderer-operator","title":"vtk_renderer operator","text":"<p>The <code>vtk_renderer</code> extension takes the output of the source video player and the output of the <code>tool_tracking_postprocessor</code> operator and renders the video stream with an overlay annotation of the label using VTK.</p> <p>VTK can be a useful addition to holohub stack since VTK is a industry leading visualization toolkit. It is important to mention that this renderer operator needs to copy the input from device memory to host due to limitations of VTK. While this is a strong limitation for VTK we believe that VTK can still be a good addition and VTK is an evolving project. Perhaps in the future we could overcome this limitation.</p>","tags":["visualization","tool tracking"]},{"location":"operators/vtk_renderer/#how-to-build-this-operator","title":"How to build this operator","text":"<p>Build the HoloHub container as described at the root README.md</p> <p>You need to create a docker image which includes VTK with the provided <code>vtk.Dockerfile</code>:</p> <pre><code>docker build -t vtk:latest -f vtk.Dockerfile .\n</code></pre> <p>Then, you can build the tool tracking application with the provided <code>Dockerfile</code>:</p> <pre><code>./dev_container launch --img vtk:latest\n</code></pre> <p>Inside the container you can build the holohub application with:</p> <pre><code>./run build &lt;application&gt; --with vtk_renderer\n</code></pre>","tags":["visualization","tool tracking"]},{"location":"operators/vtk_renderer/#parameters","title":"Parameters","text":"<ul> <li><code>videostream</code>: Input channel for the videostream, type <code>gxf::Tensor</code></li> <li>type: <code>gxf::Handle&lt;gxf::Receiver&gt;</code></li> <li><code>annotations</code>: Input channel for the annotations, type <code>gxf::Tensor</code></li> <li>type: <code>gxf::Handle&lt;gxf::Receiver&gt;</code></li> <li><code>window_name</code>: Compositor window name.</li> <li>type: <code>std::string</code></li> <li><code>width</code>: width of the renderer window.</li> <li>type: <code>int</code></li> <li><code>height</code>: height of the renderer window.</li> <li>type: <code>int</code></li> <li><code>labels</code>: labels to be displayed on the rendered image.</li> <li>type: <code>std::vector&lt;std::string&gt;&gt;</code></li> </ul>","tags":["visualization","tool tracking"]},{"location":"operators/webrtc_client/","title":"webrtc_client","text":"","tags":["WebRTC","Client","Browser","Video"]},{"location":"operators/webrtc_client/#webrtc-client-operator","title":"WebRTC Client Operator","text":"<p>The <code>webrtc_client</code> operator receives video frames through a WebRTC connection. The application using this operator needs to call the <code>offer</code> method of the operator when a new WebRTC connection is available.</p>","tags":["WebRTC","Client","Browser","Video"]},{"location":"operators/webrtc_client/#methods","title":"Methods","text":"<ul> <li><code>async def offer(self, sdp, type) -&gt; (local_sdp, local_type)</code>   Start a connection between the local computer and the peer.</li> </ul> <p>Parameters   - sdp peer Session Description Protocol object   - type peer session type</p> <p>Return values   - sdp local Session Description Protocol object   - type local session type</p>","tags":["WebRTC","Client","Browser","Video"]},{"location":"operators/webrtc_client/#outputs","title":"Outputs","text":"<ul> <li><code>output</code>: Tensor with 8 bit per component RGB data</li> <li>type: <code>Tensor</code></li> </ul>","tags":["WebRTC","Client","Browser","Video"]},{"location":"operators/webrtc_server/","title":"webrtc_server","text":"","tags":["WebRTC","Server","Browser","Video"]},{"location":"operators/webrtc_server/#webrtc-server-operator","title":"WebRTC Server Operator","text":"<p>The <code>webrtc_server</code> operator sends video frames through a WebRTC connection. The application using this operator needs to call the <code>offer</code> method of the operator when a new WebRTC connection is available.</p>","tags":["WebRTC","Server","Browser","Video"]},{"location":"operators/webrtc_server/#methods","title":"Methods","text":"<ul> <li><code>async def offer(self, sdp, type) -&gt; (local_sdp, local_type)</code>   Start a connection between the local computer and the peer.</li> </ul> <p>Parameters   - sdp peer Session Description Protocol object   - type peer session type</p> <p>Return values   - sdp local Session Description Protocol object   - type local session type</p>","tags":["WebRTC","Server","Browser","Video"]},{"location":"operators/webrtc_server/#inputs","title":"Inputs","text":"<ul> <li><code>input</code>: Tensor or numpy array with 8 bit per component RGB data</li> <li>type: <code>Tensor</code></li> </ul>","tags":["WebRTC","Server","Browser","Video"]},{"location":"operators/xr_basic_render/","title":"XR Basic Rendering Operator","text":"","tags":["Gaze tracking","OpenXR","XR","XRFrame"]},{"location":"operators/xr_basic_render/#xr-basic-rendering-operator","title":"XR Basic Rendering Operator","text":"<p>The <code>XrBasicRenderOp</code> defines and renders a basic scene to demonstrate OpenXR compatibility. It provides visuals for static primitives, controller tracking, and an ImGui window.</p> <p>See the <code>xr_hello_holoscan</code> application for a complete example demonstrating <code>XrBasicRenderOp</code> in a Holoscan SDK pipeline.</p>","tags":["Gaze tracking","OpenXR","XR","XRFrame"]},{"location":"operators/yuan_qcap/","title":"HoloHub Operators","text":"<p> Authors: David Su (Yuan) Supported platforms: amd64, arm64 Last modified: April 10, 2024 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 2 - Trusted</p> <p>This directory contains operators for the Holoscan Platform.</p>","tags":["Camera","Yuan"]},{"location":"operators/yuan_qcap/#contributing-to-holohub-operators","title":"Contributing to HoloHub Operators","text":"<p>Please review the CONTRIBUTING.md file guidelines to contribute operators.</p>","tags":["Camera","Yuan"]},{"location":"tutorials/","title":"Tutorials","text":"<p>The HoloHub tutorials are an invaluable resource for developers looking to master the Holoscan SDK and build advanced AI sensor processing applications.  This page offers a collection of comprehensive tutorials that guide users through various aspects of the Holoscan platform, from basic setup to advanced workflow optimization.</p> <p>Each tutorial is designed to provide step-by-step instructions, code examples, and best practices, making it easier for developers to integrate Holoscan's powerful features into their projects. Whether you are new to Holoscan or an experienced developer seeking to enhance your skills, these tutorials offer practical insights and hands-on exercises to help you leverage the full potential of the Holoscan platform.</p>"},{"location":"tutorials/cli_debugging/","title":"Interactively Debugging a Holoscan Application","text":"<p> Authors: Tom Birdsong (NVIDIA) Supported platforms: amd64, arm64 Last modified: August 15, 2024 Language: C++ Latest version: 0.1.0 Minimum Holoscan SDK version: 2.2.0 Tested Holoscan SDK versions: 2.2.0, 2.3.0 Contribution metric: Level 1 - Highly Reliable</p> <p>Holoscan SDK is a platform for rapidly developing low-latency AI pipelines. As part of software development we often find it useful to inspect pipeline operations and data contexts during execution. This tutorial walks through a few common scenarios to illustrate how common command line interface tools can be used in debugging an application based on Holoscan SDK.</p>","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#tutorial-sections","title":"Tutorial Sections","text":"<ol> <li>Prerequisites</li> <li>Debugging a C++ or Python application with <code>gdb</code></li> <li>Debugging a Python application with <code>pdb</code></li> <li>Debugging Symbols for Legacy Holoscan SDK</li> <li>Logging</li> </ol>","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#background","title":"Background","text":"","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#what-is-debugging","title":"What is Debugging?","text":"<p>Software debugging is the process of identifying and rectifying issues in software. - Just-in-time debugging lets us pause execution and inspect the state while a program is running.   For a C++ library such as Holoscan SDK, we rely on debugging symbols generated by the compiler   at runtime to map the execution state back to human-readable source code, which lets us understand what the program was doing when paused. Tools such as <code>gdb</code> and <code>Visual Studio Code</code> allow us to manage application   execution by setting breakpoints and watches, as well as inspect the program state such as local variable   values when paused. - Logging is one process through which we record events that occur during real-time execution of   a pipeline without interrupting the flow of execution. Log messages can provide a view of the application   state and flow that is limited to the messages the developer chooses to log. Holoscan SDK supports   logging based on popular libraries such as <code>spdlog</code> and the Python <code>logging</code> module.</p> <p>In general, both logging and just-in-time debugging are useful tools for prototyping and general development, while logging is usually better suited for application deployment. In this guide, we focus on using just-in-time debugging tools to inspect applications built on Holoscan SDK.</p>","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#what-are-debugging-symbols","title":"What are debugging symbols?","text":"<p>Debugging symbols are generated by a C++ compiler at build time to provide additional information for application debugging such as file names and line numbers. Debugging symbols must be present for debugging tools to provide meaningful information to a developer when inspecting an application's execution state.</p> <p>Holoscan SDK uses the following build profiles in its <code>run</code> script: - <code>release</code>: Instructs the C++ compiler to optimize where possible and not generate debugging symbols. We use this build type to create minimum-footprint binaries such as those in the Holoscan SDK Debian package or Python wheel distributions. - <code>rel-debug</code>: Instructs the C++ compiler to optimize where possible and generate debugging symbols. We use this build type to create developer-friendly binaries packaged in the Holoscan SDK NGC containers. Debug symbols may occasionally have inaccuracies due to release optimizations. - <code>debug</code>: Instructs the C++ compiler to minimize optimizations and prioritize debugging symbol accuracy. As of v2.3, we do not release Holoscan SDK builds of this type, but we do provide a <code>run</code> script to help developers generate their own <code>debug</code> builds on demand.</p>","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#what-are-some-common-tools-i-can-use-for-debugging-my-application","title":"What are some common tools I can use for debugging my application?","text":"<p>There are a wide variety of free and/or open source software tools available for general C++ and Python debugging, including: - NVIDIA's debugging solutions, including NVIDIA NSight and CUDA-GDB; - The GNU project DeBugger (GDB); - The built-in Python Debugger (pdb) module; - Microsoft Visual Studio Code, with a wide variety of community extensions.</p> <p>In this tutorial we will focus on the <code>GDB</code> and <code>pdb</code> command line tools. These require minimal setup and can be run via a simple terminal without a dedicated display (\"headless\"). For advanced development we recommend reviewing Visual Studio Code Development Containers with custom launch profiles, with HoloHub support coming soon.</p>","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#references","title":"References","text":"<p>To get started with debugging your Holoscan SDK application, visit the Debugging user guide section for common topics, including: - Generating debug symbols with VSCode - Live debugging for C++ and Python applications - Inspecting application crashes - Profiling and code coverage</p> <p>Visit the Logging user guide section for a thorough overview on how to set up Holoscan SDK logging in your C++ or Python application.</p>","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#prerequisites","title":"Prerequisites","text":"<p>The steps for getting started with <code>gdb</code> depend on how you are consuming Holoscan SDK. - We encourage using Holoscan SDK containers from NGC for development and we take this approach for most of the tutorial. If you are using an NGC container for Holoscan SDK v2.3.0 or later, you already have access   to debug symbols and can get started right away. - If you are using an older Holoscan SDK container from NGC, or if you are consuming Holoscan SDK through another means such as Debian packages, Python wheels, or custom installation, you will need to build Holoscan SDK with debugging symbols in order to step through Holoscan SDK code during debugging. Jump to the Legacy Holoscan SDK section to get started.</p> <p>Review the HoloHub Prerequisites along with the Endoscopy Tool Tracking requirements C++, Python to get started before continuing.</p> <p>If you have previously built the Endoscopy Tool Tracking application, you should clear your build directory before proceeding. <pre><code>./run clear-cache\n</code></pre></p>","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#debugging-a-c-application-with-gdb","title":"Debugging a C++ Application with <code>gdb</code>","text":"","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#background_1","title":"Background","text":"<p>GDB (the GNU project DeBugger) is a widely used tool for headless just-in-time debugging of C++ applications. GDB distributions are available for most Holoscan SDK supported platforms and do not require a display to set up. Refer to the GDB User Manual to get started.</p>","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#getting-started","title":"Getting Started","text":"<p>For this tutorial we will debug the Endoscopy Tool Tracking application. The tutorial <code>debug_gdb.sh</code> script is a self-contained example that will build the C++ application and launch into a <code>gdb</code> debugging session.</p> <p>Run the script to get started: <pre><code>./tutorials/cli_debugging/debug_gdb.sh\n</code></pre></p> <p>The script runs through the following steps: 1. Builds the tutorial container environment with <code>gdb</code> based on the Holoscan SDK NGC container. 2. Builds the Endoscopy Tool Tracking application in the container environment. By default we use the   <code>debug</code> build mode to generate detailed debugging symbols for the Endoscopy Tool Tracking application.   Note that this does not regenerate build symbols for Holoscan SDK, which are already packaged in   Holoscan SDK binaries in <code>rel-debug</code> mode. 3. Launches the Endoscopy Tool Tracking application with <code>gdb</code>. This step prefixes the launch command   given by <code>./run launch endoscopy_tool_tracking</code> to run with <code>gdb</code>. The command sets a few actions for <code>gdb</code> to take on startup:     - Sets a breakpoint in the <code>main</code> function of Endoscopy Tool Tracking;     - Runs the program with custom arguments until the breakpoint is hit;     - Sets a breakpoint in Holoscan SDK's <code>add_flow</code> function.</p> <p>At this point <code>gdb</code> enters an interactive session where we can inspect the Endoscopy Tool Tracking program state and advance execution.</p> <p>GDB can also be used to interactively debug C++ symbols underlying a Holoscan Python pipeline. Run the tutorial <code>debug_gdb.sh</code> script to inspect the Endoscopy Tool Tracking Python application:</p> <pre><code>./tutorials/cli_debugging/debug_gdb.sh debug python\n</code></pre> <p>When the <code>python</code> argument is provided, the script builds the Endoscopy Tool Tracking application with Python bindings and initiates debugging with GDB from the Python script entrypoint. Once symbols are loaded in GDB, we can set breakpoints and inspect the underlying state of Holoscan SDK C++ operator implementations at runtime.</p> <p>From this point we recommend referring to the GDB Manual or online tutorials to get started with interactive debugging commands.</p>","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#frequently-asked-questions-and-troubleshooting","title":"Frequently Asked Questions and Troubleshooting","text":"","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#how-can-i-verify-that-holoscan-sdk-debugging-symbols-have-been-loaded-in-gdb","title":"How can I verify that Holoscan SDK debugging symbols have been loaded in <code>gdb</code>?","text":"<p><code>gdb</code> loads debugging symbols for Holoscan SDK only when the application loads Holoscan SDK binaries. Before that time, we can set breakpoints in Holoscan SDK files, but <code>gdb</code> will not understand them yet.</p> <p>We can use <code>info sharedlibrary</code> to inspect the shared libraries that have been dynamically loaded for execution.</p> <pre><code>(gdb) info sharedlibrary\nFrom                To                  Syms Read   Shared Object Library\n0x00007ffff7fc5090  0x00007ffff7fee315  Yes         /lib64/ld-linux-x86-64.so.2\n0x00007ffff7ca3ba0  0x00007ffff7ec655d  Yes         /opt/nvidia/holoscan/lib/libholoscan_op_aja.so.2\n0x00007ffff7f90ac0  0x00007ffff7faee1f  Yes         /opt/nvidia/holoscan/lib/libholoscan_op_video_stream_replayer.so.2\n0x00007ffff7bd9810  0x00007ffff7bf4e3f  Yes         /opt/nvidia/holoscan/lib/libholoscan_op_video_stream_recorder.so.2\n...\n</code></pre> <p>We can use <code>info sources</code> to inspect the Holoscan SDK symbols are available. Note: Source paths are loaded from Holoscan SDK binaries and respect source file locations at the time the Holoscan SDK distribution was built. These paths may not reflect your filesystem if you have mounted <code>holoscan-sdk</code> somewhere other than <code>/workspace/holoscan-sdk</code>.</p> <pre><code>(gdb) info sources /workspace/holoscan-sdk/src/core\n/workspace/holoscan-sdk/src/core/application.cpp, /workspace/holoscan-sdk/src/core/services/generated/system_resource.grpc.pb.cc,\n/workspace/holoscan-sdk/src/core/services/generated/system_resource.pb.h, /workspace/holoscan-sdk/src/core/services/generated/system_resource.pb.cc,\n/workspace/holoscan-sdk/src/core/services/generated/result.grpc.pb.cc, /workspace/holoscan-sdk/src/core/services/generated/result.pb.h,\n...\n</code></pre>","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#how-can-i-pause-a-running-holoscan-sdk-application-for-debugging","title":"How can I pause a running Holoscan SDK application for debugging?","text":"<p>After launching the application with <code>gdb</code> as done in <code>debug_gdb.sh</code>, use <code>continue</code> to allow the application to run. Then, press <code>Ctrl+C</code> to force the application to pause when you want to enter interactive debugging. Use <code>backtrace</code> to view stack frames at the point where the application paused.</p>","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#how-can-i-manage-breakpoints-to-pause-the-application-sometime-in-the-future","title":"How can I manage breakpoints to pause the application sometime in the future?","text":"<ol> <li>To add a breakpoint <code>in holoscan/operators/format_converter/format_converter</code>: <pre><code>(gdb) break /workspace/holoscan-sdk/src/operators/format_converter/format_converter.cpp:compute\"\n</code></pre></li> <li>To list all current breakpoints: <pre><code>(gdb) info breakpoints\n</code></pre></li> <li>To remove breakpoints: <pre><code>(gdb) delete &lt;number(s) of breakpoint(s)&gt;\n</code></pre></li> </ol>","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#how-can-i-inspect-local-variables","title":"How can I inspect local variables?","text":"<p>Use the <code>info</code> command to inspect the values of local variables.</p> <pre><code>(gdb) info locals\n</code></pre>","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#how-can-i-attach-to-a-running-holoscan-sdk-session","title":"How can I attach to a running Holoscan SDK session?","text":"<p>Do the following to attach to a HoloHub application (C++ or Python):</p> <ol> <li> <p>Launch the container with root permissions and start the process in the background: <pre><code>./dev_container launch --img holohub:debugging --as_root\n\n# Run inside the container\n&gt;&gt;&gt; ./run launch endoscopy_tool_tracking &amp;\n</code></pre></p> </li> <li> <p>Press <code>Ctrl+C</code> to return to your interactive shell</p> </li> <li> <p>Find the process ID (PID) of the running application: <pre><code># Inside the container\n&gt;&gt;&gt; ps -ef | grep endoscopy_tool_tracking\nuser+     292     203 28 13:17 pts/9    00:00:04 /workspace/holohub/build/endoscopy_tool_tracking/applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking --data /workspace/holohub/data/endoscopy\n</code></pre></p> </li> <li> <p>Attach to the running process with <code>gdb -p</code>: <pre><code># Inside the container\n&gt;&gt;&gt; gdb -p 292\n</code></pre></p> </li> </ol> <p>From this point you can use Ctrl+C to pause application execution, then use the interactive GDB console to set breakpoints and inspect application state as usual.</p>","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#i-see-a-gdb-python-exception-in-the-gdb-log","title":"I see a <code>gdb</code> Python Exception in the <code>gdb</code> log.","text":"<p><code>gdb</code> relies on a Python module for operations such as unwinding. If the Python module is not properly referenced in the container, you may see <code>gdb</code> errors appear in the console log such as the following: <pre><code>\"Python Exception &lt;class 'NameError'&gt;: Installation error: gdb._execute_unwinders function is missing\"\n</code></pre> To resolve the error, update your <code>PYTHONPATH</code> variable to include your GDB Python directory: <pre><code>PYTHONPATH=${PYTHONPATH}:/usr/share/gdb/python\n</code></pre></p>","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#debugging-a-python-application-with-pdb","title":"Debugging a Python application with <code>pdb</code>","text":"","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#background_2","title":"Background","text":"<p>The Python Debugger module is a built-in interactive debugging tool for Python programs. Similar to <code>gdb</code>, it supports setting breakpoints for interactive, headless, just-in-time debugging. You can use <code>pdb</code> to debug Holoscan SDK Python programs on any platform supported by Holoscan SDK.</p> <p>Holoscan SDK Python libraries serve as wrappers around Holoscan SDK C++ libraries. While <code>pdb</code> may load C++ symbols, it is not well suited for setting breakpoints or stepping into underlying C++ code. <code>pdb</code> is best suited for debugging Holoscan SDK operators whose implementation lies in a Python-native <code>compute</code> method.</p>","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#prerequisites_1","title":"Prerequisites","text":"<p>The <code>pdb</code> module is built in to modern Python versions. No additional installation is required.</p> <p>We will continue to use the Holoscan SDK v2.3 development container from NGC for this section, which includes pre-installed versions of Python and Holoscan SDK.</p>","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#getting-started_1","title":"Getting Started","text":"<p>We will continue to debug the Endoscopy Tool Tracking application. The tutorial <code>debug_pdb.sh</code> script is a self-contained example that will build the application and launch into a <code>pdb</code> debugging session.</p> <p>Run the script to get started: <pre><code>./tutorials/cli_debugging/debug_pdb.sh\n</code></pre></p> <p>The script runs through the following steps: 1. Builds the HoloHub container environment based on the Holoscan SDK NGC container. 2. Builds the Endoscopy Tool Tracking Python application in the container environment. By default we use the   <code>debug</code> build mode to generate detailed C++ debugging symbols for the Endoscopy Tool Tracking application. 3. Launches the Endoscopy Tool Tracking application with <code>pdb</code>, which is invoked via the Python interpreter: <pre><code>python3 -m pdb &lt;command --args ...&gt;\n</code></pre></p> <p>This command launches the Python version of the Endoscopy Tool Tracking application with a breakpoint set on the very first line. From this point you can set additional breakpoints, inspect application state, and control program execution.</p> <p>For instance, the following snippet sets a breakpoint and then inspects the value of <code>self.source.lower()</code> during pipeline setup in the app <code>compose</code> method: <pre><code>(Pdb) break endoscopy_tool_tracking.py:77\nBreakpoint 1 at /workspace/holohub/applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking.py:77\n(Pdb) continue\n...\n&gt; /workspace/holohub/applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking.py(77)compose()\n-&gt; if self.source.lower() == \"aja\":\n(Pdb) p self.source.lower()\n'replayer'\n(Pdb)\n...\n</code></pre></p> <p>You can also add a breakpoint directly in the <code>.py</code> source code before running a program by adding a <code>breakpoint()</code> statement.</p> <p>From here we recommend referring to <code>pdb</code> documentation for common commands and debugging strategies.</p>","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#debugging-symbols-for-legacy-holoscan-sdk-versions","title":"Debugging Symbols for Legacy Holoscan SDK Versions","text":"<p>Starting from Holoscan SDK v2.3, we package debugging symbols as part of the libraries distributed in NGC Holoscan Containers, with the goal of improving ease of development and debugging. This change comes at a small cost to size and performance of the Holoscan SDK binary distribution. But what about older versions of Holoscan SDK containers?</p> <p>If you are using a legacy Holoscan SDK container (earlier than v2.3) for your development, your container does not come with debugging symbols pre-packaged. However, you can rebuild the Holoscan SDK from its source code to enable interactive debugging. We provide utilities as part of Holoscan SDK open source code to help you generate debugging symbols that will allow you to use tools such as <code>gdb</code> and <code>pdb</code> with legacy Holoscan SDK.</p> <p>The following script provides the necessary steps to rebuild Holoscan SDK version with debugging symbols and then set up for debugging with <code>gdb</code>:</p> <pre><code>./tutorials/cli_debugging/debug_legacy.sh\n</code></pre> <p>The script does the following: 1. Builds the specified Holoscan SDK version with the specified build type in a temporary tutorial folder. Refer to   background discussion for an overview of the different build types. 2. Builds the Endoscopy Tool Tracking application against the custom Holoscan SDK debug build. 3. Runs the Endoscopy Tool Tracking application with <code>gdb</code> for interactive debugging.</p> <p>For convenience, the <code>debug_legacy.sh</code> script mounts your custom Holoscan SDK installation at <code>/opt/nvidia/holoscan</code>, the default library path in the Holoscan SDK container. This effectively hides the Holoscan SDK build otherwise distributed inside the container and instead makes your custom debugging build available to build downstream applications.</p>","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#frequently-asked-questions-and-troubleshooting_1","title":"Frequently Asked Questions and Troubleshooting","text":"","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#how-can-i-use-a-custom-container-path-for-my-holoscan-sdk-debugging-build-other-than-optnvidiaholoscan","title":"How can I use a custom container path for my Holoscan SDK debugging build other than <code>/opt/nvidia/holoscan</code>?","text":"<p>You can choose to mount your custom Holoscan SDK debugging build at another path in the container with the Docker <code>-v</code> option or the HoloHub <code>dev_container</code> script <code>--local_sdk_root</code> or <code>--mount-volume</code> options. If you are mounting your build at a custom path in the Holoscan SDK container for general development, consider the following details when building and debugging: - <code>LD_LIBRARY_PATH</code> is an environment variable with a list of locations to look up for dynamic loading. By default   the Holoscan SDK container sets <code>LD_LIBRARY_PATH</code> to include <code>/opt/nvidia/holoscan</code>, and then the HoloHub <code>run</code> script   sets it again when launching an application. Edit this variable and launch your application directly to load libraries   from your custom mount by default. - <code>RPATH</code> or <code>RUNPATH</code> is an ELF header field that embeds shared library lookup locations in an executable.   HoloHub applications set <code>RPATH</code> to include <code>/opt/nvidia/holoscan</code> by default. Edit the value of <code>CMAKE_INSTALL_RPATH</code> in  <code>CMakeLists.txt</code> to remove the reference to <code>/opt/nvidia/holoscan</code> or reference your preferred mount path.</p>","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#how-can-i-launch-the-tutorial-application","title":"How can I launch the tutorial application?","text":"<p>You can simply re-run the tutorial script to rebuild and relaunch the application:</p> <pre><code>./tutorials/cli_debugging/debug_legacy.sh\n</code></pre> <p>Alternatively, run the following to relaunch the application in the debugging container without rebuilding: <pre><code># Find the custom Holoscan SDK debugging build\nINSTALL_DIR=$(realpath $(find ./tutorials/cli_debugging/tmp -type d -name \"install-*\"))\n\n# Launch the debugging container\n./dev_container launch --docker_opts \"-v $INSTALL_DIR:/opt/nvidia/holoscan --security-opt seccomp=unconfined\" --img holohub:debugging\n\n# Inside the container\n&gt;&gt;&gt; gdb -q \\\n    -ex \"break main\" \\\n    -ex \"run --data /workspace/holohub/data/endoscopy\" \\\n    -ex \"break /workspace/holoscan-sdk/src/core/application.cpp:add_flow\" \\\n    /workspace/holohub/build/endoscopy_tool_tracking/applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking\n</code></pre></p> <p>Refer to the <code>debug_legacy.sh</code> script for more details.</p>","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#logging","title":"Logging","text":"<p>Just-in-time debugging is not well suited to problems that require real-time performance analysis. Logging is usually the better choice to debug performance related issues in your Holoscan application.</p> <p>The Holoscan SDK User Guide Logging section presents a detailed overview of how to get started with logging from your application.</p>","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#logging-from-a-c-application","title":"Logging from a C++ application","text":"<p>C++ applications based on Holoscan should use the <code>HOLOSCAN_LOG_LEVEL</code> environment variable or <code>holoscan::set_log_level</code> function to set the global level of detail to log in the application. You can add inline macros such as <code>HOLOSCAN_LOG_INFO</code> AND <code>HOLOSCAN_LOG_TRACE</code> in your application code to print out log messages at runtime according to the current logging level of detail.</p> <pre><code>export HOLOSCAN_LOG_LEVEL=\"Debug\"\n./run launch endoscopy_tool_tracking\n</code></pre>","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/cli_debugging/#logging-from-a-python-application","title":"Logging from a Python application","text":"<p>Python applications based on Holoscan should use the Python <code>logging</code> module. Holoscan observes the standard logging module interface with statements such as <code>logger.info</code> and <code>logger.debug</code>. Refer to the Python <code>logging</code> module for more information.</p>","tags":["Container","Debugging","Interactive","Logging","Tools"]},{"location":"tutorials/creating-multi-node-applications/","title":"Creating Multi Node Applications","text":"<p> Authors: Jin Li (NVIDIA) Supported platforms: amd64, arm64 Last modified: February 5, 2025 Language: Python Latest version: 0.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 2 - Trusted</p> <p>In this tutorial, we will walk through the process in two scenarios of creating a multi node application from existing applications. We will demonstrate the process with Python applications but it's similar for C++ applications as well.</p> <ol> <li> <p>When we would want to divide an application that was previously running on a single node into two fragments running on two nodes. This corresponds to use cases where we want to separate the compute and visualization workloads onto two different nodes, for example in the case of surgical robotics, the visualization node should be closest to the surgeon. For this purpose we choose the example of the <code>multiai_endoscopy</code> application.</p> </li> <li> <p>When we would want to connect and combine two previously independent applications into one application with two fragments. This corresponds to the use cases where we want to run time-critical task(s) on a node closest to the data stream, and non time-critical task(s) on another node that can have a bit more latency, for example saving the inbody video recording of the surgery to cloud can have a higher latency than the real-time visualization. For this purpose we choose the example of the <code>endoscopy_tool_tracking</code> application and the <code>endoscopy_out_of_body_detection</code> application.</p> </li> </ol> <p>The SDK documentation on Creating a Distributed Application contains the necessary core concepts and description for distributed applications, please familiarize yourself with the documentation before proceeding to this tutorial.</p> <p>Please also see two SDK examples ping_distributed and video_replayer_distributed on simple examples of creating distributed applications.</p> <p>In this tutorial, we will focus on modifying existing applications you have created into distributed applications.</p>","tags":["Distributed","Fragment"]},{"location":"tutorials/creating-multi-node-applications/#scenario-1-divide-an-application-into-two-fragments","title":"Scenario 1 - Divide an application into two fragments","text":"<p>The <code>multiai_endoscopy</code> application has its app graph like below:</p> <p></p> <p>We will divide it into two fragments. The first fragment will include all operators excluding the visualizer and the second fragment will include the visualizer, as illustrated below:</p> <p></p>","tags":["Distributed","Fragment"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-1-extra-imports","title":"Changes in scenario 1 - Extra Imports","text":"<p>To created a distributed application, we will need to import the Fragment object.  </p> <pre><code>from holoscan.core import Fragment\n</code></pre>","tags":["Distributed","Fragment"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-1-changing-the-way-command-line-arguments-are-parsed","title":"Changes in scenario 1 - Changing the way command-line arguments are parsed","text":"<p>As seen in the documentation, it is recommended to parse user-defined arguments from the <code>argv ((C++/Python))</code> method/property of the application. To parse in user-defined command line arguments (such as <code>--data</code>, <code>--source</code>, <code>--labelfile</code> in this app), let's make sure to avoid arguments that are unique to the multi-fragment applications, such as  <code>--driver</code>, <code>--worker</code>, <code>--address</code>, <code>--worker-address</code>, <code>--fragments</code> (see the documentation for more details on using those arguments).  In the non-distributed application, we would have  <pre><code>if __name__ == \"__main__\":\n    parser = ArgumentParser(description=\"Multi-AI Detection Segmentation application.\")\n    parser.add_argument(...) # for the app config yaml file via --config \n    parser.add_argument(...) # for args needed in app init --source --data --labelfile\n    args = parser.parse_args()\n    # logic to define args (config_file, labelfile) needed to pass into application init and config\n    app = MultiAIDetectionSegmentation(source=args.source, data=args.data, labelfile=labelfile)\n    app.config(config_file)\n    app.run()\n</code></pre></p> <p>In the distributed application, we need to make the following changes mainly to <code>parser.parse_args</code>: <code>``python !5,6 if __name__ == \"__main__\":     parser = ArgumentParser(description=\"Multi-AI Detection Segmentation application.\")     parser.add_argument(...) # for the app config yaml file via --config      parser.add_argument(...) # for args needed in app init --source --data --labelfile     apps_argv = Application().argv # difference     args = parser.parse_args(apps_argv[1:]) # difference     # logic to define args (config_file, labelfile) needed to pass into application init and config     app = MultiAIDetectionSegmentation(source=args.source, data=args.data, labelfile=labelfile)     app.config(config_file)     app.run() <pre><code>### Changes in scenario 1 - Changing the application structure\nPreviously, we defined our non distributed applications with the `__init__()` and `compose()` methods. \n```python\nclass MultiAIDetectionSegmentation(Application):\n    def __init__(self, data, source=\"replayer\", labelfile=\"\"):\n        ...\n\n    def compose(self):\n        # define operators and add flow\n        ...\n</code></pre> Now we will define two fragments, and add and connect them in the application's</code>compose()` method: <pre><code>class Fragment1(Fragment):\n    # operators in fragment1 need the objects: sample_data_path, source, label_dict \n    def __init__(self, app, name, sample_data_path, source, label_dict):\n        super().__init__(app, name)\n        ...\n    def compose(self):\n        # define fragment1 operators\n        # add flow\n        ... \n\nclass Fragment2(Fragment):\n    # operators in fragment2 need the object: label_dict \n    def __init__(self, app, name, label_dict):\n        super().__init__(app, name)\n        ...\n    def compose(self):\n        # define the one operator in fragment2 (Holoviz)\n        # add operator\n        # no need to add_flow because there's only one operator in this fragment\n        ... \n\nclass MultiAIDetectionSegmentation(Application):\n    def __init__(self, data, source=\"replayer\", labelfile=\"\"):\n        super().__init__()\n        # set self.name\n        # get self.label_dict from labelfile,self.source, self.sample_data_path\n        ...\n\n    def compose(self):\n        # define the two fragments in this app: fragment1, fragment2\n        # pass in the objects needed to each fragment's operators when defining each fragment\n        # operators in fragment1 need the objects: sample_data_path, source, label_dict \n        fragment1 = Fragment1(self, name=\"fragment1\", source=self.source, sample_data_path=self.sample_data_path, label_dict=self.label_dict)\n        # operators in fragment2 need the object: label_dict \n        fragment2 = Fragment2(self, name=\"fragment2\", label_dict=self.label_dict)\n\n        # Connect the two fragments \n        # There are three connections between fragment1 and fragment2:\n        # (1) from the data source to Holoviz\n        source_output = self.source + \".video_buffer_output\" if self.source.lower() == \"aja\" else self.source + \".output\"\n        self.add_flow(fragment1, fragment2, {(source_output, \"holoviz.receivers\")})\n        # (2) from the detection postprocessor to Holoviz\n        self.add_flow(fragment1, fragment2, {(\"detection_postprocessor.out\" , \"holoviz.receivers\")})\n        # (3) from the segmentation postprocessor to Holoviz\n        self.add_flow(fragment1, fragment2, {(\"segmentation_postprocessor.out_tensor\" , \"holoviz.receivers\")})\n</code></pre></p>","tags":["Distributed","Fragment"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-1-defining-objects-shared-among-fragments","title":"Changes in scenario 1 - Defining objects shared among fragments","text":"<p>When creating your fragments, first make a list of all the objects each fragment's operators will need. If there are objects that are needed across multiple fragments (such as <code>self.label_dict</code> in this case), before passing them into fragments in the application's <code>compose()</code> method, create such objects in the app's <code>__init()__</code> method ideally. In the non-distributed application, in the app's <code>compose()</code> method we define <code>label_dict</code> from <code>self.labelfile</code>, and continue using <code>label_dict</code> while composing the application. In the distributed application, we move the definition of <code>label_dict</code> from <code>self.labelfile</code> into the application's <code>__init()__</code> method, and refer to <code>self.label_dict</code> in the application's <code>compose()</code> method and each fragment's <code>__init__()</code>/<code>compose()</code> methods.</p> <p>Non distributed application: <pre><code>class MultiAIDetectionSegmentation(Application):\n    def compose(self):\n        # construct the labels dictionary if the commandline arg for labelfile isn't empty\n        label_dict = self.get_label_dict(self.labelfile)\n</code></pre></p> <p>Distributed application: <pre><code>class Fragment1(Fragment):\n    def __init__(self, app, name, sample_data_path, source, label_dict):\n        super().__init__(app, name)\n\n        self.source = source\n        self.label_dict = label_dict\n        self.sample_data_path = sample_data_path\n\n    def compose(self):\n        ...\n        # Use self.label_dict\n\nclass Fragment2(Fragment):\n    def __init__(self, app, name, label_dict):\n        super().__init__(app, name)\n\n        self.label_dict = label_dict\n\n    def compose(self):\n        ...\n        # Use self.label_dict\n\nclass MultiAIDetectionSegmentation(Application):\n    def __init__(self, data, source=\"replayer\", labelfile=\"\"):\n        ...\n        self.label_dict = self.get_label_dict(labelfile)\n    def compose(self):\n        fragment1 = Fragment1(self, name=\"fragment1\", source=self.source, sample_data_path=self.sample_data_path, label_dict=self.label_dict)\n        fragment2 = Fragment2(self, name=\"fragment2\", label_dict=self.label_dict)\n        ...\n</code></pre></p>","tags":["Distributed","Fragment"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-1-adding-operators-to-app-graph","title":"Changes in scenario 1 - Adding Operators to App Graph","text":"<p>When composing a non-distributed application, operators are created in the <code>compose()</code> method, then added to the app graph one of two ways:    1. For applications with a single operator (rare), <code>add_operator()</code> should be called.    1. for applications with multiple operators, using <code>add_flow()</code> will take care of adding each operator to the app graph on top of connecting them. This applies to distributed applications as well: when composing multiple fragments, each of them are responsible for adding all their operators to the app graph in their <code>compose()</code> method. Calling <code>add_flow()</code> in the <code>compose()</code> method of the distributed application when connecting fragments together does not add the operators to the app graph. This is often relevant when breaking down a single fragment application in a multi fragments application for distributed use cases, as some fragments might end up owning a single operator, and the absence of <code>add_flow()</code> in that fragment should come with the addition of <code>add_operator()</code> instead.</p> <p>In the non distributed application: <pre><code>class MultiAIDetectionSegmentation(Application):\n    def compose(self):\n        # define operators\n        source = SourceClass(...)\n        detection_preprocessor = FormatConverterOp(...)\n        segmentation_preprocessor = FormatConverterOp(...)\n        multi_ai_inference = InferenceOp(...)\n        detection_postprocessor = DetectionPostprocessorOp(...)\n        segmentation_postprocessor = SegmentationPostprocessorOp(...)\n        holoviz = HolovizOp(...)\n\n        # add flow between operators\n        ...\n</code></pre></p> <p>In a distributed application: ```python #11-17 class Fragment1(Fragment):     def compose(self):         # define operators         source = SourceClass(...)         detection_preprocessor = FormatConverterOp(...)         segmentation_preprocessor = FormatConverterOp(...)         multi_ai_inference = InferenceOp(...)         detection_postprocessor = DetectionPostprocessorOp(...)         segmentation_postprocessor = SegmentationPostprocessorOp(...)</p> <pre><code>    # add flow between operators\n    ...\n</code></pre> <p>class Fragment2(Fragment):     def compose(self):         holoviz = HolovizOp(...)         self.add_operator(holoviz) <pre><code>### Changes in scenario 1 - Shared resources\nIn a non distributed application, there may be some shared resources defined in the application's `compose()` method, such as a `UnboundedAllocator` for various operators. When splitting the application into multiple fragments, remember to create those resources once for each fragment.\n\nNon distributed application:\n```python\nclass MultiAIDetectionSegmentation(Application):\n    def compose(self):\n        pool = UnboundedAllocator(self, name=\"pool\")\n</code></pre></p> <p>Distributed application: <pre><code>class Fragment1(Fragment):\n    def compose(self):\n        pool = UnboundedAllocator(self, name=\"pool\")\n        # ... operator definitions in fragment1 with parameter pool=pool,\n\nclass Fragment2(Fragment):\n    def compose(self):\n        pool = UnboundedAllocator(self, name=\"pool\")\n        # ... operator definitions in fragment2 with parameter pool=pool,\n</code></pre></p>","tags":["Distributed","Fragment"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-1-running-the-application","title":"Changes in scenario 1 - Running the application","text":"<p>Previously for a non distributed application, the command to launch is <code>python3 multi_ai.py --data &lt;DATA_DIR&gt;</code>, now in the distributed application we will have the option to specify a few more things: <pre><code># To run fragment 1 on current node as driver and worker:\npython3 multi_ai.py --data /workspace/holohub/data/ --driver --worker --address &lt;node 1 IP address&gt;:&lt;port&gt; --fragments fragment1\n# To run fragment 2 on current node as worker:\npython3 multi_ai.py --data /workspace/holohub/data/  --worker --address &lt;node 1 IP address&gt;:&lt;port&gt; --fragments fragment2\n</code></pre></p> <p>For more details on the commandline arguments for multi fragment apps, see the documentation.</p> <p>To run the app we create in scenario 1, please see Running the Applications.</p>","tags":["Distributed","Fragment"]},{"location":"tutorials/creating-multi-node-applications/#scenario-2-connect-two-applications-into-a-multi-node-application-with-two-fragments","title":"Scenario 2 - Connect two applications into a multi-node application with two fragments","text":"<p>In this scenario, we will combine the existing application endoscopy_tool_tracking and endoscopy_out_of_body_detection into a distributed application with 2 fragments.</p> <p>Since endoscopy_out_of_body_detection is implemented in C++, we will quickly implement the Python version of the app for our Fragment2.</p> <p>The app graph for <code>endoscopy_tool_tracking</code>:  The app graph for <code>endoscopy_out_of_body_detection</code>:  The distributed app graph we want to create: </p>","tags":["Distributed","Fragment"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-2-extra-imports","title":"Changes in scenario 2  - Extra Imports","text":"<p>Similar to scenario 1, to created a distributed application, we will need to import the Fragment object. When combining two non distributed apps into a multi-fragment application, remember to import all prebuilt Holoscan operators needed in both fragments.</p> <pre><code>from holoscan.core import Fragment\nfrom holoscan.operators import (\n    AJASourceOp,\n    FormatConverterOp,\n    HolovizOp,\n    VideoStreamRecorderOp,\n    VideoStreamReplayerOp,\n    InferenceOp,\n    InferenceProcessorOp\n)\n</code></pre>","tags":["Distributed","Fragment"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-2-changing-the-way-command-line-arguments-are-parsed","title":"Changes in scenario 2 - Changing the way command-line arguments are parsed","text":"<p>Similar to Changing the way command-line arguments are parse in scenario 1, instead of the following for the non distributed app: <pre><code>if __name__ == \"__main__\":\n    ...\n    args = parser.parse_args()\n    ...\n</code></pre> For the distributed app we will have: <pre><code>if __name__ == \"__main__\":\n    ...\n    apps_argv = Application().argv\n    args = parser.parse_args(apps_argv[1:])\n    ...\n</code></pre></p>","tags":["Distributed","Fragment"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-2-modifying-non-distributed-applications-into-a-distributed-application","title":"Changes in scenario 2 - Modifying non-distributed application(s) into a distributed application","text":"<p>In the new distributed app, we will define the app graph in <code>endoscopy_tool_tracking</code> as the new app's fragment 1.  The non distributed app <code>endoscopy_tool_tracking</code> had its <code>__init__()</code> and <code>compose()</code> methods structured like following: <pre><code>class EndoscopyApp(Application):\n    def __init__(self, data, record_type=None, source=\"replayer\"):\n        super().__init__()\n        # set self.name\n        ...\n        # get parameters for app graph composition: \n        # self.record_type, self.source, self.sample_data_path \n        ...\n    def compose(self):\n        # Create operators including:\n        # source operator (video replayer, AJA, Yuan), \n        # format converters, recorder\n        # LSTMTensorRTInferenceOp, ToolTrackingPostprocessorOp\n        # HolovizOp\n        ...\n\n        # Add flow between operators\n        ...\n</code></pre></p> <p>We can define our fragment1 modified from <code>endoscopy_tool_tracking</code> app with the following structure.</p> <pre><code>class Fragment1(Fragment):\n    def __init__(self, app, name, sample_data_path, source, record_type):\n        super().__init__(app, name)\n\n        self.source = source\n        self.sample_data_path = sample_data_path\n        self.record_type = record_type\n    def compose(self):\n        # Create operators including:\n        # source operator (video replayer, AJA, Yuan), \n        # format converters, recorder\n        # LSTMTensorRTInferenceOp, ToolTrackingPostprocessorOp\n        # HolovizOp\n        ...\n\n        # Add flow between operators\n        ...\n</code></pre> <p>We will define the app graph in <code>endoscopy_out_of_body_detection</code> as the new app's fragment 2. <pre><code>class Fragment2(Fragment):\n    def __init__(self, app, name, source, model_path, record_type):\n        super().__init__(app, name)\n\n        self.source = source\n        self.record_type = record_type\n        self.model_path = model_path\n    def compose(self):\n\n        is_aja = self.source.lower() == \"aja\"\n\n        pool = UnboundedAllocator(self, name=\"fragment2_pool\")\n        in_dtype = \"rgba8888\" if is_aja else \"rgb888\"\n\n        out_of_body_preprocessor = FormatConverterOp(\n            self,\n            name=\"out_of_body_preprocessor\",\n            pool=pool,\n            in_dtype=in_dtype,\n            **self.kwargs(\"out_of_body_preprocessor\"),\n        )\n\n        model_path_map = {\"out_of_body\": os.path.join(self.model_path, \"out_of_body_detection.onnx\")}\n        for k, v in model_path_map.items():\n            if not os.path.exists(v):\n                raise RuntimeError(f\"Could not find model file: {v}\")\n        inference_kwargs = self.kwargs(\"out_of_body_inference\")\n        inference_kwargs[\"model_path_map\"] = model_path_map\n        out_of_body_inference = InferenceOp(\n            self,\n            name=\"out_of_body_inference\",\n            allocator=pool,\n            **inference_kwargs,\n        )\n        out_of_body_postprocessor = InferenceProcessorOp(\n            self,\n            name=\"out_of_body_postprocessor\",\n            allocator=pool,\n            disable_transmitter=True,\n            **self.kwargs(\"out_of_body_postprocessor\")\n        )\n\n        # add flow between operators\n        self.add_flow(out_of_body_preprocessor, out_of_body_inference, {(\"\", \"receivers\")})\n        self.add_flow(out_of_body_inference, out_of_body_postprocessor, {(\"transmitter\", \"receivers\")})\n</code></pre> We can then define our distributed application with the following structure. Notice how the objects <code>self.record_type</code>, <code>self.source</code>, and <code>self.sample_data_path</code> are passed into each fragment.</p> <pre><code>class EndoscopyDistributedApp(Application):\n    def __init__(self, data, record_type=None, source=\"replayer\"):\n        super().__init__()\n        # set self.name\n        ...\n        # get parameters for app graph composition: \n        # self.record_type, self.source, self.sample_data_path \n        ...\n    def compose(self):\n        is_aja = self.source.lower() == \"aja\"\n\n        fragment1 = Fragment1(self, \n                            name=\"fragment1\", \n                            source = self.source, \n                            sample_data_path=os.path.join(self.sample_data_path, \"endoscopy\"), \n                            record_type=self.record_type)\n        fragment2 = Fragment2(self, \n                            name=\"fragment2\", \n                            source = self.source, \n                            model_path=os.path.join(self.sample_data_path, \"endoscopy_out_of_body_detection\"),\n                            record_type = self.record_type)\n\n        self.add_flow(fragment1, fragment2, {(\"aja.video_buffer_output\" if is_aja else \"replayer.output\", \"out_of_body_preprocessor\")})\n</code></pre>","tags":["Distributed","Fragment"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-2-combining-objects-needed-by-each-fragments-in-distributed-app-init-time","title":"Changes in scenario 2 - Combining objects needed by each fragments in distributed app init time","text":"<p>Note how the <code>__init__()</code> method in the distributed app structured above is now the place to get parameters for the app graph composition for both fragment1 and fragment 2. In this case, Fragment1 needs the following objects at <code>__init__()</code> time:  <pre><code>class Fragment1(Fragment):\n    def __init__(self, app, name, sample_data_path, source, record_type):\n        super().__init__(app, name)\n\n        self.source = source\n        self.sample_data_path = sample_data_path\n        self.record_type = record_type\n</code></pre> and Fragment 2 needs the following objects at <code>__init__()</code> time: <pre><code>class Fragment2(Fragment):\n    def __init__(self, app, name, source, model_path, record_type):\n        super().__init__(app, name)\n\n        self.source = source\n        self.record_type = record_type\n        self.model_path = model_path\n</code></pre> We need to make sure in the distributed app's <code>__init__()</code> method, we are creating the corresponding objects to pass in to each fragment's <code>__init__()</code> time.</p>","tags":["Distributed","Fragment"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-2-configuration-file","title":"Changes in scenario 2 - Configuration File","text":"<p>If you had two <code>yaml</code> files for configuring each of the non distributed applications, now in the combined distributed application you will need to combine the content of both in a <code>yaml</code> file.</p>","tags":["Distributed","Fragment"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-2-running-the-application","title":"Changes in scenario 2 - Running the application","text":"<p>In the newly created distributed application we will launch the application like below: <pre><code># To run fragment 1 on current node as driver and worker:\npython3 /workspace/holohub/applications/distributed_app/python/endoscopy_tool_tracking.py --source replayer --data /workspace/holohub/data --driver --worker --fragments fragment1 --address &lt;node 1 IP address&gt;:&lt;port&gt;\n\n# To run fragment 2 on current node as worker:\npython3 /workspace/holohub/applications/distributed_app/python/endoscopy_tool_tracking.py --data /workspace/holohub/data --source replayer --worker --fragments fragment2 --address &lt;node 1 IP address&gt;:&lt;port&gt;\n\n# To run on a single node:\npython3 /workspace/holohub/applications/distributed_app/python/endoscopy_tool_tracking.py --source replayer --data /workspace/holohub/data --driver --worker --fragments fragment1,fragment2 \n</code></pre></p> <p>For more details on the commandline arguments for multi fragment apps, see the documentation.</p> <p>To run the app we create in scenario 2, please see Running the Applications.</p>","tags":["Distributed","Fragment"]},{"location":"tutorials/creating-multi-node-applications/#configuration","title":"Configuration","text":"<p>You can run a distributed application across any combination of hardware that are compatible with the Holoscan SDK. Please see SDK Installation Prerequisites for a list of compatible hardware. </p> <p>If you would like the distributed application fragments to communicate through high speed ConnectX NICs, enable the ConnectX NIC for GPU Direct RDMA, for example the ConnectX-7 NIC on the IGX Orin Developer Kit, follow these instructions. Enabling ConnectX NICs could bring significant speedup to your distributed app connection, for example, the RJ45 ports on IGX Orin Developer Kit supports 1GbE while the QSFP28 ports connected to ConnectX-7 support up to 100 GbE.</p> <p>If connecting together two IGX Orin Developer Kits with dGPU, follow the above instructions on each devkit to enable GPU Direct RDMA through ConnectX-7, and make the hardware connection through the QSFP28 ports on the back panel. A QSFP-QSFP cable should be included in your devkit box alongside power cables etc in the inner small box. Use either one of the two port on the devkit, and make sure to find out the logical name of the port as detailed in the instructions.</p> <p></p> <p>On each machine, make sure to specify an address for the network logical name that has the QSFP cable connected, and when running your distributed application, make sure to specify that address for the <code>--driver</code> machine.</p>","tags":["Distributed","Fragment"]},{"location":"tutorials/creating-multi-node-applications/#running-the-applications","title":"Running the Applications","text":"<p>Follow Container Build instructions to build and launch the HoloHub dev container.</p>","tags":["Distributed","Fragment"]},{"location":"tutorials/creating-multi-node-applications/#scenario-1-application","title":"Scenario 1 Application","text":"<p>Before we start to launch the application, let's first run the original application. In the dev container, make sure to build and launch the <code>multiai_endoscopy</code> app, this will convert the downloaded ONNX model file into a TensorRT engine at first run. </p> <p>Build and run instructions may change in the future, please refer to the original application. <pre><code># on the node that runs fragment 1, or a node that runs the entire app\n./run build multiai_endoscopy\n./run launch multiai_endoscopy python\n</code></pre> Now we're ready to launch the distributed application in scenario 1. <pre><code>cd scenario1/\n</code></pre> To launch the two fragments on separate nodes, launch fragment 1 on node 1 as the driver node with: <pre><code># with replayer as source:\npython3 multi_ai.py --source replayer --data /workspace/holohub/data/ --driver --worker --address &lt;node 1 IP address&gt;:&lt;port number&gt; --fragments fragment1\n# with AJA video capture card as source:\npython3 multi_ai.py --source aja  --data /workspace/holohub/data/ --driver --worker --address &lt;node 1 IP address&gt;:&lt;port number&gt; --fragments fragment1\n</code></pre> and launch fragment 2 with: <pre><code>python3 multi_ai.py --worker --address &lt;node 1 IP address&gt;:&lt;port number&gt; --fragments fragment2\n</code></pre> To launch the two fragments together on a single node, simply launch the application without specifying the additional parameters: <pre><code># with replayer as source:\npython3 multi_ai.py --source replayer --data /workspace/holohub/data/ \n# with AJA video capture card as source:\npython3 multi_ai.py --source aja  --data /workspace/holohub/data/ \n</code></pre></p>","tags":["Distributed","Fragment"]},{"location":"tutorials/creating-multi-node-applications/#scenario-2-application","title":"Scenario 2 Application","text":"<p>Let's first run the original applications. In the dev container, make sure to build and launch the <code>endoscopy_tool_tracking</code> and <code>endoscopy_out_of_body_detection</code> apps, this will convert the downloaded ONNX models into TensorRT engines at first run. </p> <p>Build and run instructions may change in the future, please refer to the original applications. <pre><code># On the node that runs fragment 1 or a node that runs the entire app\n./run build endoscopy_tool_tracking\n./run launch endoscopy_tool_tracking python\n\n# On the node that runs fragment 2 or a node that runs the entire app\n./run build endoscopy_out_of_body_detection\ncd build &amp;&amp; applications/endoscopy_out_of_body_detection/endoscopy_out_of_body_detection --data ../data/endoscopy_out_of_body_detection\n</code></pre> Now we're ready to launch the distributed application in scenario 2.  <pre><code># don't forget to do this on both machines\n# configure your PYTHONPATH environment variable \nexport PYTHONPATH=/opt/nvidia/holoscan/lib/cmake/holoscan/../../../python/lib:/workspace/holohub/build/python/lib\n# run in the build directory of Holohub in order to load extensions\ncd /workspace/holohub/build\n</code></pre></p> <p>To launch the two fragments on separate nodes, launch fragment 1 on node 1 as the driver node with: <pre><code># with replayer as source:\npython3 /workspace/holohub/tutorials/creating-multi-node-applications/scenario2/endoscopy_distributed_app.py --source replayer --data /workspace/holohub/data --driver --worker --fragments fragment1 --address &lt;node 1 IP address&gt;:&lt;port number&gt;\n# with AJA video capture card as source:\npython3 /workspace/holohub/tutorials/creating-multi-node-applications/scenario2/endoscopy_distributed_app.py --source aja --driver --worker --fragments fragment1 --address &lt;node 1 IP address&gt;:&lt;port number&gt;\n</code></pre> and launch fragment 2 with: <pre><code># with replayer as source:\npython3 /workspace/holohub/tutorials/creating-multi-node-applications/scenario2/endoscopy_distributed_app.py  --worker --fragments fragment2 --address &lt;node 1 IP address&gt;:&lt;port number&gt;\n# with AJA video capture card as source:\npython3 /workspace/holohub/tutorials/creating-multi-node-applications/scenario2/endoscopy_distributed_app.py  --source aja --worker --fragments fragment2 --address &lt;node 1 IP address&gt;:&lt;port number&gt;\n</code></pre> To launch the two fragments together on a single node, simply launch the application without specifying the additional parameters: <pre><code># with replayer as source:\npython3 /workspace/holohub/tutorials/creating-multi-node-applications/scenario2/endoscopy_distributed_app.py --source replayer --data /workspace/holohub/data --driver --worker --fragments fragment1,fragment2\n# with AJA video capture card as source:\npython3 /workspace/holohub/tutorials/creating-multi-node-applications/scenario2/endoscopy_distributed_app.py --source aja --driver --worker --fragments fragment1,fragment2 \n</code></pre></p>","tags":["Distributed","Fragment"]},{"location":"tutorials/cuda_mps/","title":"CUDA MPS Tutorial for Holoscan Applications","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: December 3, 2024 Latest version: 0.1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>CUDA MPS is NVIDIA's Multi-Process Service for CUDA applications. It allows multiple CUDA applications to share a single GPU, which can be useful for running more than one Holoscan application on a single machine featuring one or more GPUs. This tutorial describes the steps to enable CUDA MPS and demonstrate few performance benefits of using it.</p>","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Steps to enable CUDA MPS</li> <li>Customization</li> <li>x86 System Performance</li> <li>IGX Orin<ol> <li>Model Benchmarking Application Setup</li> <li>Performance Benchmark Setup</li> <li>Performance Benefits on IGX Orin w/ Discrete GPU<ol> <li>Varying Number of Instances</li> <li>Varying Number of Parallel Inferences</li> </ol> </li> <li> IGX Orin w/ iGPU<ol> <li>MPS Setup on IGX-iGPU</li> <li>Performance Benefits on IGX Orin w/ Integrated GPU</li> </ol> </li> </ol> </li> </ol>","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#steps-to-enable-cuda-mps","title":"Steps to enable CUDA MPS","text":"<p>Before enabling CUDA MPS, please check whether your system supports CUDA MPS.</p> <p>CUDA MPS can be enabled by running the <code>nvidia-cuda-mps-control -d</code> command and stopped by running  <code>echo quit | nvidia-cuda-mps-control</code> command. More control commands are described  here. </p> <p>CUDA MPS does not require any changes to an existing Holoscan application; even an already compiled application binary works as it is. Therefore, a Holoscan application can work with CUDA MPS without any  changes to its source code or binary. However, a machine learning model like a TRT engine file may need to be recompiled  for the first time after enabling CUDA MPS.</p> <p>We have included a helper script in this tutorial <code>start_mps_daemon.sh</code> to enable  CUDA MPS with necessary environment variables.</p> <pre><code>./start_mps_daemon.sh\n</code></pre>","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#customization","title":"Customization","text":"<p>CUDA MPS provides many options to customize resource allocation for MPS clients. For example, it has an option to limit the maximum number of GPU threads that can  be used by every MPS client.  The <code>CUDA_MPS_ACTIVE_THREAD_PERCENTAGE</code> environment variable can be used to control this limit system-wide. This limit can also be configured by communicating the active thread percentage to the control daemon with <code>echo \"set_default_active_thread_percentage &lt;Thread Percentage&gt;\" | nvidia-cuda-mps-control</code>. Our <code>start_mps_daemon.sh</code> script takes this percentage as the first argument as well.</p> <pre><code>./start_mps_daemon.sh &lt;Active Thread Percentage&gt;\n</code></pre> <p>For different applications, one may want to set different limits on the number of GPU threads available to each of them. This can be done by setting the <code>CUDA_MPS_ACTIVE_THREAD_PERCENTAGE</code> environment variable separately for each application. It is elaborated in details here.</p> <p>There are other customizations available in CUDA MPS as well. Please refer to the CUDA MPS documentation to know more about them. Please note that concurrently running Holoscan applications may increase the GPU device memory footprint. Therefore, one needs to be careful about hitting the GPU memory size and potential delay due to page faults.</p> <p>CUDA MPS improves the performance for concurrently running Holoscan applications.  Since multiple applications can simultaneously execute more than one CUDA compute tasks with CUDA MPS, it can also improve the overall GPU utilization.</p>","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#performance-benefits-on-x86-system","title":"Performance Benefits on x86 System","text":"<p>Note: Endoscopy Tool Tracking does not work with CUDA MPS after holohub/Holoscan-SDK-v2.6.0 due to the unavailability of CUDA dynamic parallelism implemented in this PR in CUDA MPS. In case endoscopy tool tracking needs to be tested with CUDA MPS, please use the <code>holoscan-sdk-v2.6.0</code> tag or earlier.</p> <p>Suppose, we want to run the endoscopy tool tracking and ultrasound segmentation applications concurrently on an x86 workstation with RTX A6000 GPU. The below table shows the maximum end-to-end latency performance without and with CUDA MPS, where the active thread percentage is set to 40\\% for each application. It demonstrates 18% and 50% improvement in the maximum end-to-end latency for the endoscopy tool tracking and ultrasound segmentation applications, respectively.</p> Application Without MPS (ms) With MPS (ms) Endoscopy Tool Tracking 115.38 94.20 Ultrasound Segmentation 121.48 60.94 <p>In another set of experiments, we concurrently run multiple instances of the endoscopy tool tracking application in different processes. We set the active thread percentage to be 20\\% for each MPS client. The below graph shows the maximum end-to-end latency with and without CUDA MPS. The experiment demonstrates up to 36% improvement with CUDA MPS.</p> <p></p> <p>Such experiments can easily be conducted with Holoscan Flow Benchmarking to retrieve various end-to-end latency performance metrics.</p>","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#igx-orin","title":"IGX Orin","text":"<p>CUDA MPS is available on IGX Orin since CUDA 12.5. Please check you CUDA version and upgrade to CUDA 12.5+ to test CUDA MPS. We evaluate the benefits of MPS on IGX Orin with discrete and integrated GPUs. Please follow the steps outlined in Steps to enable CUDA MPS to start running the MPS server on IGX Orin. </p> <p>We use the model benchmarking application to demonstrate the benefits of CUDA MPS. In general, MPS improves performance by enabling multiple concurrent processes to share a CUDA context and scheduling resources. We show the benefits of using CUDA MPS along two dimensions: (a) increasing the workload per application instance (varying the number of parallel inferences for the same model) and (b) increasing the total number of instances. </p>","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#model-benchmarking-application-setup","title":"Model Benchmarking Application Setup","text":"<p>Please follow the steps outlined in model benchmarking to ensure that the application builds and runs properly. </p> <p>Note that you need to run the video using v4l2loopback in a separate terminal while running the model benchmarking application.</p> <p>Make sure to change the device path in the <code>model_benchmarking/python/model_benchmarking.yaml</code> file to match the values you provided in the <code>modprobe</code> command when following the v4l2loopback instructions.</p>","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#performance-benchmark-setup","title":"Performance Benchmark Setup","text":"<p>To gather performance metrics for the model benchmarking application, follow the steps outlined in Holoscan Flow Benchmarking. </p> <p>If you are running within a container, please complete Step-3 before launching the container</p> <p>We use the following steps:</p> <p>1. Patch Application:</p> <pre><code>./benchmarks/holoscan_flow_benchmarking/patch_application.sh model_benchmarking\n</code></pre> <p>2. Build Application for Benchmarking:</p> <pre><code>./run build model_benchmarking python --configure-args -DCMAKE_CXX_FLAGS=-I$PWD/benchmarks/holoscan_flow_benchmarking`\n</code></pre> <p>3. Set Up V4l2Loopback Devices:</p> <p>i. Install <code>v4l2loopback</code> and <code>v4l2loopback</code>:</p> <pre><code>sudo apt-get install v4l2loopback-dkms ffmpeg\n</code></pre> <p>ii. Determine the number of instances you would like to benchmark and set that as the value of <code>devices</code>. Then, load the <code>v4l2loopback</code> kernel module on virtual devices <code>/dev/video[*]</code>. This enables each instance to get its input from a separate virtual device.</p> <p>Example: For 3 instances, the <code>v4l2loopback</code> kernel module can be loaded on <code>/dev/video1</code>, <code>/dev/video2</code> and <code>/dev/video3</code>:</p> <pre><code>sudo modprobe v4l2loopback devices=3 video_nr=1 max_buffers=4\n</code></pre> <p>Now open 3 separate terminals.</p> <p>In terminal-1, run: <pre><code>ffmpeg -stream_loop -1 -re -i /data/ultrasound_segmentation/ultrasound_256x256.avi -pix_fmt yuyv422 -f v4l2 /dev/video1\n</code></pre></p> <p>In terminal-2, run: <pre><code>ffmpeg -stream_loop -1 -re -i /data/ultrasound_segmentation/ultrasound_256x256.avi -pix_fmt yuyv422 -f v4l2 /dev/video2\n</code></pre></p> <p>In terminal-3, run: <pre><code>ffmpeg -stream_loop -1 -re -i /data/ultrasound_segmentation/ultrasound_256x256.avi -pix_fmt yuyv422 -f v4l2 /dev/video3\n</code></pre></p> <p>4. Benchmark Application:</p> <pre><code>python benchmarks/holoscan_flow_benchmarking/benchmark.py --run-command=\"python applications/model_benchmarking/python/model_benchmarking.py -l &lt;number of parallel inferences&gt; -i\"  --language python -i &lt;number of instances&gt; -r &lt;number of runs&gt; -m &lt;number of messages&gt; --sched greedy -d &lt;outputs folder&gt; -u\n</code></pre> <p>The command executes <code>&lt;number of runs&gt;</code> runs of <code>&lt;number of instances&gt;</code> instances of the model benchmarking application with <code>&lt;number of messages&gt;</code> messages. Each instance runs <code>&lt;number of parallel inferences&gt;</code> parallel model benchmarking inferences with no post-processing and visualization (<code>-i</code>).</p> <p>Please refer to Model benchmarking options and Holoscan flow benchmarking options for more information on the various command options.</p> <p>Example: After Step-3, to benchmark 3 instances for 10 runs with 1000 messages, run:</p> <pre><code>python benchmarks/holoscan_flow_benchmarking/benchmark.py --run-command=\"python applications/model_benchmarking/python/model_benchmarking.py -l 7 -i\"  --language python -i 3 -r 10 -m 1000 --sched greedy -d myoutputs -u`\n</code></pre>","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#performance-benefits-on-igx-orin-w-discrete-gpu","title":"Performance Benefits on IGX Orin w/ Discrete GPU","text":"<p>We look at the performance benefits of MPS by varying the number of instances and number of inferences. We use the RTX A6000 GPU for our experiments. From our experiments, we observe that enabling MPS results in upto 12% improvement in maximum latency compared to the default setting.</p>","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#varying-number-of-instances","title":"Varying Number of Instances","text":"<p>We fix the number of parallel inferences to 7, number of runs to 10 and number of messages to 1000 and vary the number of instances from 3 to 7 using the <code>-i</code> parameter. Please refer to Performance Benchmark Setup for benchmarking commands.</p> <p>The graph below shows the maximum end-to-end latency of model benchmarking application with and without CUDA MPS, where the active thread percentage was set to <code>80/(number of instances)</code>. For example, for 5 instances, we set the active thread percentage to <code>80/5 = 16</code>. By provisioning resources this way, we leave some resources idle in case a client should require to use it. Please refer to CUDA MPS Resource Provisioning for more details regarding this.</p> <p>The graph is missing a bar for the case of 7 instances and 7 parallel inferences as we were unable to get the baseline to execute. However, we were able to run when MPS was enabled, highlighting the advantage of using MPS for large workloads. We see that the maximum end-to-end latency improves when MPS is enabled and the improvement is more pronounced as the number of instances increases. This is because, as the number of concurrent processes increases, MPS confines CUDA workloads to a certain predefined set of SMs. MPS combines multiple CUDA contexts from multiple processes into one, while simultaneously running them together. It reduces the number of context switches and related inferences, resulting in improved GPU utilization.</p> <p>| Maximum end-to-end Latency | | :-------------------------:| </p> <p>We also notice minor improvements in the 99.9<sup>th</sup> percentile latency and similar improvements in the 99<sup>th</sup> percentile latency.</p> 99.9<sup>th</sup> Percentile Latency 99<sup>th</sup> Percentile Latency","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#varying-number-of-parallel-inferences","title":"Varying Number of Parallel Inferences","text":"<p>We vary the number of parallel inferences to show that MPS may not be beneficial if the workload is insufficient to offset the overhead of running the MPS server. The graph below shows the result of increasing the number of parallel inferences from 3 to 7 while the number of instances is constant. </p> <p>As the number of parallel inferences increases, so does the workload, and the benefit of MPS is more evident. However, when the workload is low, CUDA MPS may not be beneficial. </p> Maximum Latency for 5 Instances","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#igx-orin-w-integrated-gpu","title":"IGX Orin w/ Integrated GPU","text":"","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#mps-setup-on-igx-igpu","title":"MPS Setup on IGX-iGPU","text":"<p>Note that we run all commands as root</p> <p>1. Please add cuda-12.5+ to <code>$PATH</code> and <code>$LD_LIBRARY_PATH</code></p> <p>If you have multiple CUDA installations, check it at <code>/usr/local/</code> directory.</p> <pre><code>echo $PATH\n/usr/local/cuda-12.6/compat:/usr/local/cuda-12.6/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin\n\necho $LD_LIBRARY_PATH\n/usr/local/cuda-12.6/compat/lib:/usr/local/cuda-12.6/compat:/usr/local/cuda-12.6/lib64:\n</code></pre> <p>2. Be sure to pass <code>-v /tmp/nvidia-mps:/tmp/nvidia-mps  -v /tmp/nvidia-log:/tmp/nvidia-log -v /usr/local/cuda-12.6:/usr/local/cuda-12.6</code> to <code>./dev_container launch</code> command to ensure that the container is connected to the MPS control and server</p> <p>Example: <pre><code>./dev_container launch --img holohub:v2.1 --docker_opts \"-v /tmp/nvidia-mps:/tmp/nvidia-mps  -v /tmp/nvidia-log:/tmp/nvidia-log -v /usr/local/cuda-12.6:/usr/local/cuda-12.6\"\n</code></pre></p> <p>3. Inside the container, be sure to set the following environment variables: <pre><code>export CUDA_VISIBLE_DEVICES=0\nexport CUDA_MPS_PIPE_DIRECTORY=/tmp/nvidia-mps\nexport CUDA_MPS_LOG_DIRECTORY=/tmp/nvidia-log\n\nexport PATH=/usr/local/cuda-12.6/bin:$PATH\nexport PATH=/usr/local/cuda-12.6/compat:$PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.6/lib64:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.6/compat:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.6/compat/lib:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=/usr/lib/aarch64-linux-gnu/nvidia:$LD_LIBRARY_PATH\n</code></pre> Our <code>$PATH</code> and <code>$LD_LIBRARY_PATH</code> values inside the container are: <pre><code>echo $PATH\n/usr/local/cuda-12.6/bin:/opt/tensorrt/bin:/usr/local/mpi/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/ucx/bin:/opt/nvidia/holoscan/bin\n\necho $LD_LIBRARY_PATH\n/usr/local/cuda-12.6/compat/lib:/usr/local/cuda-12.6/compat:/usr/local/cuda-12.6/lib64:/usr/local/cuda/compat/lib.real:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/nvidia/holoscan/lib\n</code></pre></p> <p>4. Start MPS server and control <pre><code>sudo -i\nexport CUDA_MPS_ACTIVE_THREAD_PERCENTAGE=20\nnvidia-cuda-mps-control -d\n</code></pre></p> <p>5. After steps 1-4, follow the benchmark insructions to benchmark the application</p>","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#performance-benefits-on-igx-orin-w-integrated-gpu","title":"Performance Benefits on IGX Orin w/ Integrated GPU","text":"<p>We look at the performance benefits of MPS by varying the number of application instances. We run the model benchmarking application in a mode where the inputs are always available and being read from the disk with the video replayer operator. For every instance of the application, we run 1 inference (<code>-l 1</code>) as iGPU is a smaller GPU. In this experiment, we also oversubscribe the GPU to provide the instances more opportunity to utilize the available SMs (IGX Orin iGPU has 16 SMs).</p> <p>From our experiments, we observe that enabling MPS results in 22-50%, 13-49% and 6-37% improvement in maximum latency, 99.9 percentile latency and average latency, respectively. The graphs below capture the result. In the X-axis, number of instances is show to increase from 2 to 7, and the number in the parenthesis shows the number of SMs per instance enabled by <code>CUDA_ACTIVE_THREAD_PERCENTAGE</code>.</p> <p>| Maximum end-to-end Latency | | :-------------------------:| </p> 99.9 Percentile Latency Average Latency <p>We use different number of SMs for different instances to ensure the total number of SMs requested by all the instances exceed the number of available SMs.</p>","tags":["Acceleration","Benchmarking","CUDA","MPS"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/","title":"Processing DICOM to USD with MONAI Deploy and Holoscan","text":"<p> Authors: Rahul Choudhury (NVIDIA), Andreas Heumann (NVIDIA), Cristiana Dinea (NVIDIA), Gregory Lee (NVIDIA), Jeroen Stinstra (NVIDIA), Ming Melvin Qi (NVIDIA), Tom Birdsong (NVIDIA), Wendell Hom (NVIDIA) Supported platforms: amd64, arm64 Last modified: March 20, 2024 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 2 - Trusted</p> <p></p> <p>In this tutorial we demonstrate a method leveraging a combined MONAI Deploy and Holoscan pipeline to process DICOM input data and write a resulting mesh to disk in the OpenUSD file format.</p>","tags":["DICOM","mesh","MONAI","MONAI Deploy","Omniverse","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#demonstrated-technologies","title":"Demonstrated Technologies","text":"","tags":["DICOM","mesh","MONAI","MONAI Deploy","Omniverse","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#dicom","title":"DICOM","text":"<p>The Digital Imaging and Communications in Medicine (DICOM) standard is a comprehensive standard for medical imaging. DICOM covers a wide variety of medical imaging modalities and defines standards for both image storage and communications in medicine. DICOM data often represent 2D or 3D volumes or series of volumes.</p>","tags":["DICOM","mesh","MONAI","MONAI Deploy","Omniverse","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#openusd","title":"OpenUSD","text":"<p>Universal Scene Description (OpenUSD) is an extensible ecosystem of file formats, compositors, renderers, and other plugins for comprehensive 3D scene description.</p> <p>OpenUSD serves as the backbone of the NVIDIA Omniverse cloud computing platform. Omniverse includes a variety of applications such as USD Composer for viewing and manipulating OpenUSD scenes, with features such as: - State-of-the-art cloud rendering - Live collaborative scene editing - Multi-user mixed reality</p> <p>Download the NVIDIA Omniverse launcher to get started with Omniverse. See NVIDIA OpenUSD Tutorials for getting started with the OpenUSD Python libraries we use in this tutorial.</p>","tags":["DICOM","mesh","MONAI","MONAI Deploy","Omniverse","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#monai-deploy-holoscan-pipeline","title":"MONAI Deploy + Holoscan Pipeline","text":"<p>The MONAI Deploy App SDK provides a series of operators to load and select DICOM instances and then decode the pixel data into in-memory NumPy data objects. MONAI Deploy integrates seamlessly with Holoscan pipelines.</p> <ol> <li>The DICOM Loader operator parses a set of DICOM instance files, and loads them into a set of objects representing the logical hierarchical structure of DICOM Study, Series, and Instance. Key DICOM metadata is extracted, while the image pixel data is not loaded in memory or decoded.</li> <li>The DICOM Series Selector selects relevant DICOM Series, e.g, a MR T2 series, using simple configurable selection rules.</li> <li>The Series to Volume converter decodes and combines the pixel data of the instances to a 3D NumPy array with a set of metadata for spacing, orientation, etc.</li> <li>The MONAI Deploy AI Inference Operator accepts the 3D volume and runs AI inference to segment the region of interest, which in this case is pixels representing the spleen.</li> <li>The MONAI STL Conversion Operator converts the output label volume to a mesh in STL format.</li> <li>The Holoscan \"Send Mesh to USD\" operator writes the mesh to disk in the OpenUSD format.</li> </ol>","tags":["DICOM","mesh","MONAI","MONAI Deploy","Omniverse","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#requirements","title":"Requirements","text":"<p>Please review the HoloHub README to get started with HoloHub general requirements before continuing.</p>","tags":["DICOM","mesh","MONAI","MONAI Deploy","Omniverse","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#hardware","title":"Hardware","text":"<p>This tutorial may run on an <code>amd64</code> or <code>arm64</code> workstation. - For <code>amd64</code> we rely on <code>usd-core</code> Python wheels from PyPI for OpenUSD support. - For <code>arm64</code> we rely on NVIDIA Omniverse Python libraries for OpenUSD support.</p>","tags":["DICOM","mesh","MONAI","MONAI Deploy","Omniverse","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#host-software","title":"Host Software","text":"<p>This tutorial should run in a <code>docker</code> container:</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install docker\n</code></pre>","tags":["DICOM","mesh","MONAI","MONAI Deploy","Omniverse","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#building-the-tutorial-container","title":"Building the tutorial container","text":"<p>Run the command below from the top-level HoloHub directory to build the tutorial container on the host workstation:</p> <pre><code>export NGC_CONTAINER_IMAGE_PATH=\"nvcr.io/nvidia/clara-holoscan/holoscan:v1.0.3-dgpu\"\n./dev_container build --docker_file tutorials/dicom_to_usd_with_monai_and_holoscan/Dockerfile --base_img ${NGC_CONTAINER_IMAGE_PATH} --img holohub:dicom-to-usd\n</code></pre>","tags":["DICOM","mesh","MONAI","MONAI Deploy","Omniverse","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#running-the-application","title":"Running the application","text":"<p>Run the commands below on the host workstation to launch the container and run the tutorial. The application will run the MONAI Deploy + Holoscan pipeline for AI segmentation and write results to the <code>.usd</code> output file. The mesh will also be available as a <code>mesh.stl</code> file on disk.</p> <pre><code>./dev_container launch --img holohub:dicom-to-usd # start the container\ncd ./tutorials/dicom_to_usd_with_monai_and_holoscan\npython tutorial.py --output ./output/spleen-segmentation.usd # run the tutorial\n</code></pre> <p>Download the NVIDIA Omniverse launcher to explore applications such as USD Composer for viewing and manipulating the OpenUSD output file.</p>","tags":["DICOM","mesh","MONAI","MONAI Deploy","Omniverse","OpenUSD","STL"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/","title":"Taking advantage of GPU Direct Storage on the latest NVIDIA Edge platform","text":"<p> Authors: Marcus Manos (NVIDIA) Supported platforms: arm64 Last modified: September 5, 2024 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 4 - Experimental</p>","tags":["GPU Direct Storage"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#introduction","title":"Introduction","text":"<p>Modern-day edge accelerated computing solutions constantly push the boundaries of what is possible. The success of edge computing is mainly due to a new approach to accelerated computing problems, which are viewed from both a GPU and systems perspective. With innovations such as GPU Direct Storage (GDS) and GPU Direct Storage-over-Fabrics (GDS-OF), we can load data directly onto the GPU at lightning-fast speeds. GDS allows us to transform previously offline batch workloads into online streaming solutions, bringing them into the 21st century. This tutorial demonstrates this with a sample pipeline using the latest industrial-grade edge hardware (IGX) and A6000 workstation GPU. We\u2019ll start by providing an overview of the two main types of workflows, then discuss how to set up GDS, and finally, wrap up with an example application using Nvidia Holoscan and the Nvidia Rapids software suite.</p>","tags":["GPU Direct Storage"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#overview-of-gds-and-gds-of","title":"Overview of GDS and GDS-OF","text":"<p>To give a brief introduction to the GPU Direct Storage paradigm, we first need to understand the traditional file transfer pathway between the storage device and a GPU. Traditionally, to transfer a file from the storage device to the GPU, the CPU would create a temporary cache or bounce buffer out of the system memory (RAM). This buffer would hold data transferred from the storage device. Only after the data has been transferred to the bounce buffer or the buffer is full will the data transfer from the bounce buffer to the GPU. In the case of large file transfers to repetitive file transfers, this will result in increased latency. GPU Direct Storage sends the data directly from the storage device to the GPU without the need for the temporarily allocated CPU system memory to coordinate movement with the PCIE bus. For more information, please visit the blog below. -   Introduction to GPU Direct Storage</p> <p>Before diving into the technical details of setting up GDS, we will first cover the two prominent use cases of ultra-fast GDS. The first case is the consumer use case, those developers that will take advantage of the GDS to read data in real time. These cases include: - Running Digital Signal Processing models in real-time - Streaming Video data for live analytics - Streaming data from scientific instruments such as electron microscopes.</p>","tags":["GPU Direct Storage"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#producer-use-cases","title":"Producer Use Cases","text":"<p>The other GDS use case for developers includes producing or writing data to     a source. Examples of these workflows include: - Writing streams from multiple scientific equipment into a single stream. - Archiving raw or transformed data in real-time. - Writing data to a secondary workflow.</p>","tags":["GPU Direct Storage"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#overview-of-igx","title":"Overview of IGX","text":"<p>IGX is the latest generation of Industrial grade edge AI platform. It boasts an integrated 12-core ARM CPU and 2048 Cuda core + 64 Tensor Core integrated Ampere generation GPU. This powerful duo combines many other features, such as NVME support, RJ45 networking, and more, designed to optimize any edge AI workflow. The 2 PCIE slots connected to the ConnectX 7 chip onboard the IGX are critical. This ConnectX 7 chip uniquely facilitates GDS &amp; GDS-OF for the GPU and any PCI-E expansion card. For this reason, we added a PCIE riser card and compatible SSD to the second PCIE Gen5x8 slot. \u00a0  </p> <p></p>","tags":["GPU Direct Storage"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#setting-up-gds","title":"Setting Up GDS","text":"<p>As previously mentioned, we have two open slots onboard the IGX system. The first x16 slot is for our GPU, which will take advantage of all x16 lanes for communication. The second x8 slot, however, can be used for any PCIE expansion card; we chose to use a PCIE riser to add an NVME-compatible drive to take advantage of GDS. It\u2019s important to note that we couldn\u2019t use the existing M.2 NVME boot drive since it\u2019s not connected to the Connect 7 chip onboard the IGX.</p> <p></p>","tags":["GPU Direct Storage"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#hardware-requirements","title":"Hardware Requirements","text":"<p>The first step in this process is to obtain the correct hardware. In addition to the IGX system you have, you will also need a PCIE riser card and a compatible NVME SSD with the PCIE riser you chose (e.g., if you have a PCIE M.2 riser, then you\u2019ll need an M.2 NVME SSD).</p> <p></p>","tags":["GPU Direct Storage"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#software-installation","title":"Software Installation","text":"<p>The next step is to update the software inside your IGX. Note: You can skip this step if you have a new IGX system or one that has been freshly flashed. An explanation of each command will be available at the end of this post. * Install/update GCC and Linux Headers     * <code>sudo apt update</code> * Install MOFED drivers     * <code>wget --no-check-certificate https://content.mellanox.com/ofed/MLNX_OFED-24.01-0.3.3.1/MLNX_OFED_LINUX-24.01-0.3.3.1-ubuntu22.04-aarch64.iso</code>     * <code>mkdir /mnt/iso</code>     * <code>mount -o loop MLNX_OFED_LINUX-24.01-0.3.3.1-ubuntu22.04-x86_64.iso /mnt/iso</code>     * <code>/mnt/iso/mlnxofedinstall --with-nfsrdma --with-nvmf \u2013force</code>     * <code>update-initramfs -u -k $(uname -r)</code>     * <code>sudo reboot</code> * Install cuda toolkit, cuda drivers, and GPU drivers     * <code>sudo apt-get update</code>     * <code>sudo apt-get -y install cuda-toolkit-12-5</code>     * <code>sudo apt-get install -y nvidia-driver-535-open</code>     * <code>sudo apt-get install -y cuda-drivers-535</code></p> <p>After updating the essential software, follow the instructions below to install the Nvidia-GDS application and reboot your machine. * Install the nvidia-gds application     * <code>sudo apt-get install nvidia-gds</code> * Reboot     * <code>sudo reboot</code> * Mount a new directory to the SSD     * Find which SSD is the new one:         *   <code>Lsblk</code>     * Mount a compatible file system (In our case ext4) to the drive         * <code>sudo mount -t ext4 /dev/nvme1n1</code>         * <code>/mnt/nvme/ -o data=ordered</code></p> <p>Once you have completed the software setup process, there are some additional things you can do to ensure GDS is being used correctly:</p> <ul> <li>Modify your cufile.json only to be compatible with GDS file systems<ul> <li>Open <code>/usr/local/cuda/gds/cufile.json</code></li> <li>Change <code>\u201callow_compat_mode\u201d</code> to <code>\u201cfalse\u201d</code></li> </ul> </li> </ul> <p>After this, you can run several checks to ensure that GDS is working: * Run the gdscheck script     * <code>/usr/local/cuda/gds/tools/gdscheck -p</code> * Run the gdsio application to send sample data to the mounted directory     * <code>Sudo /usr/local/cuda/gds/tools/gdsio x 0 -i 1M -s 10M -d 0 -w 1 -I 1 -V -D /mnt/nvme</code></p>","tags":["GPU Direct Storage"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#example-application","title":"Example Application","text":"","tags":["GPU Direct Storage"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#using-nvidia-holoscan-and-rapids","title":"Using Nvidia Holoscan and RAPIDS","text":"<p>Kvikio is a part of the RAPIDS ecosystem of libraries. It gives you access to the GPU Direct Storage API, CuFile. Although you can access CuFile directly, Kvikio is a Python and C++ API that gives you easy access to the underlying CuFile functionality in a straightforward way. \u00a0</p> <p>Holoscan, a powerful sensor processing SDK, plays a crucial role in enabling scientists and engineers to harness accelerated computing for their needs. It provides the essential framework for hosting a scientific pipeline. In the example below, we demonstrate how Holoscan, in conjunction with CuPy and Kvikio, can be used to read electron microscope data and identify electrons in the image.</p> <p>Feel free to run the example application by either providing your own data, or using the data generation script. Once the data is generated you can run the application with the following command:  <code>python3 holoscan_gds.py</code></p>","tags":["GPU Direct Storage"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#conclusion","title":"Conclusion","text":"<p>GPU Direct Storage (GDS) is revolutionizing edge computing by enabling ultra-fast data transfer directly to GPUs. Following the steps outlined in this tutoril, you can set up GDS on your IGX system and leverage its real-time data processing capabilities. Try it out and see the difference it makes in your workflows!</p>","tags":["GPU Direct Storage"]},{"location":"tutorials/gui_for_python_applications/","title":"Adding a GUI to Holoscan Python Applications","text":"<p> Authors: Wendell Hom (NVIDIA) Supported platforms: amd64, arm64 Last modified: September 3, 2024 Language: Python Latest version: 0.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.3.0 Contribution metric: Level 1 - Highly Reliable</p> <p>When developing Holoscan applications, incorporating a graphical user interface (GUI) can enhance usability and allow modification of the application's behavior at runtime.</p> <p>This tutorial demonstrates how GUI controls were integrated into the Florence-2 Python application using PySide6. This addition enables users to dynamically change the vision task performed by the application.</p> <p> </p>","tags":["GUI","Python","Pyside6"]},{"location":"tutorials/gui_for_python_applications/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview<ol> <li>Dockerfile</li> <li>Application Code</li> <li>GUI Code</li> </ol> </li> <li>Creating the GUI Widgets and Layout</li> <li>Starting the Holoscan Application Thread</li> <li>Adding a GUI to Your Own Application</li> </ol>","tags":["GUI","Python","Pyside6"]},{"location":"tutorials/gui_for_python_applications/#overview","title":"Overview","text":"<p>The Florence-2 application includes the typical components of a Holohub application, with the addition of a GUI component.  The main components are:</p> <ul> <li>Dockerfile: For installing additional dependencies.</li> <li>Application Code: Defines the Holoscan application and its operators.</li> <li>GUI Code: Utilizes PySide6 to add UI controls.</li> </ul>","tags":["GUI","Python","Pyside6"]},{"location":"tutorials/gui_for_python_applications/#dockerfile","title":"Dockerfile","text":"<p>The Dockerfile is used in Holohub when the application requires additional dependencies. For GUI functionality, this application's  Dockerfile installs:</p> <ul> <li><code>qt6-base-dev</code> for Qt6 framework</li> <li><code>PySide6</code> for Python bindings for Qt6 (as specified in requirements.txt)</li> </ul>","tags":["GUI","Python","Pyside6"]},{"location":"tutorials/gui_for_python_applications/#application-code","title":"Application Code","text":"<p>The Florence-2 application code is organized across several files:</p> <ul> <li><code>florence2_app.py</code>: Main application code.</li> <li><code>florence2_op.py</code>: Florence-2 model inference code.</li> <li><code>florence2_postprocessor_op.py</code>: Post-processing code to send overlays (e.g., bounding boxes, labels, segmentation masks) to Holoviz.</li> <li><code>config.yaml</code>: Default application parameters.</li> </ul> <p>The Florence-2 application can be run independently of the GUI code. E.g., the application can be run with <code>python application/florence-2-vision/florence2_app.py</code> inside the Florence-2 Docker container. This will run the application without the GUI controls.  The only code needed for GUI integration in the application code is the <code>set_parameters()</code> method in the <code>FlorenceApp</code> class. This method updates two fields in the Florence-2 operator:</p> <pre><code>class FlorenceApp(Application):\n    def set_parameters(self, task, prompt):\n        \"\"\"Set parameters for the Florence2Operator.\"\"\"\n        if self.florence_op:\n            self.florence_op.task = task\n            self.florence_op.prompt = prompt\n</code></pre> <p>These updated parameters are passed to the model during the next <code>compute()</code> method execution of the Florence-2 operator.</p>","tags":["GUI","Python","Pyside6"]},{"location":"tutorials/gui_for_python_applications/#gui-code","title":"GUI Code","text":"<p>The GUI code resides in qt_app.py. The code in this file defines a class for the main window which calls <code>setupUi()</code> and <code>runHoloscanApp()</code> when the instance is initialized.</p> <pre><code>class Window(QMainWindow):\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.setupUi()  # Setup the UI\n        self.runHoloscanApp()  # Run the Holoscan application\n</code></pre> <p>At a high level, this is all we need to launch a Python Holoscan application with a GUI. The <code>setupUi()</code> method defines the GUI widgets and layout, while <code>runHoloscanApp()</code> runs the Florence-2 application in a separate thread within the process. Details of these methods are explored in the following sections.</p>","tags":["GUI","Python","Pyside6"]},{"location":"tutorials/gui_for_python_applications/#creating-the-gui-widgets-and-layout","title":"Creating the GUI Widgets and Layout","text":"<p>The <code>setupUi()</code> method creates the GUI with a few simple widgets using PySide6 APIs.  For those unfamiliar with PySide6, this tutorial provides an introduction.</p> <pre><code>    def setupUi(self):\n        \"\"\"Setup the UI components.\"\"\"\n        self.setWindowTitle(\"Florence-2\")\n        self.resize(400, 150)\n        self.centralWidget = QWidget()\n        self.setCentralWidget(self.centralWidget)\n\n        layout = QVBoxLayout()\n\n        # Create and add dropdown for task selection\n        self.dropdown = QComboBox()\n        self.dropdown.addItems(\n            [\n                \"Object Detection\",\n                \"Caption\",\n                \"Detailed Caption\",\n                \"More Detailed Caption\",\n                \"Dense Region Caption\",\n                \"Region Proposal\",\n                \"Caption to Phrase Grounding\",\n                \"Referring Expression Segmentation\",\n                \"Open Vocabulary Detection\",\n                \"OCR\",\n                \"OCR with Region\",\n            ]\n        )\n        layout.addWidget(QLabel(\"Select an option:\"))\n        layout.addWidget(self.dropdown)\n\n        # Create and add text input for prompt\n        self.text_input = QLineEdit()\n        layout.addWidget(QLabel(\"Enter text:\"))\n        layout.addWidget(self.text_input)\n\n        # Create and add submit button\n        self.submit_button = QPushButton(\"Submit\")\n        self.submit_button.clicked.connect(self.on_submit)\n        layout.addWidget(self.submit_button)\n\n        self.centralWidget.setLayout(layout)\n</code></pre> <p>This code creates the following widgets:</p> <ul> <li>Drop-down Menu: Lists the vision tasks supported by Florence-2.</li> <li>Text input Widget: Allows text input for tasks such as Open Vocabulary Detection.</li> <li>Submit Button: Triggers the <code>on_submit()</code> method when clicked.</li> </ul> <p>When the application is running, the user selects a vision task, enters text (if  needed), and clicks \"Submit\" to change the task performed by the model. The <code>on_submit()</code> method is then invoked, calling the <code>set_parameters()</code> method  in the <code>FlorenceApp</code> class to update the operator's parameters.</p> <pre><code>    def on_submit(self):\n        \"\"\"Handle the submit button click event.\"\"\"\n        selected_option = self.dropdown.currentText()\n        entered_text = self.text_input.text()\n\n        # Set parameters in the Holoscan application\n        global gApp\n        if gApp:\n            gApp.set_parameters(selected_option, entered_text)\n</code></pre>","tags":["GUI","Python","Pyside6"]},{"location":"tutorials/gui_for_python_applications/#starting-the-holoscan-application-thread","title":"Starting the Holoscan Application Thread","text":"<p>The <code>runHoloscanApp()</code> method starts the Florence-2 application by creating an instance of <code>FlorenceWorker</code>  and running it in a thread.</p> <pre><code>    def runHoloscanApp(self):\n        \"\"\"Run the Holoscan application in a separate thread.\"\"\"\n        self.thread = QThread()\n        self.worker = FlorenceWorker()\n        self.worker.moveToThread(self.thread)\n        self.thread.started.connect(self.worker.run)\n        self.worker.finished.connect(self.thread.quit)\n        self.worker.finished.connect(self.worker.deleteLater)\n        self.thread.finished.connect(self.thread.deleteLater)\n        self.thread.start()\n</code></pre> <p>When the thread is started, it calls the <code>FlorenceWorker</code> class's <code>run()</code> method which creates and runs the Holoscan application.</p> <pre><code># Worker class to run the Holoscan application in a separate thread\nclass FlorenceWorker(QObject):\n    finished = Signal()  # Signal to indicate the worker has finished\n    progress = Signal(int)  # Signal to indicate progress (if needed)\n\n    def run(self):\n        \"\"\"Run the Holoscan application.\"\"\"\n        config_file = os.path.join(os.path.dirname(__file__), \"config.yaml\")\n        global gApp\n        gApp = app = FlorenceApp()\n        app.config(config_file)\n        app.run()\n</code></pre> <p>This covers the essential steps for creating a GUI to control your Python Holoscan applications.  To try out the application, follow the instructions provided here.</p>","tags":["GUI","Python","Pyside6"]},{"location":"tutorials/gui_for_python_applications/#adding-a-gui-to-your-own-application","title":"Adding a GUI to Your Own Application","text":"<p>To integrate a GUI into your Python application using PySide6, follow these steps:</p> <ol> <li>Ensure Qt and PySide6 dependencies are included in your Dockerfile. Verify that Qt and PySide6 package licenses meet your project requirements.</li> <li>Copy the <code>qt_app.py</code> file to your application directory.  Rename and modify the <code>FlorenceWorker</code> class to create an instance of your application. Update the import statement <code>from florence2_app import FlorenceApp</code> as necessary.</li> <li>Customize the <code>setupUi()</code> method to include the controls relevant to your application.</li> <li>Update <code>set_parameters()</code> methoed to reflect the parameters your application needs to update.</li> </ol>","tags":["GUI","Python","Pyside6"]},{"location":"tutorials/high_performance_networking/","title":"High Performance Networking with Holoscan","text":"<p> Authors: Alexis Girault (NVIDIA) Supported platforms: amd64, arm64 Last modified: March 14, 2025 Language: C++ Latest version: 0.1.0 Minimum Holoscan SDK version: 3.0.0 Tested Holoscan SDK versions: 3.0.0 Contribution metric: Level 3 - Developmental</p> <p>This tutorial demonstrates how to use the advanced networking Holoscan operator (often referred to as ANO or <code>advanced_network</code> in HoloHub) for low latency and high throughput communication through NVIDIA SmartNICs. With a properly tuned system, the advanced network operator can achieve hundreds of Gbps with latencies in the low microseconds.</p> <p>Note</p> <p>This solution is designed for users who want to create a Holoscan application that will interface with an external system or sensor over Ethernet.</p> <ul> <li>For high performance communication with systems also running Holoscan, refer to the Holoscan distributed application documentation instead.</li> <li>For JESD-compliant sensor without Ethernet support, consider the Holoscan Sensor Bridge for an FPGA-based interface to Holoscan.</li> </ul>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#prerequisites","title":"Prerequisites","text":"<p>Achieving High Performance Networking with Holoscan requires a system with an NVIDIA SmartNIC and a discrete GPU. That is the case of NVIDIA Data Center systems, or edge systems like the NVIDIA IGX platform and the NVIDIA Project DIGITS. <code>x86_64</code> systems equipped with these components are also supported, though the performance will vary greatly depending on the PCIe topology of the system (more on this below).</p> <p>In this tutorial, we will be developing on an NVIDIA IGX Orin platform with IGX SW 1.1 and an NVIDIA RTX 6000 ADA GPU, which is the configuration that is currently actively tested. The concepts should be applicable to other systems based on Ubuntu 22.04 as well. It should also work on other Linux distributions with a glibc version of 2.35 or higher by containerizing the dependencies and applications on top of an Ubuntu 22.04 image, but this is not actively tested at this time.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#background","title":"Background","text":"<p>Achieving high performance networking is a complex problem that involves many system components and configurations which we will cover in this tutorial. Two of the core concepts to achieve this are named Kernel Bypass, and GPUDirect.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#kernel-bypass","title":"Kernel Bypass","text":"<p>In this context, Kernel Bypass refers to bypassing the operating system's kernel to directly communicate with the network interface (NIC), greatly reducing the latency and overhead of the Linux network stack. There are multiple technologies that achieve this in different fashions. They're all Ethernet-based, but differ in their implementation and features. The goal of the <code>advanced_network</code> operator in Holoscan Networking is to provide a common higher-level interface to all these backends:</p> <ul> <li>RDMA: Remote Direct Memory Access, using the open-source <code>rdma-core</code> library. It differs from the other Ethernet-based backends with its server/client model and RoCE (RDMA over Ethernet) protocol. Given the extra cost and complexity to setup on both ends, it offers a simpler user interface, orders packets on arrival, and is the only one to offer a high reliability mode.</li> <li>DPDK: the Data Plane Development Kit is an open-source project part of the Linux Foundation with a strong and long-lasting community support. Its RTE Flow capability is generally considered the most flexible solution to split packets ingress and egress data.</li> <li>DOCA GPUNetIO: This NVIDIA proprietary technology differs from the other backends by transmitting and receiving packets from the NIC using a GPU kernel instead of CPU code, which is highly beneficial for CPU-bound applications.</li> <li>NVIDIA Rivermax: NVIDIA's other proprietary kernel bypass technology. For a license fee, it should offer the lowest latency and lowest resource utilization for video streaming (RTP packets).</li> </ul> Work In Progress <p>The Holoscan Advanced Networking Operator integration testing infrastructure is under active development. As such:</p> <ul> <li>The DPDK backend is supported and distributed with the <code>holoscan-networking</code> package, and is the only backend actively tested at this time.</li> <li>The DOCA GPUNetIO backend is supported and distributed with the <code>holoscan-networking</code> package, with testing infrastructure under development.</li> <li>The NVIDIA Rivermax backend is supported for Rx only when building from source, but not yet distributed nor actively tested. Tx support is under development.</li> <li>The RDMA backend is under active development and should be available soon.</li> </ul> <p>Which backend is best for your use case will depend on multiple factors, such as packet size, batch size, data type, and more. The goal of the Advanced Networking Operator is to abstract the interface to these backends, allowing developers to focus on the application logic and experiment with different configurations to identify the best technology for their use case.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#gpudirect","title":"GPUDirect","text":"<p><code>GPUDirect</code> allows the NIC to read and write data from/to a GPU without requiring to copy the data the system memory, decreasing CPU overheads and significantly reducing latency. An implementation of <code>GPUDirect</code> is supported by all the kernel bypass backends listed above.</p> <p>Warning</p> <p><code>GPUDirect</code> is only supported on Workstation/Quadro/RTX GPUs and Data Center GPUs. It is not supported on GeForce cards.</p> How does that relate to peermem or dma-buf? <p>There are two interfaces to enable <code>GPUDirect</code>:</p> <ul> <li>The <code>nvidia-peermem</code> kernel module, distributed with the NVIDIA DKMS GPU drivers.<ul> <li>Supported on Ubuntu kernels 5.4+, deprecated starting with kernel 6.8.</li> <li>Supported on NVIDIA optimized Linux kernels, including IGX OS and DGX OS.</li> <li>Supported by all MOFED drivers (requires rebuilding nvidia-dkms drivers afterwards).</li> </ul> </li> <li><code>DMA Buf</code>, supported on Linux kernels 5.12+ with NVIDIA open-source drivers 515+ and CUDA toolkit 11.7+.</li> </ul>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#1-installing-holoscan-networking","title":"1. Installing Holoscan Networking","text":"<p>We'll start with installing the <code>holoscan-networking</code> package, as it provides some utilities to help tune the system, and requires some dependencies which will help us with the system setup.</p> <p>First, add the DOCA apt repository which holds some of its dependencies:</p> IGX OS 1.1SBSA (Ubuntu 22.04)x86_64 (Ubuntu 22.04) <pre><code>export DOCA_URL=\"https://linux.mellanox.com/public/repo/doca/2.10.0/ubuntu22.04/arm64-sbsa/\"\nwget -qO- https://linux.mellanox.com/public/repo/doca/GPG-KEY-Mellanox.pub | gpg --dearmor - | sudo tee /etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub &gt; /dev/null\necho \"deb [signed-by=/etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub] $DOCA_URL ./\"  | sudo tee /etc/apt/sources.list.d/doca.list &gt; /dev/null\n\nsudo apt update\n</code></pre> <pre><code>export DOCA_URL=\"https://linux.mellanox.com/public/repo/doca/2.10.0/ubuntu22.04/arm64-sbsa/\"\nwget -qO- https://linux.mellanox.com/public/repo/doca/GPG-KEY-Mellanox.pub | gpg --dearmor - | sudo tee /etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub &gt; /dev/null\necho \"deb [signed-by=/etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub] $DOCA_URL ./\"  | sudo tee /etc/apt/sources.list.d/doca.list &gt; /dev/null\n\n# Also need the CUDA repository for holoscan: https://developer.nvidia.com/cuda-downloads?target_os=Linux\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\n\nsudo apt update\n</code></pre> <pre><code>export DOCA_URL=\"https://linux.mellanox.com/public/repo/doca/2.10.0/ubuntu22.04/x86_64/\"\nwget -qO- https://linux.mellanox.com/public/repo/doca/GPG-KEY-Mellanox.pub | gpg --dearmor - | sudo tee /etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub &gt; /dev/null\necho \"deb [signed-by=/etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub] $DOCA_URL ./\"  | sudo tee /etc/apt/sources.list.d/doca.list &gt; /dev/null\n\n# Also need the CUDA repository for holoscan: https://developer.nvidia.com/cuda-downloads?target_os=Linux\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\n\nsudo apt update\n</code></pre> <p>You can then install <code>holoscan-networking</code>:</p> Debian installationFrom source <p>COMING SOON - GTC 2025</p> <pre><code>sudo apt install -y holoscan-networking\n</code></pre> <p>You can build the Holoscan Networking libraries and sample applications from source on HoloHub:</p> <pre><code>git clone git@github.com:nvidia-holoscan/holohub.git\ncd holohub\n./dev_container build_and_install holoscan-networking   # Installed in ./install\n</code></pre> <p>If you'd like to generate the debian package from source and install it to ensure all dependencies are then present on your system, you can run:</p> <pre><code>./dev_container build_and_package holoscan-networking\nsudo apt-get install ./holoscan-networking_*.deb        # Installed in /opt/nvidia/holoscan\n</code></pre> <p>Refer to the HoloHub README for more information.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#2-required-system-setup","title":"2. Required System Setup","text":"","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#21-check-your-nic-drivers","title":"2.1 Check your NIC drivers","text":"<p>Ensure your NIC drivers are loaded:</p> <pre><code>lsmod | grep ib_core\n</code></pre> See an example output <p>This would be an expected output, where <code>ib_core</code> is listed on the left.</p> <pre><code>ib_core               442368  8 rdma_cm,ib_ipoib,iw_cm,ib_umad,rdma_ucm,ib_uverbs,mlx5_ib,ib_cm\nmlx_compat             20480  11 rdma_cm,ib_ipoib,mlxdevm,iw_cm,ib_umad,ib_core,rdma_ucm,ib_uverbs,mlx5_ib,ib_cm,mlx5_core\n</code></pre> <p>If this is empty, install the latest OFED drivers from DOCA (the DOCA APT repository should already be configured from the Holoscan Networking installation above), and reboot your system:</p> <pre><code>sudo apt update\nsudo apt install doca-ofed\nsudo reboot\n</code></pre> <p>Note</p> <p>If this is not empty, you can still install the newest OFED drivers from <code>doca-ofed</code> above. If you choose to keep your current drivers, install the following utilities for convenience later on. They include tools like <code>ibstat</code>, <code>ibv_devinfo</code>, <code>ibdev2netdev</code>, <code>mlxconfig</code>:</p> <pre><code>sudo apt update\nsudo apt install infiniband-diags ibverbs-utils mlnx-ofed-kernel-utils mft\n</code></pre> <p>Also upgrade the user space libraries to make sure your tools have all the symbols they need:</p> <pre><code>sudo apt install libibverbs1 librdmacm1 rdma-core\n</code></pre> <p>Running <code>ibstat</code> or <code>ibv_devinfo</code> will confirm your NIC interfaces are recognized by your drivers.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#22-switch-your-nic-link-layers-to-ethernet","title":"2.2 Switch your NIC Link Layers to Ethernet","text":"<p>NVIDIA SmartNICs can function in two separate modes (called link layer):</p> <ul> <li>Ethernet (ETH)</li> <li>Infiniband (IB)</li> </ul> <p>To identify the current mode, run <code>ibstat</code> or <code>ibv_devinfo</code> and look for the <code>Link Layer</code> value.</p> <pre><code>ibv_devinfo\n</code></pre> Couldn't load driver 'libmlx5-rdmav34.so' <p>If you see an error like this, you might have different versions for your OFED tools and libraries. Attempt after upgrading your user space libraries to match the version of your OFED tools like so:</p> <pre><code>sudo apt update\nsudo apt install libibverbs1 librdmacm1 rdma-core\n</code></pre> See an example output <p>In the example below, the <code>mlx5_0</code> interface is in Ethernet mode, while the <code>mlx5_1</code> interface is in Infiniband mode. Do not pay attention to the <code>transport</code> value which is always <code>InfiniBand</code>.</p> <pre><code>hca_id: mlx5_0\n        transport:                      InfiniBand (0)\n        fw_ver:                         28.38.1002\n        node_guid:                      48b0:2d03:00f4:07fb\n        sys_image_guid:                 48b0:2d03:00f4:07fb\n        vendor_id:                      0x02c9\n        vendor_part_id:                 4129\n        hw_ver:                         0x0\n        board_id:                       NVD0000000033\n        phys_port_cnt:                  1\n                port:   1\n                        state:                  PORT_ACTIVE (4)\n                        max_mtu:                4096 (5)\n                        active_mtu:             4096 (5)\n                        sm_lid:                 0\n                        port_lid:               0\n                        port_lmc:               0x00\n                        link_layer:             Ethernet\n\nhca_id: mlx5_1\n        transport:                      InfiniBand (0)\n        fw_ver:                         28.38.1002\n        node_guid:                      48b0:2d03:00f4:07fc\n        sys_image_guid:                 48b0:2d03:00f4:07fb\n        vendor_id:                      0x02c9\n        vendor_part_id:                 4129\n        hw_ver:                         0x0\n        board_id:                       NVD0000000033\n        phys_port_cnt:                  1\n                port:   1\n                        state:                  PORT_ACTIVE (4)\n                        max_mtu:                4096 (5)\n                        active_mtu:             4096 (5)\n                        sm_lid:                 0\n                        port_lid:               0\n                        port_lmc:               0x00\n                        link_layer:             InfiniBand\n</code></pre> <p>For Holoscan Networking, we want the NIC to use the ETH link layer. To switch the link layer mode, there are two possible options:</p> <ol> <li>On IGX Orin developer kits, you can switch that setting through the BIOS: see IGX Orin documentation.</li> <li> <p>On any system with a NVIDIA NIC (including the IGX Orin developer kits), you can run the commands below from a terminal:</p> <ol> <li> <p>Identify the PCI address of your NVIDIA NIC</p> ibdev2netdevlspci <pre><code>nic_pci=$(sudo ibdev2netdev -v | awk '{print $1}' | head -n1)\n</code></pre> <pre><code># `0200` is the PCI-SIG class code for Ethernet controllers\n# `0207` is the PCI-SIG class code for Infiniband controllers\n# `15b3` is the Vendor ID for Mellanox\nnic_pci=$(lspci -n | awk '($2 == \"0200:\" || $2 == \"0207:\") &amp;&amp; $3 ~ /^15b3:/ {print $1; exit}')\n</code></pre> </li> <li> <p>Set both link layers to Ethernet. <code>LINK_TYPE_P1</code> and <code>LINK_TYPE_P2</code> are for <code>mlx5_0</code> and <code>mlx5_1</code> respectively. You can choose to only set one of them. <code>ETH</code> or <code>2</code> is Ethernet mode, and <code>IB</code> or <code>1</code> is for InfiniBand.</p> <pre><code>sudo mlxconfig -d $nic_pci set LINK_TYPE_P1=ETH LINK_TYPE_P2=ETH\n</code></pre> <p>Apply with <code>y</code>.</p> See an example output <pre><code>Device #1:\n----------\n\nDevice type:    ConnectX7\nName:           P3740-B0-QSFP_Ax\nDescription:    NVIDIA Prometheus P3740 ConnectX-7 VPI PCIe Switch Motherboard; 400Gb/s; dual-port QSFP; PCIe switch5.0 X8 SLOT0 ;X16 SLOT2; secure boot;\nDevice:         0005:03:00.0\n\nConfigurations:                                      Next Boot       New\n        LINK_TYPE_P1                                ETH(2)          ETH(2)\n        LINK_TYPE_P2                                IB(1)           ETH(2)\n\nApply new Configuration? (y/n) [n] :\ny\n\nApplying... Done!\n-I- Please reboot machine to load new configurations.\n</code></pre> <ul> <li><code>Next Boot</code> is the current value that was expected to be used at the next reboot.</li> <li><code>New</code> is the value you're about to set to override <code>Next Boot</code>.</li> </ul> </li> <li> <p>Reboot your system.</p> <pre><code>sudo reboot\n</code></pre> </li> </ol> </li> </ol>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#23-configure-the-ip-addresses-of-the-nic-ports","title":"2.3 Configure the IP addresses of the NIC ports","text":"<p>First, we want to identify the logical names of your NIC interfaces. Connecting an SFP cable in just one of the ports of the NIC will help you identify which port is which. Run the following command once the cable is in place:</p> <pre><code>ibdev2netdev\n</code></pre> See an example output <p>In the example below, only <code>mlx5_1</code> has a cable connected (<code>Up</code>), and its logical ethernet name is <code>eth1</code>:</p> <pre><code>$ ibdev2netdev\nmlx5_0 port 1 ==&gt; eth0 (Down)\nmlx5_1 port 1 ==&gt; eth1 (Up)\n</code></pre> ibdev2netdev does not show the NIC <p>If you have a cable connected but it does not show Up/Down in the output of <code>ibdev2netdev</code>, you can try to parse the output of <code>dmesg</code> instead. The example below shows that <code>0005:03:00.1</code> is plugged, and that it is associated with <code>eth1</code>:</p> <pre><code>$ sudo dmesg | grep -w mlx5_core\n...\n[   11.512808] mlx5_core 0005:03:00.0 eth0: Link down\n[   11.640670] mlx5_core 0005:03:00.1 eth1: Link down\n...\n[ 3712.267103] mlx5_core 0005:03:00.1: Port module event: module 1, Cable plugged\n</code></pre> <p>The next step is to set a static IP on the interface you'd like to use so you can refer to it in your Holoscan applications. First, check if you already have any addresses configured using the ethernet interface names identified above (in our case, <code>eth0</code> and <code>eth1</code>):</p> <pre><code>ip -f inet addr show eth0\nip -f inet addr show eth1\n</code></pre> <p>If nothing appears, or you'd like to change the address, you can set an IP address through the Network Manager user interface, CLI (<code>nmcli</code>), or other IP configuration tools. In the example below, we configure the <code>eth0</code> interface with an address of <code>1.1.1.1/24</code>, and the <code>eth1</code> interface with an address of <code>2.2.2.2/24</code>.</p> One-timePersistent <pre><code>sudo ip addr add 1.1.1.1/24 dev eth0\nsudo ip addr add 2.2.2.2/24 dev eth1\n</code></pre> <p>Set these variables to your desired values:</p> <pre><code>if_name=eth0\nif_static_ip=1.1.1.1/24\n</code></pre> NetworkManagersystemd-networkd <p>Update the IP with <code>nmcli</code>:</p> <pre><code>sudo nmcli connection modify $if_name ipv4.addresses $if_static_ip\nsudo nmcli connection up $if_name\n</code></pre> <p>Create a network config file with the static IP:</p> <pre><code>cat &lt;&lt; EOF | sudo tee /etc/systemd/network/20-$if_name.network\n[Match]\nMACAddress=$(cat /sys/class/net/$if_name/address)\n\n[Network]\nAddress=$if_static_ip\nEOF\n</code></pre> <p>Apply now:</p> <pre><code>sudo systemctl restart systemd-networkd\n</code></pre> <p>Note</p> <p>If you are connecting the NIC to another NIC with a LinkX interconnect, do the same on the other system with an IP address on the same network segment. For example, to communicate with <code>1.1.1.1/24</code> above (<code>/24</code> -&gt; <code>255.255.255.0</code> submask), setup your other system with an IP between <code>1.1.1.2</code> and <code>1.1.1.254</code>, and the same <code>/24</code> submask.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#24-enable-gpudirect","title":"2.4 Enable GPUDirect","text":"<p>Assuming you already have NVIDIA drivers installed, check if the <code>nvidia_peermem</code> kernel module is loaded:</p> tune_system.py Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check topo\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check topo\n</code></pre> See an example output <pre><code>2025-03-12 14:15:07 - INFO - GPU 0: NVIDIA RTX A6000 has GPUDirect support.\n2025-03-12 14:15:27 - INFO - nvidia-peermem module is loaded.\n</code></pre> <pre><code>lsmod | grep nvidia_peermem\n</code></pre> <p>If it's not loaded, run the following command, then check again:</p> One-timePersistent <pre><code>sudo modprobe nvidia_peermem\n</code></pre> <pre><code>sudo echo \"nvidia-peermem\" &gt;&gt; /etc/modules\nsudo systemctl restart systemd-modules-load.service\n</code></pre> Error loading the <code>nvidia-peermem</code> kernel module <p>If you run into an error loading the <code>nvidia-peermem</code> kernel module, follow these steps:</p> <ol> <li>Install the <code>doca-ofed</code> package to get the latest drivers for your NIC as documented above.</li> <li>Restart your system.</li> <li>Rebuild your NVIDIA drivers with DKMS like so:</li> </ol> <pre><code>peermem_ko=$(find /lib/modules/$(uname -r) -name \"*peermem*.ko\")\nnv_dkms=$(dpkg -S \"$peermem_ko\" | cut -d: -f1)\nsudo dpkg-reconfigure $nv_dkms\nsudo modprobe nvidia_peermem\n</code></pre> Why peermem and not dma buf? <p><code>peermem</code> is currently the only GPUDirect interface supported by all our networking backends. This section will therefore provide instructions for <code>peermem</code> and not <code>dma buf</code>.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#3-optimal-system-configurations","title":"3. Optimal system configurations","text":"<p>Advanced</p> <p>The section below is for advanced users looking to extract more performance out of their system. You can choose to skip this section and return to it later if performance if your application is not satisfactory.</p> <p>While the configurations above are the minimum requirements to get a NIC and a NVIDIA GPU to communicate while bypassing the OS kernel stack, performance can be further improved in most scenarios by tuning the system as described below.</p> <p>Before diving in each of the setups below, we provide a utility script as part of the <code>holoscan-networking</code> package which provides an overview of the configurations that potentially need to be tuned on your system.</p> Work In Progress <p>This utility script is under active development and will be updated in future releases with additional checks, more actionable recommendations, and automated tuning.</p> Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check all\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check all\n</code></pre> See an example output <p>Our tuned-up IGX system with A6000 can optimize most settings:</p> <pre><code>2025-03-12 14:16:06 - INFO - CPU 0: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 1: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 2: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 3: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 4: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 5: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 6: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 7: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 8: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 9: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 10: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 11: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - cx7_0/0005:03:00.0: MRRS is correctly set to 4096.\n2025-03-12 14:16:06 - INFO - cx7_1/0005:03:00.1: MRRS is correctly set to 4096.\n2025-03-12 14:16:06 - WARNING - cx7_0/0005:03:00.0: PCIe Max Payload Size is not set to 256 bytes. Found: 128 bytes.\n2025-03-12 14:16:06 - WARNING - cx7_1/0005:03:00.1: PCIe Max Payload Size is not set to 256 bytes. Found: 128 bytes.\n2025-03-12 14:16:06 - INFO - HugePages_Total: 3\n2025-03-12 14:16:06 - INFO - HugePage Size: 1024.00 MB\n2025-03-12 14:16:06 - INFO - Total Allocated HugePage Memory: 3072.00 MB\n2025-03-12 14:16:06 - INFO - Hugepages are sufficiently allocated with at least 500 MB.\n2025-03-12 14:16:06 - INFO - GPU 0: SM Clock is correctly set to 1920 MHz (within 500 of the 2100 MHz theoretical Max).\n2025-03-12 14:16:06 - INFO - GPU 0: Memory Clock is correctly set to 8000 MHz.\n2025-03-12 14:16:06 - INFO - GPU 00000005:09:00.0: BAR1 size is 8192 MiB.\n2025-03-12 14:16:06 - INFO - GPU GPU0 has at least one PIX/PXB connection to a NIC\n2025-03-12 14:16:06 - INFO - isolcpus found in kernel boot line\n2025-03-12 14:16:06 - INFO - rcu_nocbs found in kernel boot line\n2025-03-12 14:16:06 - INFO - irqaffinity found in kernel boot line\n2025-03-12 14:16:06 - INFO - Interface cx7_0 has an acceptable MTU of 9000 bytes.\n2025-03-12 14:16:06 - INFO - Interface cx7_1 has an acceptable MTU of 9000 bytes.\n2025-03-12 14:16:06 - INFO - GPU 0: NVIDIA RTX A6000 has GPUDirect support.\n2025-03-12 14:16:06 - INFO - nvidia-peermem module is loaded.\n</code></pre> <p>Based on the results, you can figure out which of the sections below are appropriate to update configurations on your system.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#31-ensure-ideal-pcie-topology","title":"3.1 Ensure ideal PCIe topology","text":"<p>Kernel bypass and GPUDirect rely on PCIe to communicate between the GPU and the NIC at high speeds. As-such, the topology of the PCIe tree on a system is critical to ensure optimal performance.</p> <p>Run the following command to check the GPUDirect communication matrix. You are looking for a <code>PXB</code> or <code>PIX</code> connection between the GPU and the NIC interfaces to get the best performance.</p> tune_system.pynvidia-smi Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check topo\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check topo\n</code></pre> See an example output <p>On IGX developer kits, the board's internal switch is designed to connect the GPU to the NIC interfaces with a <code>PXB</code> connection, offering great performance.</p> <pre><code>2025-03-06 12:07:45 - INFO - GPU GPU0 has at least one PIX/PXB connection to a NIC\n</code></pre> <pre><code>nvidia-smi topo -mp\n</code></pre> See an example output <p>On IGX developer kits, the board's internal switch is designed to connect the GPU to the NIC interfaces with a <code>PXB</code> connection, offering great performance. <pre><code>        GPU0    NIC0    NIC1    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PXB     PXB     0-11    0               N/A\nNIC0    PXB      X      PIX\nNIC1    PXB     PIX      X\n\nLegend:\n\nX    = Self\nSYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\nNODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\nPHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\nPXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\nPIX  = Connection traversing at most a single PCIe bridge\n\nNIC Legend:\n\nNIC0: mlx5_0\nNIC1: mlx5_1\n</code></pre></p> <p>If your connection is not optimal, you might be able to improve it by moving your NIC and/or GPU on a different PCIe port, so that they can share a branch and do not require going back to the Host Bridge (the CPU) to communicate. Refer to your system manufacturer for documentation, or run the following command to inspect the topology of your system:</p> <pre><code>lspci -tv\n</code></pre> See an example output <p>Here is the PCIe tree of an IGX system. Note how the ConnectX-7 and RTX A6000 are connected to the same branch. <pre><code>-+-[0007:00]---00.0-[01-ff]----00.0  Marvell Technology Group Ltd. 88SE9235 PCIe 2.0 x2 4-port SATA 6 Gb/s Controller\n+-[0005:00]---00.0-[01-ff]----00.0-[02-09]--+-00.0-[03]--+-00.0  Mellanox Technologies MT2910 Family [ConnectX-7]\n|                                           |            \\-00.1  Mellanox Technologies MT2910 Family [ConnectX-7]\n|                                           +-01.0-[04-06]----00.0-[05-06]----08.0-[06]--\n|                                           \\-02.0-[07-09]----00.0-[08-09]----00.0-[09]--+-00.0  NVIDIA Corporation GA102GL [RTX A6000]\n|                                                                                        \\-00.1  NVIDIA Corporation GA102 High Definition Audio Controller\n+-[0004:00]---00.0-[01-ff]----00.0  Sandisk Corp WD PC SN810 / Black SN850 NVMe SSD\n+-[0001:00]---00.0-[01-ff]----00.0-[02-fc]--+-01.0-[03-34]----00.0  Realtek Semiconductor Co., Ltd. RTL8111/8168/8411 PCI Express Gigabit Ethernet Controller\n|                                           +-02.0-[35-66]----00.0  Realtek Semiconductor Co., Ltd. RTL8111/8168/8411 PCI Express Gigabit Ethernet Controller\n|                                           +-03.0-[67-98]----00.0  Device 1c00:3450\n|                                           +-04.0-[99-ca]----00.0-[9a]--+-00.0  ASPEED Technology, Inc. ASPEED Graphics Family\n|                                           |                            \\-02.0  ASPEED Technology, Inc. Device 2603\n|                                           \\-05.0-[cb-fc]----00.0  Realtek Semiconductor Co., Ltd. RTL8822CE 802.11ac PCIe Wireless Network Adapter\n\\-[0000:00]-\n</code></pre></p> <p>x86_64 compatibility</p> <p>Most x86_64 systems are not designed for this topology as they lack a discrete PCIe switch. In that case, the best connection they can achieve is <code>NODE</code>.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#32-check-the-nics-pcie-configuration","title":"3.2 Check the NIC's PCIe configuration","text":"<p>Understanding PCIe Configuration for Maximum Performance - May 27, 2022</p> <p>PCIe is used in any system for communication between different modules [including the NIC and the GPU]. This means that in order to process network traffic, the different devices communicating via the PCIe should be well configured. When connecting the network adapter to the PCIe, it auto-negotiates for the maximum capabilities supported between the network adapter and the CPU.</p> <p>The instructions below are meant to understand if your system is able to extract the maximum capabilities of your NIC, but they're not configurable. The two values that we are looking at here are the Max Payload Size (MPS - the maximum size of a PCIe packet) and the Speed (or PCIe generation).</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#max-payload-size-mps","title":"Max Payload Size (MPS)","text":"tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check mps\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check mps\n</code></pre> See an example output <p>The PCIe configuration on the IGX Orin developer kit is not able to leverage the max payload size of the NIC:</p> <pre><code>2025-03-10 16:15:54 - WARNING - cx7_0/0005:03:00.0: PCIe Max Payload Size is not set to 256 bytes. Found: 128 bytes.\n2025-03-10 16:15:54 - WARNING - cx7_1/0005:03:00.1: PCIe Max Payload Size is not set to 256 bytes. Found: 128 bytes.\n</code></pre> <p>Identify the PCIe address of your NVIDIA NIC:</p> ibdev2netdevlspci <pre><code>nic_pci=$(sudo ibdev2netdev -v | awk '{print $1}' | head -n1)\n</code></pre> <pre><code># `0200` is the PCI-SIG class code for NICs\n# `15b3` is the Vendor ID for Mellanox\nnic_pci=$(lspci -n | awk '$2 == \"0200:\" &amp;&amp; $3 ~ /^15b3:/ {print $1}' | head -n1)\n</code></pre> <p>Check current and max MPS:</p> <pre><code>sudo lspci -vv -s $nic_pci | awk '/DevCap/{s=1} /DevCtl/{s=0} /MaxPayload /{match($0, /MaxPayload [0-9]+/, m); if(s){print \"Max \" m[0]} else{print \"Current \" m[0]}}'\n</code></pre> See an example output <p>The PCIe configuration on the IGX Orin developer kit is not able to leverage the max payload size of the NIC:</p> <pre><code>Max MaxPayload 512\nCurrent MaxPayload 128\n</code></pre> <p>Note</p> <p>While your NIC might be capable of more, 256 bytes is generally the largest supported by any switch/CPU at this time.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#pcie-speedgeneration","title":"PCIe Speed/Generation","text":"<p>Identify the PCIe address of your NVIDIA NIC:</p> ibdev2netdevlspci <pre><code>nic_pci=$(sudo ibdev2netdev -v | awk '{print $1}' | head -n1)\n</code></pre> <pre><code># `0200` is the PCI-SIG class code for NICs\n# `15b3` is the Vendor ID for Mellanox\nnic_pci=$(lspci -n | awk '$2 == \"0200:\" &amp;&amp; $3 ~ /^15b3:/ {print $1}' | head -n1)\n</code></pre> <p>Check current and max Speeds:</p> <pre><code>sudo lspci -vv -s $nic_pci | awk '/LnkCap/{s=1} /LnkSta/{s=0} /Speed /{match($0, /Speed [0-9]+GT\\/s/, m); if(s){print \"Max \" m[0]} else{print \"Current \" m[0]}}'\n</code></pre> See an example output <p>On IGX, the switch is able to maximize the NIC speed, both being PCIe 5.0:</p> <pre><code>Max Speed 32GT/s\nCurrent Speed 32GT/s\n</code></pre>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#33-maximize-the-nics-max-read-request-size-mrrs","title":"3.3 Maximize the NIC's Max Read Request Size (MRRS)","text":"<p>Understanding PCIe Configuration for Maximum Performance - May 27, 2022</p> <p>PCIe Max Read Request determines the maximal PCIe read request allowed. A PCIe device usually keeps track of the number of pending read requests due to having to prepare buffers for an incoming response. The size of the PCIe max read request may affect the number of pending requests (when using data fetch larger than the PCIe MTU).</p> <p>Unlike the PCIe properties queried in the previous section, the MRRS is configurable. We recommend maxing it to 4096 bytes. Run the following to check your current settings:</p> tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check mrrs\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check mrrs\n</code></pre> <p>Identify the PCIe address of your NVIDIA NIC:</p> ibdev2netdevlspci <pre><code>nic_pci=$(sudo ibdev2netdev -v | awk '{print $1}' | head -n1)\n</code></pre> <pre><code># `0200` is the PCI-SIG class code for NICs\n# `15b3` is the Vendor ID for Mellanox\nnic_pci=$(lspci -n | awk '$2 == \"0200:\" &amp;&amp; $3 ~ /^15b3:/ {print $1}' | head -n1)\n</code></pre> <p>Check current MRRS:</p> <pre><code>sudo lspci -vv -s $nic_pci | grep DevCtl: -A2 | grep -oE \"MaxReadReq [0-9]+\"\n</code></pre> <p>Update MRRS:</p> Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --set mrrs\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --set mrrs\n</code></pre> <p>Note</p> <p>This value is reset on reboot and needs to be set every time the system boots</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#34-enable-huge-pages","title":"3.4 Enable Huge pages","text":"<p>Huge pages are a memory management feature that allows the OS to allocate large blocks of memory (typically 2MB or 1GB) instead of the default 4KB pages. This reduces the number of page table entries and the amount of memory used for translation, improving cache performance and reducing TLB (Translation Lookaside Buffer) misses, which leads to lower latencies.</p> <p>While it is naturally beneficial for CPU packets, it is also needed when routing data packets to the GPU in order to handle metadata (mbufs) on the CPU.</p> hugeadmvanilla <p>We recommend installing the <code>libhugetlbfs-bin</code> package for the <code>hugeadm</code> utility:</p> <pre><code>sudo apt update\nsudo apt install -y libhugetlbfs-bin\n</code></pre> <p>Then, check your huge page pools:</p> <pre><code>hugeadm --pool-list\n</code></pre> See an example output <p>The example below shows that this system supports huge pages of 64K, 2M (default), 32M, and 1G, but that none of them are currently allocated.</p> <pre><code>      Size  Minimum  Current  Maximum  Default\n     65536        0        0        0\n   2097152        0        0        0        *\n  33554432        0        0        0\n1073741824        0        0        0\n</code></pre> <p>And your huge page mount points:</p> <pre><code>hugeadm --list-all-mounts\n</code></pre> See an example output <p>The default huge pages are mounted on <code>/dev/hugepages</code> with a page size of 2M.</p> <pre><code>Mount Point          Options\n/dev/hugepages       rw,relatime,pagesize=2M\n</code></pre> <p>First, check your huge page pools:</p> <pre><code>ls -1 /sys/kernel/mm/hugepages/\ngrep Huge /proc/meminfo\n</code></pre> See an example output <p>The example below shows that this system supports huge pages of 64K, 2M (default), 32M, and 1G, but that none of them are currently allocated.</p> <pre><code>hugepages-1048576kB\nhugepages-2048kB\nhugepages-32768kB\nhugepages-64kB\n</code></pre> <pre><code>HugePages_Total:       0\nHugePages_Free:        0\nHugePages_Rsvd:        0\nHugePages_Surp:        0\nHugepagesize:       2048 kB\nHugetlb:               0 kB\n</code></pre> <p>And your huge page mount points:</p> <pre><code>mount | grep huge\n</code></pre> See an example output <p>The default huge pages are mounted on <code>/dev/hugepages</code> with a page size of 2M.</p> <pre><code>hugetlbfs on /dev/hugepages type hugetlbfs (rw,relatime,pagesize=2M)\n</code></pre> <p>As a rule of thumb, we recommend to start with 3 to 4 GB of total huge pages, with an individual page size of 500 MB to 1 GB (per system availability).</p> <p>There are two ways to allocate huge pages:</p> <ul> <li>in the kernel bootline (recommended to ensure contiguous memory allocation) or</li> <li>dynamically at runtime (risk of fragmentation for large page sizes)</li> </ul> <p>The example below allocates 3 huge pages of 1GB each.</p> Kernel bootlineRuntime <p>Add the flags below to the <code>GRUB_CMDLINE_LINUX</code> variable in <code>/etc/default/grub</code>:</p> <pre><code>default_hugepagesz=1G hugepagesz=1G hugepages=3\n</code></pre> Show explanation <ul> <li><code>default_hugepagesz</code>: the default huge page size to use, making them available from the default mount point, <code>/dev/hugepages</code>.</li> <li><code>hugepagesz</code>: the size of the huge pages to allocate.</li> <li><code>hugepages</code>: the number of huge pages to allocate.</li> </ul> <p>Then rebuild your GRUB configuration and reboot:</p> <pre><code>sudo update-grub\nsudo reboot\n</code></pre> <p>Allocate the 3x 1GB huge pages:</p> hugeadmvanilla <pre><code>sudo hugeadm --pool-pages-min 1073741824:3\n</code></pre> <pre><code>echo 3 | sudo tee /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages\n</code></pre> <p>Create a mount point to access the 1GB huge pages pool since that is not the default size on that system. We will name it <code>/mnt/huge</code> here.</p> One-timePersistent <pre><code>sudo mkdir -p /mnt/huge\nsudo mount -t hugetlbfs -o pagesize=1G none /mnt/huge\n</code></pre> <pre><code>echo \"nodev /mnt/huge hugetlbfs pagesize=1G 0 0\" | sudo tee -a /etc/fstab\nsudo mount /mnt/huge\n</code></pre> <p>Note</p> <p>If you work with containers, remember to mount this directory in your container as well with <code>-v /mnt/huge:/mnt/huge</code>.</p> <p>Rerunning the initial commands should now list 3 hugepages of 1GB each. 1GB will be the default huge page size if updated in the kernel bootline only.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#35-isolate-cpu-cores","title":"3.5 Isolate CPU cores","text":"<p>Note</p> <p>This optimization is less impactful when using the <code>gpunetio</code> backend since the GPU polls the NIC.</p> <p>The CPU interacting with the NIC to route packets is sensitive to perturbations, especially with smaller packet/batch sizes requiring more frequent work. Isolating a CPU in Linux prevents unwanted user or kernel threads from running on it, reducing context switching and latency spikes from noisy neighbors.</p> <p>We recommend isolating the CPU cores you will select to interact with the NIC (defined in the <code>advanced_network</code> configuration described later in this tutorial). This is done by setting additional flags on the kernel bootline.</p> <p>You can first check if any of the recommended flags were already set on the last boot:</p> tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check cmdline\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check cmdline\n</code></pre> <pre><code>cat /proc/cmdline | grep -e isolcpus -e irqaffinity -e nohz_full -e rcu_nocbs -e rcu_nocb_poll\n</code></pre> <p>Decide which cores to isolate based on your configuration. We recommend one core per queue as a rule of thumb. First, identify your core IDs:</p> <pre><code>cat /proc/cpuinfo | grep processor\n</code></pre> See an example output <p>This system has 12 cores, numbered 0 to 11: <pre><code>processor       # 0\nprocessor       # 1\nprocessor       # 2\nprocessor       # 3\nprocessor       # 4\nprocessor       # 5\nprocessor       # 6\nprocessor       # 7\nprocessor       # 8\nprocessor       # 9\nprocessor       # 10\nprocessor       # 11\n</code></pre></p> <p>As an example, the line below will isolate cores 9, 10 and 11, leaving cores 0-8 free for other tasks and hardware interrupts:</p> <pre><code>isolcpus=9-11 irqaffinity=0-8 nohz_full=9-11 rcu_nocbs=9-11 rcu_nocb_poll\n</code></pre> Show explanation Parameter Description <code>isolcpus</code> Isolates specific CPU cores from the Linux scheduler, preventing regular system tasks from running on them. This ensures dedicated cores are available exclusively for your networking tasks, reducing context switches and interruptions that can cause latency spikes. <code>irqaffinity</code> Controls which CPU cores can handle hardware interrupts. By directing network interrupts away from your isolated cores, you prevent networking tasks from being interrupted by hardware events, maintaining consistent processing time. <code>nohz_full</code> Disables regular kernel timer ticks on specified cores when they're running user space applications. This reduces overhead and prevents periodic interruptions, allowing your networking code to run with fewer disturbances. <code>rcu_nocbs</code> Offloads Read-Copy-Update (RCU) callback processing from specified cores. RCU is a synchronization mechanism in the Linux kernel that can cause periodic processing bursts. Moving this work away from your networking cores helps maintain consistent performance. <code>rcu_nocb_poll</code> Works with <code>rcu_nocbs</code> to improve how RCU callbacks are processed on non-callback CPUs. This can reduce latency spikes by changing how the kernel polls for RCU work. <p>Together, these parameters create an environment where specific CPU cores can focus exclusively on network packet processing with minimal interference from the operating system, resulting in lower and more consistent latency.</p> <p>Add these flags to the <code>GRUB_CMDLINE_LINUX</code> variable in <code>/etc/default/grub</code>, then rebuild your GRUB configuration and reboot:</p> <pre><code>sudo update-grub\nsudo reboot\n</code></pre> <p>Verify that the flags were properly set after boot by rerunning the check commands above.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#36-prevent-cpu-cores-from-going-idle","title":"3.6 Prevent CPU cores from going idle","text":"<p>When a core goes idle/to sleep, coming back online to poll the NIC can cause latency spikes and dropped packets. To prevent this, we recommend setting the scaling governor to <code>performance</code> for these CPU cores.</p> <p>Note</p> <p>Cores from a single cluster will always share the same governor.</p> <p>Bug</p> <p>We have witnessed instances where setting the governor to <code>performance</code> on only the isolated cores (dedicated to polling the NIC) does not lead to the performance gains expected. As such, we currently recommend setting the governor to <code>performance</code> for all cores which has shown to be reliably effective.</p> <p>Check the current governor for each of your cores:</p> tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check cpu-freq\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check cpu-freq\n</code></pre> See an example output <pre><code>2025-03-06 12:20:27 - WARNING - CPU 0: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 1: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 2: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 3: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 4: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 5: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 6: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 7: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 8: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 9: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 10: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 11: Governor is set to 'powersave', not 'performance'.\n</code></pre> <pre><code>cat /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor\n</code></pre> See an example output <p>In this example, all cores were defaulted to <code>powersave</code> instead of the recommended <code>performance</code>.</p> <pre><code>powersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\n</code></pre> <p>Install <code>cpupower</code> to more conveniently set the governor:</p> <pre><code>sudo apt update\nsudo apt install -y linux-tools-$(uname -r)\n</code></pre> <p>Set the governor to <code>performance</code> for all cores:</p> One-timePersistent <pre><code>sudo cpupower frequency-set -g performance\n</code></pre> <pre><code>cat &lt;&lt; EOF | sudo tee /etc/systemd/system/cpu-performance.service\n[Unit]\nDescription=Set CPU governor to performance\nAfter=multi-user.target\n\n[Service]\nType=oneshot\nExecStart=/usr/bin/cpupower -c all frequency-set -g performance\n\n[Install]\nWantedBy=multi-user.target\nEOF\nsudo systemctl enable cpu-performance.service\nsudo systemctl start cpu-performance.service\n</code></pre> <p>Running the checks above should now list <code>performance</code> as the governor for all cores. You can also run <code>sudo cpupower -c all frequency-info</code> for more details.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#37-prevent-the-gpu-from-going-idle","title":"3.7 Prevent the GPU from going idle","text":"<p>Similarly to the above, we want to maximize the GPU's clock speed and prevent it from going idle.</p> <p>Run the following command to check your current clocks and whether they're locked (persistence mode):</p> <pre><code>nvidia-smi -q | grep -i \"Persistence Mode\"\nnvidia-smi -q -d CLOCK\n</code></pre> See an example output <pre><code>    Persistence Mode: Enabled\n...\nAttached GPUs                             : 1\nGPU 00000005:09:00.0\n    Clocks\n        Graphics                          : 420 MHz\n        SM                                : 420 MHz\n        Memory                            : 405 MHz\n        Video                             : 1680 MHz\n    Applications Clocks\n        Graphics                          : 1800 MHz\n        Memory                            : 8001 MHz\n    Default Applications Clocks\n        Graphics                          : 1800 MHz\n        Memory                            : 8001 MHz\n    Deferred Clocks\n        Memory                            : N/A\n    Max Clocks\n        Graphics                          : 2100 MHz\n        SM                                : 2100 MHz\n        Memory                            : 8001 MHz\n        Video                             : 1950 MHz\n    ...\n</code></pre> <p>To lock the GPU's clocks to their max values:</p> One-timePersistent <pre><code>sudo nvidia-smi -pm 1\nsudo nvidia-smi -lgc=$(nvidia-smi --query-gpu=clocks.max.sm --format=csv,noheader,nounits)\nsudo nvidia-smi -lmc=$(nvidia-smi --query-gpu=clocks.max.mem --format=csv,noheader,nounits)\n</code></pre> <pre><code>cat &lt;&lt; EOF | sudo tee /etc/systemd/system/gpu-max-clocks.service\n[Unit]\nDescription=Max GPU clocks\nAfter=multi-user.target\n\n[Service]\nType=oneshot\nExecStart=/usr/bin/nvidia-smi -pm 1\nExecStart=/bin/bash -c '/usr/bin/nvidia-smi --lock-gpu-clocks=$(/usr/bin/nvidia-smi --query-gpu=clocks.max.sm --format=csv,noheader,nounits)'\nExecStart=/bin/bash -c '/usr/bin/nvidia-smi --lock-memory-clocks=$(/usr/bin/nvidia-smi --query-gpu=clocks.max.mem --format=csv,noheader,nounits)'\nRemainAfterExit=true\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\nsudo systemctl enable gpu-max-clocks.service\nsudo systemctl start gpu-max-clocks.service\n</code></pre> Show explanation <p>This queries the max clocks for the GPU SM (<code>clocks.max.sm</code>) and memory (<code>clocks.max.mem</code>) and sets them to the current clocks (<code>lock-gpu-clocks</code> and <code>lock-memory-clocks</code> respectively). <code>-pm 1</code> (or <code>--persistence-mode=1</code>) enables persistence mode to lock these values.</p> See an example output <pre><code>GPU clocks set to \"(gpuClkMin 2100, gpuClkMax 2100)\" for GPU 00000005:09:00.0\nAll done.\nMemory clocks set to \"(memClkMin 8001, memClkMax 8001)\" for GPU 00000005:09:00.0\nAll done.\n</code></pre> <p>You can confirm that the clocks are set to the max values by running <code>nvidia-smi -q -d CLOCK</code> again.</p> <p>Note</p> <p>Some max clocks might not be achievable in certain configurations, or due to boost clocks (SM) or rounding errors (Memory),  despite the lock commands indicating it worked. For example - on IGX - the max non-boot SM clock will be 1920 MHz, and the max memory clock will show 8000 MHz, which are satisfying compared to the initial mode.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#38-maximize-gpu-bar1-size","title":"3.8 Maximize GPU BAR1 size","text":"<p>The GPU BAR1 memory is the primary resource consumed by <code>GPUDirect</code>. It allows other PCIe devices (like the CPU and the NIC) to access the GPU's memory space. The larger the BAR1 size, the more memory the GPU can expose to these devices in a single PCIe transaction, reducing the number of transactions needed and improving performance.</p> <p>We recommend a BAR1 size of 1GB or above. Check the current BAR1 size:</p> tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check bar1-size\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check bar1-size\n</code></pre> See an example output <pre><code>2025-03-06 12:22:53 - INFO - GPU 00000005:09:00.0: BAR1 size is 8192 MiB.\n</code></pre> <pre><code>nvidia-smi -q | grep -A 3 BAR1\n</code></pre> See an example output <p>For our RTX A6000, this shows a BAR1 size of 256 MiB:</p> <pre><code>    BAR1 Memory Usage\n    Total                             : 256 MiB\n    Used                              : 13 MiB\n    Free                              : 243 MiB\n</code></pre> <p>Warning</p> <p>Resizing the BAR1 size requires:</p> <ul> <li>A BIOS with resizable BAR support</li> <li>A GPU with physical resizable BAR</li> </ul> <p>If you attempt to go forward with the instructions below without meeting the above requirements, you might render your GPU unusable.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#bios-resizable-bar-support","title":"BIOS Resizable BAR support","text":"<p>First, check if your system and BIOS support resizable BAR. Refer to your system's manufacturer documentation to access the BIOS. The Resizable BAR option is often categorized under <code>Advanced &gt; PCIe</code> settings. Enable this feature if found.</p> <p>Note</p> <p>The IGX Developer kit with IGX OS 1.1+ supports resizable BAR by default.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#gpu-resizable-bar-support","title":"GPU Resizable BAR support","text":"<p>Next, you can check if your GPU has physical resizable BAR by running the following command:</p> <pre><code>sudo lspci -vv -s $(nvidia-smi --query-gpu=pci.bus_id --format=csv,noheader) | grep BAR\n</code></pre> See an example output <p>This RTX A6000 has a resizable BAR1, currently set to 256 MiB:</p> <pre><code>Capabilities: [bb0 v1] Physical Resizable BAR\n    BAR 0: current size: 16MB, supported: 16MB\n    BAR 1: current size: 256MB, supported: 64MB 128MB 256MB 512MB 1GB 2GB 4GB 8GB 16GB 32GB 64GB\n    BAR 3: current size: 32MB, supported: 32MB\n</code></pre> <p>If your GPU is listed on this page, you can download the <code>Display Mode Selector</code> to resize the BAR1 to 8GB.</p> <ol> <li>Press <code>Join Now</code>.</li> <li>Once approved, download the <code>Display Mode Selector</code> archive.</li> <li>Unzip the archive.</li> <li>Access your system without a X-server running, either through SSH or a Virtual Console (<code>Alt+F1</code>).</li> <li>Go down the right OS and architecture folder for your system (<code>linux/aarch64</code> or <code>linux/x64</code>).</li> <li>Run the <code>displaymodeselector</code> command like so:</li> </ol> <pre><code>chmod +x displaymodeselector\nsudo ./displaymodeselector --gpumode physical_display_enabled_8GB_bar1\n</code></pre> <p>Press <code>y</code> to confirm you'd like to continue, then <code>y</code> again to apply to all the eligible adapters.</p> See an example output <pre><code>NVIDIA Display Mode Selector Utility (Version 1.67.0)\nCopyright (C) 2015-2021, NVIDIA Corporation. All Rights Reserved.\n\nWARNING: This operation updates the firmware on the board and could make\n        the device unusable if your host system lacks the necessary support.\n\nAre you sure you want to continue?\nPress 'y' to confirm (any other key to abort):\ny\nSpecified GPU Mode \"physical_display_enabled_8GB_bar1\"\n\n\nUpdate GPU Mode of all adapters to \"physical_display_enabled_8GB_bar1\"?\nPress 'y' to confirm or 'n' to choose adapters or any other key to abort:\ny\n\nUpdating GPU Mode of all eligible adapters to \"physical_display_enabled_8GB_bar1\"\n\nApply GPU Mode &lt;6&gt; corresponds to \"physical_display_enabled_8GB_bar1\"\n\nReading EEPROM (this operation may take up to 30 seconds)\n\n[==================================================] 100 %\nReading EEPROM (this operation may take up to 30 seconds)\n\nSuccessfully updated GPU mode to \"physical_display_enabled_8GB_bar1\" ( Mode 6 ).\n\nA reboot is required for the update to take effect.\n</code></pre> Error: unload the NVIDIA kernel driver first <p>If you see this error:</p> <pre><code>ERROR: In order to avoid the irreparable damage to your graphics adapter it is necessary to unload the NVIDIA kernel driver first:\n\nrmmod nvidia_uvm nvidia_drm nvidia_modeset nvidia_peermem nvidia\n</code></pre> <p>Try to unload the NVIDIA kernel driver listed in the error message above (list may vary):</p> <pre><code>sudo rmmod nvidia_uvm nvidia_drm nvidia_modeset nvidia_peermem nvidia\n</code></pre> <p>If this fails because the drivers are in use, stop the X-server first before trying again:</p> <pre><code>sudo systemctl isolate multi-user\n</code></pre> <p>Reboot your system, and check the BAR1 size again to confirm the change.</p> <pre><code>sudo reboot\n</code></pre>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#39-enable-jumbo-frames","title":"3.9 Enable Jumbo Frames","text":"<p>Jumbo frames are Ethernet frames that carry a payload larger than the standard 1500 bytes MTU (Maximum Transmission Unit). They can significantly improve network performance when transferring large amounts of data by reducing the overhead of packet headers and the number of packets that need to be processed.</p> <p>We recommend an MTU of 9000 bytes on all interfaces involved in the data path. You can check the current MTU of your interfaces:</p> tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check mtu\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check mtu\n</code></pre> See an example output <pre><code>2025-03-06 16:51:19 - INFO - Interface eth0 has an acceptable MTU of 9000 bytes.\n2025-03-06 16:51:19 - INFO - Interface eth1 has an acceptable MTU of 9000 bytes.\n</code></pre> <p>For a given <code>if_name</code> interface:</p> <pre><code>if_name=eth0\nip link show dev $if_name | grep -oE \"mtu [0-9]+\"\n</code></pre> See an example output <pre><code>mtu 1500\n</code></pre> <p>You can set the MTU for each interface like so, for a given <code>if_name</code> name identified above:</p> One-timePersistent <pre><code>sudo ip link set dev $if_name mtu 9000\n</code></pre> NetworkManagersystemd-networkd <pre><code>sudo nmcli connection modify $if_name ipv4.mtu 9000\nsudo nmcli connection up $if_name\n</code></pre> <p>Assuming you've set an IP address for the interface above, you can add the MTU to the interface's network configuration file like so:</p> <pre><code>sudo sed -i '/\\[Network\\]/a MTU=9000' /etc/systemd/network/20-$if_name.network\nsudo systemctl restart systemd-networkd\n</code></pre> Can I do more than 9000? <p>While your NIC might have a maximum MTU capability larger than 9000, we typically recommend setting the MTU to 9000 bytes, as that is the standard size for jumbo frames that's widely supported for compatibility with other network equipment. When using jumbo frames, all devices in the communication path must support the same MTU size. If any device in between has a smaller MTU, packets will be fragmented or dropped, potentially degrading performance.</p> <p>Example with the CX-7 NIC:</p> <pre><code>$ ip -d link show dev $if_name | grep -oE \"maxmtu [0-9]+\"\nmaxmtu 9978\n</code></pre>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#4-running-a-test-application","title":"4. Running a test application","text":"<p>Holoscan Networking provides a benchmarking application named <code>adv_networking_bench</code> that can be used to test the performance of the networking configuration. In this section, we'll walk you through the steps needed to configure the application for your NIC for Tx and Rx, and run a loopback test between the two interfaces with a physical link between them.</p> <p>Before continuing, ensure you have properly installed the <code>holoscan-networking</code> package as part of the installation instructions, whether from a Debian package, or building from the HoloHub source repository.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#41-update-the-loopback-configuration","title":"4.1 Update the loopback configuration","text":"","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#find-the-application-files","title":"Find the application files","text":"<p>Identify the location of the <code>adv_networking_bench</code> executable, and of the configuration file named <code>adv_networking_bench_default_tx_rx.yaml</code>, for your installation:</p> Debian installationFrom source <p>Both located under <code>/opt/nvidia/holoscan/examples/adv_networking_bench/</code>:</p> <pre><code>ls -1 /opt/nvidia/holoscan/examples/adv_networking_bench/\nadv_networking_bench\nadv_networking_bench_default_rx_multi_q.yaml\nadv_networking_bench_default_tx_rx_hds.yaml\nadv_networking_bench_default_tx_rx.yaml\nadv_networking_bench_gpunetio_tx_rx.yaml\nadv_networking_bench_rmax_rx.yaml\nCMakeLists.txt\ndefault_bench_op_rx.h\ndefault_bench_op_tx.h\ndoca_bench_op_rx.h\ndoca_bench_op_tx.h\nkernels.cu\nkernels.cuh\nmain.cpp\n</code></pre> <p>Both located under <code>./install/examples/adv_networking_bench/</code></p> <pre><code>ls -1 ./install/examples/adv_networking_bench\nadv_networking_bench\nadv_networking_bench_default_rx_multi_q.yaml\nadv_networking_bench_default_tx_rx_hds.yaml\nadv_networking_bench_default_tx_rx.yaml\nadv_networking_bench_gpunetio_tx_rx.yaml\nadv_networking_bench.py\nadv_networking_bench_rmax_rx.yaml\nCMakeLists.txt\ndefault_bench_op_rx.h\ndefault_bench_op_tx.h\ndoca_bench_op_rx.h\ndoca_bench_op_tx.h\nkernels.cu\nkernels.cuh\nmain.cpp\n</code></pre> <p>Warning</p> <p>The configuration file is also located alongide the application source code at <code>applications/adv_networking_bench/adv_networking_bench_default_tx_rx.yaml</code>. However, modifying this file will not affect the configuration used by the application executable without rebuilding the application.</p> <p>For this reason, we recommend using the configuration file located in the install tree.</p> <p>Note</p> <p>The fields in this <code>yaml</code> file will be explained in more details in a section below. For now, we'll stick to modifying the strict minimum required fields to run the application as-is on your system.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#identify-your-nics-pcie-addresses","title":"Identify your NIC's PCIe addresses","text":"<p>Retrieve the PCIe addresses of both ports of your NIC. We'll arbitrarily use the first for Tx and the second for Rx here:</p> ibdev2netdevlspci <pre><code>sudo ibdev2netdev -v | awk '{print $1}'\n</code></pre> <pre><code># `0200` is the PCI-SIG class code for NICs\n# `15b3` is the Vendor ID for Mellanox\nlspci -n | awk '$2 == \"0200:\" &amp;&amp; $3 ~ /^15b3:/ {print $1}'\n</code></pre> See an example output <pre><code>0005:03:00.0\n0005:03:00.1\n</code></pre>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#configure-the-nic-for-tx-and-rx","title":"Configure the NIC for Tx and Rx","text":"<p>Set the NIC addresses in the <code>interfaces</code> section of the <code>advanced_network</code> section. This configures your NIC independently of your application:</p> <ul> <li>Set the <code>address</code> field of the <code>tx_port</code> interface to one of these addresses. That interface will be able to transmit ethernet packets.</li> <li>Set the <code>address</code> field of the <code>rx_port</code> interface to the other address. This interface will be able to receive ethernet packets.</li> </ul> <pre><code>interfaces:\n    - name: \"tx_port\"\n    address: &lt;0000:00:00.0&gt;       # The BUS address of the interface doing Tx\n    tx:\n        ...\n    - name: \"rx_port\"\n    address: &lt;0000:00:00.0&gt;       # The BUS address of the interface doing Rx\n    rx:\n        ...\n</code></pre> See an example yaml <pre><code>interfaces:\n    - name: \"tx_port\"\n    address: 0005:03:00.0       # The BUS address of the interface doing Tx\n    tx:\n        ...\n    - name: \"rx_port\"\n    address: 0005:03:00.1       # The BUS address of the interface doing Rx\n    rx:\n        ...\n</code></pre>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#configure-the-application","title":"Configure the application","text":"<p>Modify the <code>bench_tx</code> section which configures the application itself, to create the packet headers and direct them to the NIC:</p> <ul> <li><code>eth_dst_addr</code> with the MAC address (and not the PCIe address) of the NIC interface you want to use for Rx. You can get the MAC address of your <code>if_name</code> interface with <code>cat /sys/class/net/$if_name/address</code>:</li> <li>Replacing <code>address</code> with the PCIe address of the NIC interface you want to use for Tx (same as <code>tx_port</code>'s address above).</li> </ul> <pre><code>bench_tx:\n    ...\n    eth_dst_addr: &lt;00:00:00:00:00:00&gt; # Destination MAC address - required when Rx flow_isolation=true\n    ip_src_addr: &lt;1.2.3.4&gt;  # Source IP address - required on layer 3 network\n    ip_dst_addr: &lt;5.6.7.8&gt;  # Destination IP address - required on layer 3 network\n    udp_src_port: 4096      # UDP source port\n    udp_dst_port: 4096      # UDP destination port\n    address: &lt;0000:00:00.0&gt; # Source NIC Bus ID. Should match the address of the Tx interface above\n</code></pre> Show explanation <ul> <li><code>eth_dst_addr</code> - the destination ethernet MAC address - will be embedded in the packet headers by the application. This is required here because the Rx interface above has <code>flow_isolation: true</code> (explained in more details below). In that configuration, only the packets listing the adequate destination MAC address will be accepted by the Rx interface.</li> <li>We ignore the IP fields (<code>ip_src_addr</code>, <code>ip_dst_addr</code>) for now, as we are testing on a layer 2 network by just connecting a cable between the two interfaces on our system, therefore having mock values has no impact.</li> <li><code>address</code> - the source PCIe address - needs to be defined again to tell the application itself to route the packets to the NIC interface we have configured previously for Tx.</li> <li>You might have noted the lack of a <code>eth_src_addr</code> field in the <code>bench_tx</code> section. This is because the source Ethernet MAC address can be inferred automatically from the PCIe address of the Tx interface (below).</li> </ul> See an example yaml <pre><code>bench_tx:\n    ...\n    eth_dst_addr: 48:b0:2d:ee:83:ad # Destination MAC address - required when Rx flow_isolation=true\n    ip_src_addr: &lt;1.2.3.4&gt;  # Source IP address - required on layer 3 network\n    ip_dst_addr: &lt;5.6.7.8&gt;  # Destination IP address - required on layer 3 network\n    udp_src_port: 4096      # UDP source port\n    udp_dst_port: 4096      # UDP destination port\n    address: 0005:03:00.0  # Source NIC Bus ID. Should match the address of the Tx interface above\n</code></pre>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#42-run-the-loopback-test","title":"4.2 Run the loopback test","text":"Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/examples/adv_networking_bench/adv_networking_bench adv_networking_bench_default_tx_rx.yaml\n</code></pre> <pre><code>sudo ./install/examples/adv_networking_bench/adv_networking_bench adv_networking_bench_default_tx_rx.yaml\n</code></pre> <p>The application will run indefinitely. You can stop it gracefully with <code>Ctrl-C</code>. You can also uncomment and set the <code>max_duration_ms</code> field in the <code>scheduler</code> section of the configuration file to limit the duration of the run automatically.</p> See an example output <pre><code>[info] [fragment.cpp:599] Loading extensions from configs...\n[info] [gxf_executor.cpp:264] Creating context\n[info] [main.cpp:35] Initializing advanced network operator\n[info] [main.cpp:40] Using ANO manager dpdk\n[info] [adv_network_rx.cpp:35] Adding output port bench_rx_out\n[info] [adv_network_rx.cpp:51] AdvNetworkOpRx::initialize()\n[info] [adv_network_common.h:607] Finished reading advanced network operator config\n[info] [adv_network_dpdk_mgr.cpp:373] Attempting to use 2 ports for high-speed network\n[info] [adv_network_dpdk_mgr.cpp:382] Setting DPDK log level to: Info\n[info] [adv_network_dpdk_mgr.cpp:402] DPDK EAL arguments: adv_net_operator --file-prefix=nwlrbbmqbh -l 3,11,9 --log-level=9 --log-level=pmd.net.mlx5:info -a 0005:03:00.0,txq_inline_max=0,dv_flow_en=2 -a 0005:03:00.1,txq_inline_max=0,dv_flow_en=2\nLog level 9 higher than maximum (8)\nEAL: Detected CPU lcores: 12\nEAL: Detected NUMA nodes: 1\nEAL: Detected shared linkage of DPDK\nEAL: Multi-process socket /var/run/dpdk/nwlrbbmqbh/mp_socket\nEAL: Selected IOVA mode 'VA'\nEAL: 1 hugepages of size 1073741824 reserved, but no mounted hugetlbfs found for that size\nEAL: Probe PCI driver: mlx5_pci (15b3:1021) device: 0005:03:00.0 (socket -1)\nmlx5_net: PCI information matches for device \"mlx5_0\"\nmlx5_net: enhanced MPS is enabled\nmlx5_net: port 0 MAC address is 48:B0:2D:EE:83:AC\nEAL: Probe PCI driver: mlx5_pci (15b3:1021) device: 0005:03:00.1 (socket -1)\nmlx5_net: PCI information matches for device \"mlx5_1\"\nmlx5_net: enhanced MPS is enabled\nmlx5_net: port 1 MAC address is 48:B0:2D:EE:83:AD\nTELEMETRY: No legacy callbacks, legacy socket not created\n[info] [adv_network_dpdk_mgr.cpp:298] Port 0 has no RX queues. Creating dummy queue.\n[info] [adv_network_dpdk_mgr.cpp:165] Adjusting buffer size to 9228 for headroom\n[info] [adv_network_dpdk_mgr.cpp:165] Adjusting buffer size to 9128 for headroom\n[info] [adv_network_dpdk_mgr.cpp:165] Adjusting buffer size to 9128 for headroom\n[info] [adv_network_mgr.cpp:116] Registering memory regions\n[info] [adv_network_mgr.cpp:178] Successfully allocated memory region MR_Unused_P0 at 0x100fa0000 type 2 with 9100 bytes (32768 elements @ 9228 bytes total 302383104)\n[info] [adv_network_mgr.cpp:178] Successfully allocated memory region Data_RX_GPU at 0xffff4fc00000 type 3 with 9000 bytes (51200 elements @ 9128 bytes total 467402752)\n[info] [adv_network_mgr.cpp:178] Successfully allocated memory region Data_TX_GPU at 0xffff33e00000 type 3 with 9000 bytes (51200 elements @ 9128 bytes total 467402752)\n[info] [adv_network_mgr.cpp:191] Finished allocating memory regions\n[info] [adv_network_dpdk_mgr.cpp:223] Successfully registered external memory for Data_TX_GPU\n[info] [adv_network_dpdk_mgr.cpp:223] Successfully registered external memory for Data_RX_GPU\n[info] [adv_network_dpdk_mgr.cpp:193] Mapped external memory descriptor for 0xffff4fc00000 to device 0\n[info] [adv_network_dpdk_mgr.cpp:193] Mapped external memory descriptor for 0xffff33e00000 to device 0\n[info] [adv_network_dpdk_mgr.cpp:193] Mapped external memory descriptor for 0xffff4fc00000 to device 1\n[info] [adv_network_dpdk_mgr.cpp:193] Mapped external memory descriptor for 0xffff33e00000 to device 1\n[info] [adv_network_dpdk_mgr.cpp:454] DPDK init (0005:03:00.0) -- RX: ENABLED TX: ENABLED\n[info] [adv_network_dpdk_mgr.cpp:464] Configuring RX queue: UNUSED_P0_Q0 (0) on port 0\n[info] [adv_network_dpdk_mgr.cpp:513] Created mempool RXP_P0_Q0_MR0 : mbufs=32768 elsize=9228 ptr=0x10041c380\n[info] [adv_network_dpdk_mgr.cpp:523] Max packet size needed for RX: 9100\n[info] [adv_network_dpdk_mgr.cpp:564] Configuring TX queue: ADC Samples (0) on port 0\n[info] [adv_network_dpdk_mgr.cpp:607] Created mempool TXP_P0_Q0_MR0 : mbufs=51200 elsize=9000 ptr=0x100c1fc00\n[info] [adv_network_dpdk_mgr.cpp:621] Max packet size needed with TX: 9100\n[info] [adv_network_dpdk_mgr.cpp:632] Setting port config for port 0 mtu:9082\n[info] [adv_network_dpdk_mgr.cpp:663] Initializing port 0 with 1 RX queues and 1 TX queues...\nmlx5_net: port 0 Tx queues number update: 0 -&gt; 1\nmlx5_net: port 0 Rx queues number update: 0 -&gt; 1\n[info] [adv_network_dpdk_mgr.cpp:679] Successfully configured ethdev\n[info] [adv_network_dpdk_mgr.cpp:689] Successfully set descriptors to 8192/8192\n[info] [adv_network_dpdk_mgr.cpp:704] Port 0 not in isolation mode\n[info] [adv_network_dpdk_mgr.cpp:713] Setting up port:0, queue:0, Num scatter:1 pool:0x10041c380\n[info] [adv_network_dpdk_mgr.cpp:734] Successfully setup RX port 0 queue 0\n[info] [adv_network_dpdk_mgr.cpp:756] Successfully set up TX queue 0/0\n[info] [adv_network_dpdk_mgr.cpp:761] Enabling promiscuous mode for port 0\nmlx5_net: [mlx5dr_cmd_query_caps]: Failed to query wire port regc value\nmlx5_net: port 0 Rx queues number update: 1 -&gt; 1\n[info] [adv_network_dpdk_mgr.cpp:775] Successfully started port 0\n[info] [adv_network_dpdk_mgr.cpp:778] Port 0, MAC address: 48:B0:2D:EE:83:AC\n[info] [adv_network_dpdk_mgr.cpp:1111] Applying tx_eth_src offload for port 0\n[info] [adv_network_dpdk_mgr.cpp:454] DPDK init (0005:03:00.1) -- RX: ENABLED TX: DISABLED\n[info] [adv_network_dpdk_mgr.cpp:464] Configuring RX queue: Data (0) on port 1\n[info] [adv_network_dpdk_mgr.cpp:513] Created mempool RXP_P1_Q0_MR0 : mbufs=51200 elsize=9128 ptr=0x125a5b940\n[info] [adv_network_dpdk_mgr.cpp:523] Max packet size needed for RX: 9000\n[info] [adv_network_dpdk_mgr.cpp:621] Max packet size needed with TX: 9000\n[info] [adv_network_dpdk_mgr.cpp:632] Setting port config for port 1 mtu:8982\n[info] [adv_network_dpdk_mgr.cpp:663] Initializing port 1 with 1 RX queues and 0 TX queues...\nmlx5_net: port 1 Rx queues number update: 0 -&gt; 1\n[info] [adv_network_dpdk_mgr.cpp:679] Successfully configured ethdev\n[info] [adv_network_dpdk_mgr.cpp:689] Successfully set descriptors to 8192/8192\n[info] [adv_network_dpdk_mgr.cpp:701] Port 1 in isolation mode\n[info] [adv_network_dpdk_mgr.cpp:713] Setting up port:1, queue:0, Num scatter:1 pool:0x125a5b940\n[info] [adv_network_dpdk_mgr.cpp:734] Successfully setup RX port 1 queue 0\n[info] [adv_network_dpdk_mgr.cpp:764] Not enabling promiscuous mode on port 1 since flow isolation is enabled\nmlx5_net: [mlx5dr_cmd_query_caps]: Failed to query wire port regc value\nmlx5_net: port 1 Rx queues number update: 1 -&gt; 1\n[info] [adv_network_dpdk_mgr.cpp:775] Successfully started port 1\n[info] [adv_network_dpdk_mgr.cpp:778] Port 1, MAC address: 48:B0:2D:EE:83:AD\n[info] [adv_network_dpdk_mgr.cpp:790] Adding RX flow ADC Samples\n[info] [adv_network_dpdk_mgr.cpp:998] Adding IPv4 length match for 1050\n[info] [adv_network_dpdk_mgr.cpp:1018] Adding UDP port match for src/dst 4096/4096\n[info] [adv_network_dpdk_mgr.cpp:814] Setting up RX burst pool with 8191 batches of size 81920\n[info] [adv_network_dpdk_mgr.cpp:833] Setting up RX burst pool with 8191 batches of size 20480\n[info] [adv_network_dpdk_mgr.cpp:875] Setting up TX ring TX_RING_P0_Q0\n[info] [adv_network_dpdk_mgr.cpp:901] Setting up TX burst pool TX_BURST_POOL_P0_Q0 with 10240 pointers at 0x125a0d4c0\n[info] [adv_network_dpdk_mgr.cpp:1186] Config validated successfully\n[info] [adv_network_dpdk_mgr.cpp:1199] Starting advanced network workers\n[info] [adv_network_dpdk_mgr.cpp:1278] Flushing packet on port 1\n[info] [adv_network_dpdk_mgr.cpp:1478] Starting RX Core 9, port 1, queue 0, socket 0\n[info] [adv_network_dpdk_mgr.cpp:1268] Done starting workers\n[info] [default_bench_op_tx.h:79] AdvNetworkingBenchDefaultTxOp::initialize()\n[info] [adv_network_dpdk_mgr.cpp:1637] Starting TX Core 11, port 0, queue 0 socket 0 using burst pool 0x125a0d4c0 ring 0x127690740\n[info] [default_bench_op_tx.h:113] Initialized 4 streams and events\n[info] [default_bench_op_tx.h:130] AdvNetworkingBenchDefaultTxOp::initialize() complete\n[info] [default_bench_op_rx.h:67] AdvNetworkingBenchDefaultRxOp::initialize()\n[info] [gxf_executor.cpp:1797] creating input IOSpec named 'burst_in'\n[info] [default_bench_op_rx.h:104] AdvNetworkingBenchDefaultRxOp::initialize() complete\n[info] [adv_network_tx.cpp:46] AdvNetworkOpTx::initialize()\n[info] [gxf_executor.cpp:1797] creating input IOSpec named 'burst_in'\n[info] [adv_network_common.h:607] Finished reading advanced network operator config\n[info] [gxf_executor.cpp:2208] Activating Graph...\n[info] [gxf_executor.cpp:2238] Running Graph...\n[info] [multi_thread_scheduler.cpp:300] MultiThreadScheduler started worker thread [pool name: default_pool, thread uid: 0]\n[info] [multi_thread_scheduler.cpp:300] MultiThreadScheduler started worker thread [pool name: default_pool, thread uid: 1]\n[info] [multi_thread_scheduler.cpp:300] MultiThreadScheduler started worker thread [pool name: default_pool, thread uid: 2]\n[info] [gxf_executor.cpp:2240] Waiting for completion...\n[info] [multi_thread_scheduler.cpp:300] MultiThreadScheduler started worker thread [pool name: default_pool, thread uid: 3]\n[info] [multi_thread_scheduler.cpp:300] MultiThreadScheduler started worker thread [pool name: default_pool, thread uid: 4]\n^C[info] [multi_thread_scheduler.cpp:636] Stopping multithread scheduler\n[info] [multi_thread_scheduler.cpp:694] Stopping all async jobs\n[info] [multi_thread_scheduler.cpp:218] Dispatcher thread has stopped checking jobs\n[info] [multi_thread_scheduler.cpp:679] Waiting to join all async threads\n[info] [multi_thread_scheduler.cpp:316] Worker Thread [pool name: default_pool, thread uid: 1] exiting.\n[info] [multi_thread_scheduler.cpp:702] *********************** DISPATCHER EXEC TIME : 476345.364000 ms\n\n[info] [multi_thread_scheduler.cpp:316] Worker Thread [pool name: default_pool, thread uid: 0] exiting.\n[info] [multi_thread_scheduler.cpp:316] Worker Thread [pool name: default_pool, thread uid: 3] exiting.\n[info] [multi_thread_scheduler.cpp:371] Event handler thread exiting.\n[info] [multi_thread_scheduler.cpp:703] *********************** DISPATCHER WAIT TIME : 47339.961000 ms\n\n[info] [multi_thread_scheduler.cpp:704] *********************** DISPATCHER COUNT : 197630449\n\n[info] [multi_thread_scheduler.cpp:316] Worker Thread [pool name: default_pool, thread uid: 2] exiting.\n[info] [multi_thread_scheduler.cpp:705] *********************** WORKER EXEC TIME : 983902.800000 ms\n\n[info] [multi_thread_scheduler.cpp:706] *********************** WORKER WAIT TIME : 1634522.159000 ms\n\n[info] [multi_thread_scheduler.cpp:707] *********************** WORKER COUNT : 11817369\n\n[info] [multi_thread_scheduler.cpp:316] Worker Thread [pool name: default_pool, thread uid: 4] exiting.\n[info] [multi_thread_scheduler.cpp:688] All async worker threads joined, deactivating all entities\n[info] [adv_network_rx.cpp:46] AdvNetworkOpRx::stop()\n[info] [adv_network_dpdk_mgr.cpp:1928] DPDK ANO shutdown called 2\n[info] [adv_network_tx.cpp:41] AdvNetworkOpTx::stop()\n[info] [adv_network_dpdk_mgr.cpp:1928] DPDK ANO shutdown called 1\n[info] [adv_network_dpdk_mgr.cpp:1133] Port 0:\n[info] [adv_network_dpdk_mgr.cpp:1135]  - Received packets:    0\n[info] [adv_network_dpdk_mgr.cpp:1136]  - Transmit packets:    6005066864\n[info] [adv_network_dpdk_mgr.cpp:1137]  - Received bytes:      0\n[info] [adv_network_dpdk_mgr.cpp:1138]  - Transmit bytes:      6389391347584\n[info] [adv_network_dpdk_mgr.cpp:1139]  - Missed packets:      0\n[info] [adv_network_dpdk_mgr.cpp:1140]  - Errored packets:     0\n[info] [adv_network_dpdk_mgr.cpp:1141]  - RX out of buffers:   0\n[info] [adv_network_dpdk_mgr.cpp:1143]    ** Extended Stats **\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_good_packets:          6005070000\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_good_bytes:            6389394480000\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_q0_packets:            6005070000\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_q0_bytes:              6389394480000\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_multicast_bytes:               9589\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_multicast_packets:             22\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_unicast_bytes:         6389394480000\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_multicast_bytes:               9589\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_unicast_packets:               6005070000\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_multicast_packets:             22\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_phy_packets:           6005070022\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_phy_packets:           24\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_phy_bytes:             6413414769677\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_phy_bytes:             9805\n[info] [adv_network_dpdk_mgr.cpp:1133] Port 1:\n[info] [adv_network_dpdk_mgr.cpp:1135]  - Received packets:    6004323692\n[info] [adv_network_dpdk_mgr.cpp:1136]  - Transmit packets:    0\n[info] [adv_network_dpdk_mgr.cpp:1137]  - Received bytes:      6388600255072\n[info] [adv_network_dpdk_mgr.cpp:1138]  - Transmit bytes:      0\n[info] [adv_network_dpdk_mgr.cpp:1139]  - Missed packets:      746308\n[info] [adv_network_dpdk_mgr.cpp:1140]  - Errored packets:     0\n[info] [adv_network_dpdk_mgr.cpp:1141]  - RX out of buffers:   5047027287\n[info] [adv_network_dpdk_mgr.cpp:1143]    ** Extended Stats **\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_good_packets:          6004323692\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_good_bytes:            6388600255072\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_missed_errors:         746308\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_mbuf_allocation_errors:                5047027287\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_q0_packets:            6004323692\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_q0_bytes:              6388600255072\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_q0_errors:             5047027287\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_unicast_bytes:         6389394480000\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_multicast_bytes:               9589\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_unicast_packets:               6005070000\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_multicast_packets:             22\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_multicast_bytes:               9589\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_multicast_packets:             22\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_phy_packets:           24\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_phy_packets:           6005070022\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_phy_bytes:             9805\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_phy_bytes:             6413414769677\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_out_of_buffer:         746308\n[info] [adv_network_dpdk_mgr.cpp:1935] ANO DPDK manager shutting down\n[info] [adv_network_dpdk_mgr.cpp:1622] Total packets received by application (port/queue 1/0): 6004323692\n[info] [adv_network_dpdk_mgr.cpp:1698] Total packets transmitted by application (port/queue 0/0): 6005070000\n[info] [multi_thread_scheduler.cpp:645] Multithread scheduler stopped.\n[info] [multi_thread_scheduler.cpp:664] Multithread scheduler finished.\n[info] [gxf_executor.cpp:2243] Deactivating Graph...\n[info] [multi_thread_scheduler.cpp:491] TOTAL EXECUTION TIME OF SCHEDULER : 523694.460857 ms\n\n[info] [gxf_executor.cpp:2251] Graph execution finished.\n[info] [adv_network_dpdk_mgr.cpp:1928] DPDK ANO shutdown called 0\n[info] [default_bench_op_tx.h:51] ANO benchmark TX op shutting down\n[info] [default_bench_op_rx.h:56] Finished receiver with 6388570603520/6004295680 bytes/packets received and 0 packets dropped\n[info] [default_bench_op_rx.h:61] ANO benchmark RX op shutting down\n[info] [default_bench_op_rx.h:108] AdvNetworkingBenchDefaultRxOp::freeResources() start\n[info] [default_bench_op_rx.h:116] AdvNetworkingBenchDefaultRxOp::freeResources() complete\n[info] [gxf_executor.cpp:294] Destroying context\n</code></pre> <p>To inspect the speed the data is moving through the NIC, run <code>mlnx_perf</code> on one of the interfaces in a separate terminal, concurrently with the application running:</p> <pre><code>sudo mlnx_perf -i $if_name\n</code></pre> See an example output <p>On IGX with RTX A6000, we are able to hit close to the 100 Gbps linerate with this configuration: <pre><code>  rx_vport_unicast_packets: 11,614,900\n    rx_vport_unicast_bytes: 12,358,253,600 Bps   = 98,866.2 Mbps\n            rx_packets_phy: 11,614,847\n              rx_bytes_phy: 12,404,657,664 Bps   = 99,237.26 Mbps\n rx_1024_to_1518_bytes_phy: 11,614,936\n            rx_prio0_bytes: 12,404,738,832 Bps   = 99,237.91 Mbps\n          rx_prio0_packets: 11,614,923\n</code></pre></p> Troubleshooting EAL: failed to parse device <p>Make sure to set valid PCIe addresses in the <code>address</code> fields in <code>interfaces</code>, per instructions above.</p> Invalid MAC address format <p>Make sure to set a valid MAC address in the <code>eth_dst_addr</code> field in <code>bench_tx</code>, per instructions above.</p> mlx5_common: Fail to create MR for address [...] Could not DMA map EXT memory <p>Example error:</p> <pre><code>mlx5_common: Fail to create MR for address (0xffff2fc00000)\nmlx5_common: Device 0005:03:00.0 unable to DMA map\n[critical] [adv_network_dpdk_mgr.cpp:188] Could not DMA map EXT memory: -1 err=Invalid argument\n[critical] [adv_network_dpdk_mgr.cpp:430] Failed to map MRs\n</code></pre> <p>Make sure that <code>nvidia-peermem</code> is loaded.</p> EAL: Couldn't get fd on hugepage file [..] error allocating rte services array <p>Example error:</p> <pre><code>EAL: get_seg_fd(): open '/mnt/huge/nwlrbbmqbhmap_0' failed: Permission denied\nEAL: Couldn't get fd on hugepage file\nEAL: error allocating rte services array\nEAL: FATAL: rte_service_init() failed\nEAL: rte_service_init() failed\n</code></pre> <p>Ensure you run as root, using <code>sudo</code>.</p> EAL: Cannot get hugepage information. <pre><code>EAL: x hugepages of size x reserved, no mounted hugetlbfs found for that size\n</code></pre> <p>Ensure your hugepages are mounted.</p> <pre><code>EAL: No free x kB hugepages reported on node 0\n</code></pre> <ul> <li>Ensure you have allocated hugepages.</li> <li> <p>If you have already, check if they are any free left with <code>grep Huge /proc/meminfo</code>.</p> See an example output <p>No more space here!</p> <pre><code>HugePages_Total:       2\nHugePages_Free:        0\nHugePages_Rsvd:        0\nHugePages_Surp:        0\nHugepagesize:    1048576 kB\nHugetlb:         2097152 kB\n</code></pre> </li> <li> <p>If not, you can delete dangling hugepages under your hugepage mount point. That happens when your previous application run crashes.</p> <pre><code>sudo rm -rf /dev/hugepages/* # default mount point\nsudo rm -rf /mnt/huge/*      # custom mount point\n</code></pre> </li> </ul> Could not allocate x MB of GPU memory [...] Failed to allocate GPU memory <p>Check your GPU utilization:</p> <pre><code>nvidia-smi pmon -c 1\n</code></pre> <p>You might need to kill some of the listed processes to free up GPU VRAM.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#5-building-your-own-application","title":"5. Building your own application","text":"<p>This section will guide you through building your own application using the <code>adv_networking_bench</code> as an example. Make sure to have installed <code>holoscan-networking</code> first.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#51-understand-the-configuration-parameters","title":"5.1 Understand the configuration parameters","text":"<p>Note</p> <p>The configuration below will be analyzed in the context of the application consuming it, as defined in the <code>main.cpp</code> file. You can look it up when the \"sample application code\" is referenced.</p> Debian installationFrom source <pre><code>/opt/nvidia/holoscan/examples/adv_networking_bench/main.cpp\n</code></pre> <pre><code>./applications/adv_networking_bench/cpp/main.cpp\n</code></pre> <p>If you are not yet familiar with how Holoscan applications are constructed, please refer to the Holoscan SDK documentation first.</p> <p>Let's look at the <code>adv_networking_bench_default_tx_rx.yaml</code> file below. Click on the (1) icons below to expand explanations for each annotated line.</p> <ol> <li>The cake is a lie </li> </ol> <pre><code>scheduler: # (1)!\n  check_recession_period_ms: 0\n  worker_thread_number: 5\n  stop_on_deadlock: true\n  stop_on_deadlock_timeout: 500\n  # max_duration_ms: 20000\n\nadvanced_network: # (2)!\n  cfg:\n    version: 1\n    manager: \"dpdk\" # (3)!\n    master_core: 3 # (4)!\n    debug: false\n    log_level: \"info\"\n\n    memory_regions: # (5)!\n    - name: \"Data_TX_GPU\" # (6)!\n      kind: \"device\" # (7)!\n      affinity: 0 # (8)!\n      num_bufs: 51200 # (9)!\n      buf_size: 1064 # (10)!\n    - name: \"Data_RX_GPU\"\n      kind: \"device\"\n      affinity: 0\n      num_bufs: 51200\n      buf_size: 1000\n    - name: \"Data_RX_CPU\"\n      kind: \"huge\"\n      affinity: 0\n      num_bufs: 51200\n      buf_size: 64\n\n    interfaces: # (11)!\n    - name: \"tx_port\" # (12)!\n      address: &lt;0000:00:00.0&gt; # (13)! # The BUS address of the interface doing Tx\n      tx: # (14)!\n        queues: # (15)!\n        - name: \"tx_q_0\" # (16)!\n          id: 0 # (17)!\n          batch_size: 10240 # (18)!\n          cpu_core: 11 # (19)!\n          memory_regions: # (20)!\n            - \"Data_TX_GPU\"\n          offloads: # (21)!\n            - \"tx_eth_src\"\n    - name: \"rx_port\"\n      address: &lt;0000:00:00.0&gt; # (22)! # The BUS address of the interface doing Rx\n      rx:\n        flow_isolation: true # (23)!\n        queues:\n        - name: \"rx_q_0\"\n          id: 0\n          cpu_core: 9\n          batch_size: 10240\n          output_port: \"bench_rx_out\" # (24)!\n          memory_regions: # (25)!\n            - \"Data_RX_CPU\"\n            - \"Data_RX_GPU\"\n        flows: # (26)!\n        - name: \"flow_0\" # (27)!\n          id: 0 # (28)!\n          action: # (29)!\n            type: queue\n            id: 0\n          match: # (30)!\n            udp_src: 4096\n            udp_dst: 4096\n            ipv4_len: 1050\n\nbench_rx: # (31)!\n  gpu_direct: true       # Set to true if using a GPU region for the Rx queues.\n  split_boundary: true   # Whether header and data are split for Rx (Header to CPU)\n  batch_size: 10240\n  max_packet_size: 1064\n  header_size: 64\n\nbench_tx: # (32)!\n  gpu_direct: true        # Set to true if using a GPU region for the Tx queues.\n  split_boundary: 0       # Byte boundary where header and data are split for Tx, 0 if no split\n  batch_size: 10240\n  payload_size: 1000\n  header_size: 64\n  eth_dst_addr: &lt;00:00:00:00:00:00&gt; # Destination MAC address - required when Rx flow_isolation=true\n  ip_src_addr: &lt;1.2.3.4&gt;  # Source IP address - required on layer 3 network\n  ip_dst_addr: &lt;5.6.7.8&gt;  # Destination IP address - required on layer 3 network\n  udp_src_port: 4096      # UDP source port\n  udp_dst_port: 4096      # UDP destination port\n  address: &lt;0000:00:00.0&gt; # Source NIC Bus ID. Should match the address of the Tx interface above\n</code></pre> <ol> <li>The <code>scheduler</code> section is passed to the multi threaded scheduler we declare in the <code>main()</code> function of this application. See the holoscan SDK documentation and API docs for more details. This is related to the Holoscan core library and is not specific to Holoscan Networking.</li> <li>The <code>advanced_network</code> section is passed to the <code>AdvNetworkOpRx</code> and <code>AdvNetworkOpTx</code> operators which are responsible for setting up the NIC.</li> <li><code>manager</code> is the backend networking library. default: <code>dpdk</code>. Other: <code>gpunetio</code> (DOCA GPUNet IO + DOCA Ethernet &amp; Flow). Coming soon: <code>rivermax</code>, <code>rdma</code>.</li> <li><code>master_core</code> is the ID of the CPU core used for setup. It does not need to be isolated, and is recommended to differ differ from the <code>cpu_core</code> fields below used for polling the NIC.</li> <li>The <code>memory_regions</code> section lists where the NIC will write/read data from/to when bypassing the OS kernel. Tip: when using GPU buffer regions, keeping the sum of their buffer sizes lower than 80% of your BAR1 size is generally a good rule of thumb \ud83d\udc4d.</li> <li>A descriptive name for that memory region to refer to later in the <code>interfaces</code> section.</li> <li>The type of memory region. Best options are <code>device</code> (GPU), or <code>huge</code> (pages - CPU). Also supported but not recommended are <code>malloc</code> (CPU) and <code>pinned</code> (CPU).</li> <li>The GPU ID for <code>device</code> memory regions. The NUMA node ID for CPU memory regions.</li> <li>The number of buffers in the memory region. A higher value means more time to process the data, but it takes additional space on the GPU BAR1. Too low increases the risk of dropping packets from the NIC having nowhere to write (Rx) or the risk of higher latency from buffering (Tx). Need a rule of thumb \ud83d\udc4d? 5x the <code>batch_size</code> below is a good starting point.</li> <li>The size of each buffer in the memory region. These should be equal to your maximum packet size, or less if breaking down packets (ex: header data split, see the <code>rx</code> queue below).</li> <li>The <code>interfaces</code> section lists the NIC interfaces that will be configured for the application.</li> <li>A descriptive name for that interface, currently only used for logging.</li> <li>The PCIe/bus address of that interface, as identified in previous sections.</li> <li>Each interface can have a <code>tx</code> (transmitting) or <code>rx</code> (receiving) section, or both if you'd like to configure both Tx and Rx on the same interface.</li> <li>The <code>queues</code> section lists the queues for that interface. Queues are a core concept of NICs: they handle the actual receiving or transmitting of network packets. Rx queues buffer incoming packets until they can be processed by the application, while Tx queues hold outgoing packets waiting to be sent on the network. The simplest setup uses only one receive and one transmit queue. Using more queues allows multiple streams of network traffic to be processed in parallel, as each queue can be assigned to a specific CPU core, and are assigned their own memory regions that are not shared.</li> <li>A descriptive name for that queue, currently only used for logging.</li> <li>The ID of that queue, which can be referred to later in the <code>flows</code> section.</li> <li>The number of packets per batch. The <code>advanced_network</code> Rx operator will forward packets on a timer, or when the NIC receives enough packets for a whole batch per this number. The <code>advanced_network</code> Tx operator needs to ensure it does not send more packets than this value on each <code>Operator::compute()</code> call.</li> <li>The ID of the CPU core that this queue will use to poll the NIC. Ideally one isolated core per queue.</li> <li>The list of memory regions where this queue will write/read packets from/to. The order matters: the first memory region will be used first to read/write from until it fills up one buffer (<code>buf_size</code>), after which it will move to the next region in the list and so on until the packet is fully written/read. See the <code>memory_regions</code> for the <code>rx</code> queue below for an example.</li> <li>The <code>offloads</code> section (Tx queues only) lists optional tasks that can be offloaded to the NIC. The only value currently supported is <code>tx_eth_src</code>, that lets the NIC insert the ethernet source mac address in the packet headers. Note: IP, UDP, and Ethernet Checksums or CRC are always done by the NIC currently and are not optional.</li> <li>Same as for <code>tx_port</code>. Each interface in this list should have a unique mac address. This one will do <code>rx</code> per config below.</li> <li>Whether to isolate the Rx flow. If true, any incoming packets that does not match the MAC address of this interface - or isn't directed to a queue when the <code>flows</code> section below is used - will be delegated back to Linux for processing (no kernel bypass). This is useful to let this interface handle ARP, ICMP, etc. Otherwise, any packets sent to this interface (ex: ping) will need to be processed (or dropped) by your application.</li> <li><code>rx</code> queues have an <code>output_port</code> parameter so you can attach a downstream operator to receive data from this specific queue, as can be seen in the <code>Application::compose()</code> function of the sample application. Multiple <code>rx</code> queues can share the same <code>output_port</code>. In contrast, <code>tx</code> queues have a single non-configurable port (name: <code>burst_in</code>) to which upstream operators will send all packets, which are then routed to the correct queue based on the port/queue in the burst header.</li> <li>This scenario is called HDS (Header-Data Split): the packet will first be written to a buffer in the <code>Data_RX_CPU</code> memory region, filling its <code>buf_size</code> of 64 bytes - which is consistent with the size of our header - then the rest of the packet will be written to the <code>Data_RX_GPU</code> memory region. Its <code>buf_size</code> of 1000 bytes is just what we need to write the payload size for our application, no byte wasted!</li> <li>The list of flows. Flows are responsible for routing packets to the correct queue based on various properties. If this field is missing, all packets will be routed to the first queue.</li> <li>The flow name, currently only used for logging.</li> <li>The flow <code>id</code> is used to tag the packets with what flow it arrived on. This is useful when sending multiple flows to a single queue, as the user application can differentiate which flow (i.e. rules) matched the packet based on this ID.</li> <li>What to do with packets that match this flow. The only supported action currently is <code>type: queue</code> to send the packet to a queue given its <code>id</code>.</li> <li>List of rules to match packets against. All rules must be met for a packet to match the flow. Currently supported rules include <code>udp_src</code> and <code>udp_dst</code> (port numbers), <code>ipv4_len</code> (#TODO#) etc.</li> <li>The <code>bench_rx</code> section is passed to the <code>AdvNetworkingBenchDefaultRxOp</code> operator in the <code>Application::compose()</code> function of the sample application. This operator is a custom operator implemented in <code>default_bench_op_rx.h</code> that aggregates packets received from the NIC. The parameters in this section are specific to this operator, and should align with how <code>memory_regions</code> and <code>queues</code> were configured for the <code>rx</code> interface.</li> <li>The <code>bench_tx</code> section is passed to the <code>AdvNetworkingBenchDefaultTxOp</code> operator in the <code>Application::compose()</code> function of the sample application. This operator is a custom operator implemented in <code>default_bench_op_tx.h</code> that generates dummy packets to send to the NIC. The parameters in this section up to <code>header_size</code> should align with how <code>memory_regions</code> and <code>queues</code> were configured for the <code>tx</code> interface. The following parameters up to <code>udp_dst_port</code> are used to fill-in the ethernet header of the packets. The last parameter, <code>address</code>, is used to specify which NIC interface to use for the Tx operation.</li> </ol>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#52-create-your-own-rx-operator","title":"5.2 Create your own Rx operator","text":"<p>Under construction</p> <p>This section is under construction. Refer to the implementation of the <code>AdvNetworkingBenchDefaultRxOp</code> for an example.</p> Debian installationFrom source <pre><code>/opt/nvidia/holoscan/examples/adv_networking_bench/default_bench_op_rx.h\n</code></pre> <pre><code>./applications/adv_networking_bench/cpp/default_bench_op_rx.h\n</code></pre> <p>Note</p> <p>Design investigations are expected soon for a generic packet aggregator operator.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#53-create-your-own-tx-operator","title":"5.3 Create your own Tx operator","text":"<p>Under construction</p> <p>This section is under construction. Refer to the implementation of the <code>AdvNetworkingBenchDefaultTxOp</code> for an example.</p> Debian installationFrom source <pre><code>/opt/nvidia/holoscan/examples/adv_networking_bench/default_bench_op_tx.h\n</code></pre> <pre><code>./applications/adv_networking_bench/cpp/default_bench_op_tx.h\n</code></pre> <p>Note</p> <p>Designs investigations are expected soon for a generic way to prepare packets to send to the NIC.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/#54-build-with-cmake","title":"5.4 Build with CMake","text":"Debian installationFrom source <ol> <li>Create a source directory and write your source file(s) for your application (and custom operators if needed)</li> <li> <p>Create a <code>CMakeLists.txt</code> file in your source directory like this one:</p> <pre><code>cmake_minimum_required(VERSION 3.20)\nproject(my_app CXX) # Add CUDA if writing .cu kernels\n\nfind_package(holoscan 2.6 REQUIRED CONFIG PATHS \"/opt/nvidia/holoscan\")\nfind_package(holoscan-networking REQUIRED CONFIG PATHS \"/opt/nvidia/holoscan\")\n\n# Create an executable\nadd_executable(my_app\n    my_app.cpp\n    ...\n)\ntarget_include_directories(my_app\n    PRIVATE\n        my_include_dirs/\n        ...\n)\ntarget_link_libraries(my_app\n    PRIVATE\n        holoscan::core\n        holoscan::ops::advanced_network_rx\n        holoscan::ops::advanced_network_tx\n        my_other_dependencies\n        ...\n)\n\n# Copy the config file to the build directory for convenience referring to it\nadd_custom_target(my_app_config_yaml\n    COMMAND ${CMAKE_COMMAND} -E copy_if_different \"${CMAKE_CURRENT_SOURCE_DIR}/my_app_config.yaml\" ${CMAKE_CURRENT_BINARY_DIR}\n    DEPENDS \"${CMAKE_CURRENT_SOURCE_DIR}/my_app_config.yaml\"\n)\nadd_dependencies(my_app my_app_config_yaml)\n</code></pre> </li> <li> <p>Build your application like so:</p> <pre><code># Your chosen paths\nsrc_dir=\".\"\nbuild_dir=\"build\"\n\n# Configure the build\ncmake -S \"$src_dir\" -B \"$build_dir\"\n\n# Build the application\ncmake --build \"$build_dir\" -j\n</code></pre> Failed to detect a default CUDA architecture. <p>Add the path to your installation of <code>nvcc</code> to your <code>PATH</code>, or pass its to the cmake configuration command like so (adjust to your CUDA/nvcc installation path):</p> <pre><code>cmake -S \"$src_dir\" -B \"$build_dir\" -D CMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc\n</code></pre> </li> <li> <p>Run your application like so:</p> <pre><code>\"./$build_dir/my_app my_app_config.yaml\"\n</code></pre> </li> </ol> <ol> <li>Create an application directory under <code>applications/</code> in your clone of the HoloHub repository, and write your source file(s) for your application (and custom operators if needed).</li> <li> <p>Add the following to the <code>application/CMakeLists.txt</code> file:</p> <pre><code>add_holohub_application(my_app DEPENDS OPERATORS advanced_network)\n</code></pre> </li> <li> <p>Create a <code>CMakeLists.txt</code> file in your application directory like this one:</p> <pre><code>cmake_minimum_required(VERSION 3.20)\nproject(my_app CXX) # Add CUDA if writing .cu kernels\n\nfind_package(holoscan 2.6 REQUIRED CONFIG PATHS \"/opt/nvidia/holoscan\")\n\n# Create an executable\nadd_executable(my_app\n    my_app.cpp\n    ...\n)\ntarget_include_directories(my_app\n    PRIVATE\n        my_include_dirs/\n        ...\n)\ntarget_link_libraries(my_app\n    PRIVATE\n        holoscan::core\n        holoscan::ops::advanced_network_rx\n        holoscan::ops::advanced_network_tx\n        my_other_dependencies\n        ...\n)\n\n# Copy the config file to the build directory for convenience referring to it\nadd_custom_target(my_app_config_yaml\n    COMMAND ${CMAKE_COMMAND} -E copy_if_different \"${CMAKE_CURRENT_SOURCE_DIR}/my_app_config.yaml\" ${CMAKE_CURRENT_BINARY_DIR}\n    DEPENDS \"${CMAKE_CURRENT_SOURCE_DIR}/my_app_config.yaml\"\n)\nadd_dependencies(my_app my_app_config_yaml)\n</code></pre> </li> <li> <p>Build your application like so:</p> <pre><code>./dev_container build_and_run my_app --no_run\n</code></pre> </li> <li> <p>Run your application like so:</p> <pre><code>./dev_container launch --img holohub:my_app --docker_opts \"-u 0 --privileged\" --bash -c \"./build/my_app/applications/my_app my_app_config.yaml\"\n</code></pre> <p>or, if you have set up a shortcut to run your application with its config file through its <code>metadata.json</code> (see other apps for examples):</p> <pre><code>./dev_container build_and_run --no_build --container_args \" -u 0 --privileged\"\n</code></pre> </li> </ol>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/holoscan-bootcamp/","title":"NVIDIA Holoscan Bootcamp","text":"<p> Authors: Denis Leshchev (NVIDIA), Adam Thompson (NVIDIA), Nicolas Lebovitz (NVIDIA) Supported platforms: amd64, arm64 Last modified: October 7, 2024 Language: Python Latest version: 0.1.0 Minimum Holoscan SDK version: 2.2.0 Tested Holoscan SDK versions: 2.3.0 Contribution metric: Level 1 - Highly Reliable</p>","tags":["Python","Sensor Processing","Bootcamp"]},{"location":"tutorials/holoscan-bootcamp/#overview","title":"Overview","text":"<p>This is a meta-tutorial for deploying and running NVIDIA Holoscan Bootcamp lab materials.</p>","tags":["Python","Sensor Processing","Bootcamp"]},{"location":"tutorials/holoscan-bootcamp/#description","title":"Description","text":"<p>NVIDIA Holoscan Bootcamp was conducted by the NVIDIA team in collaboration with OpenHackathons on September 24th, 2024 (event link). The bootcamp included a lecture and a hands-on Python-based lab. The lab materials are freely available for downloading at OpenHackathons repo.</p> <p>NVIDIA Holoscan Bootcamp lab materials include a Jupyter notebook for self-paced studying of the core Holoscan concepts.</p> <p>The notebook covers the following topics: - Creating custom Holoscan operators - Connecting operators to form an application - Build Holoscan applications with GPU-accelerated Python packages - How to build a sensor to visualization pipeline - Measuring and optimizing application performance - How to build complex sensor processing workflows using multiple fragments and the advanced schedulers</p> <p>The notebook guides users through the topics with example code and diagrams, and includes exercises to help them practice the concepts they've learned.</p> <p>To run the lab materials, clone the repo and follow the deployment guide.</p>","tags":["Python","Sensor Processing","Bootcamp"]},{"location":"tutorials/holoscan-playground-on-aws/","title":"Holoscan Playground on AWS","text":"<p> Authors: Jin Li (NVIDIA) Supported platforms: amd64, arm64 Last modified: March 26, 2024 Latest version: 0.1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p>","tags":["Amazon Web Services","AWS","Cloud","EC2"]},{"location":"tutorials/holoscan-playground-on-aws/#overview","title":"Overview","text":"<p>The Holoscan on AWS EC2 experience is an easy way for having a first try at the Holoscan SDK. The Holoscan SDK documentation lists out the hardware prerequisites. If you have a compatible hardware at hand, please get started with the SDK on your hardware. Otherwise, you could utilize an AWS EC2 instance to have a first look at the Holoscan SDK by following this guide. </p> <p>We estimate the time needed to follow this guide is around 1 hour, after which you could feel free to explore more of the SDK examples and applications. Please note that for the g5.xlarge instance type utilized, the cost is $1.006/hour.</p> <ol> <li> <p>The AWS experience is intended as a trial environment of the Holoscan SDK, not as a full time development environment. Some limitations of running the SDK on an EC2 instance are: </p> </li> <li> <p>An EC2 instance does not have the capability of live input sources, including video capture cards like AJA and Deltacast, or the onboard HDMI input port on devkits. An EC2 instance does not have ConnectX networking capabilities available on devkits.</p> </li> <li> <p>Display forwarding from EC2 to your local machine depends on internet connectivity and results in heavy latency, so if you would like to develop applications with display, it is not ideal.</p> </li> </ol>","tags":["Amazon Web Services","AWS","Cloud","EC2"]},{"location":"tutorials/holoscan-playground-on-aws/#launch-ec2-instance","title":"Launch EC2 Instance","text":"<p>Type in the name that you want to give to the instance.</p> <p></p> <p>In the <code>Application and OS Images (Amazon Machine Image)</code> window, search for <code>NVIDIA</code>.</p> <p></p> <p>From the results, switch to <code>AWS Marketplace AMIs</code> and choose <code>NVIDIA GPU-Optimized AMI</code>.</p> <p></p> <p>Select <code>Continue</code> after viewing the details of this AMI.</p> <p></p> <p>The selected AMI should look like this in the view to create an instance:</p> <p></p> <p>For <code>Instance type</code>, select <code>g5.xlarge</code>.  Note: If you see an error similar to <code>The selected instance type is not supported in the zone (us-west-2d). Please select a different instance type or subnet.</code>, go down to <code>Network settings</code>, click on <code>Edit</code>, and try changing the <code>Subnet</code> selection. Note: If <code>g5.xlarge</code> is not available in any region/subnet accessible to you, <code>p3.2xlarge</code> should also work.</p> <p></p> <p>For <code>Key pair</code>, create a new key pair, enter the key pair name as you like, and store the file as prompted. After clicking on <code>Create key pair</code>, the file <code>your-name.pem</code> will be automatically downloaded by the browser to the Downloads folder. Then select the key pair in the view to create an instance.</p> <p></p> <p>Configure the <code>Network settings</code>. Click on <code>Edit</code> to start.  * If you got an error in the <code>Instance type</code> selection about <code>g5.xlarge</code> being unavailable, try changing your <code>Subnet</code> selection in <code>Network settings</code>. Otherwise, there is no need to change the <code>Subnet</code> selection. * Make sure your <code>Auto-assign public IP</code> is enabled, otherwise you would have issues ssh\u2019ing into the instance.  * Select a security group with a public IP address where you plan to ssh from, if one doesn\u2019t exist yet, select <code>create security group</code>.      * If you\u2019re already on the local machine you plan to ssh from, select <code>My IP</code> under <code>Source Type</code>.      * To add other machines that you plan to ssh from, select <code>Custom</code> under <code>Source Type</code> and enter your public IP address under <code>Source</code>. You can find the public IP address of the machine by going to https://www.whatsmyip.org/ from the machine. </p> <p></p> <p>Keep the default 128 GB specification in <code>Configure storage</code>. </p> <p></p> <p>Your Summary on the right side should look like this:</p> <p></p> <p>Please note that with a different instance type, Storage (volumes) may look different too.</p> <p>Click <code>Launch instance</code>, and you should see a <code>Success</code> notification.</p> <p></p> <p>Now go back to the <code>Instances</code> window to view the <code>Status check</code> of the instance we had just launched. It should show <code>Initializing</code> for a few minutes:</p> <p></p> <p>And later it should show <code>2/2 checks passed</code>:</p> <p></p> <p>Now we\u2019re ready to ssh into the instance.</p>","tags":["Amazon Web Services","AWS","Cloud","EC2"]},{"location":"tutorials/holoscan-playground-on-aws/#ssh-into-ec2-instance","title":"SSH into EC2 Instance","text":"<p>Click on the instance ID, and you should see this layout for instance details. Click on the <code>Connect</code> button on the top right.</p> <p></p> <p>Under the <code>SSH client</code> tab there are the SSH instructions. Note that the username <code>root</code> is guessed, and for the AMI we chose, it should be <code>ubuntu</code>. The private key file that you saved from when you were configuring the instance should be on the machine that you are ssh\u2019ing from. </p> <p>Add <code>-X</code> to the ssh command to enable display forwarding.</p> <p></p>","tags":["Amazon Web Services","AWS","Cloud","EC2"]},{"location":"tutorials/holoscan-playground-on-aws/#setting-up-display-forwarding-from-ec2-instance","title":"Setting up Display Forwarding from EC2 Instance","text":"<p>Holoscan SDK has examples and applications that depend on seeing a display. For this experience, we will do X11 forwarding.</p>","tags":["Amazon Web Services","AWS","Cloud","EC2"]},{"location":"tutorials/holoscan-playground-on-aws/#on-ec2-instance","title":"On EC2 Instance","text":"<p>First,install the package needed for a simple forwarding test, xeyes.</p> <pre><code>sudo apt install -y x11-apps\n</code></pre> <p>Next, run \u201cxeyes\u201d in the terminal, and you should get a display window popping up on the machine you\u2019re ssh\u2019ing from: <pre><code>xeyes\n</code></pre></p> <p>https://github.com/jin-nvidia/holohub/assets/60405124/57c76bed-ca16-458b-8740-1e4351ca63f7</p> <p>If you run into display issues, ensure the machine you\u2019re ssh\u2019ing from has X11 forwarding enabled. Please see the Troubleshooting section.</p>","tags":["Amazon Web Services","AWS","Cloud","EC2"]},{"location":"tutorials/holoscan-playground-on-aws/#in-a-docker-container-on-ec2-instance","title":"In a Docker Container On EC2 Instance","text":"<p>Now you have enabled display forwarding on the EC2 instance bare metal, let\u2019s take it one step further to enable display forwarding from a Docker container on the EC2 instance.</p> <pre><code>XSOCK=/tmp/.X11-unix\nXAUTH=/tmp/.docker.xauth\n# the error \u201cfile does not exist\u201d is expected at the next command\nxauth nlist $DISPLAY | sed -e 's/^..../ffff/' | sudo xauth -f $XAUTH nmerge -\nsudo chmod 777 $XAUTH\ndocker run -ti -e DISPLAY=$DISPLAY -v $XSOCK:$XSOCK -v $XAUTH:$XAUTH -e XAUTHORITY=$XAUTH --net host ubuntu:latest\n</code></pre> <p>Within the container:</p> <p><pre><code>apt update &amp;&amp; apt install -y x11-apps\nxeyes\n</code></pre> Press  <code>ctrl + D</code> to exit the Docker container. Now we have enabled display forwarding from both EC2 bare metal and containerized environments!</p>","tags":["Amazon Web Services","AWS","Cloud","EC2"]},{"location":"tutorials/holoscan-playground-on-aws/#run-holoscan","title":"Run Holoscan","text":"","tags":["Amazon Web Services","AWS","Cloud","EC2"]},{"location":"tutorials/holoscan-playground-on-aws/#install-holoscan","title":"Install Holoscan","text":"<p>There are several ways to install the Holoscan SDK. For the quickest way to get started, we will choose the Holoscan Docker container that already has all dependencies set up.  We run nvidia-smi in the EC2 instance to check that there are drivers installed:</p> <p></p> <p>Follow the overview of https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/containers/holoscan. Some modifications are made to the original commands due to the EC2 environment, <code>--gpus all</code> and <code>-v $XAUTH:$XAUTH -e XAUTHORITY=$XAUTH</code>. <pre><code># install xhost util\nsudo apt install -y x11-xserver-utils\n\nxhost +local:docker\n\nnvidia_icd_json=$(find /usr/share /etc -path '*/vulkan/icd.d/nvidia_icd.json' -type f,l -print -quit 2&gt;/dev/null | grep .) || (echo \"nvidia_icd.json not found\" &gt;&amp;2 &amp;&amp; false)\n\nexport NGC_CONTAINER_IMAGE_PATH=\"nvcr.io/nvidia/clara-holoscan/holoscan:v0.6.0-dgpu\"\n\ndocker run -it --rm --net host \\\n  --gpus all \\\n   -v $XAUTH:$XAUTH -e XAUTHORITY=$XAUTH \\\n  -v /tmp/.X11-unix:/tmp/.X11-unix \\\n  -v $nvidia_icd_json:$nvidia_icd_json:ro \\\n  -e NVIDIA_DRIVER_CAPABILITIES=graphics,video,compute,utility,display \\\n  -e DISPLAY=$DISPLAY \\\n  --ipc=host \\\n  --cap-add=CAP_SYS_PTRACE \\\n  --ulimit memlock=-1 \\\n  ${NGC_CONTAINER_IMAGE_PATH}\n</code></pre></p>","tags":["Amazon Web Services","AWS","Cloud","EC2"]},{"location":"tutorials/holoscan-playground-on-aws/#sanity-check-with-holoscan-hello-world","title":"Sanity Check with Holoscan Hello World","text":"<pre><code>/opt/nvidia/holoscan/examples/hello_world/cpp/hello_world\n</code></pre>","tags":["Amazon Web Services","AWS","Cloud","EC2"]},{"location":"tutorials/holoscan-playground-on-aws/#examples","title":"Examples","text":"<p>Refer to each one of the Holoscan SDK examples on GitHub. You will find these examples installed under <code>/opt/nvidia/holoscan/examples/</code>.</p>","tags":["Amazon Web Services","AWS","Cloud","EC2"]},{"location":"tutorials/holoscan-playground-on-aws/#video-replayer-example","title":"Video Replayer Example","text":"<p>Let\u2019s take a look at the video replayer example which is a basic video player app. Since we are in the Docker container, there\u2019s no need to manually download data as it already exists in the container.</p> <p>Run the video_replayer example</p> <p><pre><code>cd /opt/nvidia/holoscan \n./examples/video_replayer/cpp/video_replayer\n</code></pre> You should see a window like below</p> <p>https://github.com/jin-nvidia/holohub/assets/60405124/7ae99409-ca42-4c38-b495-84a59648b671</p> <p>Please note that it is normal for the video stream to be lagging behind since it is forwarded from a docker container on a EC2 instance to your local machine. How much the forwarded video will lag heavily depends on the internet connection. When running Holoscan applications on the edge, we should have significantly less latency lag.</p> <p>You can close the sample application by pressing ctrl +C.</p> <p>Now that we have run a simple video replayer, let\u2019s explore the examples a little more. </p>","tags":["Amazon Web Services","AWS","Cloud","EC2"]},{"location":"tutorials/holoscan-playground-on-aws/#tensor-interoperability-example","title":"Tensor Interoperability Example","text":"","tags":["Amazon Web Services","AWS","Cloud","EC2"]},{"location":"tutorials/holoscan-playground-on-aws/#the-c-tensor-interop-example","title":"The C++ Tensor Interop example","text":"<p>Since we used the Debian package install, run the C++ tensor interopability example by </p> <pre><code>/opt/nvidia/holoscan/examples/tensor_interop/cpp/tensor_interop\n</code></pre> <p>Please refer to the README and the source file to see how we can have interoperability between a native operator (<code>ProcessTensorOp</code>) and two wrapped GXF Codelets (<code>SendTensor</code> and <code>ReceiveTensor</code>). For the Holoscan documentation on tensor interop in the C++ API, please see Interoperability between GXF and native C++ operators.</p>","tags":["Amazon Web Services","AWS","Cloud","EC2"]},{"location":"tutorials/holoscan-playground-on-aws/#the-python-tensor-interop-example","title":"The Python Tensor Interop example","text":"<p>The Python Tensor Interop example demonstrates interoperability between a native Python operator (<code>ImageProcessingOp</code>) and two operators that wrap existing C++ based operators,  (<code>VideoStreamReplayerOp</code> and <code>HolovizOp</code>) through the Holoscan Tensor object. </p> <p>Run the Python example by </p> <p><pre><code>python3 /opt/nvidia/holoscan/examples/tensor_interop/python/tensor_interop.py\n</code></pre> This example applies a Gaussian filtering to each frame of an endoscopy video stream and displays the filtered (blurred) video stream. You should see a window like below</p> <p>https://github.com/jin-nvidia/holohub/assets/60405124/b043637b-5fd9-4ee1-abc5-0dae069e785f</p> <p>The native Python operator is defined at tensor_interop.py#L37. We can see in the initialization <code>__init__()</code> of the operator, <code>self.count</code> was initialize to 1. In the <code>setup()</code> method, the input message, output message and the parameter <code>sigma</code> is defined. The <code>compute()</code> method is what gets called every time. In the <code>compute()</code> method, first we receive the upstream tensor by </p> <pre><code>in_message = op_input.receive(\"input_tensor\")\n</code></pre> <p>Please note that <code>input_tensor</code> is the name defined in <code>setup()</code>. </p> <p><code>cp_array</code> is the CuPy array that holds the output value after the Gaussian filter, and we can see that the way the CuPy array gets transmitted downstream is  <pre><code>out_message = dict()\n\u2026\n# add each CuPy array to the out_message dictionary\nout_message[key] = cp_array\n\u2026\nop_output.emit(out_message, \"output_tensor\")\n</code></pre></p> <p>Please note that <code>output_tensor</code> is the name defined in <code>setup()</code>. </p> <p>Since there is only one input and one output port, when connecting the native Python operator <code>ImageProcessingOp</code> to its upstream and downstream operators, we do not need to specify the in/out name for <code>ImageProcessingOp</code>: <pre><code>self.add_flow(source, image_processing)\nself.add_flow(image_processing, visualizer, {(\"\", \"receivers\")})\n</code></pre></p> <p>Otherwise, with each <code>add_flow()</code>, the input and output port names need to be specified when multiple ports are present.</p> <p>For more information on tensor interop in Python API, please see the Holoscan documentation Interoperability between wrapped and native Python operators. </p> <p>Now that we have seen an example of tensor interop for single tensors per port, let\u2019s look at the next example where there are multiple tensors in the native operator\u2019s output port.</p>","tags":["Amazon Web Services","AWS","Cloud","EC2"]},{"location":"tutorials/holoscan-playground-on-aws/#holoviz-example","title":"Holoviz Example","text":"<p>Let\u2019s take a look at the Holoviz example. Run the example <pre><code>python3 /opt/nvidia/holoscan/examples/holoviz/python/holoviz_geometry.py\n</code></pre></p> <p>You should get something like below on the display</p> <p>https://github.com/jin-nvidia/holohub/assets/60405124/6d79845a-66bd-4448-9646-284b90c5e5f3</p> <p>Please take your time to look through holoviz_geometry.py for how each one of the shapes and text in the native Holoscan Python operator is defined. </p> <p>Let\u2019s also dive into how we can add to <code>out_message</code> and pass to Holoviz various tensors at the same time, including the frame itself, <code>box_coords</code>,  <code>triangle_coords</code>, <code>cross_coords</code>, <code>oval_coords</code>, the time varying <code>point_coords</code>, and <code>label_coords</code>. </p> <pre><code># define the output message dictionary where box_coords is a numpy array and \u201cboxes\u201d is the tensor name\nout_message = {\n            \"boxes\": box_coords,\n            \"triangles\": triangle_coords,\n            \"crosses\": cross_coords,\n            \"ovals\": oval_coords,\n            \"points\": point_coords,\n            \"label_coords\": label_coords,\n            \"dynamic_text\": dynamic_text,\n}\n\n# emit the output message \nop_output.emit(out_message, \"outputs\")\n</code></pre> <p>We can also see that each tensor name is referenced by the <code>tensors</code> parameter in the instantiation of a Holoviz operator at line 249. </p> <p>This is a great example and reference not only for passing different shapes to Holoviz, but also creating and passing multiple tensors within one message from a native Holoscan Python operator to the downstream operators.</p> <p>For more information on the Holoviz module, please see the Holoscan documentation. </p> <p>Exit from the Docker container by ctrl+D.</p>","tags":["Amazon Web Services","AWS","Cloud","EC2"]},{"location":"tutorials/holoscan-playground-on-aws/#applications","title":"Applications","text":"<p>To run the reference applications on Holoscan, let\u2019s go to HoloHub - a central repository for users and developers to share reusable operators and sample applications.</p> <p>On the EC2 instance, clone the HoloHub repo: <pre><code>cd ~\ngit clone https://github.com/nvidia-holoscan/holohub.git\ncd holohub\n</code></pre> To set up and build HoloHub, we will go with the option <code>Building dev container</code>: Run the following command from the holohub directory to build the development container: <pre><code>./dev_container build\n</code></pre></p> <p>Check the tag for the container we had just build:</p> <p><pre><code>docker images\n</code></pre> There should be an image with repository:tag similar to <code>holohub:ngc-vx.y.z-dgpu</code> where <code>x.y.z</code> is the latest SDK version. We will set this as <code>HOLOHUB_IMAGE</code>: <pre><code># make sure to replace 0.6.0 with the actual SDK version\nexport HOLOHUB_IMAGE=holohub:ngc-v0.6.0-dgpu\n</code></pre> Next, launch the dev container for HoloHub. On a regular machine we can do so by <code>./dev_container launch</code>, however we need to make a few adjustments to the command again since we\u2019re running on an EC2 instance: <pre><code>docker run -it --rm --net host  -v /etc/group:/etc/group:ro -v /etc/passwd:/etc/passwd:ro -v $PWD:/workspace/holohub -w /workspace/holohub --gpus all -e NVIDIA_DRIVER_CAPABILITIES=graphics,video,compute,utility,display -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY --group-add video -v /etc/vulkan/icd.d/nvidia_icd.json:/etc/vulkan/icd.d/nvidia_icd.json:ro  -v $XAUTH:$XAUTH -e XAUTHORITY=$XAUTH $HOLOHUB_IMAGE\n</code></pre> Please refer to HoloHub for instructions on building each application.</p>","tags":["Amazon Web Services","AWS","Cloud","EC2"]},{"location":"tutorials/holoscan-playground-on-aws/#endoscopy-tool-tracking-application","title":"Endoscopy Tool Tracking Application","text":"<p>Build the sample app and run: <pre><code>./run build endoscopy_tool_tracking\n./run launch endoscopy_tool_tracking cpp\n</code></pre> You should see a window like:</p> <p>https://github.com/jin-nvidia/holohub/assets/60405124/8eb93c50-d893-4b2c-897b-57de94b91371</p> <p>Note: Be prepared to wait a few minutes as we\u2019re running the app for the first time, and it will convert the ONNX model to a TensorRT engine. The conversion happens only for the first time, after that, each time we run the app the TensorRT engine is already present.</p> <p>Please visit HoloHub to see the application graph, different input types (although on the EC2 instance we can not use a live source such as the AJA capture card), and the construction of the same application in C++ vs in Python.</p>","tags":["Amazon Web Services","AWS","Cloud","EC2"]},{"location":"tutorials/holoscan-playground-on-aws/#multi-ai-ultrasound-application","title":"Multi AI Ultrasound Application","text":"<p>In the last application we saw how to run AI inference on the video source. Next, let\u2019s see how we can run inference with multiple AI models at the same time within a Holoscan application, enabled by Holoscan Inference Module. Build the application in applications/multiai_ultrasound <pre><code>./run build multiai_ultrasound\n</code></pre></p> <p>Launch the Python application: <pre><code>./run launch multiai_ultrasound python\n</code></pre> You should see a window like below:</p> <p>https://github.com/jin-nvidia/holohub/assets/60405124/9d347b44-d635-4cc6-b013-7d26e3e4e2be</p> <p>You can find more information on Holoscan Inference Module here, including the parameters you can specify to define inference configuration, how to specify the multiple (or single) model(s) you want to run, and how the Holoscan Inference Module functions as an operator within the Holoscan SDK framework.</p> <p>Please see the application graph and more on HoloHub for how the multi AI inference connects to the rest of the operators, the definition of the same application in Python vs in C++, and how the multiai_ultrasound.yaml config file defines parameters for each operator especially the Holoscan Inference Module.</p>","tags":["Amazon Web Services","AWS","Cloud","EC2"]},{"location":"tutorials/holoscan-playground-on-aws/#stop-ec2-instance","title":"Stop EC2 Instance","text":"<p>Now that you have run several Holoscan Examples and HoloHub Applications, please continue exploring the rest of Examples and Applications, and when you\u2019re ready, stop the instance by going back to EC2 page with the list of <code>Instances</code>, select the launched instance, and select <code>Stop Instance</code> in the dropdown <code>Instance state</code>.</p> <p></p>","tags":["Amazon Web Services","AWS","Cloud","EC2"]},{"location":"tutorials/holoscan-playground-on-aws/#troubleshooting","title":"Troubleshooting","text":"<p>If you receive a display forwarding error such as  <pre><code>unable to open display \"localhost:10.0\"\n</code></pre> <pre><code>Glfw Error 65544: X11: Failed to open display localhost:10.0\nterminate called after throwing an instance of 'std::runtime_error'\n  what():  Failed to initialize glfw\n</code></pre> Please see below to find the suggestion for your OS.</p>","tags":["Amazon Web Services","AWS","Cloud","EC2"]},{"location":"tutorials/holoscan-playground-on-aws/#from-a-linux-local-machine","title":"From a Linux Local Machine","text":"<ul> <li>Ensure that <code>-X</code> is added to the ssh command when connecting to the EC2 instance.</li> </ul>","tags":["Amazon Web Services","AWS","Cloud","EC2"]},{"location":"tutorials/holoscan-playground-on-aws/#from-a-windows-local-machine","title":"From a Windows Local Machine","text":"<ul> <li>Ensure that <code>-X</code> is added to the ssh command when connecting to the EC2 instance.</li> <li>Try using MobaXTerm to establish a SSH connection with X11 forwarding enabled.</li> </ul>","tags":["Amazon Web Services","AWS","Cloud","EC2"]},{"location":"tutorials/holoscan-playground-on-aws/#from-a-mac-local-machine","title":"From a Mac Local Machine","text":"<ul> <li>Download Quartz, reboot, and enable the following. </li> </ul> <p>Once Quartz is downloaded it will automatically launch when running display forwarding apps like <code>xeyes</code>.</p> <ul> <li>Ensure that <code>-X</code> is added to the ssh command when connecting to the EC2 instance.</li> </ul>","tags":["Amazon Web Services","AWS","Cloud","EC2"]},{"location":"tutorials/holoscan_container_vscode/","title":"Holoscan SDK Visual Studio Code Dev Container Template","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: January 27, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.3.0 Tested Holoscan SDK versions: 2.3.0 Contribution metric: Level 1 - Highly Reliable</p>","tags":["VS Code","Dev Container","Tutorial"]},{"location":"tutorials/holoscan_container_vscode/#overview","title":"Overview","text":"<p>A Dev Container (short for Development Container) is a lightweight, isolated environment for developing software. It's a self-contained directory that contains all the dependencies and tools needed to develop a software project without polluting the host machine or interfering with other projects.</p> <p>This directory contains a pre-configured Dev Container designed for Holoscan SDK using the Holoscan NGC Container with Visual Studio Code. This Dev Container comes with the complete Holoscan SDK and source code, sample applications, and all the tools needed to build your next application using Holoscan SDK.</p> <p>Follow this step-by-step guide to get started!</p>","tags":["VS Code","Dev Container","Tutorial"]},{"location":"tutorials/holoscan_container_vscode/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker</li> <li>VS Code with the Dev Container Extension Pack</li> <li>Install Dev Container Extension Pack via command line     <pre><code>code --install-extension ms-vscode-remote.remote-containers\n</code></pre></li> <li>NVIDIA Container Toolkit</li> <li>NVIDIA CUDA Toolkit</li> <li>Holoscan SDK container 2.3 or later</li> </ul>","tags":["VS Code","Dev Container","Tutorial"]},{"location":"tutorials/holoscan_container_vscode/#dev-container-setup","title":"Dev Container Setup","text":"<ol> <li>Download everything in this directory to a folder of your choice. This folder can also be used to store your project(s) later. See Directory Structure.</li> <li>Open .devcontainer/devcontainer.json, find and change the value of <code>HOLOSCAN_SDK_IMAGE</code> to a version of Holoscan SDK container image you want to use.   \ud83d\udca1  Important: Holoscan SDK container v2.3 or later is required to step into Holoscan SDK source code in debug mode.</li> <li>Start VS Code, open the Command Palette by pressing <code>F1</code>, <code>Ctrl+Shift+P</code>, or navigating to <code>View &gt; Command Palette</code>, type to select <code>Dev Containers: Open Folder in Container...</code> and select the folder where you stored step 1.</li> <li>Wait for the Dev Container to start up. Building the container and installing all the required extensions will take a while. (Switch to the <code>Output</code> view and select the <code>Server</code> option from the dropdown to check the status of extension installations.)</li> </ol> <p>When everything is ready, you should see the following in the VS Code Explorer sidebar.</p> <p></p> <p>The <code>My Workspace</code> section contains all the files that you've copied. The <code>Holoscan SDK</code> section includes the Holoscan example applications from <code>~/examples</code>.</p>","tags":["VS Code","Dev Container","Tutorial"]},{"location":"tutorials/holoscan_container_vscode/#directory-structure","title":"Directory Structure","text":"<pre><code>/workspace\n   \u251c\u2500\u2500 .vscode/            # Visual Studio Code debugging configuration files\n   \u251c\u2500\u2500 holoscan-sdk/       # Holoscan SDK source code\n   \u2514\u2500\u2500 my/                 # Your workspace directory - this is the directory created and mounted from step 1 above.\n       \u251c\u2500\u2500 .devcontainer/  # Dev Container configurations\n       \u251c\u2500\u2500 src/            # A folder set up to store your project(s)\n       \u2514\u2500\u2500 README.md       # This file\n</code></pre> <p>\ud83d\udca1 Note: The Dev Container is configured with a default user account <code>holoscan</code> using user ID <code>1000</code> and group ID <code>1000</code> in the devcontainer.json file.   - To change the user name, find and replace <code>USERNAME</code> and <code>remoteUser</code>.  - To change the user ID, find and replace <code>USER_UID</code> and <code>userUid</code>.  - To change the group ID, find and replace <code>USER_GID</code> and <code>userGid</code>.</p>","tags":["VS Code","Dev Container","Tutorial"]},{"location":"tutorials/holoscan_container_vscode/#debugging-applications","title":"Debugging Applications","text":"","tags":["VS Code","Dev Container","Tutorial"]},{"location":"tutorials/holoscan_container_vscode/#debugging-a-c-application","title":"Debugging a C++ Application","text":"<p>\ud83d\udca1 Tip: Open this <code>README.md</code> file in the Dev Container will help you navigate to the linked files faster.      This file can be found inside the Dev Container under <code>/workspace/my/README.md</code>.</p> <p>This section will walk you through the steps to debug the Hello World application using VS Code.</p> <p>Open the hello_world.cpp file in VS Code. (If the link doesn't work, click on Explorer from the sidebar and expand the holoscan-sdk folder. Find and open <code>hello_world.cpp</code> under the <code>examples/hello_world/cpp</code> directory. Let's put a breakpoint on this line: <code>auto app = holoscan::make_application&lt;HelloWorldApp&gt;();</code> (Feel free to put more breakpoints wherever you want).</p> <p>Now, let's switch to the Run and Debug panel on the sidebar, and then click on the dropdown box to the right of the Start button.</p> <p>Select <code>(gdb) examples/hello_world/cpp</code> from the list of available launch configurations.</p> <p>Hit F5 on the keyboard or click the green arrow to start debugging. VS Code shall hit the breakpoint and stop as the screenshot shows below: </p> <p>\ud83d\udca1 Tip: What happens when you hit F5? VS Code looks up the launch profile for <code>(gdb) examples/hello_world/cpp</code> in the .vscode/launch.json file and starts the debugger with the appropriate configurations and arguments.</p>","tags":["VS Code","Dev Container","Tutorial"]},{"location":"tutorials/holoscan_container_vscode/#debugging-a-python-application","title":"Debugging a Python Application","text":"<p>There are a few options when debugging a Python application. In the .vscode/launch.json file, you may find the following options to debug the Hello World application:</p> <ul> <li>Python: Debug Current File: with this option selected, open hello_world.py file and hit F5. It shall stop at any breakpoints selected.</li> <li>Python C++ Debug: similar to the previous option, this launch profile allows you to debug both the Python and C++ code.   Open holoscan-sdk/src/core/application.cpp and find the <code>Application::run()</code> function. Let's put a breakpoint inside this function. Navigate back to the hello_world.py file and hit F5. When the debug session starts, it stops at the top of the main application file and brings up a prompt in the terminal asking for superuser access.</li> </ul> <pre><code>Superuser access is required to attach to a process. Attaching as superuser can potentially harm your computer. Do you want to continue? [y/N]\n</code></pre> <p>You may answer <code>Y</code> or <code>y</code> to continue the debug session. The debugger shall now stop at the breakpoint you've set in the <code>application.cpp</code> file.</p> <ul> <li>(gdb) examples/hello_world/python: this third launch profile option allows you to debug the C++ code only.  Put a breakpoint in the <code>Application::run()</code> function inside the holoscan-sdk/src/core/application.cpp file.</li> </ul> <p>\ud83d\udca1 Tip: you must open a Python file and make sure the file tab is active to debug when using the first two launch profiles.*</p>","tags":["VS Code","Dev Container","Tutorial"]},{"location":"tutorials/holoscan_container_vscode/#troubleshooting","title":"Troubleshooting","text":"","tags":["VS Code","Dev Container","Tutorial"]},{"location":"tutorials/holoscan_container_vscode/#cannot-start-debugger","title":"Cannot Start Debugger","text":"<ul> <li>Configured debug type 'cppdbg' is not supported.</li> <li>Configured debug type 'debugpy' is not supported.</li> <li>Configured debug type 'pythoncpp' is not supported.</li> <li>Configured debug type 'python' is not supported.</li> </ul> <p>If you encounter the above errors, please ensure all the required extensions are installed in VS Code. It may take a while to install them for the first time.</p>","tags":["VS Code","Dev Container","Tutorial"]},{"location":"tutorials/holoscan_container_vscode/#cannot-set-breakpoints","title":"Cannot Set Breakpoints","text":"<p>If you cannot set breakpoints, please ensure all the required extensions are installed in VS Code. It may take a while to install them for the first time.</p>","tags":["VS Code","Dev Container","Tutorial"]},{"location":"tutorials/integrate_external_libs_into_pipeline/","title":"Best Practices to integrate external libraries into Holoscan pipelines","text":"<p> Authors: Meiran Peng (NVIDIA) Supported platforms: amd64, arm64 Last modified: July 18, 2024 Language: Python Latest version: 0.1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The Holoscan SDK is part of NVIDIA Holoscan, the AI sensor processing platform that combines hardware systems for low-latency sensor and network connectivity, optimized libraries for data processing and AI, and core microservices to run streaming, imaging, and other applications, from embedded to edge to cloud. It can be used to build streaming AI pipelines for a variety of domains, including medical devices, high-performance computing at the edge, industrial inspection, and more.</p> <p>With the Holoscan SDK, one can develop an end-to-end GPU-accelerated pipeline with RDMA support. However, with increasing requirements for pre-processing and post-processing beyond inference-only pipelines, integration with other powerful, GPU-accelerated libraries is needed.</p> <p>One of the key features of the Holoscan SDK is its seamless interoperability with other libraries.</p> <p>This tutorial explains how to leverage this capability in your applications. For detailed examples of integrating various libraries with Holoscan applications, refer to the following sections: - Tensor Interoperability   - Integrate MatX library - DLPack support in C++   - Integrate RAPIDS cuCIM library   - Integrate CV-CUDA library   - Integrate OpenCV with CUDA Module   - Integrate PyTorch library - CUDA Interoperability   - Integrate CUDA Python library   - Integrate CuPy library</p>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#interoperability-features","title":"Interoperability Features","text":"","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#dlpack-support","title":"DLPack Support","text":"<p>The Holoscan SDK supports DLPack, enabling efficient data exchange between deep learning frameworks.</p>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#array-interface-support","title":"Array Interface Support","text":"<p>The SDK also supports the array interface, including: - <code>__array_interface__</code> - <code>__cuda_array_interface__</code></p> <p>This allows for seamless integration with various Python libraries such as: - CuPy - PyTorch - JAX - TensorFlow - Numba</p>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#technical-details","title":"Technical Details","text":"<p>The <code>Tensor</code> class is a wrapper around the <code>DLManagedTensorContext</code> struct, which holds the <code>DLManagedTensor</code> object (a DLPack structure).</p> <p>For more information on interoperability, refer to the following sections in the Holoscan SDK documentation: - Interoperability between GXF and native C++ operators - Interoperability between wrapped and native Python operators</p>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#cuda-array-interfacedlpack-support","title":"CUDA Array Interface/DLPack Support","text":"<p>The following Python libraries have adopted the CUDA Array Interface and/or DLPack standards, enabling seamless interoperability with Holoscan Tensors:</p> <ul> <li>CuPy</li> <li>CV-CUDA</li> <li>PyTorch</li> <li>Numba</li> <li>PyArrow</li> <li>mpi4py</li> <li>ArrayViews</li> <li>JAX</li> <li>PyCUDA</li> <li>DALI: the NVIDIA Data Loading Library\u00a0:</li> <li>TensorGPU objects\u00a0expose the CUDA Array Interface.</li> <li>The External Source operator\u00a0consumes objects exporting the CUDA Array Interface.</li> <li>The RAPIDS stack:</li> <li>cuCIM</li> <li>cuDF</li> <li>cuML</li> <li>cuSignal</li> <li>RMM</li> </ul> <p>For more details on using the CUDA Array Interface and DLPack with various libraries, see CuPy's Interoperability guide.</p>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#using-holoscan-tensors-in-python","title":"Using Holoscan Tensors in Python","text":"<p>The Holoscan SDK's Python API provides the <code>holoscan.as_tensor()</code> method to convert objects supporting the (CUDA) Array Interface or DLPack to a Holoscan Tensor. The <code>holoscan.Tensor</code> object itself also supports these interfaces, allowing for easy integration with compatible libraries.</p> <p>Example usage:</p> <pre><code>import cupy as cp\nimport numpy as np\nimport torch\nimport holoscan as hs\n\n# Create tensors using different libraries\ntorch_cpu_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\ntorch_gpu_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]], device=\"cuda\")\nnumpy_tensor = np.array([[1, 2, 3], [4, 5, 6]])\ncupy_tensor = cp.array([[1, 2, 3], [4, 5, 6]])\n\n# Convert to Holoscan Tensors\ntorch_cpu_to_holoscan = hs.as_tensor(torch_cpu_tensor)\ntorch_gpu_to_holoscan = hs.as_tensor(torch_gpu_tensor)\nnumpy_to_holoscan = hs.as_tensor(numpy_tensor)\ncupy_to_holoscan = hs.as_tensor(cupy_tensor)\n</code></pre>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#tensor-interoperability","title":"Tensor Interoperability","text":"","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#integrate-matx-library","title":"Integrate MatX library","text":"<p>MatX library (An efficient C++17 GPU numerical computing library with Python-like syntax) is an open-source, efficient C++17 GPU numerical computing library created by NVIDIA. It provides a NumPy-like interface for GPU-accelerated numerical computing, enabling developers to write high-performance, GPU-accelerated code with ease.</p>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#installation","title":"Installation","text":"<p>MatX is a header-only library. Using it in your own projects is as simple as including only the core <code>matx.h</code> file.</p> <p>Please refer to the MatX documentation for detailed instructions on building and using the MatX library.</p> <p>The following is a sample CMakeLists.txt file for a project that uses MatX:</p> <pre><code>cmake_minimum_required(VERSION 3.20)\nproject(my_app CXX)\n\n# Holoscan\nfind_package(holoscan 2.2 REQUIRED CONFIG\n             PATHS \"/opt/nvidia/holoscan\" \"/workspace/holoscan-sdk/install\")\n\n# Enable cuda language\nset(CMAKE_CUDA_ARCHITECTURES \"70;80\")\nenable_language(CUDA)\n\n# Download MatX (from 'main' branch)\ninclude(FetchContent)\nFetchContent_Declare(\n  MatX\n  GIT_REPOSITORY https://github.com/NVIDIA/MatX.git\n  GIT_TAG main\n)\nFetchContent_MakeAvailable(MatX)\n\nadd_executable(my_app\n  my_app.cpp\n)\n\ntarget_link_libraries(my_app\n  PRIVATE\n  holoscan::core\n  # ...\n  matx::matx\n)\n</code></pre>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#sample-code","title":"Sample code","text":"<p>The following are the sample applications that use the MatX library to integrate with Holoscan SDK.</p> <ul> <li>Multi AI Application with SSD Detection and MONAI Endoscopic Tool Segmentation</li> <li><code>applications/multiai_endoscopy</code></li> <li>Network Radar Pipeline</li> <li><code>applications/network_radar_pipeline/cpp</code></li> <li>Simple Radar Pipeline Application</li> <li><code>applications/simple_radar_pipeline/cpp</code></li> </ul> <p>On the GPU</p> <ul> <li>https://github.com/nvidia-holoscan/holohub/blob/main/applications/multiai_endoscopy/cpp/post-proc-matx-gpu/multi_ai.cu</li> </ul> <pre><code>#include &lt;holoscan/holoscan.hpp&gt;\n#include &lt;matx.h&gt;\n\n// ...\n\nvoid compute(InputContext&amp; op_input, OutputContext&amp; op_output,\n             ExecutionContext&amp; context) override {\n  // Get input message and make output message\n  auto in_message = op_input.receive&lt;gxf::Entity&gt;(\"in\").value();\n  // ...\n  auto boxes = in_message.get&lt;Tensor&gt;(\"inference_output_detection_boxes\");\n  auto scores = in_message.get&lt;Tensor&gt;(\"inference_output_detection_scores\");\n  int32_t Nb = scores-&gt;shape()[1];  // Number of boxes\n  auto Nl = matx::make_tensor&lt;int&gt;({});  // Number of label boxes\n  // ...\n  auto boxesl_mx = matx::make_tensor&lt;float&gt;({1, Nl(), 4});\n  (boxesl_mx = matx::remap&lt;1&gt;(boxes_ix_mx, ixl_mx)).run();\n  // ...\n  // Holoscan tensors to MatX tensors\n  auto boxes_mx = matx::make_tensor&lt;float&gt;((float*)boxes-&gt;data(), {1, Nb, 4});\n  // ...\n  // MatX to Holoscan tensor\n  auto boxes_hs = std::make_shared&lt;holoscan::Tensor&gt;(boxesls_mx.GetDLPackTensor());\n  // ...\n}\n</code></pre> <p>On the CPU</p> <ul> <li>https://github.com/nvidia-holoscan/holohub/blob/main/applications/multiai_endoscopy/cpp/post-proc-matx-cpu/multi_ai.cpp</li> </ul> <p>MatX library usage on the CPU is similar to the GPU version, but the <code>run()</code> function is called with <code>matx::SingleThreadHostExecutor()</code> to run the operation on the CPU.</p> <pre><code>#include &lt;holoscan/holoscan.hpp&gt;\n#include &lt;matx.h&gt;\n\n// ...\n\nvoid compute(InputContext&amp; op_input, OutputContext&amp; op_output,\n             ExecutionContext&amp; context) override {\n  // Get input message and make output message\n  auto in_message = op_input.receive&lt;gxf::Entity&gt;(\"in\").value();\n  // ...\n  auto boxesh = in_message.get&lt;Tensor&gt;(\"inference_output_detection_boxes\");  // (1, num_boxes, 4)\n  auto scoresh = in_message.get&lt;Tensor&gt;(\"inference_output_detection_scores\");  // (1, num_boxes)\n  int32_t Nb = scoresh-&gt;shape()[1];  // Number of boxes\n  auto Nl = matx::make_tensor&lt;int&gt;({});  // Number of label boxes\n  // ...\n  auto boxes = copy_device2vec&lt;float&gt;(boxesh);\n  // Holoscan tensors to MatX tensors\n  auto boxes_mx = matx::make_tensor&lt;float&gt;(boxes.data(), {1, Nb, 4});\n  // ...\n  auto boxesl_mx = matx::make_tensor&lt;float&gt;({1, Nl(), 4});\n  (boxesl_mx = matx::remap&lt;1&gt;(boxes_ix_mx, ixl_mx)).run(matx::SingleThreadHostExecutor());\n  // ...\n  // MatX to Holoscan tensor\n  auto boxes_hs = std::make_shared&lt;holoscan::Tensor&gt;(boxesls_mx.GetDLPackTensor());\n  // ...\n}\n</code></pre>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#integrate-rapids-cucim-library","title":"Integrate RAPIDS cuCIM library","text":"<p>RAPIDS cuCIM (Compute Unified Device Architecture Clara IMage) is an open-source, accelerated computer vision and image processing software library for multidimensional images used in biomedical, geospatial, material and life science, and remote sensing use cases.</p> <p>See the supported Operators in cuCIM documentation.</p> <p>cuCIM offers interoperability with CuPy. We can initialize CuPy arrays directly from Holoscan Tensors and use the arrays in cuCIM operators for processing without memory transfer between host and device.</p>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#installation_1","title":"Installation","text":"<p>Follow the cuCIM documentation to install the RAPIDS cuCIM library.</p>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#sample-code_1","title":"Sample code","text":"<pre><code>import cupy as cp\nimport cucim.skimage.exposure as cu_exposure\nfrom cucim.skimage.util import img_as_ubyte\nfrom cucim.skimage.util import img_as_float\n\ndef CustomizedcuCIMOperator(Operator):\n    ### Other implementation of __init__, setup()... etc.\n\n    def compute(self, op_input, op_output, context):\n        message = op_input.receive(\"input_tensor\")\n        input_tensor = message.get()\n        # Directly use Holoscan tensor to initialize CuPy array\n        cp_array = cp.asarray(input_tensor)\n\n        cp_array = img_as_float(cp_array)\n        cp_res=cu_exposure.equalize_adapthist(cp_array)\n        cp_array = img_as_ubyte(cp_res)\n\n        # Emit CuPy array memory as an item in a `holoscan.TensorMap`\n        op_output.emit(dict(out_tensor=cp_array), \"out\")\n</code></pre>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#integrate-cv-cuda-library","title":"Integrate CV-CUDA library","text":"<p>CV-CUDA is an open-source, graphics processing unit (GPU)-accelerated library for cloud-scale image processing and computer vision developed jointly by NVIDIA and the ByteDance Applied Machine Learning teams. CV-CUDA helps developers build highly efficient pre- and post-processing pipelines that can improve throughput by more than 10x while lowering cloud computing costs.</p> <p>See the supported CV-CUDA Operators in the CV-CUDA developer guide</p>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#installation_2","title":"Installation","text":"<p>Follow the CV-CUDA documentation to install the CV-CUDA library.</p> <p>Requirement: CV-CUDA &gt;= 0.2.1 (From which version DLPack interop is supported)</p>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#sample-code_2","title":"Sample code","text":"<p>CV-CUDA is implemented with DLPack standards. A CV-CUDA tensor can directly access a Holoscan Tensor.</p> <p>Refer to the Holoscan CV-CUDA sample application for an example of how to use CV-CUDA with Holoscan SDK.</p> <pre><code>import cvcuda\n\nclass CustomizedCVCUDAOp(Operator):\n    def __init__(self, *args, **kwargs):\n\n        # Need to call the base class constructor last\n        super().__init__(*args, **kwargs)\n\n    def setup(self, spec: OperatorSpec):\n        spec.input(\"input_tensor\")\n        spec.output(\"output_tensor\")\n\n    def compute(self, op_input, op_output, context):\n        message = op_input.receive(\"input_tensor\")\n        input_tensor = message.get()\n\n        cvcuda_input_tensor = cvcuda.as_tensor(input_tensor,\"HWC\")\n\n        cvcuda_resize_tensor = cvcuda.resize(\n                    cvcuda_input_tensor,\n                    (\n                        640,\n                        640,\n                        3,\n                    ),\n                    cvcuda.Interp.LINEAR,\n                )\n\n        buffer = cvcuda_resize_tensor.cuda()\n\n        # Emits an `holoscan.TensorMap` with a single entry `out_tensor`\n        op_output.emit(dict(out_tensor=buffer), \"output_tensor\")\n</code></pre>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#integrate-opencv-with-cuda-module","title":"Integrate OpenCV with CUDA Module","text":"<p>OpenCV (Open Source Computer Vision Library) is a comprehensive open-source library that contains over 2500 algorithms covering Image &amp; Video Manipulation, Object and Face Detection, OpenCV Deep Learning Module and much more.</p> <p>OpenCV also supports GPU acceleration and includes a CUDA module which is a set of classes and functions to utilize CUDA computational capabilities. It is implemented using NVIDIA CUDA Runtime API and provides utility functions, low-level vision primitives, and high-level algorithms.</p>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#installation_3","title":"Installation","text":"<p>Prerequisites: - OpenCV &gt;= 4.8.0 (From which version, OpenCV GpuMat supports initialization with GPU Memory pointer)</p> <p>Install OpenCV with its CUDA module following the guide in opencv/opencv_contrib</p> <p>We also recommend referring to the Holoscan Endoscopy Depth Estimation application container as an example of how to build an image with Holoscan SDK and OpenCV CUDA.</p>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#sample-code_3","title":"Sample code","text":"<p>The data type of OpenCV is GpuMat which implements neither the cuda_array_interface nor the standard DLPack. To achieve the end-to-end GPU accelerated pipeline/application, we need to implement 2 functions to convert the GpuMat to CuPy array which can be accessed directly with Holoscan Tensor and vice versa.</p> <p>Refer to the Holoscan Endoscopy Depth Estimation sample application for an example of how to use the OpenCV operator with Holoscan SDK.</p> <ol> <li>Conversion from GpuMat to CuPy Array</li> </ol> <p>The GpuMat object of OpenCV Python bindings provides a cudaPtr method that can be used to access the GPU memory address of a GpuMat object. This memory pointer can be utilized to initialize a CuPy array directly, allowing for efficient data handling by avoiding unnecessary data transfers between the host and device.</p> <p>Refer to the function below, which is used to create a CuPy array from a GpuMat. For more details, see the source code in holohub/applications/endoscopy_depth_estimation-gpumat_to_cupy.</p> <pre><code>import cv2\nimport cupy as cp\n\ndef gpumat_to_cupy(gpu_mat: cv2.cuda.GpuMat) -&gt; cp.ndarray:\n    w, h = gpu_mat.size()\n    size_in_bytes = gpu_mat.step * w\n    shapes = (h, w, gpu_mat.channels())\n    assert gpu_mat.channels() &lt;=3, \"Unsupported GpuMat channels\"\n\n    dtype = None\n    if gpu_mat.type() in [cv2.CV_8U,cv2.CV_8UC1,cv2.CV_8UC2,cv2.CV_8UC3]:\n        dtype = cp.uint8\n    elif gpu_mat.type() == cv2.CV_8S:\n        dtype = cp.int8\n    elif gpu_mat.type() == cv2.CV_16U:\n        dtype = cp.uint16\n    elif gpu_mat.type() == cv2.CV_16S:\n        dtype = cp.int16\n    elif gpu_mat.type() == cv2.CV_32S:\n        dtype = cp.int32\n    elif gpu_mat.type() == cv2.CV_32F:\n        dtype = cp.float32\n    elif gpu_mat.type() == cv2.CV_64F:\n        dtype = cp.float64\n\n    assert dtype is not None, \"Unsupported GpuMat type\"\n\n    mem = cp.cuda.UnownedMemory(gpu_mat.cudaPtr(), size_in_bytes, owner=gpu_mat)\n    memptr = cp.cuda.MemoryPointer(mem, offset=0)\n    cp_out = cp.ndarray(\n        shapes,\n        dtype=dtype,\n        memptr=memptr,\n        strides=(gpu_mat.step, gpu_mat.elemSize(), gpu_mat.elemSize1()),\n    )\n    return cp_out\n</code></pre> <p>Note: In this function, we used the UnownedMemory API to create the CuPy array. In some cases, the OpenCV operators will allocate new device memory which needs to be handled by CuPy and the lifetime is not limited to one operator but the whole pipeline. In this case, the CuPy array initiated from the GpuMat shall know the owner and keep the reference to the object. Check the CuPy documentation for more details on CuPy interoperability.</p> <ol> <li>Conversion from Holoscan Tensor to GpuMat via CuPy array</li> </ol> <p>With the release of OpenCV 4.8, the Python bindings for OpenCV now support the initialization of GpuMat objects directly from GPU memory pointers. This capability facilitates more efficient data handling and processing by allowing direct interaction with GPU-resident data, bypassing the need for data transfer between host and device memory.</p> <p>Within pipeline applications based on Holoscan SDK, the GPU Memory pointer can be obtained through the <code>__cuda_array_interface__</code> interface provided by CuPy arrays.</p> <p>Refer to the functions outlined below for creating GpuMat objects utilizing CuPy arrays. For a detailed implementation, see the source code provided in holohub/applications/endoscopy_depth_estimation-gpumat_from_cp_array.</p> <pre><code>import cv2\nimport cupy as cp\nimport holoscan as hs\nfrom holoscan.gxf import Entity\n\ndef gpumat_from_cp_array(arr: cp.ndarray) -&gt; cv2.cuda.GpuMat:\n    assert len(arr.shape) in (2, 3), \"CuPy array must have 2 or 3 dimensions to be a valid GpuMat\"\n    type_map = {\n        cp.dtype('uint8'): cv2.CV_8U,\n        cp.dtype('int8'): cv2.CV_8S,\n        cp.dtype('uint16'): cv2.CV_16U,\n        cp.dtype('int16'): cv2.CV_16S,\n        cp.dtype('int32'): cv2.CV_32S,\n        cp.dtype('float32'): cv2.CV_32F,\n        cp.dtype('float64'): cv2.CV_64F\n    }\n    depth = type_map.get(arr.dtype)\n    assert depth is not None, \"Unsupported CuPy array dtype\"\n    channels = 1 if len(arr.shape) == 2 else arr.shape[2]\n    mat_type = depth + ((channels - 1) &lt;&lt; 3)\n\n     mat = cv2.cuda.createGpuMatFromCudaMemory(\n      arr.__cuda_array_interface__['shape'][1::-1],\n      mat_type,\n      arr.__cuda_array_interface__['data'][0]\n  )\n    return mat\n</code></pre> <ol> <li>Integrate OpenCV Operators inside customized Operator</li> </ol> <p>The demonstration code is provided below. For the complete source code, please refer to the holohub/applications/endoscopy_depth_estimation-customized Operator.</p> <pre><code>   def compute(self, op_input, op_output, context):\n        stream = cv2.cuda_Stream()\n        message = op_input.receive(\"in\")\n\n        cp_frame = cp.asarray(message.get(\"\"))  # CuPy array\n        cv_frame = gpumat_from_cp_array(cp_frame)  # GPU OpenCV mat\n\n        ## Call OpenCV Operator\n        cv_frame = cv2.cuda.XXX(hsv_merge, cv2.COLOR_HSV2RGB)\n\n        cp_frame = gpumat_to_cupy(cv_frame)\n        cp_frame = cp.ascontiguousarray(cp_frame)\n\n        op_output.emit(dict(out_tensor=cp_frame), \"out\")\n</code></pre>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#integrate-pytorch-library","title":"Integrate PyTorch library","text":"<p>PyTorch is a popular open-source machine learning library developed by Facebook's AI Research lab. It provides a flexible and dynamic computational graph that allows for easy model building and training. PyTorch also supports GPU acceleration, making it ideal for deep learning applications that require high-performance computing.</p> <p>Since PyTorch tensors support the array interface and DLPack (link), they can be interoperable with other array/tensor objects including Holoscan Tensors.</p>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#installation_4","title":"Installation","text":"<p>Follow the PyTorch documentation to install the PyTorch library.</p> <p>e.g., for CUDA 12.x with pip:</p> <pre><code>python3 -m pip install torch torchvision torchaudio\n</code></pre>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#sample-code_4","title":"Sample code","text":"<p>The following is a sample application that demonstrates how to use PyTorch with Holoscan SDK:</p> <pre><code>import torch\n\ndef CustomizedTorchOperator(Operator):\n    ### Other implementation of __init__, setup()... etc.\n\n    def compute(self, op_input, op_output, context):\n        message = op_input.receive(\"input_tensor\")\n        input_tensor = message.get()\n        # Directly use Holoscan tensor to initialize PyTorch tensor\n        torch_tensor = torch.as_tensor(input_tensor, device=\"cuda\")\n\n        torch_tensor *= 2\n\n        # Emit PyTorch tensor memory as a `holoscan.Tensor` item in a `holoscan.TensorMap`\n        op_output.emit(dict(out_tensor=torch_tensor), \"out\")\n</code></pre>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#cuda-interoperability","title":"CUDA Interoperability","text":"","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#integrate-cuda-python-library","title":"Integrate CUDA Python library","text":"<p>CUDA Python is a Python library that provides Cython/Python wrappers for CUDA driver and runtime APIs. It offers a convenient way to leverage GPU acceleration for complex computations, making it ideal for high-performance applications that require intensive numerical processing.</p> <p>When using CUDA Python with the Holoscan SDK, you need to use the Primary context (CUDA doc link) by calling <code>cuda.cuDevicePrimaryCtxRetain()</code> (link).</p> <p>Since the Holoscan Operator is executed in an arbitrary non-main thread, you may need to set the CUDA context using the cuda.cuCtxSetCurrent() method in the <code>Operator.compute()</code> method.</p>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#installation_5","title":"Installation","text":"<p>Follow the instructions in the CUDA Python documentation to install the CUDA Python library.</p> <p>CUDA Python can be installed using <code>pip</code>:</p> <pre><code>python3 -m pip install cuda-python\n</code></pre>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#sample-code_5","title":"Sample code","text":"<p>Please see the example application (cuda_example.py) that demonstrates how to use CUDA Python with the Holoscan SDK.</p> <p>In this example, we define a <code>CudaOperator</code> class that encapsulates the CUDA context, stream, and memory management. The <code>CudaOperator</code> class provides methods for allocating device memory, building CUDA kernels, launching kernels, and cleaning up. We also define three operators: <code>CudaTxOp</code>, <code>ApplyGainOp</code>, and <code>CudaRxOp</code>, which perform data initialization, apply a gain operation, and process the output data, respectively. The output of the <code>CudaRxOp</code> operator is passed to both a <code>ProbeOp</code> operator, which inspects the data and prints the metadata information, and a <code>HolovizOp</code> operator, which visualizes the data using the Holoviz module.</p> <p>There are four examples in the <code>CudaRxOp.compute()</code> method that demonstrate different ways to handle data conversion and transfer between tensor libraries. These examples include creating 1) a NumPy array from CUDA memory, 2) converting a NumPy array to a CuPy array, 3) creating a CUDA array interface object, and 4) creating a CuPy array from CUDA memory.</p> <p><code>__cuda_array_interface__</code> is a dictionary that provides a standard interface for exchanging array data between different libraries in Python. It contains metadata such as the shape, data type, and memory location of the array. By using this interface, you can efficiently transfer tensor data between two libraries without copying the data.</p> <p>In the following example, we create a <code>CudaArray</code> class to represent a CUDA array interface object and populate it with the necessary metadata. This object can then be passed to the <code>op_output.emit()</code> method to transfer the data to downstream operators.</p> <p>In the <code>__cuda_array_interface__</code> dictionary, the <code>stream</code> field is the CUDA stream associated with the data. When passing a <code>cuda.cuda.CUstream</code> object (the variable named <code>stream</code>) to the <code>stream</code> field, you need to convert it to an integer using <code>int(stream)</code>:</p> <pre><code>class CudaRxOp(CudaOperator):\n    # ...\n    def compute(self, op_input, op_output, context):\n        # ...\n        class CudaArray:\n            \"\"\"Class to represent a CUDA array interface object.\"\"\"\n\n            pass\n\n        cuda_array = CudaArray()\n\n        # Reference: https://numba.readthedocs.io/en/stable/cuda/cuda_array_interface.html\n        cuda_array.__cuda_array_interface__ = {\n            \"shape\": (self._frame_height, self._frame_width, self._frame_channels),\n            \"typestr\": np.dtype(np.uint8).str,  # \"|u1\"\n            \"descr\": [(\"\", np.dtype(np.uint8).str)],\n            \"stream\": int(stream),\n            \"version\": 3,\n            \"strides\": None,\n            \"data\": (int(value), False),\n        }\n        # ...\n</code></pre> <p>Please don't confuse this with the <code>cuda.cuda.CUstream.getPtr()</code> method. If you use the <code>stream.getPtr()</code> method, it will return a pointer to the CUDA stream object, not the stream ID. To get the stream ID, you need to convert the <code>stream</code> object to an integer using <code>int(stream)</code>. Otherwise, you will get an error that is difficult to debug, like this:</p> <pre><code>[error] [tensor.cpp:479] Runtime call \"Failure during call to cudaEventRecord\" in line 479 of file ../python/holoscan/core/tensor.cpp failed with 'context is destroyed' (709)\n[error] [gxf_wrapper.cpp:84] Exception occurred for operator: 'rx' - RuntimeError: Error occurred in CUDA runtime API call\n</code></pre>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#integrate-cupy-library","title":"Integrate CuPy library","text":"<p>CuPy is an open-source array library for GPU-accelerated computing with a NumPy-compatible API. It provides a convenient way to perform high-performance numerical computations on NVIDIA GPUs, making it ideal for applications that require efficient data processing and manipulation.</p>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#installation_6","title":"Installation","text":"<p>CuPy can be installed using <code>pip</code>:</p> <pre><code>python3 -m pip install cupy-cuda12x  # for CUDA v12.x\n</code></pre> <p>For more detailed installation instructions, refer to the CuPy documentation.</p>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#sample-code_6","title":"Sample code","text":"<p>Please see the example application (cupy_example.py) that demonstrates how to use CuPy with the Holoscan SDK.</p> <p>This example performs the same operations as the previous example but uses CuPy instead of CUDA Python. The <code>CudaOperator</code> class is modified to use CuPy arrays, and the <code>ApplyGainOp</code> operator is updated to use CuPy functions for array manipulation. The <code>CudaTxOp</code> and <code>CudaRxOp</code> operators are also updated to work with CuPy arrays.</p> <p>With CuPy, you can conveniently perform GPU-accelerated computations on multidimensional arrays, making it an excellent choice for high-performance data processing tasks in Holoscan applications.</p> <p>Please note that CuPy does not fully support certain CUDA APIs, such as <code>cupy.cuda.driver.occupancyMaxPotentialBlockSize()</code>. While the driver API may be available (link), it is currently undocumented (link) and lacks support for calling the API with RawKernel's pointer (link), or using CUDA Python's <code>cuda.cuOccupancyMaxPotentialBlockSize()</code> Driver API with CuPy-generated RawKernel functions.</p> <p>Currently, direct assignment of a non-default CUDA stream to HolovizOp in Holoscan applications is not supported without utilizing the <code>holoscan.resources.CudaStreamPool</code> resource. CuPy also has limited support for custom stream management, necessitating reliance on the default stream in this context.</p> <p>For more detailed information, please refer to the following resources: - New RawKernel Calling Convention / Kernel Occupancy Functions \u00b7 Issue #3684 \u00b7 cupy/cupy \u00b7 GitHub - CUDA Stream Support:   - Enhancing stream support in CuPy's default memory pool \u00b7 Issue #8068 \u00b7 cupy/cupy   - cupy.cuda.ExternalStream \u2014 CuPy 13.1.0 documentation</p>","tags":["CV-CUDA","OpenCV","cuCIM","Holoscan"]},{"location":"tutorials/local-llama/","title":"Deploying Llama-2 70b model on the edge with IGX Orin","text":"\ud83e\udd99 Tutorial: Deploying Llama-2 70b model on the edge with IGX Orin \ud83e\udd99","tags":["Chatbot","CUDA","HuggingFace","Llama","LLM"]},{"location":"tutorials/local-llama/#introduction","title":"Introduction:","text":"<p>With the recent release of the Llama-2 family of models, there has been an excess of excitement in the LLM community due to these models being released freely for research and commercial use. Upon their release, the 70b version of the Llama-2 model quickly rose to the top place on HuggingFace's Open LLM Leaderboard. Additionally, thanks to the publishing of the model weights, fine-tuned versions of these models are consistently being released and raising the bar for the top performing open-LLM. This most recent release of Llama-2 provides some of the first legitimate open-source alternatives to the previously unparalleled performance of closed-source LLMs. This enables developers to deploy these Llama-2 models locally, and benefit from being able to use some of the most advanced LLMs ever created, while also keeping all of their data on their own host machines.</p> <p>The only edge device that is capable of running the Llama-2 70b locally is the NVIDIA IGX Orin. In order to get the Llama-2 70b model running inference, all you need is an IGX Orin, a mouse, a keyboard, and to follow the tutorial below.</p>","tags":["Chatbot","CUDA","HuggingFace","Llama","LLM"]},{"location":"tutorials/local-llama/#overview","title":"Overview:","text":"<p>This tutorial will walk you through how to run a quantized version of Meta's Llama-2 70b model as the backend LLM for a Gradio chatbot app, all running on an NVIDIA IGX Orin. Specifically, we will use Llama.cpp, a project that ports Llama models into C and C++ with CUDA acceleration, to load and run the quantized Llama-2 models. We will setup Llama.cpp's <code>api_like_OAI.py</code> Flask app that emulates the OpenAI API. This will then enable us to create a Gradio chatbot app that utilizes the popular OpenAI API Python library to interact with our local Llama-2 model. Thus, at the conclusion of this tutorial you will have a chatbot app that rivals the performance of closed-source models, while keeping all of your data local and running everything self-contained on an NVIDIA IGX Orin.</p>","tags":["Chatbot","CUDA","HuggingFace","Llama","LLM"]},{"location":"tutorials/local-llama/#hardware-requirements","title":"Hardware Requirements: \ud83d\udc49\ud83d\udcbb","text":"<ul> <li>NVIDIA IGX Orin with:</li> <li>RTX A6000 dGPU</li> <li>500 GB SSD</li> </ul>","tags":["Chatbot","CUDA","HuggingFace","Llama","LLM"]},{"location":"tutorials/local-llama/#dependencies","title":"Dependencies: \ud83d\udce6","text":"<ul> <li>NVIDIA Drivers</li> <li>CUDA Toolkit &gt;= 11.8</li> <li>Python &gt;= 3.8</li> <li><code>build-essential</code> apt package (gcc, g++, etc.)</li> <li>Cmake &gt;= 3.17</li> </ul>","tags":["Chatbot","CUDA","HuggingFace","Llama","LLM"]},{"location":"tutorials/local-llama/#cloning-and-building-llamacpp","title":"Cloning and building Llama.cpp \u2692\ufe0f:","text":"<ol> <li> <p>Clone Llama.cpp: <pre><code>git clone https://github.com/ggerganov/llama.cpp.git\n</code></pre></p> </li> <li> <p>Checkout a stable commit of llama.cpp: <pre><code>cd llama.cpp\ngit checkout e519621010cac02c6fec0f8f3b16cda0591042c0 # Commit date: 9/27/23\n</code></pre></p> </li> <li> <p>Follow cuBLAS build instructions for Llama.cpp to provide BLAS acceleration using the CUDA cores of your NVIDIA GPU. Navigate to the <code>/Llama.cpp</code> directory: <pre><code>cd llama.cpp\n</code></pre> Using <code>make</code>: <pre><code>make LLAMA_CUBLAS=1\n</code></pre></p> </li> </ol> <p>By successfully executing these commands you will now be able to run Llama models on your local machine with BLAS acceleration!</p>","tags":["Chatbot","CUDA","HuggingFace","Llama","LLM"]},{"location":"tutorials/local-llama/#downloading-llama-2-70b","title":"Downloading Llama-2 70B \u2b07\ufe0f\ud83d\udcbe:","text":"<p>In order to use Llama-2 70b as it is provided by Meta, you\u2019d need 140 GB of VRAM (70b params x 2 bytes = 140 GB in FP16). However, by utilizing model quantization, we can reduce the computational and memory costs of running inference by representing the weights and activations as low-precision data types, like int8 and int4, instead of higher-precision data types like FP16 and FP32. To learn more about quantization, check out: The Ultimate Guide to Deep Learning Model Quantization.</p> <p>Llama.cpp uses quantized models that are stored in the GGUF format. Browse to TheBloke on Huggingface.co, who provides hundred of the latest quantized models. Feel free to choose a GGUF model that suits your needs. However, for this tutorial, we will use TheBloke's 4-bit medium GGUF quantization of Meta\u2019s LLama-2-70B-Chat model. 1. Download the GGUF model from Huggingface.co.</p> <p> This model requires ~43 GB of VRAM. <pre><code>cd /media/m2 # Download the model to your SSD drive\nmkdir models # Create a directory for GGUF models\ncd models\nwget https://huggingface.co/TheBloke/Llama-2-70B-chat-GGUF/resolve/main/llama-2-70b-chat.Q4_K_M.gguf\n</code></pre></p>","tags":["Chatbot","CUDA","HuggingFace","Llama","LLM"]},{"location":"tutorials/local-llama/#running-llama-2-70b","title":"Running Llama-2 70B \ud83e\udd16:","text":"<ol> <li> <p>Return to the home directory of Llama.cpp: <pre><code>cd &lt;your_parent_dir&gt;/llama.cpp\n</code></pre></p> </li> <li> <p>Run Llama.cpp\u2019s example server application to set up a HTTP API server and a simple web front end to interact with our Llama model: <pre><code>./server -m /media/m2/models/llama-2-70b-chat.Q4_K_M.gguf -ngl 1000 -c 4096 --alias llama_2\n</code></pre></p> </li> <li> <p><code>-m</code>: indicates the location of our model.</p> </li> <li><code>-ngl</code>: the number of layers to offload to the GPU (1000 ensures all layers are).</li> <li><code>-c</code>: the size of the prompt context.</li> <li><code>--alias</code>: name given to our model for access through the API.</li> </ol> <p>After executing, you should see the below output indicating the model being loaded to VRAM and the specs of the model: <pre><code>ggml_init_cublas: found 1 CUDA devices:\n  Device 0: NVIDIA RTX A6000, compute capability 8.6\n{\"timestamp\":1695853185,\"level\":\"INFO\",\"function\":\"main\",\"line\":1294,\"message\":\"build info\",\"build\":1279,\"commit\":\"e519621\"}\n{\"timestamp\":1695853185,\"level\":\"INFO\",\"function\":\"main\",\"line\":1296,\"message\":\"system info\",\"n_threads\":6,\"total_threads\":12,\"system_info\":\"AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \"}\nllama_model_loader: loaded meta data with 19 key-value pairs and 723 tensors from /media/m2/models/llama-2-70b-chat.Q4_K_M.gguf (version GGUF V2 (latest))\n**Verbose llama_model_loader output removed for conciseness**\nllm_load_print_meta: format         = GGUF V2 (latest)\nllm_load_print_meta: arch           = llama\nllm_load_print_meta: vocab type     = SPM\nllm_load_print_meta: n_vocab        = 32000\nllm_load_print_meta: n_merges       = 0\nllm_load_print_meta: n_ctx_train    = 4096\nllm_load_print_meta: n_ctx          = 4096\nllm_load_print_meta: n_embd         = 8192\nllm_load_print_meta: n_head         = 64\nllm_load_print_meta: n_head_kv      = 8\nllm_load_print_meta: n_layer        = 80\nllm_load_print_meta: n_rot          = 128\nllm_load_print_meta: n_gqa          = 8\nllm_load_print_meta: f_norm_eps     = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps = 1.0e-05\nllm_load_print_meta: n_ff           = 28672\nllm_load_print_meta: freq_base      = 10000.0\nllm_load_print_meta: freq_scale     = 1\nllm_load_print_meta: model type     = 70B\nllm_load_print_meta: model ftype    = mostly Q4_K - Medium\nllm_load_print_meta: model params   = 68.98 B\nllm_load_print_meta: model size     = 38.58 GiB (4.80 BPW) \nllm_load_print_meta: general.name   = LLaMA v2\nllm_load_print_meta: BOS token = 1 '&lt;s&gt;'\nllm_load_print_meta: EOS token = 2 '&lt;/s&gt;'\nllm_load_print_meta: UNK token = 0 '&lt;unk&gt;'\nllm_load_print_meta: LF token  = 13 '&lt;0x0A&gt;'\nllm_load_tensors: ggml ctx size =    0.23 MB\nllm_load_tensors: using CUDA for GPU acceleration\nllm_load_tensors: mem required  =  140.86 MB (+ 1280.00 MB per state)\nllm_load_tensors: offloading 80 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloading v cache to GPU\nllm_load_tensors: offloading k cache to GPU\nllm_load_tensors: offloaded 83/83 layers to GPU\nllm_load_tensors: VRAM used: 40643 MB\n....................................................................................................\nllama_new_context_with_model: kv self size  = 1280.00 MB\nllama_new_context_with_model: compute buffer total size =  561.47 MB\nllama_new_context_with_model: VRAM scratch buffer: 560.00 MB\n\nllama server listening at http://127.0.0.1:8080\n\n{\"timestamp\":1695853195,\"level\":\"INFO\",\"function\":\"main\",\"line\":1602,\"message\":\"HTTP server listening\",\"hostname\":\"127.0.0.1\",\"port\":8080}\n</code></pre></p> <p>Now, you can interact with the simple web front end by browsing to http://127.0.0.1:8080. Use the provided chat interface to query the Llama-2 model and experiment with manipulating the provided hyperparameters to tune the responses to your liking.</p>","tags":["Chatbot","CUDA","HuggingFace","Llama","LLM"]},{"location":"tutorials/local-llama/#setting-up-a-local-openai-server","title":"Setting up a local OpenAI server \ud83d\udda5\ufe0f:","text":"<p>Llama.cpp includes a nifty Flask app <code>api_like_OAI.py</code>. This Flask app sets up a server that emulates the OpenAI API. Its trick is that it converts the OpenAI API requests into the format expected by the Llama model, and forwards the captured requests to our local Llama-2 model. This allows you to use the popular OpenAI Python backend, and thus, countless powerful LLM libraries like LangChain, Scikit-LLM, Haystack, and more. However, instead of your data being sent to OpenAI\u2019s servers, it is all processed locally on your machine! 1. In order to run the OpenAI API server and our eventual Gradio chat app, we need to open a new terminal and install a few Python dependencies: <pre><code>cd tutorials/local-llama\npip install -r requirements.txt\n</code></pre> 2. This then allows us to run the Flask server: <pre><code>cd &lt;your_parent_dir&gt;/llama.cpp/examples/server/\npython api_like_OAI.py\n</code></pre> 3. The server should begin running almost immediately and give you the following output:</p>","tags":["Chatbot","CUDA","HuggingFace","Llama","LLM"]},{"location":"tutorials/local-llama/#creating-the-gradio-chat-app","title":"Creating the Gradio Chat App \ud83d\udcac\ud83d\udcf1:","text":"<ol> <li>Create a new project directory and a <code>chatbot.py</code> file that contains the following code:</li> </ol> <pre><code>import gradio as gr\nimport openai\n\n# Indicate we'd like to send the request\n# to our local model, not OpenAI's servers\nopenai.api_base = \"http://127.0.0.1:8081\"\nopenai.api_key = \"\"\n\n\ndef to_oai_chat(history):\n    \"\"\"Converts the gradio chat history format to\n    the OpenAI chat history format:\n\n    Gradio format: ['&lt;user message&gt;', '&lt;bot message&gt;']\n    OpenAI format: [{'role': 'user', 'content': '&lt;user message&gt;'},\n                    {'role': 'assistant', 'content': '&lt;bot_message&gt;'}]\n\n    Additionally, this adds the 'system' message to the chat to tell the\n    assistant how to act.\n    \"\"\"\n    chat = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful AI Assistant who ends all of your responses with &lt;/BOT&gt;\",\n        }\n    ]\n\n    for msg_pair in history:\n        if msg_pair[0]:\n            chat.append({\"role\": \"user\", \"content\": msg_pair[0]})\n        if msg_pair[1]:\n            chat.append({\"role\": \"assistant\", \"content\": msg_pair[1]})\n    return chat\n\n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot(height=650)\n    msg = gr.Textbox()\n    clear = gr.Button(\"Clear\")\n\n    def user(user_message, history):\n        \"\"\"Appends a submitted question to the history\"\"\"\n        return \"\", history + [[user_message, None]]\n\n    def bot(history):\n        \"\"\"Sends the chat history to our Llama-2 model server\n        so that the model can respond appropriately\n        \"\"\"\n        # Gradio chat -&gt; OpenAI chat\n        oai_chat = to_oai_chat(history)\n\n        # Send chat history to our Llama-2 server\n        response = openai.ChatCompletion.create(\n            messages=oai_chat,\n            stream=True,\n            model=\"llama_2\",\n            temperature=0,\n            # Used to stop runaway responses\n            stop=[\"&lt;/BOT&gt;\"],\n        )\n\n        history[-1][1] = \"\"\n        for response_chunk in response:\n            # Filter through meta-data in the HTTP response to get response text\n            next_token = response_chunk[\"choices\"][0][\"delta\"].get(\"content\")\n            if next_token:\n                history[-1][1] += next_token\n                # Update the Gradio app with the streamed response\n                yield history\n\n    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(bot, chatbot, chatbot)\n    clear.click(lambda: None, None, chatbot, queue=False)\n\ndemo.queue()\ndemo.launch()\n</code></pre> <ol> <li> <p>Begin running the Gradio chat app: <pre><code>python chatbot.py\n</code></pre></p> </li> <li> <p>The chat app should now be accessible at http://127.0.0.1:7860:</p> </li> </ol> <p>You're now set up to interact with the Llama-2 70b model, with everything running locally! If you want to take this project further, you can experiment with different system messages to suit your needs or add the ability to interact with your local documents using frameworks like LangChain. Enjoy experimenting!</p>","tags":["Chatbot","CUDA","HuggingFace","Llama","LLM"]},{"location":"tutorials/local-llama/#sources","title":"Sources:","text":"<ul> <li>https://huggingface.co/meta-llama/Llama-2-70b-chat-hf/tree/main</li> <li>https://huggingface.co/docs/optimum/concept_guides/quantization</li> <li>https://deci.ai/quantization-and-quantization-aware-training/</li> <li>https://www.gradio.app/guides/creating-a-custom-chatbot-with-blocks#add-streaming-to-your-chatbot</li> </ul>","tags":["Chatbot","CUDA","HuggingFace","Llama","LLM"]},{"location":"tutorials/self_supervised_training/","title":"Self-Supervised Contrastive Learning for Surgical videos","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: August 19, 2024 Language: Python Latest version: 0.1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable The focus of this repo is to walkthrough the process of doing Self-Supervised Learning using Contrastive Pre-training on Surgical Video data.  As part of the walk-through we will guide through the steps needed to pre-process and extract the frames from the public Cholec80 Dataset. This will be required to run the tutorial.</p> <p>The repo is organized as follows -  * <code>Contrastive_learning_Notebook.ipynb</code> walks through the process of SSL in a tutorial style * <code>train_simclr_multiGPU.py</code> enables running of \"pre-training\" on surgical data across multiple GPUs through the CLI * <code>downstream_task_tool_segmentation.py</code> shows the process of \"fine-tuning\" for a downstream task starting from a pretrained checkpoint using MONAI</p>","tags":["Computer Vision","Learning","Medical","Self-Supervised","Surgical","Video"]},{"location":"tutorials/self_supervised_training/#dataset","title":"Dataset","text":"<p>To run through the full tutorial, it is required that the user downloads Cholec80 dataset. Additional preprocessing of the videos to extract individual frames can be performed using the python helper file as follows:</p> <p><code>python extract_frames.py --datadir &lt;path_to_cholec80_dataset&gt;</code> </p>","tags":["Computer Vision","Learning","Medical","Self-Supervised","Surgical","Video"]},{"location":"tutorials/self_supervised_training/#adapt-to-your-own-dataset","title":"Adapt to your own dataset","text":"<p>To run this with your own dataset, you will need to extract the frames and modify the <code>Pytorch Dataset/Dataloader</code> accordingly. For SSL pre-training, a really simple CSV file formatted as follows can be used.  <pre><code>&lt;path_to_frame&gt;,&lt;label&gt;\n</code></pre> where <code>&lt;label&gt;</code> can be a class/score for a downstream task, and is NOT used during pre-training.</p> <pre><code># Snippet of csv file\n/workspace/data/cholec80/frames/train/video01/1.jpg,0\n/workspace/data/cholec80/frames/train/video01/2.jpg,0\n/workspace/data/cholec80/frames/train/video01/3.jpg,0\n/workspace/data/cholec80/frames/train/video01/4.jpg,0\n/workspace/data/cholec80/frames/train/video01/5.jpg,0\n....\n</code></pre>","tags":["Computer Vision","Learning","Medical","Self-Supervised","Surgical","Video"]},{"location":"tutorials/self_supervised_training/#environment","title":"Environment","text":"<p>All environment/dependencies are captured in the Dockerfile. The exact software within the base container are described here.</p>","tags":["Computer Vision","Learning","Medical","Self-Supervised","Surgical","Video"]},{"location":"tutorials/self_supervised_training/#create-docker-imagecontainer","title":"Create Docker Image/Container","text":"<pre><code>DATA_DIR=\"/mnt/sdb/data\"  # location of Cholec80 dataset\ndocker build -t surg_video_ssl_2202:latest Docker/\n\n# sample Docker command (may need to update based on local setup)\ndocker run -it --gpus=\"device=1\" \\\n    --name=SURGSSL_EXPS \\\n    -v $DATA_DIR:/workspace/data \\\n    -v `pwd`:/workspace/codes -w=/workspace/codes/ \\\n    -p 8888:8888 \\\n    --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 \\\n    surg_video_ssl_2202 jupyter lab\n</code></pre> <p>For environment dependencies refer to the Dockerfile</p>","tags":["Computer Vision","Learning","Medical","Self-Supervised","Surgical","Video"]},{"location":"tutorials/self_supervised_training/#launch-training","title":"Launch Training","text":"<p>PRE-TRAINING <pre><code># Training on single GPU with `efficientnet_b0` backbone\npython3 train_simclr_multigpu.py --gpus 1 --backbone efficientnet_b0 --batch_size 64\n\n# Training on single GPU with `resnet50` backbone\npython3 train_simclr_multigpu.py --gpus 4 --backbone resnet50 --batch_size 128\n</code></pre></p> <p>DOWNSTREAM TASK - Segmentation This script shows an example of taking the checkpoint above and integrating it into MONAI. </p> <pre><code># Fine-Tuning on \"GPU 1\" with 10% of the dataset, while freezing the encoder\npython3 downstream_task_tool_segmentation.py --gpu 1 --perc 10 --exp simclr --freeze\n</code></pre>","tags":["Computer Vision","Learning","Medical","Self-Supervised","Surgical","Video"]},{"location":"tutorials/self_supervised_training/#modelcheckpoints-information","title":"Model/Checkpoints information","text":"<p>As part of this tutorial, we are also releasing a few different checkpoints for users. These are detailed below. </p> <p>NOTE : These checkpoints were trained using an internal dataset of Chelecystectomy videos provided by Activ Surgical and NOT the Cholec80 dataset. </p>","tags":["Computer Vision","Learning","Medical","Self-Supervised","Surgical","Video"]},{"location":"tutorials/self_supervised_training/#pre-trained-backbones","title":"Pre-Trained Backbones","text":"<ul> <li>ResNet18        - link</li> <li>ResNet50        - link</li> <li>efficientnet_b0 - link</li> </ul>","tags":["Computer Vision","Learning","Medical","Self-Supervised","Surgical","Video"]},{"location":"tutorials/self_supervised_training/#tool-segmentation-model","title":"Tool Segmentation Model","text":"<ul> <li>MONAI - FlexibleUNet (efficientnet_b0) - link</li> </ul>","tags":["Computer Vision","Learning","Medical","Self-Supervised","Surgical","Video"]},{"location":"tutorials/self_supervised_training/#holoscan-sdk","title":"Holoscan SDK","text":"<p>This tool Segmentation Model can be used to build a Holoscan App, using the process under section \"Bring your own Model\" within the Holoscan SDK User guide.</p>","tags":["Computer Vision","Learning","Medical","Self-Supervised","Surgical","Video"]},{"location":"tutorials/self_supervised_training/#resources","title":"Resources","text":"<p>[1] Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. (2020). A simple framework for contrastive learning of visual representations. In International conference on machine learning (pp. 1597-1607). PMLR. (link)</p> <p>[2] Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G. (2020). Big self-supervised models are strong semi-supervised learners. NeurIPS 2021 (link).</p> <p>[3][Pytorch Lightning SSL Tutorial](https://pytorch-lightning.readthedocs.io/en/latest/notebooks/course_UvA-DL/13-contrastive-learning.html) | Github</p> <p>[4] Ramesh, S., Srivastav, V., Alapatt, D., Yu, T., Muarli, A., et. al. (2023).  Dissecting Self-Supervised Learning Methods for Surgical Computer Vision. arXiv preprint arXiv:2207.00449. (link)</p>","tags":["Computer Vision","Learning","Medical","Self-Supervised","Surgical","Video"]},{"location":"tutorials/windows_vm/","title":"Interoperability between Holoscan and a Windows Application on a Single Machine","text":"<p> Authors: NVIDIA (NVIDIA) Supported platforms: amd64 Last modified: March 14, 2025 Language: bash Latest version: 1.0 Minimum Holoscan SDK version: 2.6.0 Tested Holoscan SDK versions: 2.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p></p>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#overview","title":"Overview","text":"<p>This tutorial enables Holoscan and Windows applications to run concurrently on the same machine or node.</p>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Description</li> <li>Platform Requirements</li> <li>Windows VM Setup Instructions</li> <li>Software Pre-requisites</li> <li>GPU Passthrough<ul> <li>Two Different GPUs (e.g., RTX A4000 and RTX A6000)</li> <li>Two Identical GPUs (e.g., 2x RTX A4000)</li> </ul> </li> <li>Windows VM Configuration for Passed-through GPU<ul> <li>Install NVIDIA Driver in Windows VM</li> </ul> </li> <li>Communication Performance between Linux Host and Windows VM</li> <li>Running Holoscan DDS App and Windows VM App</li> </ul>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#description","title":"Description","text":"<p>Many legacy graphics applications, particularly in medical devices, are developed and operated on the Windows platform. A significant number of these medical devices rely on Windows OS for their functionality. To integrate AI/ML and sensor processing capabilities from NVIDIA Holoscan into such Windows-based systems, we previously introduced a \"sidecar\" architecture. This architecture involved an AI compute node running Holoscan interoperating with a Windows node via a DDS link, showcased in the Holoscan DDS reference application. This tutorial extends the options for such use-cases by providing developers with a straightforward and clear system design to enable interoperability between Holoscan applications and Windows applications running on the same physical machine. It demonstrates how to achieve efficient communication and processing without the need for separate hardware nodes.</p> <p></p> <p>In this tutorial, Holoscan runs on an x86_64 workstation hosting Ubuntu 22.04. Two RTX A4000 GPUs are plugged into the x86_64 workstation. Using Linux KVM, a Windows VM is created on the host Ubuntu Linux. One RTX A4000 GPU is passed through to the Windows VM using VFIO. The other RTX A4000 GPU is reserved for the host Ubuntu OS and used by Holoscan. We provide a step-by-step guide on how to achieve this setup. Furthermore, we also demonstrate how a Holoscan application reads USB-based camera frames and sends the frames via DDS to an application running on the Windows VM. Finally, the Windows VM application renders the camera frames on the screen using its dedicated RTX A4000 GPU.</p>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#platform-requirements","title":"Platform Requirements","text":"<p>The setup described in this tutorial requires an x86_64 workstation and ProViz class of NVIDIA GPUs. The exact platform details are provided below:</p> <ul> <li>CPU: AMD Ryzen 9 7950X 16-Core Processor</li> <li>OS: Ubuntu 22.04</li> <li>Kernel Version: 6.8.0-48-generic</li> <li>GPU: 2x NVIDIA RTX A4000</li> </ul> <p>Although we used two same GPUs in this tutorial, combinations of two different GPUs, for example, RTX A4000 and RTX A6000, can also be used.</p> <p>Since x86_64 workstations are very much diverse, the steps to enable the setup described in this tutorial may not work as-is on all x86_64 workstations. Performances may also vary across different x86_64 workstations.</p>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#windows-vm-setup-instructions","title":"Windows VM Setup Instructions","text":"","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#software-pre-requisites","title":"Software Pre-requisites","text":"<p>Install the NVIDIA Holoscan SDK and the NVIDIA GPU driver on the host Ubuntu 22.04.</p> <p>Install KVM and QEMU</p> <pre><code>sudo apt update\nsudo apt install qemu-kvm libvirt-clients libvirt-daemon-system bridge-utils virt-manager ovmf\n</code></pre> <p>Check if KVM works</p> <pre><code>$ kvm-ok\nINFO: /dev/kvm exists\nKVM acceleration can be used\n</code></pre> <p>If KVM does not work, kindly figure out how KVM can be enabled for your specific Linux kernel version.</p>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#gpu-passthrough","title":"GPU Passthrough","text":"<p>A GPU can be passed through to a virtual machine using Linux's VFIO driver. The VM is, then, capable of using the passed through GPU directly without any host operating system intervention. If a machine has two different GPUs, then it is much more straightforward to passthrough one of the GPUs to a VM. A bit more configuration is required if two GPUs are same. We show how one of the two identical GPUs can be passed through to a VM with the VFIO driver.</p>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#two-different-gpus-eg-rtx-a4000-and-rtx-a6000","title":"Two Different GPUs (e.g., RTX A4000 and RTX A6000)","text":"","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#identify-the-gpu-to-be-passed-through","title":"Identify the GPU to be Passed Through","text":"<pre><code>$ lspci | grep -i NVIDIA\n17:00.0 VGA compatible controller [0300]: NVIDIA Corporation Device [RTX A4000] [10de:24b0] (rev a1)\n17:00.1 Audio device [0403]: NVIDIA Corporation Device [10de:228b] (rev a1)\n65:00.0 VGA compatible controller [0300]: NVIDIA Corporation Device [RTX A6000] [10de:2230] (rev a1)\n65:00.1 Audio device [0403]: NVIDIA Corporation Device [10de:1aef] (rev a1)\n</code></pre> <p>Let's assume that we want to passthrough the RTX A4000 GPU to a VM (in this tutorial it's a Windows VM). We note the PCI ID of RTX A4000 as <code>10de:24b0</code>.</p> <p>Note: Make sure that the RTX A4000 GPU to be passed through is not currently used for displaying to the monitor.</p>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#update-grub-configuration","title":"Update GRUB Configuration","text":"<p>Open the <code>/etc/default/grub</code> file and find the following line:</p> <pre><code>GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash\"\n</code></pre> <p>Update the above line to the following to add the previously noted PCI ID of RTX A4000 for the VFIO driver:</p> <pre><code>GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash intel_iommu=on amd_iommu=on iommu=pt vfio-pci.ids=10de:24b0\"\n</code></pre> <p>The above line enables IOMMU and passes the PCI ID of RTX A4000 to the VFIO driver for Linux kernel.</p>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#update-linux-kernel-module-configuration","title":"Update Linux Kernel Module Configuration","text":"<p>Create a new file <code>/etc/modprobe.d/vfio.conf</code> and add the following lines:</p> <pre><code>options vfio-pci ids=10de:24b0\nsoftdep nvidia pre: vfio-pci\n</code></pre> <p>The above lines ensure that the NVIDIA driver is loaded after the VFIO driver. Therefore, VFIO takes over the RTX A4000 GPU, and the RTX A6000 GPU is used by the default NVIDIA driver.</p>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#update-initial-ramfs-and-grub","title":"Update Initial RAMFS and GRUB","text":"<pre><code>sudo update-initramfs -u # optionally add -k all to update all kernels\nsudo update-grub\n</code></pre>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#check-if-vfio-is-loaded-for-passed-through-gpu","title":"Check if VFIO is loaded for Passed-through GPU","text":"<p>Reboot the system: <code>sudo reboot</code>. Now, check the loaded driver for RTX A4000:</p> <pre><code>$ lspci -nnk -d 10de:24b0\n17:00.0 VGA compatible controller [0300]: NVIDIA Corporation [RTX A4000] [10de:24b0] (rev a1)\n    Subsystem: NVIDIA Corporation [RTX A4000] [10de:14ad]\n    Kernel driver in use: vfio-pci\n    Kernel modules: nvidiafb, nouveau, nvidia_drm, nvidia\n</code></pre>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#two-identical-gpus-eg-2x-rtx-a4000","title":"Two Identical GPUs (e.g., 2x RTX A4000)","text":"<p>In case of two identical GPUs, the PCI IDs of the GPUs are the same.</p> <pre><code>$ lspci -nn | grep \"NVIDIA\"\n01:00.0 VGA compatible controller [0300]: NVIDIA Corporation GA104GL [RTX A4000] [10de:24b0] (rev a1)\n01:00.1 Audio device [0403]: NVIDIA Corporation GA104 High Definition Audio Controller [10de:228b] (rev a1)\n09:00.0 VGA compatible controller [0300]: NVIDIA Corporation GA104GL [RTX A4000] [10de:24b0] (rev a1)\n09:00.1 Audio device [0403]: NVIDIA Corporation GA104 High Definition Audio Controller [10de:228b] (rev a1)\n</code></pre> <p>Therefore, we cannot just pass PCI ID <code>10de:24b0</code> to the VFIO driver to pass through one RTX A4000 to a VM and keep the other one for the host Linux.</p> <p>To mitigate the issue, we force the Linux kernel to load the VFIO driver for one of the RTX A4000 GPUs by identifying the GPU by its PCI bus address at the time of booting Linux. Let's assume that we want to pass through the RTX A4000 GPU at PCI bus address <code>01:00.0</code> to a VM.</p> <p>Note: Make sure that the RTX A4000 GPU to be passed through is not currently used for displaying to the monitor. <code>nvidia-smi</code> can be used to figure out which GPU is being used for display.</p> <p>The following instructions are only verified for Ubuntu 22.04 and Linux kernel 6.8.0-48-generic. For other Ubuntu and Linux kernel versions, the instructions may not work.</p>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#update-grub-configuration_1","title":"Update GRUB Configuration","text":"<p>Open the <code>/etc/default/grub</code> file and find the following line:</p> <pre><code>GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash\"\n</code></pre> <p>Update the above line to the following to enable IOMMU:</p> <pre><code>GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash intel_iommu=on amd_iommu=on iommu=pt vfio-pci.ids=\"\n</code></pre>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#create-a-script-to-bind-the-rtx-a4000-gpu-to-vfio-driver","title":"Create a script to bind the RTX A4000 GPU to VFIO driver","text":"<p>Create a script <code>/usr/local/bin/vfio-pci-override.sh</code> with the following content:</p> <pre><code>#!/bin/sh\n\necho \"vfio-pci\" &gt; /sys/bus/pci/devices/0000\\:01\\:00.0/driver_override\necho \"vfio-pci\" &gt; /sys/bus/pci/devices/0000\\:01\\:00.0/iommu_group/devices/0000\\:01\\:00.0/driver_override\necho \"vfio-pci\" &gt; /sys/bus/pci/devices/0000\\:01\\:00.0/iommu_group/devices/0000\\:01\\:00.1/driver_override\nmodprobe -i vfio-pci\n</code></pre> <p>Kindly, note that we are using the PCI bus address <code>0000:01:00.0</code> of the RTX A4000 in the above script. In specific cases, the PCI bus address may be different and needs to be updated.</p>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#install-vfio-driver-with-the-above-script","title":"Install VFIO driver with the above script","text":"<p>Create a new file <code>/etc/modprobe.d/vfio.conf</code> and add the following lines:</p> <pre><code>install vfio-pci /usr/local/bin/vfio-pci-override.sh\nsoftdep nvidia pre: vfio-pci\n</code></pre>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#update-initial-ramfs-and-grub_1","title":"Update Initial RAMFS and GRUB","text":"<pre><code>sudo update-initramfs -u # optionally add -k all to update all kernels\nsudo update-grub\n</code></pre>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#check-if-vfio-is-loaded-for-passed-through-gpu_1","title":"Check if VFIO is loaded for Passed-through GPU","text":"<p>Reboot the system: <code>sudo reboot</code>. Now, check the loaded driver for RTX A4000:</p> <pre><code>$ lspci -nnk -d 10de:24b0\n01:00.0 VGA compatible controller [0300]: NVIDIA Corporation GA104GL [RTX A4000] [10de:24b0] (rev a1)\n    Subsystem: NVIDIA Corporation GA104GL [RTX A4000] [10de:14ad]\n    Kernel driver in use: vfio-pci\n    Kernel modules: nvidiafb, nouveau, nvidia_drm, nvidia\n09:00.0 VGA compatible controller [0300]: NVIDIA Corporation GA104GL [RTX A4000] [10de:24b0] (rev a1)\n    Subsystem: NVIDIA Corporation GA104GL [RTX A4000] [10de:14ad]\n    Kernel driver in use: nvidia\n    Kernel modules: nvidiafb, nouveau, nvidia_drm, nvidia\n</code></pre> <p>In the above output, we can see that different kernel drivers are loaded for the two RTX A4000 GPUs.</p>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#windows-vm-configuration-for-passed-through-gpu","title":"Windows VM Configuration for Passed-through GPU","text":"<p>A Windows VM can be created using <code>virt-manager</code> or <code>virsh</code> (see references if needed). We assume that the Windows VM is already created and running. We show how to assign the passed-through RTX A4000 GPU to the Windows VM.</p> <p>In the \"Hardware Details\" tab of <code>virt-manager</code> GUI, add a new PCI device by selecting <code>Add Hardware -&gt; PCI Host Device</code> option. Select the RTX A4000 GPU at PCI bus address <code>01:00.0</code> (and also other functions of the PCI device such as <code>01:00.1</code>).</p> <p></p> <p>If the GPU is assigned to the VM correctly, then the left panel of the VM hardware details and the XML of the assigned GPU in <code>virt-manager</code> window should look like the following:</p> <p></p>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#add-vnc-display-for-windows-vm","title":"Add VNC Display for Windows VM","text":"<p>Add VNC display server via \"Add Hardware\" option in <code>virt-manager</code> so that the Windows VM display can be controlled via VNC server running on Linux host.</p> <p></p>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#install-nvidia-driver-in-windows-vm","title":"Install NVIDIA Driver in Windows VM","text":"<p>Download the NVIDIA driver for RTX A4000 GPU from the NVIDIA website.</p> <p>After installing the NVIDIA driver, reboot the Windows VM. Then, open command-prompt by typing <code>cmd</code> in the Windows search bar and type <code>nvidia-smi</code>. The following output should be displayed:</p> <p></p>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#replicate-monitor-display-to-vnc-display","title":"Replicate Monitor Display to VNC Display","text":"<p>Right now, we have two screen for the Windows VM: physical monitor (VGA) and VNC display. It could be difficult to control mouse and keyboard with both the displays. Therefore, the monitor display can be replicated to the monitor display so that the Windows VM's physical monitor display can be controlled via the VNC display.</p> <p>The following configuration shows that the screens are replicated in Windows VM:</p> <p></p>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#communication-performance-between-host-and-vm","title":"Communication Performance between Host and VM","text":"<p>The performance of a Windows VM running on a Linux under KVM-based virtualization is well-studied in the research literature. The performance is dependent on specific hardware and applications. In this tutorial, we provide the data-transfer performance in our hardware configuration.</p> <p>Note: Firewalls in both Windows VM and Linux host need to be properly configured to allow network traffic between the two OS. Configuring exact firewall rules is out-of-scope of this tutorial. However, we provide a few tips below as guiding help on this topic.</p> <p>Allowing Network Traffic between VM and Host</p> <p>Use a \"bridge device\" for the VM's network configuration.</p> <p></p> <p>In Windows VM search bar, type \"Windows Defender Firewall with Advanced Security\" and open the application. For both \"Inbound Rules\" and \"Outbound Rules\", add new rules to allow TCP and UDP traffic on common (or all) port numbers.</p> <p></p> <p>In the host Linux, it may not be needed to configure the firewall to allow network traffic between VM and host. However, in case it does not work, <code>iptables</code> or <code>ufw</code> can be used to configure traffic on specific ports.</p> <p>Configuring firewall</p> <p>Turning off the firewall, either in Windows VM, or in host Linux, is not recommended. However, to test the setup in this tutorial, firewalls can be turned off. In Windows, type \"firewall\" in the search bar and turn off the firewall. The setting will look like below:</p> <p></p>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#iperf3-performance-test","title":"iperf3 Performance Test","text":"<p>Install iperf3 in both Windows VM and host Linux.</p> <p>We need the IP address of the Windows VM and the host Linux. In our setup, the Linux server IP is <code>192.168.122.1</code> and the Windows VM client IP is <code>192.168.122.147</code>. To check the host Linux IP address, run <code>ifconfig</code> and the IP address of the <code>virbr0</code> interface is the IP address of the host. The Windows VM IP address can be checked in <code>virt-manager</code> GUI in the NIC setting, or by running <code>ipconfig</code> in the Command Prompt.</p>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#host-linux-server-and-windows-vm-client","title":"Host Linux Server and Windows VM Client","text":"<p>In host Linux, run the following command:</p> <pre><code>iperf3 -s -B 192.168.122.1\n</code></pre> <p>In the Windows VM, run the following command for a 5 second test: <pre><code>iperf3.exe -c 192.168.122.1 -t 5\n</code></pre></p> <p>Output in Linux should look like below: <pre><code>$ iperf3 -s -B 192.168.122.1\n-----------------------------------------------------------\nServer listening on 5201\n-----------------------------------------------------------\nAccepted connection from 192.168.122.147, port 50807\n[  5] local 192.168.122.1 port 5201 connected to 192.168.122.147 port 50808\n[ ID] Interval           Transfer     Bitrate\n[  5]   0.00-1.00   sec  1.98 GBytes  17.0 Gbits/sec\n[  5]   1.00-2.00   sec  1.88 GBytes  16.1 Gbits/sec\n[  5]   2.00-3.00   sec  2.01 GBytes  17.3 Gbits/sec\n[  5]   3.00-4.00   sec  2.01 GBytes  17.3 Gbits/sec\n[  5]   4.00-5.00   sec  2.00 GBytes  17.2 Gbits/sec\n[  5]   5.00-5.02   sec  46.6 MBytes  16.0 Gbits/sec\n- - - - - - - - - - - - - - - - - - - - - - - - -\n[ ID] Interval           Transfer     Bitrate\n[  5]   0.00-5.02   sec  9.93 GBytes  17.0 Gbits/sec                  receiver\n</code></pre></p> <p>Output in Windows VM should look like below:</p> <p></p> <p>For Linux host to Windows, we have observed 21.9 Gbits/sec transfer rate for a 1 minute test.</p>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#host-linux-client-and-windows-vm-server","title":"Host Linux Client and Windows VM Server","text":"<p>The above commands are reversed for host Linux client and Windows VM server. In the Windows VM, run the following command:</p> <pre><code>iperf3.exe -s -B 192.168.122.147\n</code></pre> <p>In the host Linux, run the following command for a 5 second test:</p> <pre><code>iperf3 -c 192.168.122.147 -t 60\n</code></pre> <p>For a 1 minute test, we have observed 4.33 Gbits/sec transfer rate from Windows VM to host.</p>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#running-holoscan-dds-app-and-windows-vm-app","title":"Running Holoscan DDS App and Windows VM App","text":"<p>If the above configurations are done correctly, then interoperating between a Holoscan application and Windows VM application is straightforward. For this tutorial, we will use the Holoscan DDS app and a simple Windows application. The Holoscan DDS app is used in <code>publisher</code> mode where it reads frames from a USB camera and sends the frames via DDS. The Windows application (available upon request) receives the frames via DDS and renders the frame on the screen using the RTX A4000 GPU with the help of OpenGL.</p> <p>In Linux host, run the Holoscan DDS app in <code>publisher</code> mode:</p> <pre><code>./run launch dds_video --extra_args \"-p\"\n</code></pre> <p>In Windows VM, running the renderer application shows the camera input from Linux host:</p> <p></p>","tags":["VM","Windows","Tutorial"]},{"location":"tutorials/windows_vm/#references","title":"References","text":"<ul> <li>https://wiki.archlinux.org/title/PCI_passthrough_via_OVMF</li> <li>https://www.makeuseof.com/create-windows-virtual-machine-in-linux-with-kvm/</li> </ul>","tags":["VM","Windows","Tutorial"]},{"location":"workflows/","title":"Workflows","text":"<p>Workflows are pre-configured sequences of operations that help users accomplish specific tasks efficiently. These workflows combine multiple Holoscan components and settings into ready-to-use solutions, making it easier to get started with common use cases.</p>"},{"location":"workflows/ai_surgical_video/","title":"Real-Time End-to-end AI Surgical Video Workflow","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: amd64, arm64 Last modified: March 14, 2025 Language: Python Latest version: 1.0 Minimum Holoscan SDK version: 3.0.0 Tested Holoscan SDK versions: 3.0.0 Contribution metric: Level 1 - Highly Reliable</p> <p> Fig.1: The overall diagram illustrating the end-to-end pipeline for real-time AI surgical video processing. This workflow achieves an end-to-end latency of 37ms on average with a maximum of 54ms. The latency breakdown shown below the diagram indicates HSB latency (Avg=21ms, Max=28ms) and AI Application latency (Avg=16ms, Max=26ms), demonstrating the high-performance capabilities of this solution.</p>","tags":["Workflow","End-to-End Application","Multiai","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#overview","title":"Overview","text":"<p>This reference application offers developers a modular, end-to-end pipeline that spans the entire sensor processing workflow\u2014from sensor data ingestion and accelerated computing to AI inference, real-time visualization, and data stream output.</p> <p>Specifically, we demonstrate a comprehensive real-time end-to-end AI surgical video pipeline that includes:</p> <ol> <li>Sensor I/O: Integration with Holoscan Sensor Bridge, enabling GPU Direct data ingestion for ultra low-latency input of surgical video feeds.</li> <li>Out-of-body detection to determine if the endoscope is inside or outside the patient's body, ensuring patient privacy by removing identifiable information.</li> <li>Dynamic flow condition based on out-of-body detection results.</li> <li>De-identification: pixelate the image to anonymize outside of body elements like people's faces.</li> <li>Multi-AI: Enabling simultaneous execution of multiple models at inference. Surgical Tool processing with:</li> <li>SSD detection for surgical tool detection</li> <li>MONAI segmentation for endoscopic tool segmentation</li> </ol>","tags":["Workflow","End-to-End Application","Multiai","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#architecture","title":"Architecture","text":"<p> Fig.2: The workflow diagram representing all the holoscan operators (in green) and holoscan sensor bridge operators (in yellow). The source can be a Holoscan Sensor Bridge, an AJA Card or a video replayer.</p> <p> Fig.3: Endoscopy image from a partial nephrectomy procedure (surgical removal of the diseased portion of the kidney) showing AI tool segmentation results when the camera is inside the body and a deidentified (pixelated) output image when the camera is outside of the body.</p>","tags":["Workflow","End-to-End Application","Multiai","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#1-out-of-body-detection","title":"1. Out-of-Body Detection","text":"<p>The workflow first determines if the endoscope is inside or outside the patient's body using an AI model.</p>","tags":["Workflow","End-to-End Application","Multiai","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#2-dynamic-flow-control","title":"2. Dynamic Flow Control","text":"<ul> <li>If outside the body: The video is deidentified through pixelation to protect privacy</li> <li>If inside the body: The video is processed by the multi-AI pipeline</li> </ul>","tags":["Workflow","End-to-End Application","Multiai","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#3-multi-ai-processing","title":"3. Multi-AI Processing","text":"<p>When inside the body, two AI models run concurrently:</p> <ul> <li>SSD detection model identifies surgical tools with bounding boxes</li> <li>MONAI segmentation model provides pixel-level segmentation of tools</li> </ul>","tags":["Workflow","End-to-End Application","Multiai","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#4-visualization","title":"4. Visualization","text":"<p>The HolovizOp displays the processed video with overlaid AI results, including:</p> <ul> <li>Bounding boxes around detected tools</li> <li>Segmentation masks for tools</li> <li>Text labels for detected tools</li> </ul>","tags":["Workflow","End-to-End Application","Multiai","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#requirements","title":"Requirements","text":"","tags":["Workflow","End-to-End Application","Multiai","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#software","title":"Software","text":"<ul> <li>Holoscan SDK <code>v3.0</code>:   Holohub command takes care of this dependency when using Holohub container. However, you can install the Holoscan SDK via one of the methods specified in the SDK user guide.</li> <li>Holoscan Sensor Bridge <code>v2.0</code>: Please see the Quick start guide for building the Holoscan Sensor Bridge docker container.</li> </ul>","tags":["Workflow","End-to-End Application","Multiai","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#models","title":"Models","text":"<p>This workflow utilizes the following three AI models:</p> Model Description File \ud83d\udce6\ufe0f Out-of-body Detection Model Detects if endoscope is inside or outside the body <code>anonymization_model.onnx</code> \ud83d\udce6\ufe0f SSD Detection for Endoscopy Surgical Tools Detects surgical tools with bounding boxes <code>epoch24_nms.onnx</code> \ud83d\udce6\ufe0f MONAI Endoscopic Tool Segmentation Provides pixel-level segmentation of tools <code>model_endoscopic_tool_seg_sanitized_nhwc_in_nchw_out.onnx</code>","tags":["Workflow","End-to-End Application","Multiai","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#sample-data","title":"Sample Data","text":"<ul> <li>\ud83d\udce6\ufe0f Orsi partial nephrectomy procedures - Sample endoscopy video data for use with the <code>replayer</code> source</li> </ul> <p>Note: The directory specified by <code>--data</code> at runtime is assumed to contain three subdirectories, corresponding to the NGC resources specified in Models and Sample Data: <code>orsi</code>, <code>monai_tool_seg_model</code> and <code>ssd_model</code>. These resources will be automatically downloaded to the Holohub data directory when building the application.</p>","tags":["Workflow","End-to-End Application","Multiai","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#quick-start-guide","title":"Quick Start Guide","text":"","tags":["Workflow","End-to-End Application","Multiai","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#using-aja-card-or-replayer-as-io","title":"Using AJA Card or Replayer as I/O","text":"<pre><code>./dev_container build_and_run ai_surgical_video\n</code></pre>","tags":["Workflow","End-to-End Application","Multiai","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#using-holoscan-sensor-bridge-as-io","title":"Using Holoscan Sensor Bridge as I/O","text":"<p>When using the workflow with <code>--source hsb</code>, it requires the Holoscan Sensor Bridge software to be installed. You can build a Holoscan Sensor Bridge container using the following commands:</p> <pre><code>git clone https://github.com/nvidia-holoscan/holoscan-sensor-bridge.git\ncd holoscan-sensor-bridge\ngit checkout 2.0.0\n./docker/build.sh\n</code></pre> <p>This will build a docker image called <code>hololink-demo:2.0.0</code>.</p> <p>Once you have built the Holoscan Sensor Bridge container, you can build the Holohub container using the following command:</p> <pre><code>./dev_container build_and_run --base_img hololink-demo:2.0.0 --img holohub:link ai_surgical_video\n</code></pre>","tags":["Workflow","End-to-End Application","Multiai","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#advanced-usage","title":"Advanced Usage","text":"","tags":["Workflow","End-to-End Application","Multiai","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#building-the-application","title":"Building the Application","text":"<p>First, you need to run the Holohub container:</p> <pre><code>./dev_container launch --img holohub:link \n</code></pre> <p>Then, you can build the workflow using the following command:</p> <pre><code>./run build ai_surgical_video\n</code></pre>","tags":["Workflow","End-to-End Application","Multiai","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#running-the-application","title":"Running the Application","text":"","tags":["Workflow","End-to-End Application","Multiai","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#use-holohub-container-from-outside-of-the-container","title":"Use Holohub Container from Outside of the Container","text":"<p>Using the Holohub container, you can run the workflow without building it again:</p> <pre><code>./dev_container build_and_run --base_img hololink-demo:2.0.0 --img holohub:link --no_build ai_surgical_video\n</code></pre> <p>However, if you want to build the workflow, you can just remove the <code>--no_build</code> flag:</p> <pre><code>./dev_container build_and_run --base_img hololink-demo:2.0.0 --img holohub:link ai_surgical_video\n</code></pre>","tags":["Workflow","End-to-End Application","Multiai","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#use-holohub-container-from-inside-the-container","title":"Use Holohub Container from Inside the Container","text":"<p>First, you need to run the Holohub container:</p> <pre><code>./dev_container launch --img holohub:link \n</code></pre> <p>To run the Python application, you can make use of the run script:</p> <pre><code>./run launch ai_surgical_video\n</code></pre> <p>Alternatively, you can run the application directly:</p> <pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;/workflows/ai_surgical_video/python\npython3 ai_surgical_video.py --source hsb --data &lt;DATA_DIR&gt; --config &lt;CONFIG_FILE&gt;\n</code></pre> <p>TIP: You can get the exact \"Run command\" along with \"Run environment\" and \"Run workdir\" by executing:</p> <pre><code>./run launch ai_surgical_video --dryrun\n</code></pre>","tags":["Workflow","End-to-End Application","Multiai","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#command-line-arguments","title":"Command Line Arguments","text":"<p>The application accepts the following command line arguments:</p> Argument Description Default <code>-s, --source</code> Source of video input: <code>replayer</code>, <code>aja</code>, or <code>hsb</code> <code>replayer</code> <code>-c, --config</code> Path to a custom configuration file <code>config.yaml</code> in the application directory <code>-d, --data</code> Path to the data directory containing model and video files Uses the <code>HOLOHUB_DATA_PATH</code> environment variable <code>--headless</code> Run in headless mode (no visualization) False <code>--fullscreen</code> Run in fullscreen mode False <code>--camera-mode</code> Camera mode to use [0,1,2,3] <code>0</code> <code>--frame-limit</code> Exit after receiving this many frames No limit <code>--hololink</code> IP address of Hololink board <code>192.168.0.2</code> <code>--log-level</code> Logging level to display <code>20</code> <code>--ibv-name</code> IBV device to use First available InfiniBand device <code>--ibv-port</code> Port number of IBV device <code>1</code> <code>--expander-configuration</code> I2C Expander configuration (0 or 1) <code>0</code> <code>--pattern</code> Configure to display a test pattern (0-11) None <code>--ptp-sync</code> After reset, wait for PTP time to synchronize False <code>--skip-reset</code> Don't call reset on the hololink device False","tags":["Workflow","End-to-End Application","Multiai","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#benchmarking","title":"Benchmarking","text":"<p>Please refer to Holoscan Benchmarking for how to perform benchmarking for this workflow.</p>","tags":["Workflow","End-to-End Application","Multiai","SSD","Detection","MONAI","Segmentation"]}]}