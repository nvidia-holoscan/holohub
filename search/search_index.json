{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"tags/","title":"Tags","text":""},{"location":"tags/#tag:3d-reconstruction","title":"3D Reconstruction","text":"<ul> <li>            Surgical Scene Reconstruction (latest)          </li> <li>            XR + Gaussian Splatting (latest)          </li> </ul>"},{"location":"tags/#tag:3d-slicer","title":"3D Slicer","text":"<ul> <li>            Medical Imaging Segmentation with Vista-3D (latest)          </li> <li>            OpenIGTLink 3D Slicer (latest)          </li> <li>            OpenIGTLink Tx/Rx (latest)          </li> </ul>"},{"location":"tags/#tag:ai","title":"AI","text":"<ul> <li>            Medical Imaging Operators (latest)          </li> </ul>"},{"location":"tags/#tag:aja","title":"AJA","text":"<ul> <li>            AJA Video Capture (latest)          </li> <li>            AJASourceOp (latest)          </li> <li>            Colonoscopy Polyp Segmentation (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> </ul>"},{"location":"tags/#tag:asr","title":"ASR","text":"<ul> <li>            EHR Agent Framework (latest)          </li> <li>            FM Radio Automatic Speech Recognition (latest)          </li> <li>            Real-time Riva ASR to local-LLM (latest)          </li> <li>            Speech-to-text + Large Language Model (latest)          </li> </ul>"},{"location":"tags/#tag:aws","title":"AWS","text":"<ul> <li>            Holoscan Playground on AWS (latest)          </li> </ul>"},{"location":"tags/#tag:academia","title":"Academia","text":"<ul> <li>            Ultrasound Post-Processing Filter Design (latest)          </li> </ul>"},{"location":"tags/#tag:anonymization","title":"Anonymization","text":"<ul> <li>            PixelatorOp (latest)          </li> </ul>"},{"location":"tags/#tag:apple-vision-pro","title":"Apple Vision Pro","text":"<ul> <li>            NVIDIA CloudXR Runtime for XR Applications (latest)          </li> </ul>"},{"location":"tags/#tag:asynchronous-queues","title":"Asynchronous Queues","text":"<ul> <li>            Distributed Endoscopy Tool Tracking with gRPC (latest)          </li> <li>            Distributed H.264 gRPC Streaming (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> </ul>"},{"location":"tags/#tag:audio","title":"Audio","text":"<ul> <li>            EHR Agent Framework (latest)          </li> <li>            FM Radio Automatic Speech Recognition (latest)          </li> <li>            Real-time Riva ASR to local-LLM (latest)          </li> <li>            Software Defined Radio FM Demodulation (latest)          </li> <li>            Speech-to-text + Large Language Model (latest)          </li> </ul>"},{"location":"tags/#tag:auth-and-api","title":"Auth and API","text":"<ul> <li>            Chat with NVIDIA NIM (latest)          </li> <li>            FHIR Client (latest)          </li> <li>            NVIDIA NV-CLIP NIM (latest)          </li> <li>            Speech-to-text + Large Language Model (latest)          </li> </ul>"},{"location":"tags/#tag:bci","title":"BCI","text":"<ul> <li>            Kernel Flow BCI Real-Time Visualization (latest)          </li> </ul>"},{"location":"tags/#tag:backprojection","title":"Backprojection","text":"<ul> <li>            Streaming Synthetic Aperture Radar (latest)          </li> </ul>"},{"location":"tags/#tag:base","title":"Base","text":"<ul> <li>            InferenceOperator (latest)          </li> </ul>"},{"location":"tags/#tag:bayer-rgb-pipeline-optimization","title":"Bayer RGB Pipeline Optimization","text":"<ul> <li>            High-Speed Endoscopy (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> </ul>"},{"location":"tags/#tag:beamform","title":"Beamform","text":"<ul> <li>            Ultrasound Post-Processing Filter Design (latest)          </li> </ul>"},{"location":"tags/#tag:beamforming","title":"Beamforming","text":"<ul> <li>            Ultrasound Beamforming with MATLAB (latest)          </li> </ul>"},{"location":"tags/#tag:benchmarking","title":"Benchmarking","text":"<ul> <li>            Exclusive Display Benchmark (latest)          </li> <li>            Green Context Benchmarking (latest)          </li> <li>            Holoscan Flow Benchmarking (latest)          </li> <li>            Holoscan Release Benchmarking (latest)          </li> <li>            Holoviz sRGB (latest)          </li> <li>            Model Benchmarking (latest)          </li> <li>            Real-time Thread Scheduling Benchmark (latest)          </li> </ul>"},{"location":"tags/#tag:bidirectional","title":"Bidirectional","text":"<ul> <li>            Video Streaming Demo (latest)          </li> <li>            VideoStreamingOps (latest)          </li> </ul>"},{"location":"tags/#tag:bridge","title":"Bridge","text":"<ul> <li>            Holoscan ROS2 Bridge (latest)          </li> <li>            Isaac Sim Holoscan Bridge (latest)          </li> </ul>"},{"location":"tags/#tag:buffer","title":"Buffer","text":"<ul> <li>            Asynchronous Lock-free Buffer (latest)          </li> </ul>"},{"location":"tags/#tag:bundle","title":"Bundle","text":"<ul> <li>            MonaiBundleInferenceOperator (latest)          </li> </ul>"},{"location":"tags/#tag:cuda","title":"CUDA","text":"<ul> <li>            CUDA MPS Tutorial (latest)          </li> <li>            Deploying Llama-2 70b model on the edge (latest)          </li> <li>            Green Context Benchmarking (latest)          </li> </ul>"},{"location":"tags/#tag:cuda-holoviz-integration","title":"CUDA Holoviz Integration","text":"<ul> <li>            Florence-2 (latest)          </li> </ul>"},{"location":"tags/#tag:cupti","title":"CUPTI","text":"<ul> <li>            Green Context Benchmarking (latest)          </li> </ul>"},{"location":"tags/#tag:cv-cuda","title":"CV CUDA","text":"<ul> <li>            Endoscopy Depth Estimation (latest)          </li> <li>            Holoscan CvCuda Interop Ops (latest)          </li> <li>            Integrate External Libraries into a Holoscan Pipeline (latest)          </li> <li>            Laser Detection (latest)          </li> <li>            Simple CV-CUDA (latest)          </li> </ul>"},{"location":"tags/#tag:camera","title":"Camera","text":"<ul> <li>            AJASourceOp (latest)          </li> <li>            ApriltagDetectorOp (latest)          </li> <li>            Deltacast VideoMaster Operators (latest)          </li> <li>            EVT Camera Calibration (latest)          </li> <li>            EmergentSourceOp (latest)          </li> <li>            Intel RealSense Camera Visualizer (latest)          </li> <li>            Laser Detection (latest)          </li> <li>            QCAPSourceOp (latest)          </li> <li>            SAM 2: Segment Anything in Images and Videos (latest)          </li> <li>            Stereo Vision (latest)          </li> <li>            USB Camera Calibration (latest)          </li> <li>            VPI Stereo Vision (latest)          </li> <li>            WebRTC Video Client (latest)          </li> <li>            Yolo Object Detection (latest)          </li> </ul>"},{"location":"tags/#tag:cardiac-keypoints-detection","title":"Cardiac Keypoints Detection","text":"<ul> <li>            Multi-AI Ultrasound (latest)          </li> <li>            Multi-AI Ultrasound (latest)          </li> <li>            Multi-AI Ultrasound (latest)          </li> </ul>"},{"location":"tags/#tag:clara-viz","title":"Clara Viz","text":"<ul> <li>            ClaraVizOperator (latest)          </li> </ul>"},{"location":"tags/#tag:classification","title":"Classification","text":"<ul> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Multi-AI Ultrasound (latest)          </li> <li>            Multi-AI Ultrasound (latest)          </li> <li>            Multi-AI Ultrasound (latest)          </li> </ul>"},{"location":"tags/#tag:client","title":"Client","text":"<ul> <li>            Video Streaming Client Demo (C++) (latest)          </li> <li>            Video Streaming Demo (latest)          </li> <li>            VideoStreamingClientOp (latest)          </li> <li>            VideoStreamingOps (latest)          </li> <li>            streaming_client (latest)          </li> </ul>"},{"location":"tags/#tag:cloud","title":"Cloud","text":"<ul> <li>            Holoscan Playground on AWS (latest)          </li> <li>            StreamingServerOps (latest)          </li> <li>            VideoStreamingClientOp (latest)          </li> <li>            VideoStreamingOps (latest)          </li> </ul>"},{"location":"tags/#tag:cloudxr","title":"CloudXR","text":"<ul> <li>            NVIDIA CloudXR Runtime for XR Applications (latest)          </li> </ul>"},{"location":"tags/#tag:colonoscopy","title":"Colonoscopy","text":"<ul> <li>            Polyp Detection (latest)          </li> </ul>"},{"location":"tags/#tag:communications","title":"Communications","text":"<ul> <li>            Basic Networking Ping (latest)          </li> <li>            WebRTC Video Server (latest)          </li> </ul>"},{"location":"tags/#tag:computer-vision-and-perception","title":"Computer Vision and Perception","text":"<ul> <li>            Body Pose Estimation (latest)          </li> <li>            Depth Anything V2 (latest)          </li> <li>            Distributed Endoscopy Tool Tracking with gRPC (latest)          </li> <li>            EVT Camera Calibration (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Florence-2 (latest)          </li> <li>            Holoscan CvCuda Interop Ops (latest)          </li> <li>            Holoviz HDR (latest)          </li> <li>            Holoviz YUV (latest)          </li> <li>            Holoviz sRGB (latest)          </li> <li>            Holoviz vsync (latest)          </li> <li>            Image Processing with MATLAB GPU Coder (latest)          </li> <li>            In-Out Body Detection (latest)          </li> <li>            Intel RealSense Camera Visualizer (latest)          </li> <li>            Laser Detection (latest)          </li> <li>            NVIDIA NV-CLIP NIM (latest)          </li> <li>            Object Detection using PyTorch Faster R-CNN (latest)          </li> <li>            Orthorectification with NVIDIA OptiX (latest)          </li> <li>            PVA-Accelerated Image Sharpening (latest)          </li> <li>            Qt Video Replayer (latest)          </li> <li>            Real-Time Face and Text Deidentification (latest)          </li> <li>            SAM 2: Segment Anything in Images and Videos (latest)          </li> <li>            Simple CV-CUDA (latest)          </li> <li>            Stereo Vision (latest)          </li> <li>            TAO PeopleNet Detection Model (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            USB Camera Calibration (latest)          </li> <li>            VILA Live (latest)          </li> <li>            VPI Stereo Vision (latest)          </li> <li>            WebRTC Holoviz Server (latest)          </li> <li>            Yolo Object Detection (latest)          </li> </ul>"},{"location":"tags/#tag:connectx","title":"ConnectX","text":"<ul> <li>            0.1          </li> </ul>"},{"location":"tags/#tag:convert","title":"Convert","text":"<ul> <li>            Convert Depth to Screen Space (latest)          </li> </ul>"},{"location":"tags/#tag:converter","title":"Converter","text":"<ul> <li>            DICOMSeriesToVolumeOperator (latest)          </li> <li>            STLConversionOperator (latest)          </li> <li>            SendMeshToUSDOp (latest)          </li> <li>            Tensor to File (latest)          </li> <li>            TensorToVideoBufferOp (latest)          </li> </ul>"},{"location":"tags/#tag:dds","title":"DDS","text":"<ul> <li>            Body Pose Estimation (latest)          </li> <li>            DDS Operators (latest)          </li> <li>            DDSOperatorBase (latest)          </li> <li>            DDSShapesSubscriberOp (latest)          </li> <li>            DDSVideoPublisherOp (latest)          </li> <li>            Real-time Video Streaming with DDS (latest)          </li> </ul>"},{"location":"tags/#tag:dicom","title":"DICOM","text":"<ul> <li>            DICOMDataLoaderOperator (latest)          </li> <li>            DICOMEncapsulatedPDFWriterOperator (latest)          </li> <li>            DICOMSegmentationWriterOperator (latest)          </li> <li>            DICOMSeriesSelectorOperator (latest)          </li> <li>            DICOMSeriesToVolumeOperator (latest)          </li> <li>            DICOMTextSRWriterOperator (latest)          </li> </ul>"},{"location":"tags/#tag:dpdk","title":"DPDK","text":"<ul> <li>            0.1          </li> <li>            Advanced Network Operators (latest)          </li> <li>            Advanced Networking Benchmark (latest)          </li> <li>            High Performance Networking with Holoscan (latest)          </li> <li>            Radar Signal Processing over Network (latest)          </li> </ul>"},{"location":"tags/#tag:data-loading","title":"Data Loading","text":"<ul> <li>            NiftiDataLoader (latest)          </li> </ul>"},{"location":"tags/#tag:debugging","title":"Debugging","text":"<ul> <li>            Interactively Debugging a Holoscan Application (latest)          </li> <li>            VSCode Dev Container for Holoscan (latest)          </li> </ul>"},{"location":"tags/#tag:deidentification","title":"Deidentification","text":"<ul> <li>            PixelatorOp (latest)          </li> <li>            Real-Time Face and Text Deidentification (latest)          </li> </ul>"},{"location":"tags/#tag:deltacast","title":"Deltacast","text":"<ul> <li>            Deltacast VideoMaster Operators (latest)          </li> <li>            Deltacast Videomaster Receiver (latest)          </li> <li>            Deltacast Videomaster Transmitter (latest)          </li> </ul>"},{"location":"tags/#tag:demo","title":"Demo","text":"<ul> <li>            Video Streaming Demo (latest)          </li> </ul>"},{"location":"tags/#tag:deployment","title":"Deployment","text":"<ul> <li>            Deploying Llama-2 70b model on the edge (latest)          </li> <li>            Holoscan &amp; Windows Application (latest)          </li> <li>            Holoscan Playground on AWS (latest)          </li> </ul>"},{"location":"tags/#tag:depth","title":"Depth","text":"<ul> <li>            Convert Depth to Screen Space (latest)          </li> <li>            Intel RealSense Camera Visualizer (latest)          </li> <li>            Stereo Vision (latest)          </li> <li>            VPI Stereo Vision (latest)          </li> </ul>"},{"location":"tags/#tag:depth-conversion","title":"Depth Conversion","text":"<ul> <li>            Medical Image Viewer in XR (latest)          </li> </ul>"},{"location":"tags/#tag:designer","title":"Designer","text":"<ul> <li>            Ultrasound Post-Processing Filter Design (latest)          </li> </ul>"},{"location":"tags/#tag:detection","title":"Detection","text":"<ul> <li>            EVT Camera Calibration (latest)          </li> <li>            Florence-2 (latest)          </li> <li>            In-Out Body Detection (latest)          </li> <li>            Laser Detection (latest)          </li> <li>            Medical Image Viewer in XR (latest)          </li> <li>            Object Detection using PyTorch Faster R-CNN (latest)          </li> <li>            Polyp Detection (latest)          </li> <li>            Real-Time AI End-to-End Surgical Video Workflow (latest)          </li> <li>            Real-Time Face and Text Deidentification (latest)          </li> <li>            SSD Detection for Endoscopy Tools (latest)          </li> <li>            Simple Radar Pipeline (latest)          </li> <li>            Simple Radar Pipeline (latest)          </li> <li>            TAO PeopleNet Detection Model (latest)          </li> <li>            USB Camera Calibration (latest)          </li> </ul>"},{"location":"tags/#tag:development","title":"Development","text":"<ul> <li>            Adding a GUI to Holoscan Python Applications (latest)          </li> <li>            Interactively Debugging a Holoscan Application (latest)          </li> <li>            NVIDIA Holoscan Bootcamp lab materials (latest)          </li> <li>            Using holohub operators in external applications (latest)          </li> <li>            VSCode Dev Container for Holoscan (latest)          </li> </ul>"},{"location":"tags/#tag:display","title":"Display","text":"<ul> <li>            Deltacast Videomaster Receiver (latest)          </li> </ul>"},{"location":"tags/#tag:distributed","title":"Distributed","text":"<ul> <li>            Advanced Networking Benchmark (latest)          </li> <li>            Basic Networking Ping (latest)          </li> <li>            Body Pose Estimation (latest)          </li> <li>            Creating Multi-Node Holoscan Applications (latest)          </li> <li>            Distributed Endoscopy Tool Tracking with gRPC (latest)          </li> <li>            Distributed H.264 Endoscopy Tool Tracking (latest)          </li> <li>            Distributed H.264 gRPC Streaming (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            FHIR Client (latest)          </li> <li>            Power Spectral Density with cuNumeric (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> </ul>"},{"location":"tags/#tag:drone","title":"Drone","text":"<ul> <li>            Orthorectification with NVIDIA OptiX (latest)          </li> </ul>"},{"location":"tags/#tag:dynamic","title":"Dynamic","text":"<ul> <li>            Ultrasound Post-Processing Filter Design (latest)          </li> </ul>"},{"location":"tags/#tag:edge-accelerated-inference","title":"Edge Accelerated Inference","text":"<ul> <li>            Depth Anything V2 (latest)          </li> </ul>"},{"location":"tags/#tag:encoder","title":"Encoder","text":"<ul> <li>            VideoEncoderRequestOp (latest)          </li> </ul>"},{"location":"tags/#tag:encoding","title":"Encoding","text":"<ul> <li>            GStreamer Video Recorder (latest)          </li> </ul>"},{"location":"tags/#tag:endoscopy","title":"Endoscopy","text":"<ul> <li>            Colonoscopy Polyp Segmentation (latest)          </li> <li>            Deltacast Videomaster Transmitter (latest)          </li> <li>            Distributed H.264 Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Depth Estimation (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Endoscopy Tool Segmentation using MONAI (latest)          </li> <li>            H.264 Endoscopy Tool Tracking (latest)          </li> <li>            H.264 Video Decode (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> <li>            Hyperspectral Image Segmentation (latest)          </li> <li>            Multi AI Detection and Tool Segmentation (latest)          </li> <li>            NVIDIA Video Codec: Endoscopy Tool Tracking (latest)          </li> <li>            OpenIGTLink 3D Slicer (latest)          </li> <li>            PVA-Accelerated Image Sharpening (latest)          </li> <li>            ProHawk Video Replayer (latest)          </li> <li>            Simple CV-CUDA (latest)          </li> <li>            Surgical Scene Reconstruction (latest)          </li> <li>            Surgical Tool Segmentation and AR Overlay (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> </ul>"},{"location":"tags/#tag:extended-reality","title":"Extended Reality","text":"<ul> <li>            Medical Image Viewer in XR (latest)          </li> <li>            Multi AI and AR Visualization (latest)          </li> <li>            Surgical Tool Segmentation and AR Overlay (latest)          </li> <li>            XR + Gaussian Splatting (latest)          </li> <li>            XR + Holoviz (latest)          </li> <li>            XR Basic Rendering Operator (latest)          </li> <li>            XR Demo (latest)          </li> <li>            XR Operators (latest)          </li> <li>            XrBeginFrameOp (latest)          </li> <li>            XrEndFrameOp (latest)          </li> <li>            XrFrameOp (latest)          </li> <li>            XrTransformControlOp (latest)          </li> <li>            XrTransformOp (latest)          </li> <li>            XrTransformRenderOp (latest)          </li> </ul>"},{"location":"tags/#tag:ffmpeg","title":"FFmpeg","text":"<ul> <li>            NVIDIA Video Codec SDK: Video File Reader (latest)          </li> </ul>"},{"location":"tags/#tag:fm-demodulation","title":"FM demodulation","text":"<ul> <li>            FM Radio Automatic Speech Recognition (latest)          </li> </ul>"},{"location":"tags/#tag:file","title":"File","text":"<ul> <li>            GStreamer Video Recorder (latest)          </li> <li>            Tensor to File (latest)          </li> </ul>"},{"location":"tags/#tag:file-reader","title":"File Reader","text":"<ul> <li>            NVIDIA Video Codec SDK: Video File Reader (latest)          </li> </ul>"},{"location":"tags/#tag:filter","title":"Filter","text":"<ul> <li>            Ultrasound Post-Processing Filter Design (latest)          </li> </ul>"},{"location":"tags/#tag:gpudirect","title":"GPUDirect","text":"<ul> <li>            0.1          </li> <li>            Advanced Network Operators (latest)          </li> <li>            Advanced Networking Benchmark (latest)          </li> <li>            Deltacast Videomaster Receiver (latest)          </li> <li>            Deltacast Videomaster Transmitter (latest)          </li> <li>            GPU Direct Storage on IGX (latest)          </li> <li>            High Performance Networking with Holoscan (latest)          </li> <li>            Radar Signal Processing over Network (latest)          </li> <li>            VITA 49 Power Spectral Density (latest)          </li> </ul>"},{"location":"tags/#tag:gpunetio","title":"GPUNetIO","text":"<ul> <li>            0.1          </li> <li>            High Performance Networking with Holoscan (latest)          </li> </ul>"},{"location":"tags/#tag:gstreamer","title":"GStreamer","text":"<ul> <li>            GStreamer Video Recorder (latest)          </li> </ul>"},{"location":"tags/#tag:gaussian-splatting","title":"Gaussian Splatting","text":"<ul> <li>            Surgical Scene Reconstruction (latest)          </li> <li>            XR + Gaussian Splatting (latest)          </li> </ul>"},{"location":"tags/#tag:green-context","title":"Green Context","text":"<ul> <li>            Green Context Benchmarking (latest)          </li> </ul>"},{"location":"tags/#tag:h264","title":"H.264","text":"<ul> <li>            NVIDIA Video Codec SDK: Video File Reader (latest)          </li> <li>            NVIDIA Video Codec: Encode-Decode Video (latest)          </li> <li>            NVIDIA Video Codec: Endoscopy Tool Tracking (latest)          </li> <li>            NVIDIA Video Codec: H.264 File Decoder (latest)          </li> <li>            NVIDIA Video Codec: Video Writer (latest)          </li> </ul>"},{"location":"tags/#tag:h265","title":"H.265","text":"<ul> <li>            NVIDIA Video Codec SDK: Video File Reader (latest)          </li> <li>            NVIDIA Video Codec: Encode-Decode Video (latest)          </li> <li>            NVIDIA Video Codec: Endoscopy Tool Tracking (latest)          </li> <li>            NVIDIA Video Codec: H.264 File Decoder (latest)          </li> <li>            NVIDIA Video Codec: Video Writer (latest)          </li> </ul>"},{"location":"tags/#tag:hevc","title":"HEVC","text":"<ul> <li>            NVIDIA Video Codec SDK: Video File Reader (latest)          </li> <li>            NVIDIA Video Codec: Encode-Decode Video (latest)          </li> <li>            NVIDIA Video Codec: Endoscopy Tool Tracking (latest)          </li> <li>            NVIDIA Video Codec: H.264 File Decoder (latest)          </li> <li>            NVIDIA Video Codec: Video Writer (latest)          </li> </ul>"},{"location":"tags/#tag:hpc","title":"HPC","text":"<ul> <li>            0.1          </li> <li>            High Performance Networking with Holoscan (latest)          </li> </ul>"},{"location":"tags/#tag:hardware-accelerated-decode","title":"Hardware Accelerated Decode","text":"<ul> <li>            H.264 Video Decode (latest)          </li> </ul>"},{"location":"tags/#tag:healthcare-ai","title":"Healthcare AI","text":"<ul> <li>            Colonoscopy Polyp Segmentation (latest)          </li> <li>            Deltacast Videomaster Transmitter (latest)          </li> <li>            Distributed H.264 Endoscopy Tool Tracking (latest)          </li> <li>            Distributed H.264 gRPC Streaming (latest)          </li> <li>            EHR Agent Framework (latest)          </li> <li>            Endoscopy Depth Estimation (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Endoscopy Tool Segmentation using MONAI (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            FHIR Client (latest)          </li> <li>            H.264 Endoscopy Tool Tracking (latest)          </li> <li>            H.264 Video Decode (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> <li>            Hyperspectral Image Segmentation (latest)          </li> <li>            Imaging AI Whole Body Segmentation (latest)          </li> <li>            In-Out Body Detection (latest)          </li> <li>            Medical Imaging Segmentation with Vista-3D (latest)          </li> <li>            Multi AI Detection and Tool Segmentation (latest)          </li> <li>            Multi AI and AR Visualization (latest)          </li> <li>            Multi-AI Ultrasound (latest)          </li> <li>            Multi-AI Ultrasound (latest)          </li> <li>            Multi-AI Ultrasound (latest)          </li> <li>            NVIDIA Video Codec: Endoscopy Tool Tracking (latest)          </li> <li>            OpenIGTLink 3D Slicer (latest)          </li> <li>            Polyp Detection (latest)          </li> <li>            ProHawk Video Replayer (latest)          </li> <li>            Real-Time AI End-to-End Surgical Video Workflow (latest)          </li> <li>            Real-time Riva ASR to local-LLM (latest)          </li> <li>            SSD Detection for Endoscopy Tools (latest)          </li> <li>            Self-Supervised Learning for Surgical videos (latest)          </li> <li>            Speech-to-text + Large Language Model (latest)          </li> <li>            Surgical Scene Reconstruction (latest)          </li> <li>            Surgical Tool Segmentation and AR Overlay (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            Ultrasound Beamforming with MATLAB (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            Volume rendering using ClaraViz (latest)          </li> </ul>"},{"location":"tags/#tag:healthcare-interop","title":"Healthcare Interop","text":"<ul> <li>            EHR Agent Framework (latest)          </li> <li>            EHR Query LLM Operators (latest)          </li> <li>            FHIR Client (latest)          </li> <li>            FhirClientOperator (latest)          </li> <li>            FhirResourceSanitizerOp (latest)          </li> <li>            Imaging AI Whole Body Segmentation (latest)          </li> <li>            Medical Imaging Operators (latest)          </li> <li>            ZeroMQPublisherOp (latest)          </li> <li>            ZeroMQSubscriberOp (latest)          </li> </ul>"},{"location":"tags/#tag:hemodynamics","title":"Hemodynamics","text":"<ul> <li>            Kernel Flow BCI Real-Time Visualization (latest)          </li> </ul>"},{"location":"tags/#tag:holoviz","title":"Holoviz","text":"<ul> <li>            Colonoscopy Polyp Segmentation (latest)          </li> <li>            Deltacast Videomaster Receiver (latest)          </li> <li>            Depth Anything V2 (latest)          </li> <li>            Distributed H.264 Endoscopy Tool Tracking (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> <li>            Holoviz HDR (latest)          </li> <li>            Holoviz UI (latest)          </li> <li>            Holoviz YUV (latest)          </li> <li>            Holoviz sRGB (latest)          </li> <li>            Holoviz vsync (latest)          </li> <li>            Image Processing with MATLAB GPU Coder (latest)          </li> <li>            Intel RealSense Camera Visualizer (latest)          </li> <li>            Medical Image Viewer in XR (latest)          </li> <li>            Object Detection using PyTorch Faster R-CNN (latest)          </li> <li>            ProHawk Video Replayer (latest)          </li> <li>            Streaming Synthetic Aperture Radar (latest)          </li> <li>            TAO PeopleNet Detection Model (latest)          </li> <li>            USB Camera Calibration (latest)          </li> <li>            WebRTC Holoviz Server (latest)          </li> <li>            XR Demo (latest)          </li> </ul>"},{"location":"tags/#tag:hounsfield-scale-transfer-functions","title":"Hounsfield Scale Transfer Functions","text":"<ul> <li>            Volume rendering using ClaraViz (latest)          </li> </ul>"},{"location":"tags/#tag:huggingface","title":"Huggingface","text":"<ul> <li>            Deploying Llama-2 70b model on the edge (latest)          </li> </ul>"},{"location":"tags/#tag:human-body-pose-estimation","title":"Human Body Pose Estimation","text":"<ul> <li>            Body Pose Estimation (latest)          </li> </ul>"},{"location":"tags/#tag:hyperspectral-imaging","title":"Hyperspectral Imaging","text":"<ul> <li>            Hyperspectral Image Segmentation (latest)          </li> </ul>"},{"location":"tags/#tag:ip","title":"IP","text":"<ul> <li>            Advanced Network Operators (latest)          </li> <li>            BasicNetworkOp Tx/Rx (latest)          </li> </ul>"},{"location":"tags/#tag:image-conversion","title":"Image Conversion","text":"<ul> <li>            PNGConverterOperator (latest)          </li> </ul>"},{"location":"tags/#tag:image-processing","title":"Image Processing","text":"<ul> <li>            ApriltagDetectorOp (latest)          </li> <li>            EVT Camera Calibration (latest)          </li> <li>            GammaCorrectionOp (latest)          </li> <li>            Holoscan CvCuda Interop Ops (latest)          </li> <li>            Holoviz YUV (latest)          </li> <li>            Holoviz vsync (latest)          </li> <li>            Hyperspectral Image Segmentation (latest)          </li> <li>            Image Processing with MATLAB GPU Coder (latest)          </li> <li>            NVIDIA NV-CLIP NIM (latest)          </li> <li>            Orthorectification with NVIDIA OptiX (latest)          </li> <li>            PixelatorOp (latest)          </li> <li>            Real-Time Face and Text Deidentification (latest)          </li> <li>            Stereo Vision (latest)          </li> <li>            USB Camera Calibration (latest)          </li> </ul>"},{"location":"tags/#tag:imaging","title":"Imaging","text":"<ul> <li>            Imaging AI Whole Body Segmentation (latest)          </li> <li>            Medical Imaging Segmentation with Vista-3D (latest)          </li> <li>            Streaming Synthetic Aperture Radar (latest)          </li> </ul>"},{"location":"tags/#tag:industry","title":"Industry","text":"<ul> <li>            Ultrasound Post-Processing Filter Design (latest)          </li> </ul>"},{"location":"tags/#tag:inference","title":"Inference","text":"<ul> <li>            InferenceOperator (latest)          </li> <li>            LSTMTensorRTInferenceOp (latest)          </li> <li>            MonaiBundleInferenceOperator (latest)          </li> <li>            MonaiSegInferenceOperator (latest)          </li> </ul>"},{"location":"tags/#tag:interface","title":"Interface","text":"<ul> <li>            Holoscan ROS2 Bridge (latest)          </li> </ul>"},{"location":"tags/#tag:interoperability","title":"Interoperability","text":"<ul> <li>            DICOM to OpenUSD mesh segmentation (latest)          </li> <li>            GPU Direct Storage on IGX (latest)          </li> <li>            Holoscan &amp; Windows Application (latest)          </li> <li>            Integrate External Libraries into a Holoscan Pipeline (latest)          </li> <li>            Self-Supervised Learning for Surgical videos (latest)          </li> <li>            Using holohub operators in external applications (latest)          </li> </ul>"},{"location":"tags/#tag:isaac-sim","title":"Isaac Sim","text":"<ul> <li>            Isaac Sim Holoscan Bridge (latest)          </li> </ul>"},{"location":"tags/#tag:llm","title":"LLM","text":"<ul> <li>            Chat with NVIDIA NIM (latest)          </li> <li>            Deploying Llama-2 70b model on the edge (latest)          </li> <li>            EHR Query LLM Operators (latest)          </li> <li>            FHIR Client (latest)          </li> <li>            FhirClientOperator (latest)          </li> <li>            FhirResourceSanitizerOp (latest)          </li> <li>            HoloChat (latest)          </li> <li>            Real-time Riva ASR to local-LLM (latest)          </li> <li>            Speech-to-text + Large Language Model (latest)          </li> <li>            VILA Live (latest)          </li> <li>            ZeroMQPublisherOp (latest)          </li> <li>            ZeroMQSubscriberOp (latest)          </li> </ul>"},{"location":"tags/#tag:lstm","title":"LSTM","text":"<ul> <li>            Distributed Endoscopy Tool Tracking with gRPC (latest)          </li> <li>            Distributed H.264 Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            LSTMTensorRTInferenceOp (latest)          </li> </ul>"},{"location":"tags/#tag:large-vision-model","title":"Large Vision Model","text":"<ul> <li>            VILA Live (latest)          </li> </ul>"},{"location":"tags/#tag:latency","title":"Latency","text":"<ul> <li>            Asynchronous Lock-free Buffer (latest)          </li> </ul>"},{"location":"tags/#tag:learning","title":"Learning","text":"<ul> <li>            Self-Supervised Learning for Surgical videos (latest)          </li> </ul>"},{"location":"tags/#tag:lidar","title":"Lidar","text":"<ul> <li>            Velodyne VLP-16 Lidar Viewer (latest)          </li> <li>            VelodyneLidarOp (latest)          </li> </ul>"},{"location":"tags/#tag:linux","title":"Linux","text":"<ul> <li>            Asynchronous Lock-free Buffer (latest)          </li> </ul>"},{"location":"tags/#tag:loader","title":"Loader","text":"<ul> <li>            DICOMDataLoaderOperator (latest)          </li> </ul>"},{"location":"tags/#tag:monai","title":"MONAI","text":"<ul> <li>            DICOM to OpenUSD mesh segmentation (latest)          </li> <li>            Medical Imaging Operators (latest)          </li> <li>            MonaiBundleInferenceOperator (latest)          </li> <li>            MonaiSegInferenceOperator (latest)          </li> <li>            Real-Time AI End-to-End Surgical Video Workflow (latest)          </li> <li>            Self-Supervised Learning for Surgical videos (latest)          </li> </ul>"},{"location":"tags/#tag:mps","title":"MPS","text":"<ul> <li>            CUDA MPS Tutorial (latest)          </li> </ul>"},{"location":"tags/#tag:medical-imaging","title":"Medical Imaging","text":"<ul> <li>            ClaraVizOperator (latest)          </li> <li>            DICOM to OpenUSD mesh segmentation (latest)          </li> <li>            DICOMDataLoaderOperator (latest)          </li> <li>            DICOMEncapsulatedPDFWriterOperator (latest)          </li> <li>            DICOMSegmentationWriterOperator (latest)          </li> <li>            DICOMSeriesSelectorOperator (latest)          </li> <li>            DICOMSeriesToVolumeOperator (latest)          </li> <li>            DICOMTextSRWriterOperator (latest)          </li> <li>            Imaging AI Whole Body Segmentation (latest)          </li> <li>            InferenceOperator (latest)          </li> <li>            Medical Imaging Operators (latest)          </li> <li>            Medical Imaging Segmentation with Vista-3D (latest)          </li> <li>            MonaiBundleInferenceOperator (latest)          </li> <li>            MonaiSegInferenceOperator (latest)          </li> <li>            NiftiDataLoader (latest)          </li> <li>            PNGConverterOperator (latest)          </li> <li>            PublisherOperator (latest)          </li> <li>            Real-time Riva ASR to local-LLM (latest)          </li> <li>            STLConversionOperator (latest)          </li> <li>            Speech-to-text + Large Language Model (latest)          </li> <li>            Ultrasound Post-Processing Filter Design (latest)          </li> </ul>"},{"location":"tags/#tag:monocular-depth-estimation","title":"Monocular Depth Estimation","text":"<ul> <li>            Depth Anything V2 (latest)          </li> <li>            Endoscopy Depth Estimation (latest)          </li> </ul>"},{"location":"tags/#tag:multimodal-model","title":"Multimodal Model","text":"<ul> <li>            Florence-2 (latest)          </li> <li>            NVIDIA NV-CLIP NIM (latest)          </li> <li>            VILA Live (latest)          </li> </ul>"},{"location":"tags/#tag:nats","title":"NATS","text":"<ul> <li>            Live Streaming Data Web Dashboard with NATS (latest)          </li> </ul>"},{"location":"tags/#tag:nic","title":"NIC","text":"<ul> <li>            0.1          </li> </ul>"},{"location":"tags/#tag:nifti","title":"NIfTI","text":"<ul> <li>            NiftiDataLoader (latest)          </li> </ul>"},{"location":"tags/#tag:nlp","title":"NLP","text":"<ul> <li>            Deploying Llama-2 70b model on the edge (latest)          </li> </ul>"},{"location":"tags/#tag:npp","title":"NPP","text":"<ul> <li>            NppFilterOp (latest)          </li> </ul>"},{"location":"tags/#tag:nrrd-processing","title":"NRRD processing","text":"<ul> <li>            Medical Imaging Segmentation with Vista-3D (latest)          </li> </ul>"},{"location":"tags/#tag:nvidia-video-codec","title":"NVIDIA Video Codec","text":"<ul> <li>            NVIDIA Video Codec: Encode-Decode Video (latest)          </li> <li>            NVIDIA Video Codec: Endoscopy Tool Tracking (latest)          </li> <li>            NVIDIA Video Codec: H.264 File Decoder (latest)          </li> <li>            NVIDIA Video Codec: Video Writer (latest)          </li> </ul>"},{"location":"tags/#tag:nvidia-video-codec-sdk","title":"NVIDIA Video Codec SDK","text":"<ul> <li>            NVIDIA Video Codec SDK: Video File Reader (latest)          </li> </ul>"},{"location":"tags/#tag:natural-language-and-conversational-ai","title":"Natural Language and Conversational AI","text":"<ul> <li>            Chat with NVIDIA NIM (latest)          </li> <li>            HoloChat (latest)          </li> </ul>"},{"location":"tags/#tag:network","title":"Network","text":"<ul> <li>            Video Streaming Client Demo (C++) (latest)          </li> <li>            Video Streaming Server Demo (C++) (latest)          </li> <li>            streaming_client (latest)          </li> <li>            streaming_server (latest)          </li> </ul>"},{"location":"tags/#tag:networking","title":"Networking","text":"<ul> <li>            0.1          </li> <li>            StreamingServerOps (latest)          </li> <li>            VideoStreamingClientOp (latest)          </li> <li>            VideoStreamingOps (latest)          </li> </ul>"},{"location":"tags/#tag:networking-and-distributed-computing","title":"Networking and Distributed Computing","text":"<ul> <li>            Advanced Network Operators (latest)          </li> <li>            Advanced Networking Benchmark (latest)          </li> <li>            Basic Networking Ping (latest)          </li> <li>            BasicNetworkOp Tx/Rx (latest)          </li> <li>            Creating Multi-Node Holoscan Applications (latest)          </li> <li>            DDS Operators (latest)          </li> <li>            DDSOperatorBase (latest)          </li> <li>            DDSShapesSubscriberOp (latest)          </li> <li>            DDSVideoPublisherOp (latest)          </li> <li>            EHR Query LLM Operators (latest)          </li> <li>            FhirClientOperator (latest)          </li> <li>            FhirResourceSanitizerOp (latest)          </li> <li>            GRPC Operators (latest)          </li> <li>            High Performance Networking with Holoscan (latest)          </li> <li>            OpenIGTLink 3D Slicer (latest)          </li> <li>            Radar Signal Processing over Network (latest)          </li> <li>            Real-time Video Streaming with DDS (latest)          </li> <li>            VITA 49 Power Spectral Density (latest)          </li> <li>            WebRTC Holoviz Server (latest)          </li> <li>            WebRTC Video Client (latest)          </li> <li>            WebRTC Video Server (latest)          </li> <li>            WebRTCClientOp (latest)          </li> <li>            WebRTCServerOp (latest)          </li> <li>            ZeroMQPublisherOp (latest)          </li> <li>            ZeroMQSubscriberOp (latest)          </li> </ul>"},{"location":"tags/#tag:neural-rendering","title":"Neural Rendering","text":"<ul> <li>            Surgical Scene Reconstruction (latest)          </li> </ul>"},{"location":"tags/#tag:neuroscience","title":"Neuroscience","text":"<ul> <li>            Kernel Flow BCI Real-Time Visualization (latest)          </li> </ul>"},{"location":"tags/#tag:opencv","title":"OpenCV","text":"<ul> <li>            Integrate External Libraries into a Holoscan Pipeline (latest)          </li> </ul>"},{"location":"tags/#tag:openusd","title":"OpenUSD","text":"<ul> <li>            DICOM to OpenUSD mesh segmentation (latest)          </li> <li>            SendMeshToUSDOp (latest)          </li> </ul>"},{"location":"tags/#tag:openxr","title":"OpenXR","text":"<ul> <li>            XR + Gaussian Splatting (latest)          </li> </ul>"},{"location":"tags/#tag:operator","title":"Operator","text":"<ul> <li>            Ultrasound Post-Processing Filter Design (latest)          </li> </ul>"},{"location":"tags/#tag:optical-sensing","title":"Optical Sensing","text":"<ul> <li>            Kernel Flow BCI Real-Time Visualization (latest)          </li> </ul>"},{"location":"tags/#tag:optimization","title":"Optimization","text":"<ul> <li>            Asynchronous Lock-free Buffer (latest)          </li> <li>            CUDA MPS Tutorial (latest)          </li> <li>            GPU Direct Storage on IGX (latest)          </li> <li>            Holoscan SDK Response Time Analysis (latest)          </li> <li>            Image Processing with MATLAB GPU Coder (latest)          </li> <li>            PVA-Accelerated Image Sharpening (latest)          </li> </ul>"},{"location":"tags/#tag:optix","title":"Optix","text":"<ul> <li>            Orthorectification with NVIDIA OptiX (latest)          </li> </ul>"},{"location":"tags/#tag:pdf","title":"PDF","text":"<ul> <li>            DICOMEncapsulatedPDFWriterOperator (latest)          </li> </ul>"},{"location":"tags/#tag:png","title":"PNG","text":"<ul> <li>            PNGConverterOperator (latest)          </li> </ul>"},{"location":"tags/#tag:psd","title":"PSD","text":"<ul> <li>            Power Spectral Density with cuNumeric (latest)          </li> <li>            VITA 49 Power Spectral Density (latest)          </li> </ul>"},{"location":"tags/#tag:pva","title":"PVA","text":"<ul> <li>            PVA-Accelerated Image Sharpening (latest)          </li> </ul>"},{"location":"tags/#tag:performance","title":"Performance","text":"<ul> <li>            Asynchronous Lock-free Buffer (latest)          </li> <li>            Green Context Benchmarking (latest)          </li> <li>            Holoscan Release Benchmarking (latest)          </li> <li>            Holoscan SDK Response Time Analysis (latest)          </li> <li>            Real-time Thread Scheduling Benchmark (latest)          </li> </ul>"},{"location":"tags/#tag:point-cloud","title":"Point Cloud","text":"<ul> <li>            Velodyne VLP-16 Lidar Viewer (latest)          </li> <li>            VelodyneLidarOp (latest)          </li> </ul>"},{"location":"tags/#tag:polyphase-resampling","title":"Polyphase Resampling","text":"<ul> <li>            FM Radio Automatic Speech Recognition (latest)          </li> </ul>"},{"location":"tags/#tag:processing","title":"Processing","text":"<ul> <li>            Isaac Sim Holoscan Bridge (latest)          </li> </ul>"},{"location":"tags/#tag:publisher","title":"Publisher","text":"<ul> <li>            Holoscan ROS2 Publisher/Subscriber Examples (latest)          </li> <li>            PublisherOperator (latest)          </li> <li>            ROS2 VB1940 (Eagle) Camera (latest)          </li> </ul>"},{"location":"tags/#tag:python","title":"Python","text":"<ul> <li>            Adding a GUI to Holoscan Python Applications (latest)          </li> </ul>"},{"location":"tags/#tag:qt","title":"Qt","text":"<ul> <li>            Florence-2 (latest)          </li> <li>            Qt Video Replayer (latest)          </li> <li>            QtVideoOp (latest)          </li> </ul>"},{"location":"tags/#tag:quantized-model-inference","title":"Quantized Model Inference","text":"<ul> <li>            Real-time Riva ASR to local-LLM (latest)          </li> </ul>"},{"location":"tags/#tag:quantum-computing","title":"Quantum Computing","text":"<ul> <li>            CUDA Quantum Variational Quantum Eigensolver (VQE) (latest)          </li> </ul>"},{"location":"tags/#tag:rag","title":"RAG","text":"<ul> <li>            EHR Agent Framework (latest)          </li> <li>            HoloChat (latest)          </li> </ul>"},{"location":"tags/#tag:rdma","title":"RDMA","text":"<ul> <li>            0.1          </li> <li>            Advanced Network Operators (latest)          </li> <li>            Deltacast Videomaster Receiver (latest)          </li> <li>            Deltacast Videomaster Transmitter (latest)          </li> <li>            High Performance Networking with Holoscan (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> </ul>"},{"location":"tags/#tag:ros2","title":"ROS2","text":"<ul> <li>            Holoscan ROS2 Bridge (latest)          </li> <li>            Holoscan ROS2 Publisher/Subscriber Examples (latest)          </li> <li>            ROS2 VB1940 (Eagle) Camera (latest)          </li> </ul>"},{"location":"tags/#tag:rt-detr","title":"RT-DETR","text":"<ul> <li>            Polyp Detection (latest)          </li> </ul>"},{"location":"tags/#tag:rti-connext","title":"RTI Connext","text":"<ul> <li>            Body Pose Estimation (latest)          </li> <li>            DDS Operators (latest)          </li> <li>            DDSOperatorBase (latest)          </li> <li>            DDSShapesSubscriberOp (latest)          </li> <li>            DDSVideoPublisherOp (latest)          </li> <li>            Real-time Video Streaming with DDS (latest)          </li> </ul>"},{"location":"tags/#tag:real-time","title":"Real-Time","text":"<ul> <li>            Asynchronous Lock-free Buffer (latest)          </li> <li>            Ultrasound Post-Processing Filter Design (latest)          </li> </ul>"},{"location":"tags/#tag:real-time","title":"Real-time","text":"<ul> <li>            Live Streaming Data Web Dashboard with NATS (latest)          </li> <li>            Real-time Thread Scheduling Benchmark (latest)          </li> <li>            Video Streaming Client Demo (C++) (latest)          </li> <li>            Video Streaming Server Demo (C++) (latest)          </li> <li>            streaming_client (latest)          </li> <li>            streaming_server (latest)          </li> </ul>"},{"location":"tags/#tag:receiver","title":"Receiver","text":"<ul> <li>            Deltacast Videomaster Receiver (latest)          </li> </ul>"},{"location":"tags/#tag:recording","title":"Recording","text":"<ul> <li>            GStreamer Video Recorder (latest)          </li> </ul>"},{"location":"tags/#tag:rendering","title":"Rendering","text":"<ul> <li>            Convert Depth to Screen Space (latest)          </li> <li>            Endoscopy Depth Estimation (latest)          </li> <li>            Holoviz UI (latest)          </li> <li>            Holoviz sRGB (latest)          </li> <li>            Holoviz vsync (latest)          </li> <li>            Slang Gamma Correction (latest)          </li> <li>            Slang Simple Compute Kernel (latest)          </li> <li>            VolumeRendererOp (latest)          </li> <li>            XR + Holoviz (latest)          </li> </ul>"},{"location":"tags/#tag:rivermax","title":"Rivermax","text":"<ul> <li>            0.1          </li> </ul>"},{"location":"tags/#tag:robotics","title":"Robotics","text":"<ul> <li>            Holoscan ROS2 Bridge (latest)          </li> <li>            Holoscan ROS2 Publisher/Subscriber Examples (latest)          </li> <li>            Isaac Sim Holoscan Bridge (latest)          </li> <li>            ROS2 VB1940 (Eagle) Camera (latest)          </li> <li>            VelodyneLidarOp (latest)          </li> </ul>"},{"location":"tags/#tag:sched_deadline","title":"SCHED_DEADLINE","text":"<ul> <li>            Asynchronous Lock-free Buffer (latest)          </li> </ul>"},{"location":"tags/#tag:sdp-exchange","title":"SDP Exchange","text":"<ul> <li>            WebRTC Video Client (latest)          </li> </ul>"},{"location":"tags/#tag:sr","title":"SR","text":"<ul> <li>            DICOMTextSRWriterOperator (latest)          </li> </ul>"},{"location":"tags/#tag:ssd","title":"SSD","text":"<ul> <li>            Multi AI Detection and Tool Segmentation (latest)          </li> <li>            Real-Time AI End-to-End Surgical Video Workflow (latest)          </li> <li>            SSD Detection for Endoscopy Tools (latest)          </li> </ul>"},{"location":"tags/#tag:st2084","title":"ST2084","text":"<ul> <li>            Holoviz HDR (latest)          </li> </ul>"},{"location":"tags/#tag:stl","title":"STL","text":"<ul> <li>            DICOM to OpenUSD mesh segmentation (latest)          </li> <li>            Medical Imaging Operators (latest)          </li> <li>            STLConversionOperator (latest)          </li> <li>            SendMeshToUSDOp (latest)          </li> </ul>"},{"location":"tags/#tag:scheduler","title":"Scheduler","text":"<ul> <li>            Async Lock-free Buffer (latest)          </li> </ul>"},{"location":"tags/#tag:scheduling","title":"Scheduling","text":"<ul> <li>            Green Context Benchmarking (latest)          </li> </ul>"},{"location":"tags/#tag:segmentation","title":"Segmentation","text":"<ul> <li>            Colonoscopy Polyp Segmentation (latest)          </li> <li>            DICOMSegmentationWriterOperator (latest)          </li> <li>            Endoscopy Tool Segmentation using MONAI (latest)          </li> <li>            Florence-2 (latest)          </li> <li>            Hyperspectral Image Segmentation (latest)          </li> <li>            Medical Imaging Segmentation with Vista-3D (latest)          </li> <li>            MonaiSegInferenceOperator (latest)          </li> <li>            Multi AI Detection and Tool Segmentation (latest)          </li> <li>            Multi AI and AR Visualization (latest)          </li> <li>            OpenIGTLink 3D Slicer (latest)          </li> <li>            Real-Time AI End-to-End Surgical Video Workflow (latest)          </li> <li>            SAM 2: Segment Anything in Images and Videos (latest)          </li> <li>            Surgical Tool Segmentation and AR Overlay (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            Volume rendering using ClaraViz (latest)          </li> </ul>"},{"location":"tags/#tag:selector","title":"Selector","text":"<ul> <li>            DICOMSeriesSelectorOperator (latest)          </li> </ul>"},{"location":"tags/#tag:server","title":"Server","text":"<ul> <li>            StreamingServerOps (latest)          </li> <li>            Video Streaming Demo (latest)          </li> <li>            Video Streaming Server Demo (C++) (latest)          </li> <li>            VideoStreamingOps (latest)          </li> <li>            streaming_server (latest)          </li> </ul>"},{"location":"tags/#tag:signal-processing","title":"Signal Processing","text":"<ul> <li>            Basic Pulse Description Word (PDW) Generator (latest)          </li> <li>            FFT (latest)          </li> <li>            FM Radio Automatic Speech Recognition (latest)          </li> <li>            HighRatePSD (latest)          </li> <li>            IIO Controller Operators (latest)          </li> <li>            Industrial I/O (IIO) - ADALM-Pluto SDR Integration (latest)          </li> <li>            Live Streaming Data Web Dashboard with NATS (latest)          </li> <li>            LowRatePSD (latest)          </li> <li>            NppFilterOp (latest)          </li> <li>            Power Spectral Density with cuNumeric (latest)          </li> <li>            Radar Signal Processing over Network (latest)          </li> <li>            Simple Radar Pipeline (latest)          </li> <li>            Simple Radar Pipeline (latest)          </li> <li>            Software Defined Radio FM Demodulation (latest)          </li> <li>            Streaming Synthetic Aperture Radar (latest)          </li> <li>            Ultrasound Beamforming with MATLAB (latest)          </li> <li>            V49PsdPacketizer (latest)          </li> <li>            VITA 49 Power Spectral Density (latest)          </li> <li>            Velodyne VLP-16 Lidar Viewer (latest)          </li> <li>            data_writer (latest)          </li> </ul>"},{"location":"tags/#tag:slang","title":"Slang","text":"<ul> <li>            Slang Gamma Correction (latest)          </li> <li>            Slang Simple Compute Kernel (latest)          </li> <li>            SlangShaderOp (latest)          </li> </ul>"},{"location":"tags/#tag:stereo-vision","title":"Stereo Vision","text":"<ul> <li>            Stereo Vision (latest)          </li> <li>            VPI Stereo Vision (latest)          </li> <li>            XR + Holoviz (latest)          </li> <li>            XR Demo (latest)          </li> </ul>"},{"location":"tags/#tag:streaming","title":"Streaming","text":"<ul> <li>            AJA Video Capture (latest)          </li> <li>            Deltacast Videomaster Transmitter (latest)          </li> <li>            Distributed Endoscopy Tool Tracking with gRPC (latest)          </li> <li>            Live Streaming Data Web Dashboard with NATS (latest)          </li> <li>            NVIDIA Video Codec SDK: Video File Reader (latest)          </li> <li>            NVIDIA Video Codec: Encode-Decode Video (latest)          </li> <li>            NVIDIA Video Codec: H.264 File Decoder (latest)          </li> <li>            NVIDIA Video Codec: Video Writer (latest)          </li> <li>            OpenIGTLink Tx/Rx (latest)          </li> <li>            StreamingServerOps (latest)          </li> <li>            Video Streaming Client Demo (C++) (latest)          </li> <li>            Video Streaming Demo (latest)          </li> <li>            Video Streaming Server Demo (C++) (latest)          </li> <li>            VideoStreamingClientOp (latest)          </li> <li>            VideoStreamingOps (latest)          </li> <li>            streaming_client (latest)          </li> <li>            streaming_server (latest)          </li> </ul>"},{"location":"tags/#tag:subscriber","title":"Subscriber","text":"<ul> <li>            Holoscan ROS2 Publisher/Subscriber Examples (latest)          </li> <li>            ROS2 VB1940 (Eagle) Camera (latest)          </li> </ul>"},{"location":"tags/#tag:surgical-ai","title":"Surgical AI","text":"<ul> <li>            Distributed H.264 Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Segmentation using MONAI (latest)          </li> <li>            H.264 Endoscopy Tool Tracking (latest)          </li> <li>            Multi AI Detection and Tool Segmentation (latest)          </li> <li>            NVIDIA Video Codec: Endoscopy Tool Tracking (latest)          </li> <li>            Self-Supervised Learning for Surgical videos (latest)          </li> <li>            Surgical Tool Segmentation and AR Overlay (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> </ul>"},{"location":"tags/#tag:synthetic-aperture-beamforming","title":"Synthetic Aperture Beamforming","text":"<ul> <li>            Streaming Synthetic Aperture Radar (latest)          </li> <li>            Ultrasound Beamforming with MATLAB (latest)          </li> </ul>"},{"location":"tags/#tag:tcp","title":"TCP","text":"<ul> <li>            BasicNetworkOp Tx/Rx (latest)          </li> </ul>"},{"location":"tags/#tag:temporal-deformation","title":"Temporal Deformation","text":"<ul> <li>            Surgical Scene Reconstruction (latest)          </li> </ul>"},{"location":"tags/#tag:tensor","title":"Tensor","text":"<ul> <li>            Live Streaming Data Web Dashboard with NATS (latest)          </li> <li>            Tensor to File (latest)          </li> <li>            TensorToVideoBufferOp (latest)          </li> </ul>"},{"location":"tags/#tag:tensor-optimization","title":"Tensor Optimization","text":"<ul> <li>            Body Pose Estimation (latest)          </li> <li>            Simple CV-CUDA (latest)          </li> <li>            Yolo Object Detection (latest)          </li> </ul>"},{"location":"tags/#tag:tensorrt","title":"TensorRT","text":"<ul> <li>            Distributed H.264 Endoscopy Tool Tracking (latest)          </li> <li>            LSTMTensorRTInferenceOp (latest)          </li> </ul>"},{"location":"tags/#tag:text","title":"Text","text":"<ul> <li>            DICOMTextSRWriterOperator (latest)          </li> </ul>"},{"location":"tags/#tag:threading","title":"Threading","text":"<ul> <li>            Real-time Thread Scheduling Benchmark (latest)          </li> </ul>"},{"location":"tags/#tag:tools","title":"Tools","text":"<ul> <li>            Ultrasound Post-Processing Filter Design (latest)          </li> </ul>"},{"location":"tags/#tag:totalsegmentator","title":"TotalSegmentator","text":"<ul> <li>            Imaging AI Whole Body Segmentation (latest)          </li> <li>            Volume rendering using ClaraViz (latest)          </li> </ul>"},{"location":"tags/#tag:udp","title":"UDP","text":"<ul> <li>            Advanced Network Operators (latest)          </li> <li>            Advanced Networking Benchmark (latest)          </li> <li>            Basic Networking Ping (latest)          </li> <li>            Basic Pulse Description Word (PDW) Generator (latest)          </li> <li>            BasicNetworkOp Tx/Rx (latest)          </li> <li>            VITA 49 Power Spectral Density (latest)          </li> <li>            Velodyne VLP-16 Lidar Viewer (latest)          </li> </ul>"},{"location":"tags/#tag:uff","title":"UFF","text":"<ul> <li>            Ultrasound Post-Processing Filter Design (latest)          </li> </ul>"},{"location":"tags/#tag:ui","title":"UI","text":"<ul> <li>            Adding a GUI to Holoscan Python Applications (latest)          </li> <li>            Qt Video Replayer (latest)          </li> <li>            QtVideoOp (latest)          </li> </ul>"},{"location":"tags/#tag:ultrasound","title":"Ultrasound","text":"<ul> <li>            Multi-AI Ultrasound (latest)          </li> <li>            Multi-AI Ultrasound (latest)          </li> <li>            Multi-AI Ultrasound (latest)          </li> <li>            OpenIGTLink 3D Slicer (latest)          </li> <li>            Ultrasound Beamforming with MATLAB (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            Ultrasound Post-Processing Filter Design (latest)          </li> </ul>"},{"location":"tags/#tag:v4l2","title":"V4L2","text":"<ul> <li>            Video Streaming Client Demo (C++) (latest)          </li> <li>            Video Streaming Demo (latest)          </li> </ul>"},{"location":"tags/#tag:vb1940","title":"VB1940","text":"<ul> <li>            ROS2 VB1940 (Eagle) Camera (latest)          </li> </ul>"},{"location":"tags/#tag:vm","title":"VM","text":"<ul> <li>            Holoscan &amp; Windows Application (latest)          </li> </ul>"},{"location":"tags/#tag:vs-code","title":"VS Code","text":"<ul> <li>            VSCode Dev Container for Holoscan (latest)          </li> </ul>"},{"location":"tags/#tag:vtk","title":"VTK","text":"<ul> <li>            Multi AI and AR Visualization (latest)          </li> <li>            Surgical Tool Segmentation and AR Overlay (latest)          </li> </ul>"},{"location":"tags/#tag:vector-database","title":"Vector Database","text":"<ul> <li>            EHR Agent Framework (latest)          </li> <li>            HoloChat (latest)          </li> </ul>"},{"location":"tags/#tag:video","title":"Video","text":"<ul> <li>            AJA Video Capture (latest)          </li> <li>            Body Pose Estimation (latest)          </li> <li>            Colonoscopy Polyp Segmentation (latest)          </li> <li>            DDSVideoPublisherOp (latest)          </li> <li>            Deltacast Videomaster Receiver (latest)          </li> <li>            Deltacast Videomaster Transmitter (latest)          </li> <li>            Depth Anything V2 (latest)          </li> <li>            Distributed Endoscopy Tool Tracking with gRPC (latest)          </li> <li>            Distributed H.264 Endoscopy Tool Tracking (latest)          </li> <li>            Distributed H.264 gRPC Streaming (latest)          </li> <li>            Endoscopy Depth Estimation (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Endoscopy Out of Body Detection (latest)          </li> <li>            Endoscopy Tool Segmentation using MONAI (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Florence-2 (latest)          </li> <li>            GStreamer Video Recorder (latest)          </li> <li>            H.264 Endoscopy Tool Tracking (latest)          </li> <li>            H.264 Video Decode (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> <li>            High-Speed Endoscopy (latest)          </li> <li>            Holoviz YUV (latest)          </li> <li>            Image Processing with MATLAB GPU Coder (latest)          </li> <li>            In-Out Body Detection (latest)          </li> <li>            Laser Detection (latest)          </li> <li>            Multi AI Detection and Tool Segmentation (latest)          </li> <li>            Multi-AI Ultrasound (latest)          </li> <li>            Multi-AI Ultrasound (latest)          </li> <li>            Multi-AI Ultrasound (latest)          </li> <li>            NVIDIA Video Codec: Encode-Decode Video (latest)          </li> <li>            NVIDIA Video Codec: Endoscopy Tool Tracking (latest)          </li> <li>            NVIDIA Video Codec: H.264 File Decoder (latest)          </li> <li>            NVIDIA Video Codec: Video Writer (latest)          </li> <li>            Object Detection using PyTorch Faster R-CNN (latest)          </li> <li>            OpenIGTLink 3D Slicer (latest)          </li> <li>            PVA-Accelerated Image Sharpening (latest)          </li> <li>            ProHawk Video Replayer (latest)          </li> <li>            ProhawkOp (latest)          </li> <li>            Qt Video Replayer (latest)          </li> <li>            QtVideoOp (latest)          </li> <li>            Real-Time Face and Text Deidentification (latest)          </li> <li>            Real-time Video Streaming with DDS (latest)          </li> <li>            SAM 2: Segment Anything in Images and Videos (latest)          </li> <li>            SSD Detection for Endoscopy Tools (latest)          </li> <li>            Self-Supervised Learning for Surgical videos (latest)          </li> <li>            Simple CV-CUDA (latest)          </li> <li>            Stereo Vision (latest)          </li> <li>            StreamingServerOps (latest)          </li> <li>            TAO PeopleNet Detection Model (latest)          </li> <li>            TensorToVideoBufferOp (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            UCX-based Distributed Endoscopy Tool Tracking (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            VILA Live (latest)          </li> <li>            VPI Stereo Vision (latest)          </li> <li>            Video Streaming Client Demo (C++) (latest)          </li> <li>            Video Streaming Demo (latest)          </li> <li>            Video Streaming Server Demo (C++) (latest)          </li> <li>            VideoEncoderRequestOp (latest)          </li> <li>            VideoStreamingClientOp (latest)          </li> <li>            VideoStreamingOps (latest)          </li> <li>            WebRTC Video Client (latest)          </li> <li>            WebRTC Video Server (latest)          </li> <li>            WebRTCClientOp (latest)          </li> <li>            WebRTCServerOp (latest)          </li> <li>            Yolo Object Detection (latest)          </li> <li>            streaming_client (latest)          </li> <li>            streaming_server (latest)          </li> <li>            visualizer_icardio (latest)          </li> </ul>"},{"location":"tags/#tag:video-reading","title":"Video Reading","text":"<ul> <li>            NVIDIA Video Codec SDK: Video File Reader (latest)          </li> </ul>"},{"location":"tags/#tag:visualization","title":"Visualization","text":"<ul> <li>            Body Pose Estimation (latest)          </li> <li>            ClaraVizOperator (latest)          </li> <li>            Colonoscopy Polyp Segmentation (latest)          </li> <li>            Depth Anything V2 (latest)          </li> <li>            Distributed Endoscopy Tool Tracking with gRPC (latest)          </li> <li>            EVT Camera Calibration (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Exclusive Display Benchmark (latest)          </li> <li>            GRPC Operators (latest)          </li> <li>            H.264 Endoscopy Tool Tracking (latest)          </li> <li>            Holoviz HDR (latest)          </li> <li>            Holoviz UI (latest)          </li> <li>            Holoviz YUV (latest)          </li> <li>            Holoviz sRGB (latest)          </li> <li>            Holoviz vsync (latest)          </li> <li>            Hyperspectral Image Segmentation (latest)          </li> <li>            Image Processing with MATLAB GPU Coder (latest)          </li> <li>            In-Out Body Detection (latest)          </li> <li>            Intel RealSense Camera Visualizer (latest)          </li> <li>            Kernel Flow BCI Real-Time Visualization (latest)          </li> <li>            Live Streaming Data Web Dashboard with NATS (latest)          </li> <li>            Medical Image Viewer in XR (latest)          </li> <li>            Multi-AI Ultrasound (latest)          </li> <li>            Multi-AI Ultrasound (latest)          </li> <li>            Multi-AI Ultrasound (latest)          </li> <li>            NVIDIA Video Codec: Endoscopy Tool Tracking (latest)          </li> <li>            Object Detection using PyTorch Faster R-CNN (latest)          </li> <li>            OpenIGTLink 3D Slicer (latest)          </li> <li>            Orthorectification with NVIDIA OptiX (latest)          </li> <li>            PVA-Accelerated Image Sharpening (latest)          </li> <li>            Qt Video Replayer (latest)          </li> <li>            QtVideoOp (latest)          </li> <li>            Real-time Video Streaming with DDS (latest)          </li> <li>            SAM 2: Segment Anything in Images and Videos (latest)          </li> <li>            SSD Detection for Endoscopy Tools (latest)          </li> <li>            Simple CV-CUDA (latest)          </li> <li>            SlangShaderOp (latest)          </li> <li>            Stereo Vision (latest)          </li> <li>            ToolTrackingPostprocessorOp (latest)          </li> <li>            USB Camera Calibration (latest)          </li> <li>            Ultrasound Beamforming with MATLAB (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            Ultrasound Bone Scoliosis Segmentation (latest)          </li> <li>            VPI Stereo Vision (latest)          </li> <li>            Velodyne VLP-16 Lidar Viewer (latest)          </li> <li>            Volume rendering using ClaraViz (latest)          </li> <li>            VolumeLoaderOp (latest)          </li> <li>            VolumeRendererOp (latest)          </li> <li>            VtkRendererOp (latest)          </li> <li>            WebRTC Holoviz Server (latest)          </li> <li>            WebRTC Video Client (latest)          </li> <li>            XR + Holoviz (latest)          </li> <li>            XR Demo (latest)          </li> <li>            XR Operators (latest)          </li> <li>            Yolo Object Detection (latest)          </li> </ul>"},{"location":"tags/#tag:volume","title":"Volume","text":"<ul> <li>            DICOMSeriesToVolumeOperator (latest)          </li> <li>            Medical Image Viewer in XR (latest)          </li> <li>            Volume rendering using ClaraViz (latest)          </li> <li>            VolumeLoaderOp (latest)          </li> <li>            VolumeRendererOp (latest)          </li> </ul>"},{"location":"tags/#tag:volume-rendering","title":"Volume Rendering","text":"<ul> <li>            Kernel Flow BCI Real-Time Visualization (latest)          </li> </ul>"},{"location":"tags/#tag:volumetric-reconstruction","title":"Volumetric Reconstruction","text":"<ul> <li>            Imaging AI Whole Body Segmentation (latest)          </li> </ul>"},{"location":"tags/#tag:vulkan","title":"Vulkan","text":"<ul> <li>            XR Operators (latest)          </li> </ul>"},{"location":"tags/#tag:webrtc","title":"WebRTC","text":"<ul> <li>            WebRTC Holoviz Server (latest)          </li> <li>            WebRTC Video Client (latest)          </li> <li>            WebRTC Video Server (latest)          </li> <li>            WebRTCClientOp (latest)          </li> <li>            WebRTCServerOp (latest)          </li> </ul>"},{"location":"tags/#tag:writer","title":"Writer","text":"<ul> <li>            DICOMEncapsulatedPDFWriterOperator (latest)          </li> <li>            DICOMSegmentationWriterOperator (latest)          </li> <li>            Tensor to File (latest)          </li> </ul>"},{"location":"tags/#tag:xr","title":"XR","text":"<ul> <li>            NVIDIA CloudXR Runtime for XR Applications (latest)          </li> </ul>"},{"location":"tags/#tag:yolo-detection","title":"YOLO Detection","text":"<ul> <li>            Yolo Object Detection (latest)          </li> </ul>"},{"location":"tags/#tag:zeromq","title":"ZeroMQ","text":"<ul> <li>            EHR Agent Framework (latest)          </li> <li>            FHIR Client (latest)          </li> </ul>"},{"location":"tags/#tag:async_buffer","title":"async_buffer","text":"<ul> <li>            Async Lock-free Buffer (latest)          </li> </ul>"},{"location":"tags/#tag:bounding-box","title":"bounding box","text":"<ul> <li>            Polyp Detection (latest)          </li> </ul>"},{"location":"tags/#tag:color-space-conversion","title":"color space conversion","text":"<ul> <li>            Holoviz HDR (latest)          </li> </ul>"},{"location":"tags/#tag:compute","title":"compute","text":"<ul> <li>            Slang Gamma Correction (latest)          </li> <li>            Slang Simple Compute Kernel (latest)          </li> <li>            SlangShaderOp (latest)          </li> </ul>"},{"location":"tags/#tag:correction","title":"correction","text":"<ul> <li>            GammaCorrectionOp (latest)          </li> </ul>"},{"location":"tags/#tag:cuda","title":"cuda","text":"<ul> <li>            GStreamer Bridge Operators (latest)          </li> <li>            Slang Gamma Correction (latest)          </li> <li>            Slang Simple Compute Kernel (latest)          </li> <li>            SlangShaderOp (latest)          </li> </ul>"},{"location":"tags/#tag:deadline_scheduling","title":"deadline_scheduling","text":"<ul> <li>            Async Lock-free Buffer (latest)          </li> </ul>"},{"location":"tags/#tag:event_based_scheduler","title":"event_based_scheduler","text":"<ul> <li>            Async Lock-free Buffer (latest)          </li> </ul>"},{"location":"tags/#tag:filter-presets","title":"filter presets","text":"<ul> <li>            ProHawk Video Replayer (latest)          </li> </ul>"},{"location":"tags/#tag:format-conversion","title":"format conversion","text":"<ul> <li>            AJA Video Capture (latest)          </li> </ul>"},{"location":"tags/#tag:frame-rate-synchronization","title":"frame rate synchronization","text":"<ul> <li>            Holoviz vsync (latest)          </li> </ul>"},{"location":"tags/#tag:grpc","title":"gRPC","text":"<ul> <li>            Distributed Endoscopy Tool Tracking with gRPC (latest)          </li> <li>            Distributed H.264 gRPC Streaming (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            Endoscopy Tool Tracking (latest)          </li> <li>            FM Radio Automatic Speech Recognition (latest)          </li> <li>            GRPC Operators (latest)          </li> </ul>"},{"location":"tags/#tag:gamma","title":"gamma","text":"<ul> <li>            GammaCorrectionOp (latest)          </li> </ul>"},{"location":"tags/#tag:gstreamer","title":"gstreamer","text":"<ul> <li>            GStreamer Bridge Operators (latest)          </li> </ul>"},{"location":"tags/#tag:iio","title":"iio","text":"<ul> <li>            IIO Controller Operators (latest)          </li> <li>            Industrial I/O (IIO) - ADALM-Pluto SDR Integration (latest)          </li> </ul>"},{"location":"tags/#tag:libiio","title":"libiio","text":"<ul> <li>            IIO Controller Operators (latest)          </li> <li>            Industrial I/O (IIO) - ADALM-Pluto SDR Integration (latest)          </li> </ul>"},{"location":"tags/#tag:multimedia","title":"multimedia","text":"<ul> <li>            GStreamer Bridge Operators (latest)          </li> </ul>"},{"location":"tags/#tag:raytracing","title":"raytracing","text":"<ul> <li>            Slang Simple Compute Kernel (latest)          </li> <li>            SlangShaderOp (latest)          </li> </ul>"},{"location":"tags/#tag:recording","title":"recording","text":"<ul> <li>            GStreamer Bridge Operators (latest)          </li> </ul>"},{"location":"tags/#tag:rendering","title":"rendering","text":"<ul> <li>            SlangShaderOp (latest)          </li> </ul>"},{"location":"tags/#tag:shader","title":"shader","text":"<ul> <li>            Slang Gamma Correction (latest)          </li> <li>            Slang Simple Compute Kernel (latest)          </li> <li>            SlangShaderOp (latest)          </li> </ul>"},{"location":"tags/#tag:shadertoy","title":"shadertoy","text":"<ul> <li>            Slang Simple Compute Kernel (latest)          </li> <li>            SlangShaderOp (latest)          </li> </ul>"},{"location":"tags/#tag:shading","title":"shading","text":"<ul> <li>            Slang Gamma Correction (latest)          </li> <li>            Slang Simple Compute Kernel (latest)          </li> <li>            SlangShaderOp (latest)          </li> </ul>"},{"location":"tags/#tag:streaming","title":"streaming","text":"<ul> <li>            GStreamer Bridge Operators (latest)          </li> </ul>"},{"location":"tags/#tag:video","title":"video","text":"<ul> <li>            GStreamer Bridge Operators (latest)          </li> </ul>"},{"location":"applications/adv_networking_bench/","title":"Advanced Networking Benchmark","text":"<p>     \u25b6 Run Locally  Authors: Cliff Burdick (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: October 20, 2025 Latest version: 1.3 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>[!TIP] Review the High Performance Networking tutorial for guided instructions to configure your system and test the Advanced Network library.</p> <p>This is a sample application to measure a lower bound on performance for the Advanced Network library by receiving packets, optionally doing work on them, and freeing the buffers. While only freeing the packets is an unrealistic workload, it's useful to see at a high level whether the application is able to keep up with the bare minimum amount of work to do. The application contains both a transmitter and receiver that are designed to run on different systems, and may be configured independently.</p> <p>The performance of this application depends heavily on a properly-configured system and choosing the best tuning parameters that are acceptable for the workload. To configure the system please see the documentation here. With the system tuned, the application performance will be dictated by batching size and whether GPUDirect is enabled.</p> <p>At this time both the transmitter and receiver are written to handle an Ethernet+IP+UDP packet with a configurable payload. Other modes may be added in the future. Also, for simplicity, the transmitter and receiver are configured to a single packet size.</p>","tags":["Networking and Distributed Computing","Distributed","UDP","GPUDirect","DPDK"]},{"location":"applications/adv_networking_bench/#transmit","title":"Transmit","text":"<p>The transmitter sends a UDP packet with an incrementing sequence of bytes after the UDP header. The batch size configured dictates how many packets the benchmark operator sends to the NIC in each tick. Typically with the same number of CPU cores the transmitter will run faster than the receiver, so this parameter may be used to throttle the sender somewhat by making the batches very small.</p>","tags":["Networking and Distributed Computing","Distributed","UDP","GPUDirect","DPDK"]},{"location":"applications/adv_networking_bench/#receiver","title":"Receiver","text":"<p>The receiver receives the UDP packets in either CPU-only mode, header-data split mode, or GPU-only mode. - CPU-only mode will receive the packets in CPU memory, copy the payload contents to a host-pinned staging buffer,   and free the buffers. - Header-data split mode: the user may configure separate memory regions for the header and data. The header is   sent to the CPU, and all bytes afterwards are sent to the GPU. Header-data split should achieve higher   rates than CPU mode since the amount of data to the CPU can be orders of magnitude lower compared to running   in CPU-only mode. - GPU-only mode: all bytes of the packets are received in GPU memory.</p>","tags":["Networking and Distributed Computing","Distributed","UDP","GPUDirect","DPDK"]},{"location":"applications/adv_networking_bench/#configuration","title":"Configuration","text":"<p>The application can be configured to do either Rx, Tx, or both, using different configuration files, found in this directory.</p>","tags":["Networking and Distributed Computing","Distributed","UDP","GPUDirect","DPDK"]},{"location":"applications/adv_networking_bench/#receive-configuration","title":"Receive Configuration","text":"<ul> <li><code>header_data_split</code>: bool   Turn on GPUDirect header-data split mode</li> <li><code>batch_size</code>: integer   Size in packets for a single batch. This should be a multiple of the advanced_network queue batch size.   A larger batch size consumes more memory since any work will not start unless this batch size is filled. Consider   reducing this value if errors are occurring.</li> <li><code>max_packet_size</code>: integer   Maximum packet size expected. This value includes all headers up to and including UDP.</li> </ul>","tags":["Networking and Distributed Computing","Distributed","UDP","GPUDirect","DPDK"]},{"location":"applications/adv_networking_bench/#transmit-configuration","title":"Transmit Configuration","text":"<ul> <li><code>batch_size</code>: integer   Size in packets for a single batch. This batch size is used to send to the NIC, and   will loop sending that many packets for each burst.</li> <li><code>payload_size</code>: integer   Size of the payload to send after all L2-L4 headers</li> </ul>","tags":["Networking and Distributed Computing","Distributed","UDP","GPUDirect","DPDK"]},{"location":"applications/adv_networking_bench/#requirements","title":"Requirements","text":"<p>This application requires all configuration and requirements from the Advanced Network library first.</p>","tags":["Networking and Distributed Computing","Distributed","UDP","GPUDirect","DPDK"]},{"location":"applications/adv_networking_bench/#build-instructions","title":"Build Instructions","text":"","tags":["Networking and Distributed Computing","Distributed","UDP","GPUDirect","DPDK"]},{"location":"applications/adv_networking_bench/#dpdk","title":"DPDK","text":"<pre><code>./holohub build adv_networking_bench --language=cpp\n</code></pre>","tags":["Networking and Distributed Computing","Distributed","UDP","GPUDirect","DPDK"]},{"location":"applications/adv_networking_bench/#gpunetio","title":"GPUNetIO","text":"<pre><code>./holohub build adv_networking_bench --language=cpp --build-args=\"--target gpunetio\" --configure-args=\"-D ANO_MGR:STRING=gpunetio\"\n</code></pre>","tags":["Networking and Distributed Computing","Distributed","UDP","GPUDirect","DPDK"]},{"location":"applications/adv_networking_bench/#rivermax","title":"Rivermax","text":"<pre><code>./holohub build adv_networking_bench --language=cpp --build-args=\"--target rivermax\" --configure-args=\"-D ANO_MGR:STRING=rivermax\"\n</code></pre>","tags":["Networking and Distributed Computing","Distributed","UDP","GPUDirect","DPDK"]},{"location":"applications/adv_networking_bench/#run-instructions","title":"Run Instructions","text":"","tags":["Networking and Distributed Computing","Distributed","UDP","GPUDirect","DPDK"]},{"location":"applications/adv_networking_bench/#dpdk_1","title":"DPDK","text":"<p>First, edit the <code>adv_networking_bench_default_tx_rx.yaml</code> file to set the <code>eth_dst_addr</code> and <code>address</code> fields (fields with the <code>&lt;&gt;</code> placeholders) based on your system interfaces.</p> <p>Then:</p> <pre><code>./holohub run adv_networking_bench --docker-opts \"-u root --privileged\" --language cpp\n</code></pre> <p>To run with a different configuration file than the default <code>adv_networking_bench_default_tx_rx</code>, you need to call the application explicitly at this point:</p> <pre><code># Start the container interactively\n./holohub run-container adv_networking_bench \\\n--language cpp \\\n  --docker-opts=\"-u root --privileged -w /workspace/holohub/\"\n\n# &lt;- Now in the container environment -&gt;\n\n# Build the app if you made any changes\n./holohub build adv_networking_bench --language=cpp\n\n# Run with another configuration\n./build/adv_networking_bench/applications/adv_networking_bench/cpp/adv_networking_bench adv_networking_bench_default_rx_multi_q.yaml\n</code></pre>","tags":["Networking and Distributed Computing","Distributed","UDP","GPUDirect","DPDK"]},{"location":"applications/adv_networking_bench/#gpunetio_1","title":"GPUNetIO","text":"<pre><code># Ensure the gdrdrv kernel module is loaded on the system\nlsmod | grep gdrdrv\n\n# Start the container interactively\n./holohub run-container adv_networking_bench \\\n  --language cpp \\\n  --build-args=\"--target gpunetio\" \\\n  --docker-opts=\"-u root --privileged --device /dev/gdrdrv -w /workspace/holohub\"\n\n# &lt;- Now in the container environment -&gt;\n\n# Build the app if you made any changes\n./holohub build adv_networking_bench \\\n  --language=cpp \\\n  --configure-args=\"-D ANO_MGR:STRING=gpunetio\"\n\n# Run with GPUNetIO configuration\n./build/adv_networking_bench/applications/adv_networking_bench/cpp/adv_networking_bench adv_networking_bench_gpunetio_tx_rx.yaml\n</code></pre>","tags":["Networking and Distributed Computing","Distributed","UDP","GPUDirect","DPDK"]},{"location":"applications/adv_networking_bench/#rivermax_1","title":"Rivermax","text":"<pre><code># Start the container with the right target and the license file\n./holohub run-container adv_networking_bench \\\n  --language=cpp \\\n  --build-args=\"--target rivermax\" \\\n  --docker-opts=\"\\\n    -u root --privileged \\\n    -v /opt/mellanox/rivermax/rivermax.lic:/opt/mellanox/rivermax/rivermax.lic \\\n    -w /workspace/holohub/ \\\n  \"\n\n# &lt;- Now in the container environment -&gt;\n\n# Build the app if you made any changes\n./holohub build adv_networking_bench \\\n  --language=cpp \\\n  --configure-args=\"-D ANO_MGR:STRING=rivermax\"\n\n# Run with Rivermax configuration\n./build/adv_networking_bench/applications/adv_networking_bench/cpp/adv_networking_bench adv_networking_bench_rmax_rx.yaml\n</code></pre>","tags":["Networking and Distributed Computing","Distributed","UDP","GPUDirect","DPDK"]},{"location":"applications/adv_networking_bench/#test-instructions","title":"Test Instructions","text":"<p>To run the tests, you need to build the application with testing enabled:</p> <pre><code>./holohub build adv_networking_bench --configure-args=\"-D BUILD_TESTING:BOOL=ON\"\n</code></pre> <p>Then, you can run the tests inside the container with ctest:</p> <pre><code># One-shot...\n./holohub run-container adv_networking_bench \\\n  --docker-opts \"-u 0 --privileged -w /workspace/holohub/build/adv_networking_bench/\" \\\n  -- ctest\n\n# ...or in interactive mode ...\n./holohub run-container adv_networking_bench \\\n  --docker-opts \"-u 0 --privileged -w /workspace/holohub/build/adv_networking_bench/\"\n\n# ... then run the tests\nctest\n</code></pre> <p>You can use ctest flags to filter the tests, show verbose output, etc. Example:</p> <pre><code># List tests\nctest --show-only\n\n# Run a specific test with verbose output\nctest --verbose -R dpdk-1500\n\n# Run a specific test and output failure\nctest --output-on-failure -R dpdk-1500\n\n# Re-run failed tests\nctest --rerun-failed --output-on-failure\n</code></pre>","tags":["Networking and Distributed Computing","Distributed","UDP","GPUDirect","DPDK"]},{"location":"applications/aja_video_capture/","title":"AJA Video Capture","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: October 9, 2025 Latest version: 3.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3, 2.0.0, 2.1.0, 2.2.0, 3.0.0 Contribution metric: Level 0 - Core Stable</p> <p>Minimal example to demonstrate the use of the aja source operator to capture device input and stream to holoviz operator.</p> <p>Visit the SDK User Guide to setup the AJA Card.</p>","tags":["Streaming","Video","format conversion","AJA"]},{"location":"applications/aja_video_capture/#quick-start","title":"Quick Start","text":"<pre><code>./holohub run --build-args=\"--target holohub-aja\" aja_video_capture --language &lt;cpp/python&gt;\n</code></pre>","tags":["Streaming","Video","format conversion","AJA"]},{"location":"applications/aja_video_capture/#settings","title":"Settings","text":"<p>To evaluate the AJA example using alternative resolutions, you may modify the aja_capture.yaml configuration file as needed. For instance, to test a resolution format of 1280 x 720 at 60 Hz, you can specify the following parameters in the aja section of the configuration :</p> <pre><code>```bash\n  aja:\n    width: 1280\n    height: 720\n    framerate: 60\n```\n</code></pre>","tags":["Streaming","Video","format conversion","AJA"]},{"location":"applications/aja_video_capture/#migration-notes","title":"Migration Notes","text":"<p>Holoscan SDK AJA support is migrated from the core Holoscan SDK library to the HoloHub community repository in Holoscan SDK v3.0.0. Projects depending on AJA support should accordingly update include and linking paths to reference HoloHub.</p> <p>C++/CMake projects should update <code>holoscan::ops::aja</code> to <code>holoscan::aja</code></p> <p>Python projects should update <code>import holoscan.operators.AJASourceOp</code> to <code>import holohub.aja_source.AJASourceOp</code></p>","tags":["Streaming","Video","format conversion","AJA"]},{"location":"applications/asr_to_llm/","title":"Real-time Riva ASR to local-LLM","text":"<p>     \u25b6 Run Locally  Authors: Nigel Nelson (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 4 - Experimental</p> <p>This application streams microphone input to NVIDIA Riva Automatic Speech Recognition (ASR), which once the user specifies they are done speaking, passes the transcribed text to an LLM running locally that then summarizes this text.</p> <p>While this workflow in principle could be used for a number of domains, the app is currently configured to be healthcare specific. The current LLM prompt is created for radiology interpretation, but this can be easily changed in the YAML file to tailor the LLM's output to a wide array of potential use cases.</p>","tags":["Healthcare AI","Audio","ASR","Quantized Model Inference","LLM","Medical Imaging"]},{"location":"applications/asr_to_llm/#example-output","title":"Example output","text":"<p>Example output can be found at example_output.md  Description of output fields: Final Transcription: Riva's transcription of the provided mic input LLM Summary:* The LLM's output summarization</p>","tags":["Healthcare AI","Audio","ASR","Quantized Model Inference","LLM","Medical Imaging"]},{"location":"applications/asr_to_llm/#yaml-configuration","title":"YAML Configuration","text":"<p>The directions for the LLM are determined by the <code>stt_to_nlp.yaml</code> file. As you see from our example, the directions for the LLM are made via natural language, and can result in very different applications.</p> <p>With the current YAML configuration, the resulting prompt to the LLM is:</p> <pre><code>&lt;|system|&gt;\nYou are a veteran radiologist, who can answer any medical related question.\n\n&lt;|user|&gt;\nTranscript from Radiologist:\n{**transcribed text inserted here**}\nRequest(s):\nMake a summary of the transcript (and correct any transcription errors in CAPS).\nCreate a Patient Summary with no medical jargon.\nCreate a full radiological report write-up.\nGive likely ICD-10 Codes.\nSuggest follow-up steps.\n\n&lt;|assistant|&gt;\n</code></pre>","tags":["Healthcare AI","Audio","ASR","Quantized Model Inference","LLM","Medical Imaging"]},{"location":"applications/asr_to_llm/#setup-instructions","title":"Setup Instructions","text":"","tags":["Healthcare AI","Audio","ASR","Quantized Model Inference","LLM","Medical Imaging"]},{"location":"applications/asr_to_llm/#install-riva","title":"Install Riva:","text":"<p>First, you must follow the Riva local deployment quickstart guide. For x86 and ARM64 devices with dGPU follow the \"Data Center\" instructions, for ARM64 devices with iGPU follow the \"Embedded\" instructions.</p> <ul> <li>Note: to minimize the Riva install size you can change the <code>config.sh</code> file in the <code>riva_quickstart_vX.XX.X</code> directory such that it specifies to only install the ASR models (Riva has more features but only ASR is needed for this app). To do this, find the <code>sevice_enabled_*</code> variables and set them as shown below: <pre><code>service_enabled_asr=true\nservice_enabled_nlp=false\nservice_enabled_tts=false\nservice_enabled_nmt=false\n</code></pre></li> </ul> <p>\u26a0\ufe0f Note: If you are using ARM64 w/ iGPU or an x86 platform the quick-start scripts should work as intended. However, if you are using ARM64 w/ dGPU, you will need to make the following modifications to the Riva Quick-start scripts:</p> <p>In <code>riva_init.sh</code> make the following changes to ensure the ARM64 version of NGC-CLI is downloaded and your dGPU is used to run the container: <pre><code># download required models\n-if [[ $riva_target_gpu_family == \"tegra\" ]]; then\n-    docker run -it -d --rm -v $riva_model_loc:/data \\\n+if [[ $riva_target_gpu_family == \"non-tegra\" ]]; then\n+            docker run -it -d --rm --gpus '\"'$gpus_to_use'\"' -v $riva_model_loc:/data \\\n                -e \"NGC_CLI_API_KEY=$NGC_CLI_API_KEY\" \\\n</code></pre> Then in <code>riva_start.sh</code> make the changes below to ensure your Riva server has access to your sound devices: <pre><code>docker rm $riva_daemon_speech &amp;&gt; /dev/null\n-if [[ $riva_target_gpu_family == \"tegra\" ]]; then\n+if [[ $riva_target_gpu_family == \"non-tegra\" ]]; then\n    docker_run_args=\"-p 8000:8000 -p 8001:8001 -p 8002:8002 -p 8888:8888 --device /dev/bus/usb --device /dev/snd\"\n</code></pre></p>","tags":["Healthcare AI","Audio","ASR","Quantized Model Inference","LLM","Medical Imaging"]},{"location":"applications/asr_to_llm/#setup-instructions_1","title":"Setup Instructions:","text":"<p>Download the quantized Mistral 7B LLM from HugginFace.co: <pre><code>wget -nc -P &lt;your_model_dir&gt; https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GGUF/resolve/main/mistral-7b-openorca.Q8_0.gguf\n</code></pre></p>","tags":["Healthcare AI","Audio","ASR","Quantized Model Inference","LLM","Medical Imaging"]},{"location":"applications/asr_to_llm/#run-instructions","title":"Run instructions","text":"<p>Build and launch the <code>holohub:asr_to_llm</code> container: <pre><code>./holohub run-container asr_to_llm --add-volume &lt;your_model_dir&gt;\n</code></pre> Run the application and use the <code>--list-devices</code> arg to determine which microphone to use: <pre><code>python &lt;streaming_asr_to_llm_dir&gt;/asr_to_llm.py --list-devices\n</code></pre> Then run the application with the <code>--input-device</code> arg to specify the correct microphone: <pre><code>python &lt;streaming_asr_to_llm_dir&gt;/asr_to_llm.py --input-device &lt;device-index&gt;\n</code></pre></p> <p>Once <code>asr_to_llm.py</code> is running, you will see output from ALSA for loading the selected audio device and also from llama_cpp for loading the LLM onto GPU memory. Once this is complete it will immediately begin printing out the transcribed text. To signal that the audio you wish to transcribe is complete, enter <code>x</code> on the keyboard. This will terminate the ASR and microphone instance, and feed the complete transcribed text into the LLM for summarization.</p>","tags":["Healthcare AI","Audio","ASR","Quantized Model Inference","LLM","Medical Imaging"]},{"location":"applications/asr_to_llm/#stopping-instructions","title":"Stopping Instructions","text":"<p>Note: The <code>python asr_to_llm.py</code> command will complete on its own once the LLM is finished summarizing the transcription * Stopping Riva services: <pre><code>bash &lt;Riva_install_dir&gt;riva_stop.sh\n</code></pre></p>","tags":["Healthcare AI","Audio","ASR","Quantized Model Inference","LLM","Medical Imaging"]},{"location":"applications/asr_to_llm/#asr_to_llm-application-arguments","title":"ASR_To_LLM Application arguments","text":"<p>The <code>asr_to_llm.py</code> can receive several cli arguments:</p> <p><code>--input-device</code>: The index of the input audio device to use. <code>--list-devices</code>: List input audio device indices. <code>--sample-rate-hz</code>: The number of frames per second in audio streamed from the selected microphone. <code>--file-streaming-chunk</code>: A maximum number of frames in a audio chunk sent to server.</p>","tags":["Healthcare AI","Audio","ASR","Quantized Model Inference","LLM","Medical Imaging"]},{"location":"applications/asr_to_llm/#implementation-details","title":"Implementation Details","text":"<p>This application adapted the speech_to_text_llm Holohub application to transcribe audio in real-time using Riva ASR, as well as ensure that the complete app runs 100% locally.</p> <p>The LLM currently used in this application is Mistral-7B-OpenOrca-GGUF, which is a quantized Mistal 7B model that is finetuned on the OpenOrca dataset. However, any model in the GGUF file format will work as long as it can fit within your device's VRAM constraints.</p> <p>The inference engine used to run the LLM is llama-cpp-python, which is a Python binding for llama.cpp. The reason for this is that the underlying llama.cpp library is hardware agnostic, dependency free, and it runs quantized LLMs with very high throughput.</p> <p>The RivaStreamingOp is a Holoscan SDK adaptation of the transcribe_mic.py script that is part of the Riva python-clients repository.</p>","tags":["Healthcare AI","Audio","ASR","Quantized Model Inference","LLM","Medical Imaging"]},{"location":"applications/asr_to_llm/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode asr_to_llm\n</code></pre>","tags":["Healthcare AI","Audio","ASR","Quantized Model Inference","LLM","Medical Imaging"]},{"location":"applications/asr_to_llm/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"<p>There are two launch profiles configured for this application:</p> <ol> <li>(debugpy) asr_to_llm/python: Launch asr_to_llm using a launch profile that enables debugging of Python code.</li> <li>(pythoncpp) asr_to_llm/python: Launch asr_to_llm using a launch profile that enables debugging of Python and C++ code.</li> </ol>","tags":["Healthcare AI","Audio","ASR","Quantized Model Inference","LLM","Medical Imaging"]},{"location":"applications/async_buffer_deadline/","title":"An Example of Async Lock-free Buffer with SCHED_DEADLINE","text":"<p>     \u25b6 Run Locally  Authors: HoloHub Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: October 9, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 3.5.0 Tested Holoscan SDK versions: 3.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>A simple application to measure the impact of async lock-free buffer communication between operators with earliest deadline first (<code>SCHED_DEADLINE</code>) scheduling policy of Linux.</p>","tags":["Scheduler","async_buffer","deadline_scheduling","event_based_scheduler"]},{"location":"applications/async_buffer_deadline/#overview","title":"Overview","text":"<p>This application demonstrates how different kinds of buffer connectors can impact the performance in terms of message latency. It uses Linux's <code>SCHED_DEADLINE</code> scheduler to ensure predictable timing for operators.</p> <p>The application consists of: - Two transmitter operators (PingTxOp) that generate ping messages at different rates - One receiver operator (PingRxOp) that processes messages from both transmitters - Optional async buffer connectors between transmitters and receiver - Earliest deadline first scheduling using Linux's <code>SCHED_DEADLINE</code> policy</p> <p>The application measures and logs: - Message latency from transmission to reception - Observed periods between messages - Performance impact of async buffer usage</p>","tags":["Scheduler","async_buffer","deadline_scheduling","event_based_scheduler"]},{"location":"applications/async_buffer_deadline/#requirements","title":"Requirements","text":"<p>This application requires: 1. Linux with <code>SCHED_DEADLINE</code> support 2. Root privileges in the container because of <code>SCHED_DEADLINE</code> 3. Holoscan SDK 3.5.0 or later</p>","tags":["Scheduler","async_buffer","deadline_scheduling","event_based_scheduler"]},{"location":"applications/async_buffer_deadline/#build-and-run-instructions","title":"Build and Run Instructions","text":"<p>Note: Please make sure the following command is run before running the application:</p> <pre><code>sudo sysctl -w kernel.sched_rt_runtime_us=-1\n</code></pre> <p>To build and run the application:</p> <pre><code># Basic run with default settings (100 messages, no async buffer)\n./holohub run async_buffer_deadline --as-root --docker-opts='--ulimit rtprio=99 --cap-add=CAP_SYS_NICE'\n\n# Run with async buffer enabled\n./holohub run async_buffer_deadline -- --async-buffer --as-root --docker-opts='--ulimit rtprio=99 --cap-add=CAP_SYS_NICE'\n\n# Run with custom message count and periods\n./holohub run async_buffer_deadline -- --messages 50 --tx1-period 15 --tx2-period 25 --async-buffer --as-root --docker-opts='--ulimit rtprio=99 --cap-add=CAP_SYS_NICE'\n</code></pre> <p><code>--as-root --docker-opts='--ulimit rtprio=99 --cap-add=CAP_SYS_NICE'</code> is required to run <code>SCHED_DEADLINE</code> application in a container.</p>","tags":["Scheduler","async_buffer","deadline_scheduling","event_based_scheduler"]},{"location":"applications/async_buffer_deadline/#command-line-options","title":"Command Line Options","text":"<ul> <li><code>-h, --help</code>: Display help information</li> <li><code>-m &lt;COUNT&gt;, --messages &lt;COUNT&gt;</code>: Number of messages to send (default: 100)</li> <li><code>-a, --async-buffer</code>: Enable async buffer connector</li> <li><code>-x &lt;MS&gt;, --tx1-period &lt;MS&gt;</code>: Set TX1 period in milliseconds (default: 20, min: 10)</li> <li><code>-y &lt;MS&gt;, --tx2-period &lt;MS&gt;</code>: Set TX2 period in milliseconds (default: 20, min: 15)</li> </ul>","tags":["Scheduler","async_buffer","deadline_scheduling","event_based_scheduler"]},{"location":"applications/async_buffer_deadline/#output","title":"Output","text":"<p>The application generates several CSV files: - <code>tx1.csv</code>: Latency measurements for TX1 messages - <code>tx2.csv</code>: Latency measurements for TX2 messages - <code>rx_in1_periods.csv</code>: Observed message intervals for RX input 1 - <code>rx_in2_periods.csv</code>: Observed message intervals for RX input 2</p>","tags":["Scheduler","async_buffer","deadline_scheduling","event_based_scheduler"]},{"location":"applications/basic_networking_ping/","title":"Basic Networking Ping","text":"<p>     \u25b6 Run Locally  Authors: Cliff Burdick (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: February 4, 2026 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0, 3.9.0, 3.10.0, 3.11.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application takes the existing ping example that runs over Holoscan ports and instead uses the basic network operator to run over a UDP socket.</p> <p>The basic network operator allows users to send and receive UDP messages over a standard Linux socket. Separate transmit and receive operators are provided so they can run independently and better suit the needs of the application.</p>","tags":["Networking and Distributed Computing","Distributed","UDP","Communications"]},{"location":"applications/basic_networking_ping/#configuration","title":"Configuration","text":"<p>The application is configured using the file basic_networking_ping_rx.yaml or basic_networking_ping_tx.yaml, where RX will receive packets and TX will transmit. Depending on how the machine is configured, the IP and UDP port likely need to be configured. All other settings do not need to be changed.</p> <p>Please refer to the basic network operator documentation for more configuration information.</p>","tags":["Networking and Distributed Computing","Distributed","UDP","Communications"]},{"location":"applications/basic_networking_ping/#quick-start","title":"Quick Start","text":"<p>Use the following to build and run the application:</p> <pre><code># Start the receiver\n./holohub run basic_networking_ping rx\n# Start the transmitter\n./holohub run basic_networking_ping tx\n</code></pre> <p>For using different language implementations, use the <code>--language</code> argument, for instance:</p> <pre><code>./holohub run basic_networking_ping rx --language cpp\n./holohub run basic_networking_ping tx --language python\n</code></pre> <p>For using different configuration files, use the <code>--run-args</code> argument.</p> <pre><code>./holohub run basic_networking_ping --run-args basic_networking_ping_rx.yaml\n</code></pre>","tags":["Networking and Distributed Computing","Distributed","UDP","Communications"]},{"location":"applications/basic_networking_ping/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode\n</code></pre>","tags":["Networking and Distributed Computing","Distributed","UDP","Communications"]},{"location":"applications/basic_networking_ping/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Networking and Distributed Computing","Distributed","UDP","Communications"]},{"location":"applications/basic_networking_ping/#c","title":"C++","text":"<p>There are three launch profiles configured for this application:</p> <ol> <li>(gdb) basic_networking_ping/cpp RX: Launch Basic Networking Ping with the RX configurations.</li> <li>(gdb) basic_networking_ping/cpp TX: Launch Basic Networking Ping with the TX configurations.</li> <li>(compound) basic_networking_ping/cpp TX &amp; RX: Launch both 1 and 2 in parallel.    This launch profile launches the receiver followed by the transmitter.</li> </ol>","tags":["Networking and Distributed Computing","Distributed","UDP","Communications"]},{"location":"applications/basic_networking_ping/#python","title":"Python","text":"<p>There are several launch profiles configured for this application:</p> <ol> <li>(debugpy) basic_networking_ping/python RX: Launch Basic Networking Ping with the RX configurations.    This launch profile enables debugging of Python code.</li> <li>(debugpy) basic_networking_ping/python TX: Launch Basic Networking Ping with the TX configurations.    This launch profile enables debugging of Python code.</li> <li>(pythoncpp) basic_networking_ping/python TX: Launch Basic Networking Ping with the RX configurations.    This launch profile enables debugging of Python and C++ code.</li> <li>(pythoncpp) basic_networking_ping/python TX: Launch Basic Networking Ping with the TX configurations.    This launch profile enables debugging of Python and C++ code.</li> <li>(compound) basic_networking_ping/python TX &amp; RX: Launch both 1 and 2 in parallel.    This launch profile launches the receiver followed by the transmitter.</li> </ol>","tags":["Networking and Distributed Computing","Distributed","UDP","Communications"]},{"location":"applications/bci_visualization/","title":"Kernel Flow BCI Real-Time Reconstruction and Visualization","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA), Kernel Team (Kernel), Mimi Liao (NVIDIA), Julien Dubois (Kernel), Victor Szczepanski (Kernel), Gabe Lerner (Kernel) Supported platforms: x86_64, aarch64 Language: Python Last modified: January 21, 2026 Latest version: 1.0.0 Minimum Holoscan SDK version: 3.7.0 Tested Holoscan SDK versions: 3.7.0, 3.9.0 Contribution metric: Level 2 - Trusted</p> <p> Example 3D visualization </p>","tags":["Visualization","BCI","Hemodynamics","Volume Rendering","Neuroscience","Optical Sensing"]},{"location":"applications/bci_visualization/#overview","title":"Overview","text":"<p>This Holohub application demonstrates how to perform real-time source reconstruction and visualization of streaming functional brain data from the Kernel Flow 2 system. The application was developed and tested on an NVIDIA Jetson Thor paired with a Kernel Flow 2 headset. To lower the barrier to entry, we also provide recorded datasets and a data replayer, enabling developers to build and experiment with visualization and classification pipelines within the Holoscan framework without requiring access to the hardware.</p> <p>This example processes streaming moments from the distribution of time-of-flight histograms. These moments can originate either from the Kernel Flow SDK when connected to the Kernel hardware, or from the included shared near-infrared spectroscopy format (SNIRF) replayer. The moments are then combined with the sensors' spatial geometry and an average anatomical head model to produce source-reconstructed outputs similar to what was published in previous work. </p> <p>To visualize the reconstructed 3D volumes, this application utilizes both the VolumeRendererOp operator and HolovizOp for real-time 3D rendering and interactive visualization.</p> <p>For optimal efficiency and smooth user experience, we employ an event-based scheduler that decouples the reconstruction and visualization pipelines. This allows each stage to run on separate threads, resulting in higher rendering quality and more responsive interaction.</p>","tags":["Visualization","BCI","Hemodynamics","Volume Rendering","Neuroscience","Optical Sensing"]},{"location":"applications/bci_visualization/#background","title":"Background","text":"<p>Kernel Flow is a multimodal non-invasive brain measurement system. It combines the relatively high resolution of time-domain functional near-infrared spectroscopy (TD-fNIRS) with the fast temporal resolution of electroencephalography (EEG) into a compact and scalable form factor that enables a new class of non-invasive Brain-Computer Interface (BCI) applications. </p> <p>The differentiating technology underlying the performance of the Kernel Flow system is the time-resolved detectors and high-speed laser drivers. Short (~100ps) pulses of near-infrared laser light (690nm &amp; 905nm) are emitted into the user's head with a repetition rate of 20 MHz. The photons in these laser pulses scatter through the scalp, skull, and cerebrospinal fluid before reaching the brain and then scattering back out. When the photons emerge from the scalp, we use single-photon sensitive detectors to timestamp exactly how much time the photon took to traverse through the head. The amount of time photons take to reach the detector is proportional to the path length traveled by the photon and the average depth it was able to reach. </p> <p>This simulation shows the relationship between photon scattering paths (black lines) and the measured time of flight (blue sections).  </p> <p> The relationship between photon path lengths and measured time </p> <p>As you can see, later times correspond to photons that have travelled farther into the tissue. In a given second, we are timestamping over 10 billion individual photons, which generates an enormous amount of data. After compression, the data production rate of Kernel Flow is ~1GB/min.</p> <p>As the photons scatter through the tissue, many of the photons are absorbed by cells and molecules in the tissue. In particular, the wavelengths we use are particularly sensitive to hemoglobin and its two states: oxyhemoglobin and deoxyhemoglobin, which allow us to follow the locations in the brain that are demanding and consuming oxygen and is an indirect measure of neuronal activity. These same biophysical principles are behind the pulse oximeters that are found in smart watches and finger-clip sensors! For more detailed information about the biophysics, see this review article.</p> <p>With the Kernel Flow headset we have combined 120 laser sources and 240 of our custom sensors to collect over 3000 measurement paths that criss-cross the head with a frame rate of 4.75Hz. When visualized, these paths resemble this:</p> <p> The 3000+ measurements that are made with a Kernel Flow </p> <p>We call each of these measurement paths a \"channel\" and the measurement is made in \"sensor space\" (i.e. from the perspective of the detector). In order to have a more anatomical representation of the data, it is common to transform the sensor-space data into source-space (i.e. where the changes in hemoglobin concentrations likely occurred in the brain, based on what was observed at the sensor) by solving an inverse problem, commonly called source reconstruction. This inverse problem requires complex modeling that is computationally expensive but highly parallelizable. </p> <p>In this Holohub application, we demonstrate a real-time source reconstruction pipeline that runs on a Jetson Thor at the native framerate of the Kernel Flow system (4.75 Hz) and visualizes the 3D data using volume rendering techniques.</p>","tags":["Visualization","BCI","Hemodynamics","Volume Rendering","Neuroscience","Optical Sensing"]},{"location":"applications/bci_visualization/#requirements","title":"Requirements","text":"<p>This application was developed to run on an NVIDIA Jetson Thor Developer kit. Any Holoscan SDK supported platform should work. </p> <p>To run the application you need a streaming Kernel Flow data source. This can be either:</p> <pre><code>* Kernel Flow hardware and SDK\n* Recorded `.snirf` files running with our data replayer.\n</code></pre>","tags":["Visualization","BCI","Hemodynamics","Volume Rendering","Neuroscience","Optical Sensing"]},{"location":"applications/bci_visualization/#quick-start","title":"Quick Start","text":"","tags":["Visualization","BCI","Hemodynamics","Volume Rendering","Neuroscience","Optical Sensing"]},{"location":"applications/bci_visualization/#1-download-required-data","title":"1. Download Required Data","text":"<p>The required data can be found on Hugging Face. The dataset includes: - SNIRF data file (<code>data.snirf</code>): Recorded brain activity measurements. More recorded snirf file can be found on OpenNeuro. - Anatomy masks (<code>anatomy_labels_high_res.nii.gz</code>): Brain tissue segmentation (skin, skull, CSF, gray matter, white matter) - Reconstruction matrices: Pre-computed Jacobian and voxel information</p> <p>To prepare the required data for this application, download the dataset into <code>holohub/data/bci_visualization</code> as described in the expected data folder structure.</p> <p>From the <code>holohub</code> directory, use the following command: <pre><code>hf download KernelCo/holohub_bci_visualization --repo-type dataset --local-dir data/bci_visualization\n</code></pre></p>","tags":["Visualization","BCI","Hemodynamics","Volume Rendering","Neuroscience","Optical Sensing"]},{"location":"applications/bci_visualization/#expected-data-folder-structure","title":"Expected Data Folder Structure","text":"<p>Download the correct folder matching your device type along with the other files into <code>data/bci_visualization</code>. Your <code>data/bci_visualization</code> folder should have the following structure:</p> <pre><code>holohub/data/bci_visualization/\n\u251c\u2500\u2500 anatomy_labels_high_res.nii.gz      # Brain segmentation\n\u251c\u2500\u2500 data.snirf                          # SNIRF format brain activity data\n\u251c\u2500\u2500 extinction_coefficients_mua.csv     # Absorption coefficients for HbO/HbR\n\u251c\u2500\u2500 flow_channel_map.json               # Sensor-source channel mapping\n\u251c\u2500\u2500 flow_mega_jacobian.npy              # Pre-computed sensitivity matrix (channels \u2192 voxels)\n\u2514\u2500\u2500 voxel_info/                         # Voxel geometry and optical properties\n    \u251c\u2500\u2500 affine.npy                      # 4x4 affine transformation matrix\n    \u251c\u2500\u2500 idxs_significant_voxels.npy     # Indices of voxels with sufficient sensitivity\n    \u251c\u2500\u2500 ijk.npy                         # Voxel coordinates in volume space\n    \u251c\u2500\u2500 mua.npy                         # Absorption coefficient per voxel\n    \u251c\u2500\u2500 musp.npy                        # Reduced scattering coefficient per voxel\n    \u251c\u2500\u2500 resolution.npy                  # Voxel resolution (mm)\n    \u251c\u2500\u2500 wavelengths.npy                 # Measurement wavelengths (690nm, 905nm)\n    \u2514\u2500\u2500 xyz.npy                         # Voxel coordinates in anatomical space (mm)\n</code></pre>","tags":["Visualization","BCI","Hemodynamics","Volume Rendering","Neuroscience","Optical Sensing"]},{"location":"applications/bci_visualization/#2-run-the-application","title":"2. Run the Application","text":"<p>Stream moments from data.snirf <pre><code>./holohub run bci_visualization\n</code></pre></p> <p>Stream moments from Flow headset via Kernel SDK <pre><code>./holohub run bci_visualization --docker-opts=\"-e KERNEL_SDK=1 -v /etc/kernel.com/kortex.json:/etc/kernel.com/kortex.json\"\n</code></pre></p>","tags":["Visualization","BCI","Hemodynamics","Volume Rendering","Neuroscience","Optical Sensing"]},{"location":"applications/bci_visualization/#pipeline-overview","title":"Pipeline Overview","text":"<p>The application consists of two main pipelines running on separate threads:</p>","tags":["Visualization","BCI","Hemodynamics","Volume Rendering","Neuroscience","Optical Sensing"]},{"location":"applications/bci_visualization/#reconstruction-pipeline","title":"Reconstruction Pipeline","text":"<p>Transforms sensor-space measurements into 3D brain activity maps:</p> <pre><code>graph LR\n    A[SNIRF Stream] --&gt; B[Stream Operator]\n    B --&gt; C[Build RHS]\n    C --&gt; D[Normalize]\n    D --&gt; E[Regularized Solver]\n    E --&gt; F[Convert to Voxels]\n    F --&gt; G[Voxel to Volume]\n\n    style A fill:#e1f5ff\n    style G fill:#ffe1f5</code></pre> <p>Key Steps: 1. Stream Operator: Reads SNIRF data and emits time-of-flight moments 2. Build RHS: Constructs the right-hand side of the inverse problem from the streaming moments taking into account the channel mapping 3. Normalize: Normalizes the moments to hard-coded upper bounds 4. Regularized Solver: Solves the inverse problem 5. Convert to Voxels: Maps solution to 3D voxel coordinates with HbO/HbR conversion 6. Voxel to Volume: Resamples to match anatomy mask, applies adaptive normalization</p>","tags":["Visualization","BCI","Hemodynamics","Volume Rendering","Neuroscience","Optical Sensing"]},{"location":"applications/bci_visualization/#visualization-pipeline","title":"Visualization Pipeline","text":"<p>Renders 3D brain volumes with real-time interaction:</p> <pre><code>graph LR\n    G[Voxel to Volume] --&gt; H[Volume Renderer]\n    H --&gt; I[Color Buffer Passthrough]\n    I --&gt; J[HolovizOp]\n    J --&gt; H\n\n    style G fill:#ffe1f5\n    style J fill:#e1ffe1</code></pre> <p>Key Steps: 1. Volume Renderer: GPU-accelerated ray-casting with ClaraViz (tissue segmentation + activation overlay) 2. Color Buffer Passthrough: Queue management with POP policy to prevent frame stacking 3. HolovizOp: Interactive 3D display with camera controls (bidirectional camera pose feedback)</p>","tags":["Visualization","BCI","Hemodynamics","Volume Rendering","Neuroscience","Optical Sensing"]},{"location":"applications/bci_visualization/#volume-renderer-configuration","title":"Volume Renderer Configuration","text":"<p>The <code>config.json</code> file in the data folder configures the ClaraViz volume renderer. For detailed documentation, see the VolumeRenderer operator documentation and ClaraViz proto definitions.</p>","tags":["Visualization","BCI","Hemodynamics","Volume Rendering","Neuroscience","Optical Sensing"]},{"location":"applications/bci_visualization/#key-configuration-parameters","title":"Key Configuration Parameters","text":"","tags":["Visualization","BCI","Hemodynamics","Volume Rendering","Neuroscience","Optical Sensing"]},{"location":"applications/bci_visualization/#1-rendering-quality","title":"1. Rendering Quality","text":"<p><pre><code>{\n  \"timeSlot\": 100\n}\n</code></pre> - <code>timeSlot</code> (milliseconds): Rendering time budget per frame   - Higher values = better quality   - Lower values = faster rendering</p>","tags":["Visualization","BCI","Hemodynamics","Volume Rendering","Neuroscience","Optical Sensing"]},{"location":"applications/bci_visualization/#2-transfer-functions","title":"2. Transfer Functions","text":"<p>The transfer function maps voxel values to colors and opacity. This application uses three components.</p>","tags":["Visualization","BCI","Hemodynamics","Volume Rendering","Neuroscience","Optical Sensing"]},{"location":"applications/bci_visualization/#component-1-brain-tissue-base-graywhite-matter","title":"Component 1: Brain Tissue Base (Gray/White Matter)","text":"<p><pre><code>{\n  \"activeRegions\": [3, 4],\n  \"range\": { \"min\": 0, \"max\": 1 },\n  \"opacity\": 0.5,\n  \"opacityProfile\": \"SQUARE\",\n  \"diffuseStart\": { \"x\": 1, \"y\": 1, \"z\": 1 },\n  \"diffuseEnd\": { \"x\": 1, \"y\": 1, \"z\": 1 }\n}\n</code></pre> - <code>activeRegions</code>: Tissue types to render   - <code>0</code>: Skin, <code>1</code>: Skull, <code>2</code>: CSF, <code>3</code>: Gray matter, <code>4</code>: White matter, <code>5</code>: Air   - Here: <code>[3, 4]</code> = gray and white matter only - <code>range</code>: <code>[0, 1]</code> = full normalized value range - <code>opacity</code>: <code>0.5</code> = semi-transparent base layer - <code>opacityProfile</code>: <code>\"SQUARE\"</code> = constant opacity throughout range - <code>diffuseStart/End</code>: <code>[1, 1, 1]</code> = white base color</p>","tags":["Visualization","BCI","Hemodynamics","Volume Rendering","Neuroscience","Optical Sensing"]},{"location":"applications/bci_visualization/#component-2-negative-activation-deactivation-blue","title":"Component 2: Negative Activation / Deactivation (Blue)","text":"<p><pre><code>{\n  \"activeRegions\": [3, 4],\n  \"range\": { \"min\": 0, \"max\": 0.4 },\n  \"opacity\": 1.0,\n  \"opacityProfile\": \"SQUARE\",\n  \"diffuseStart\": { \"x\": 0.0, \"y\": 0.0, \"z\": 1.0 },\n  \"diffuseEnd\": { \"x\": 0.0, \"y\": 0.0, \"z\": 0.5 }\n}\n</code></pre> - <code>range</code>: <code>[0, 0.4]</code> = lower 40% of normalized range (deactivation) - <code>opacity</code>: <code>1.0</code> = fully opaque - <code>opacityProfile</code>: <code>\"SQUARE\"</code> = constant opacity - <code>diffuseStart/End</code>: <code>[0, 0, 1]</code> \u2192 <code>[0, 0, 0.5]</code> = bright blue to dark blue gradient</p>","tags":["Visualization","BCI","Hemodynamics","Volume Rendering","Neuroscience","Optical Sensing"]},{"location":"applications/bci_visualization/#component-3-positive-activation-red","title":"Component 3: Positive Activation (Red)","text":"<p><pre><code>{\n  \"activeRegions\": [3, 4],\n  \"range\": { \"min\": 0.6, \"max\": 1 },\n  \"opacity\": 1.0,\n  \"opacityProfile\": \"SQUARE\",\n  \"diffuseStart\": { \"x\": 0.5, \"y\": 0.0, \"z\": 0.0 },\n  \"diffuseEnd\": { \"x\": 1.0, \"y\": 0.0, \"z\": 0.0 }\n}\n</code></pre> - <code>range</code>: <code>[0.6, 1]</code> = upper 40% of normalized range (activation) - <code>opacity</code>: <code>1.0</code> = fully opaque - <code>opacityProfile</code>: <code>\"SQUARE\"</code> = constant opacity - <code>diffuseStart/End</code>: <code>[0.5, 0, 0]</code> \u2192 <code>[1, 0, 0]</code> = dark red to bright red gradient</p>","tags":["Visualization","BCI","Hemodynamics","Volume Rendering","Neuroscience","Optical Sensing"]},{"location":"applications/bci_visualization/#3-blending","title":"3. Blending","text":"<p><pre><code>{\n  \"blendingProfile\": \"BLENDED_OPACITY\"\n}\n</code></pre> - <code>blendingProfile</code>: How overlapping components combine</p>","tags":["Visualization","BCI","Hemodynamics","Volume Rendering","Neuroscience","Optical Sensing"]},{"location":"applications/bci_visualization/#visualization-strategy","title":"Visualization Strategy","text":"<p>The three-component approach creates a layered visualization:</p> <ol> <li>Base layer (white, 50% opacity): Shows overall brain structure (gray + white matter) throughout the full range [0, 1]</li> <li>Blue overlay (100% opacity): Highlights low values [0, 0.4] representing decreased hemoglobin.</li> <li>Red overlay (100% opacity): Highlights high values [0.6, 1] representing increased hemoglobin.</li> <li>Neutral range [0.4, 0.6]: Only shows the white base layer (no significant change)</li> </ol>","tags":["Visualization","BCI","Hemodynamics","Volume Rendering","Neuroscience","Optical Sensing"]},{"location":"applications/body_pose_estimation/","title":"Body Pose Estimation","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 18, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 2 - Trusted</p> <p>Body pose estimation is a computer vision task that involves recognizing specific points on the human body in images or videos. A model is used to infer the locations of keypoints from the source video which is then rendered by the visualizer. </p>","tags":["Computer Vision and Perception","Distributed","Tensor Optimization","Human Body Pose Estimation","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/body_pose_estimation/#model","title":"Model","text":"<p>This application uses YOLO11 pose model from Ultralytics for body pose estimation. The model is downloaded when building the application.</p>","tags":["Computer Vision and Perception","Distributed","Tensor Optimization","Human Body Pose Estimation","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/body_pose_estimation/#data","title":"Data","text":"<p>This application downloads a pre-recorded video from Pexels when the application is built for use with this application.  Please review the license terms from Pexels.</p> <p>NOTE: The user is responsible for checking if the dataset license is fit for the intended purpose.</p>","tags":["Computer Vision and Perception","Distributed","Tensor Optimization","Human Body Pose Estimation","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/body_pose_estimation/#input","title":"Input","text":"<p>This app supports three different input options.  If you have a v4l2 compatible device plugged into your machine such as a webcam, you can run this application with option 1.  Otherwise you can run this application using a pre-recorded video with option 2.  For more advanced use cases, option 3 shows how to publish/subscribe to a DDS video stream to for interprocess applications.</p> <ol> <li>v4l2 compatible input device (default, see V4L2 Support below)</li> <li>pre-recorded video (see Video Replayer Support below)</li> <li>DDS video stream (see DDS Support below)</li> </ol> <p>To see the list of v4l2 devices connected to your machine, install <code>v4l-utils</code> if it's not already installed:</p> <pre><code>sudo apt-get install v4l-utils\n</code></pre> <p>Then run:</p> <pre><code>v4l2-ctl --list-devices\n</code></pre>","tags":["Computer Vision and Perception","Distributed","Tensor Optimization","Human Body Pose Estimation","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/body_pose_estimation/#run-instructions","title":"Run Instructions","text":"","tags":["Computer Vision and Perception","Distributed","Tensor Optimization","Human Body Pose Estimation","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/body_pose_estimation/#v4l2-support","title":"V4L2 Support","text":"<p>This application supports v4l2 compatible devices as input.  To run this application with your v4l2 compatible device, please plug in your input device and run: <pre><code>./holohub run body_pose_estimation\n</code></pre></p> <p>By default, this application expects the input device to be mounted at <code>/dev/video0</code>.  If this is not the case, please update <code>applications/body_pose_estimation/body_pose_estimation.yaml</code> and set it to use the corresponding input device before running the application.  You can also override the default input device on the command line by running: <pre><code>./holohub run body_pose_estimation --run-args=\"--video_device /dev/video0\"\n</code></pre></p>","tags":["Computer Vision and Perception","Distributed","Tensor Optimization","Human Body Pose Estimation","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/body_pose_estimation/#video-replayer-support","title":"Video Replayer Support","text":"<p>If you don't have a v4l2 compatible device plugged in, you may also run this application on a pre-recorded video. To launch the application using the Video Stream Replayer as the input source, run:</p> <pre><code>./holohub run body_pose_estimation --run-args=\"--source replayer\"\n</code></pre> <p>or using the mode <code>replayer</code> defined in the <code>metadata.json</code> file:</p> <pre><code>./holohub run body_pose_estimation replayer\n</code></pre>","tags":["Computer Vision and Perception","Distributed","Tensor Optimization","Human Body Pose Estimation","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/body_pose_estimation/#dds-support","title":"DDS Support","text":"<p>This application supports using a DDS video stream as the input as well as publishing the output video stream back to DDS. To enable DDS, the application must first be built with the DDS operators enabled. Only the subscriber or publisher operators need to be enabled for the sake of input or output video streams, respectively, but to enable both use the following:</p> <pre><code>./holohub build --local body_pose_estimation --build-with \"dds_video_subscriber;dds_video_publisher\"\n</code></pre> <p>Note that building these operators requires RTI Connext be installed. See the DDS Operator Documentation for more information on how to build the operators. If using a development container, see the additional instructions below.</p> <p>To use a DDS video stream as the input to the application, use the <code>-s=dds</code> argument when running the application:</p> <pre><code>./holohub run --local body_pose_estimation --run-args=\"-s=dds\"\n</code></pre> <p>or using the mode <code>dds</code> defined in the <code>metadata.json</code> file:</p> <pre><code>./holohub run body_pose_estimation dds --local\n</code></pre> <p>To publish the output result to DDS, edit the <code>body_pose_estimation.yaml</code> configuration file so that the <code>dds_publisher</code> <code>enable</code> option is <code>true</code>:</p> <pre><code>dds_publisher:\n  enable: true\n</code></pre> <p>Note that the default DDS video stream IDs use by the application are <code>0</code> for the input and <code>1</code> for the output. These can be changed using the <code>stream_id</code> settings in the <code>dds_source</code> and <code>dds_publisher</code> sections of the configuration file, respectively.</p> <p>To produce the DDS input stream or to view the output stream generated by this application, the dds_video application can be used. For example, the following will use the <code>dds_video</code> application to capture video from the default V4L2 device and publish it to DDS so that it can be received as input by this application:</p> <pre><code>./holohub run --local dds_video --run-args=\"-p -i 0\"\n</code></pre> <p>And the following will use the <code>dds_video</code> application to receive and render the output published by this application:</p> <pre><code>./holohub run --local dds_video --run-args=\"-s -i 1\"\n</code></pre>","tags":["Computer Vision and Perception","Distributed","Tensor Optimization","Human Body Pose Estimation","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/body_pose_estimation/#using-a-development-container-with-dds-support","title":"Using a Development Container with DDS Support","text":"<p>Installing RTI Connext into the development container is not currently supported, so enabling DDS support with this application requires RTI Connext be installed onto the host and then mounted into the container at runtime. To mount RTI Connext into the container, ensure that the <code>NDDSHOME</code> and <code>CONNEXTDDS_ARCH</code> environment variables are set (which can be done using the RTI <code>setenv</code> script) then use the following:</p> <pre><code>./holohub run-container body_pose_estimation --docker-opts=\"-v $NDDSHOME:/opt/dds -e NDDSHOME=/opt/dds -e CONNEXTDDS_ARCH=$CONNEXTDDS_ARCH\"\n</code></pre>","tags":["Computer Vision and Perception","Distributed","Tensor Optimization","Human Body Pose Estimation","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/body_pose_estimation/#known-issues","title":"Known Issues","text":"<p>There is a known issue running this application on IGX w/ iGPU and on Jetson AGX (see #500). The workaround is to update the device to avoid picking up the libnvv4l2.so library.</p> <pre><code>cd /usr/lib/aarch64-linux-gnu/\nls -l libv4l2.so.0.0.999999\nsudo rm libv4l2.so.0.0.999999\nsudo ln -s libv4l2.so.0.0.0.0  libv4l2.so.0.0.999999\n</code></pre>","tags":["Computer Vision and Perception","Distributed","Tensor Optimization","Human Body Pose Estimation","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/colonoscopy_segmentation/","title":"Colonoscopy Polyp Segmentation","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: February 4, 2026 Latest version: 1.0 Minimum Holoscan SDK version: 3.0.0 Tested Holoscan SDK versions: 3.0.0, 3.9.0, 3.10.0, 3.11.0 Contribution metric: Level 1 - Highly Reliable</p> <p>Full workflow including a generic visualization of segmentation results from a polyp segmentation models.</p>","tags":["Healthcare AI","Visualization","AJA","Endoscopy","Segmentation","Holoviz","Video"]},{"location":"applications/colonoscopy_segmentation/#quick-start","title":"Quick Start","text":"<pre><code>./holohub run colonoscopy_segmentation\n</code></pre>","tags":["Healthcare AI","Visualization","AJA","Endoscopy","Segmentation","Holoviz","Video"]},{"location":"applications/colonoscopy_segmentation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8+</li> <li>The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the colonoscopy data (replayer). Follow the setup instructions from the user guide to use the AJA capture card.</li> </ul>","tags":["Healthcare AI","Visualization","AJA","Endoscopy","Segmentation","Holoviz","Video"]},{"location":"applications/colonoscopy_segmentation/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI Colonoscopy Segmentation of Polyps</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Healthcare AI","Visualization","AJA","Endoscopy","Segmentation","Holoviz","Video"]},{"location":"applications/colonoscopy_segmentation/#build-instructions","title":"Build Instructions","text":"<p>To build the application in a runtime container:</p> <pre><code>./holohub build colonoscopy_segmentation\n</code></pre> <p>Or, to build the application in the host environment, install the application dependencies and then run:</p> <pre><code>./holohub build --local colonoscopy_segmentation\n</code></pre>","tags":["Healthcare AI","Visualization","AJA","Endoscopy","Segmentation","Holoviz","Video"]},{"location":"applications/colonoscopy_segmentation/#run-instructions","title":"Run Instructions","text":"","tags":["Healthcare AI","Visualization","AJA","Endoscopy","Segmentation","Holoviz","Video"]},{"location":"applications/colonoscopy_segmentation/#application-presets","title":"Application Presets","text":"<p>To run the application with the pre-recorded colonoscopy sample video</p> <pre><code>./holohub run colonoscopy_segmentation replayer\n</code></pre> <p>To run the application with input from an AJA video capture card:</p> <pre><code>./holohub run colonoscopy_segmentation aja\n</code></pre>","tags":["Healthcare AI","Visualization","AJA","Endoscopy","Segmentation","Holoviz","Video"]},{"location":"applications/colonoscopy_segmentation/#application-configurations","title":"Application Configurations","text":"<p>To pass runtime arguments to the application:</p> <pre><code>./holohub run colonoscopy_segmentation --run-args=\"--contours --source=\\\"replayer\\\"\"\n</code></pre>","tags":["Healthcare AI","Visualization","AJA","Endoscopy","Segmentation","Holoviz","Video"]},{"location":"applications/colonoscopy_segmentation/#cli-parameters","title":"CLI Parameters","text":"<pre><code>usage: colonoscopy_segmentation.py [-h] [-s {replayer,aja}] [-c CONFIG] [-d DATA] [--contours | --no-contours]\n\nColonoscopy segmentation demo application.\n\noptions:\n  -h, --help            show this help message and exit\n  -s {replayer,aja}, --source {replayer,aja}\n                        If 'replayer', replay a prerecorded video. If 'aja' use an AJA capture card as the source (default: replayer).\n  -c CONFIG, --config CONFIG\n                        Set config path to override the default config file location\n  -d DATA, --data DATA  Set the data path\n  --contours, --no-contours\n                        Show segmentation contours instead of mask (default: False)\n</code></pre>","tags":["Healthcare AI","Visualization","AJA","Endoscopy","Segmentation","Holoviz","Video"]},{"location":"applications/colonoscopy_segmentation/#vs-code-dev-container","title":"VS Code Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode\n</code></pre>","tags":["Healthcare AI","Visualization","AJA","Endoscopy","Segmentation","Holoviz","Video"]},{"location":"applications/colonoscopy_segmentation/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"<p>There are two launch profiles configured for this application:</p> <ol> <li>(debugpy) colonoscopy_segmentation/python: Launch colonoscopy_segmentation using a launch profile that enables debugging of Python code.</li> <li>(pythoncpp) colonoscopy_segmentation/python: Launch colonoscopy_segmentation using a launch profile that enables debugging of Python and C++ code.</li> </ol> <p>Note: the launch profile starts the application with Video Replayer. To adjust the arguments of the application, open launch.json, find the launch profile named <code>(debugpy) colonoscopy_segmentation/python</code>, and adjust the <code>args</code> field as needed.</p>","tags":["Healthcare AI","Visualization","AJA","Endoscopy","Segmentation","Holoviz","Video"]},{"location":"applications/cuda_quantum/","title":"CUDA Quantum Variational Quantum Eigensolver (VQE)","text":"<p>     \u25b6 Run Locally  Authors: Sean Huver (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: October 21, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 4 - Experimental</p>","tags":["Quantum Computing"]},{"location":"applications/cuda_quantum/#variational-quantum-eigensolver-vqe","title":"Variational Quantum Eigensolver (VQE)","text":"<p>The Variational Quantum Eigensolver (VQE) is a quantum algorithm designed to approximate the ground state energy of quantum systems. This energy, represented by what is called the Hamiltonian of the system, is central to multiple disciplines, including drug discovery, material science, and condensed matter physics. The goal of VQE is to find the state that minimizes the expectation value of this Hamiltonian, which corresponds to the ground state energy.</p> <p>At its core, VQE is a lighthouse example of the synergy between classical and quantum computing, requiring them both to tackle problems traditionally deemed computationally intractable. Even in the current landscape where fault-tolerant quantum computing\u2014a stage where quantum computers are resistant to errors\u2014is not yet realized, VQE is seen as a practical tool. This is due to its design as a 'near-term' algorithm, built to operate on existing noisy quantum hardware. </p>","tags":["Quantum Computing"]},{"location":"applications/cuda_quantum/#key-components-of-vqe","title":"Key Components of VQE","text":"<ol> <li> <p>Hamiltonian: This represents the total energy of the quantum system, which is known ahead of time. In VQE, we aim to find the lowest eigenvalue (ground state energy) of this Hamiltonian.</p> </li> <li> <p>Ansatz (or trial wavefunction): The ansatz is the initial guess for the state of the quantum system, represented by a parameterized quantum circuit. It's crucial for this state to be a good representation, as the quality of the ansatz can heavily influence the final results. VQE iteratively refines the parameters of this ansatz to approximate the true ground state of the Hamiltonian.</p> </li> </ol>","tags":["Quantum Computing"]},{"location":"applications/cuda_quantum/#vqe-mechanism","title":"VQE Mechanism","text":"<p>The VQE operates by employing a hybrid quantum-classical approach:</p> <ol> <li>Quantum Circuit Parameterization: VQE begins with a parameterized quantum circuit, effectively serving as an initial guess or representation of the system's state.</li> <li>Evaluation and Refinement: The quantum system's energy is evaluated using the current quantum circuit parameters. Classical optimization algorithms then adjust these parameters in a quest to minimize the energy.</li> <li>Iterative Process: The combination of quantum evaluation and classical refinement is iterative. Over multiple cycles, the parameters are tuned to get increasingly closer to the true ground state energy.</li> </ol>","tags":["Quantum Computing"]},{"location":"applications/cuda_quantum/#integration-with-holoscan-and-cuda-quantum","title":"Integration with Holoscan and CUDA Quantum","text":"<ul> <li>NVIDIA Holoscan SDK: The Holoscan SDK is designed for efficient handling of high-throughput, low-latency GPU tasks. Within the context of VQE, the Holoscan SDK facilitates the rapid classical computations necessary for parameter adjustments and optimization. The <code>ClassicalComputeOp</code> in the provided code sample is an example of this SDK in action, preparing the quantum circuits efficiently.</li> <li>CUDA Quantum: CUDA Quantum is a framework that manages hybrid quantum-classical workflows. For VQE, CUDA Quantum processes quantum data and executes quantum operations. The <code>QuantumComputeOp</code> operator in the code uses the cuQuantum simulator backend, but the user may optionally switch out the simulator for a real quantum cloud backend provided by either IonQ or Quantinuum (see CUDA Quantum backend documentation).</li> </ul> <p>Holoscan ensures swift and efficient classical computations, while CUDA Quantum manages the quantum components with precision.</p>","tags":["Quantum Computing"]},{"location":"applications/cuda_quantum/#usage","title":"Usage","text":"<p>To run the application, you need to have CUDA Quantum, Qiskit, and Holoscan installed. You also need an IBM Quantum account to use their quantum backends.</p> <ol> <li> <p>Clone the repository and navigate to the <code>cuda_quantum</code> directory containing.</p> </li> <li> <p>Install the requirements <code>pip install -r requirements.txt</code></p> </li> <li> <p>Either use or replace the <code>'hamiltonian'</code> in <code>cuda_quantum.yaml</code> dependent on the physical system you wish to model.</p> </li> <li> <p>Run the application with the command <code>python cuda_quantum.py</code>.</p> </li> </ol>","tags":["Quantum Computing"]},{"location":"applications/cuda_quantum/#operators","title":"Operators","text":"<p>The application uses three types of operators:</p> <ul> <li> <p><code>ClassicalComputeOp</code>: This operator performs classical computations. It also creates a quantum kernel representing the initial ansatz, or guess of the state of the system, and a Hamiltonian.</p> </li> <li> <p><code>QuantumComputeOp</code>: This operator performs quantum computations. It uses the quantum kernel and Hamiltonian from <code>ClassicalComputeOp</code> to iterate towards the ground state energy and parameter using VQE.</p> </li> <li> <p><code>PrintOp</code>: This operator prints the result from <code>QuantumComputeOp</code>.</p> </li> </ul>","tags":["Quantum Computing"]},{"location":"applications/cuda_quantum/#operator-connections","title":"Operator Connections","text":"<p>The operators are connected as follows:</p> <pre><code>flowchart LR\n    ClassicalComputeOp --&gt; QuantumComputeOp\n    QuantumComputeOp --&gt; PrintOp</code></pre> <p><code>ClassicalComputeOp</code> sends the quantum kernel and Hamiltonian to <code>QuantumComputeOp</code>, which computes the energy and parameter and sends the result to <code>PrintOp</code>.</p>","tags":["Quantum Computing"]},{"location":"applications/cuda_quantum/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode cuda_quantum\n</code></pre>","tags":["Quantum Computing"]},{"location":"applications/cuda_quantum/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"<p>There are two launch profiles configured for this application:</p> <ol> <li>(debugpy) cuda_quantum/python: Launch cuda_quantum using a launch profile that enables debugging of Python code.</li> <li>(pythoncpp) cuda_quantum/python: Launch cuda_quantum using a launch profile that enables debugging of Python and C++ code.</li> </ol> <p>Note: to adjust the arguments of the application, open launch.json, find the launch profile named <code>(debugpy) cuda_quantum/python</code>, and adjust the <code>args</code> field as needed.</p>","tags":["Quantum Computing"]},{"location":"applications/cunumeric_integration/","title":"Power Spectral Density with cuNumeric","text":"<p>     \u25b6 Run Locally  Authors: Adam Thompson (NVIDIA) Supported platforms: x86_64 Language: Python Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 2 - Trusted</p> <p>cuNumeric is an drop-in replacement for NumPy that aims to provide a distributed and accelerated drop-in replacement for the NumPy API on top of the Legion runtime. It works best for programs that have very large arrays of data that can't fit in the the memory of a single GPU or node.</p> <p>In this example application, we are using the cuNumeric library within a Holoscan application graph to determine the Power Spectral Density (PSD) of an incoming signal waveform. Notably, this is simply achieved by taking the absolute value of the FFT of a data array.</p> <p>The main objectives of this demonstration are to: - Highlight developer productivity in building an end-to-end streaming application with Holoscan and cuNumeric - Demonstrate how to scale a given workload to multiple GPUs using cuNumeric</p>","tags":["Signal Processing","Distributed","PSD"]},{"location":"applications/cunumeric_integration/#running-the-application","title":"Running the Application","text":"<p>Prior to running the application, the user needs to install the necessary dependencies. This is most easily done in an Anaconda environment.</p> <pre><code>conda create --name holoscan-cunumeric-demo python=3.9\nconda activate holoscan-cunumeric-demo\nconda install -c nvidia -c conda-forge -c legate cunumeric cupy\npip install holoscan\n</code></pre> <p>The cuNumeric PSD processing pipeline example can then be run via <pre><code>legate --gpus 2 applications/cunumeric_integration/cunumeric_psd.py\n</code></pre></p> <p>While running the application, you can confirm multi GPU utilization via watching <code>nvidia-smi</code> or using another GPU utilization tool</p> <p>To run the same application without cuNumeric, simply change <code>import cunumeric as np</code> to <code>import cupy as np</code> in the code and run <pre><code>python applications/cunumeric_integration/cunumeric_psd.py\n</code></pre></p>","tags":["Signal Processing","Distributed","PSD"]},{"location":"applications/cvcuda_basic/","title":"Simple CV-CUDA","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates seamless interoperability between Holoscan tensors and CV-CUDA tensors. The image processing pipeline is just a simple flip of the video orientation.</p> <p>Note that the C++ version of this application currently requires extra code to handle conversion back and forth between CV-CUDA and Holoscan tensor types. On the Python side, the conversion is trivial due to the support for the DLPack Python specification in both CV-CUDA and Holoscan. We provide two operators to handle the interoperability between CVCUDA and Holoscan tensors.</p>","tags":["Computer Vision and Perception","Video","Tensor Optimization","CV CUDA","Endoscopy","Visualization"]},{"location":"applications/cvcuda_basic/#quick-start-with-holohub-cli-container","title":"Quick Start with HoloHub CLI Container","text":"<p>Run the following command to build and run the CV-CUDA sample application in a Docker container:</p> <pre><code>./holohub run cvcuda_basic\n</code></pre>","tags":["Computer Vision and Perception","Video","Tensor Optimization","CV CUDA","Endoscopy","Visualization"]},{"location":"applications/cvcuda_basic/#advanced-build-steps","title":"Advanced Build Steps","text":"","tags":["Computer Vision and Perception","Video","Tensor Optimization","CV CUDA","Endoscopy","Visualization"]},{"location":"applications/cvcuda_basic/#using-the-docker-file","title":"Using the docker file","text":"<p>This application requires a compiled version of CV-CUDA. For simplicity a DockerFile is available. To generate the container run:</p> <pre><code>./holohub build-container cvcuda_basic\n</code></pre> <p>The C++ version of the application can then be built by launching this container and using the provided <code>holohub</code> CLI.</p> <pre><code>./holohub run-container cvcuda_basic\n./holohub build cvcuda_basic\n</code></pre>","tags":["Computer Vision and Perception","Video","Tensor Optimization","CV CUDA","Endoscopy","Visualization"]},{"location":"applications/cvcuda_basic/#running-the-application","title":"Running the Application","text":"<p>This application uses the endoscopy dataset as an example. The build command above will automatically download it. This application is then run inside the container.</p> <pre><code>./holohub run-container cvcuda_basic\n</code></pre> <p>The Python version of the simple CV-CUDA pipeline example can be run via <pre><code>python applications/cvcuda_basic/python/cvcuda_basic.py --data=/workspace/holohub/data/endoscopy\n</code></pre></p> <p>or using the HoloHub CLI entrypoint:</p> <pre><code>./holohub run cvcuda_basic --language=python --local\n</code></pre> <p>The C++ version of the simple CV-CUDA pipeline example can then be run via <pre><code>./build/applications/cvcuda_basic/cpp/cvcuda_basic --data=/workspace/holohub/data/endoscopy\n</code></pre></p> <p>or using the run script</p> <pre><code>./holohub run cvcuda_basic --language=cpp --local\n</code></pre>","tags":["Computer Vision and Perception","Video","Tensor Optimization","CV CUDA","Endoscopy","Visualization"]},{"location":"applications/cvcuda_basic/#vs-code-dev-container","title":"VS Code Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode cvcuda_basic\n</code></pre>","tags":["Computer Vision and Perception","Video","Tensor Optimization","CV CUDA","Endoscopy","Visualization"]},{"location":"applications/cvcuda_basic/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Computer Vision and Perception","Video","Tensor Optimization","CV CUDA","Endoscopy","Visualization"]},{"location":"applications/cvcuda_basic/#c","title":"C++","text":"<p>Use the <code>**(gdb) cvcuda_basic/cpp**</code> launch profile configured for this application to debug the application.</p>","tags":["Computer Vision and Perception","Video","Tensor Optimization","CV CUDA","Endoscopy","Visualization"]},{"location":"applications/cvcuda_basic/#python","title":"Python","text":"<p>There are two launch profiles configured for this Python application:</p> <ol> <li>(debugpy) cvcuda_basic/python: Launch cvcuda_basic using a launch profile that enables debugging of Python code.</li> <li>(pythoncpp) cvcuda_basic/python: Launch cvcuda_basic using a launch profile that enables debugging of Python and C++ code.</li> </ol>","tags":["Computer Vision and Perception","Video","Tensor Optimization","CV CUDA","Endoscopy","Visualization"]},{"location":"applications/dds/dds_video/","title":"DDS Video: Real-time Video Streaming with RTI Connext","text":"<p>     \u25b6 Run Locally  Authors: Ian Stewart (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>The DDS Video application demonstrates how video frames can be written to or read from a DDS databus in order to provide flexible integration between Holoscan applications and other applications (using Holoscan or not) via DDS.</p> <p>The application can be run as either a publisher or as a subscriber. In either case, it will use the VideoFrame data topic registered by the <code>DDSVideoPublisherOp</code> or <code>DDSVideoSubscriberOp</code> operators in order to write or read the video frame data to/from the DDS databus, respectively.</p> <p>When run as a publisher, the source for the input video frames will come from an attached V4L2-compatible camera via the <code>V4L2VideoCaptureOp</code> operator.</p> <p>When run as a subscriber, the application will use Holoviz to render the received video frames to the display. In addition to the video stream, the subscriber application will also subscribe to the <code>Square</code>, <code>Circle</code>, and <code>Triangle</code> topics as used by the RTI Shapes Demo. Any shapes received by this subscriber will also be overlaid on top of the Holoviz output.</p> <p></p>","tags":["Networking and Distributed Computing","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/dds/dds_video/#prerequisites","title":"Prerequisites","text":"<ul> <li>This application requires an installation of RTI Connext Express to provide access to the DDS domain. To obtain a license/activation key, please click here. Please see the usage rules for Connext Express.</li> <li>V4L2 capable device</li> </ul> <p>[!NOTE] Instructions below are based on the `.run' installer from RTI Connext. Refer to the Linux installation for details.</p>","tags":["Networking and Distributed Computing","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/dds/dds_video/#quick-start","title":"Quick Start","text":"<pre><code># Start the publisher\n./holohub run dds_video --docker-opts=\"-v $HOME/rti_connext_dds-7.3.0:/opt/rti.com/rti_connext_dds-7.3.0/\" --run-args=\"-p\"\n\n# Start the subscriber\n./holohub run dds_video --docker-opts=\"-v $HOME/rti_connext_dds-7.3.0:/opt/rti.com/rti_connext_dds-7.3.0/\" --run-args=\"-s\"\n</code></pre>","tags":["Networking and Distributed Computing","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/dds/dds_video/#building-the-application","title":"Building the Application","text":"<p>To build on an IGX devkit (using the <code>armv8</code> architecture), follow the instructions to build Connext DDS applications for embedded Arm targets up to, and including, step 5 (Installing Java and setting JREHOME).</p> <p>To build the application, the <code>RTI_CONNEXT_DDS_DIR</code> CMake variable must point to the installation path for RTI Connext. This can be done automatically by setting the <code>NDDSHOME</code> environment variable to the RTI Connext installation directory (such as when using the RTI <code>setenv</code> scripts), or manually at build time, e.g.:</p> <pre><code>./holohub build --local dds_video --configure-args=\"-DRTI_CONNEXT_DDS_DIR=~/rti/rti_connext_dds-7.3.0\"\n</code></pre>","tags":["Networking and Distributed Computing","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/dds/dds_video/#building-with-a-container","title":"Building with a Container","text":"<p>Due to the license requirements of RTI Connext it is not currently supported to install RTI Connext into a development container. Instead, Connext should be installed onto the host as above and then the development container can be launched with the RTI Connext folder mounted at runtime. To do so, ensure that the <code>NDDSHOME</code> and <code>CONNEXTDDS_ARCH</code> environment variables are set (which can be done using the RTI <code>setenv</code> script) and use the following:</p> <pre><code># 1. Build and launch the container\n./holohub run-container dds_video --docker-opts=\"-v $HOME/rti_connext_dds-7.3.0:/opt/rti.com/rti_connext_dds-7.3.0/\"\n# 3. Build the application\n./holohub build dds_video\n# Continue to the next section to run the application with the publisher. \n# Open a new terminal to repeat step #2 and launch a new container for the subscriber.\n</code></pre>","tags":["Networking and Distributed Computing","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/dds/dds_video/#running-the-application","title":"Running the Application","text":"<p>Both a publisher and subscriber process must be launched to see the result of writing to and reading the video stream from DDS, respectively.</p> <p>To run the publisher process, use the <code>-p</code> option:</p> <pre><code>$ ./holohub run --no-local-build dds_video --run-args=\"-p\"\n</code></pre> <p>To run the subscriber process, use the <code>-s</code> option:</p> <pre><code>$ ./holohub run --no-local-build dds_video --run-args=\"-s\"\n</code></pre> <p>If running the application generates an error about <code>RTI Connext DDS No Source for License information</code>, ensure that the RTI Connext license has either been installed system-wide or the <code>NDDSHOME</code> environment variable has been set to point to your user's RTI Connext installation path.</p> <p>Note that these processes can be run on the same or different systems, so long as they are both discoverable by the other via RTI Connext. If the processes are run on different systems then they will communicate using UDPv4, for which optimizations have been defined in the default <code>qos_profiles.xml</code> file. These optimizations include increasing the buffer size used by RTI Connext for network sockets, and so the systems running the application must also be configured to increase their maximum send and receive socket buffer sizes. This can be done by running the <code>set_socket_buffer_sizes.sh</code> script within this directory:</p> <pre><code>$ ./set_socket_buffer_sizes.sh\n</code></pre> <p>For more details, see the RTI Connext Guide to Improve DDS Network Performance on Linux Systems</p> <p>The QoS profiles used by the application can also be modified by editing the <code>qos_profiles.xml</code> file in the application directory. For more information about modifying the QoS profiles, see the RTI Connext Basic QoS tutorial or the RTI Connext QoS Reference Guide.</p>","tags":["Networking and Distributed Computing","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/dds/dds_video/#publishing-shapes-from-the-rti-shapes-demo","title":"Publishing Shapes from the RTI Shapes Demo","text":"<p>The RTI Shapes Demo can be used to publish shapes which are then read and overlaid onto the video stream by this application. However, the domain participant QoS used by this application is not compatible with the default DDS QoS settings, so the RTI Shapes Demo must be configured to use the QoS settings provided by this application.  To do this, follow these steps:</p> <ol> <li>Launch the RTI Shapes Demo</li> <li>Select <code>Controls</code>, then <code>Configuration</code> from the menu bar</li> <li>Click <code>Stop</code> to disable the default domain participant</li> <li>Click <code>Manage QoS</code></li> <li>Click <code>Add</code> then navigate to and select the <code>qos_profiles.xml</code> file in this    application's directory.</li> <li>Click <code>OK</code> to close the <code>Manage QoS</code> window.</li> <li>In the <code>Choose the profile</code> drop-down, select <code>HoloscanDDSTransport::SHMEM+LAN</code></li> <li>Click <code>Start</code> to join the domain.</li> </ol> <p>Once the Shapes Demo is running and has joined the domain of a running <code>dds_video</code> subscriber, shapes published by the application should be rendered on top of the subscriber's video stream.</p>","tags":["Networking and Distributed Computing","DDS","RTI Connext","Video","Visualization"]},{"location":"applications/deltacast_receiver/","title":"Deltacast Videomaster Receiver","text":"<p>     \u25b6 Run Locally  Authors: Laurent Radoux (DELTACAST), Pierre PERICK (DELTACAST) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: January 12, 2026 Latest version: 1.0 Minimum Holoscan SDK version: 3.0.0 Tested Holoscan SDK versions: 3.6.0 Contribution metric: Level 2 - Trusted</p> <p>This application demonstrates the use of videomaster_source to receive and display video streams from a Deltacast capture card using Holoviz for visualization.</p>","tags":["Video","Deltacast","Receiver","Display","Holoviz","RDMA","GPUDirect"]},{"location":"applications/deltacast_receiver/#requirements","title":"Requirements","text":"<p>This application uses the DELTACAST.TV capture card for input stream. Contact DELTACAST.TV for more details on how to access the SDK and setup your environment.</p>","tags":["Video","Deltacast","Receiver","Display","Holoviz","RDMA","GPUDirect"]},{"location":"applications/deltacast_receiver/#build-instructions","title":"Build Instructions","text":"<p>See instructions from the top level README on how to build this application. Note that this application requires to provide the VideoMaster_SDK_DIR if it is not located in a default location on the system. This can be done with the following command, from the top level Holohub source directory:</p> <pre><code>./holohub build --local deltacast_receiver --configure-args=\"-DVideoMaster_SDK_DIR=&lt;Path to VideoMasterSDK&gt;\"\n</code></pre>","tags":["Video","Deltacast","Receiver","Display","Holoviz","RDMA","GPUDirect"]},{"location":"applications/deltacast_receiver/#run-instructions","title":"Run Instructions","text":"","tags":["Video","Deltacast","Receiver","Display","Holoviz","RDMA","GPUDirect"]},{"location":"applications/deltacast_receiver/#c-application","title":"C++ Application","text":"<p>From the build directory, run the command:</p> <pre><code>./applications/deltacast_receiver/cpp/deltacast_receiver\n</code></pre>","tags":["Video","Deltacast","Receiver","Display","Holoviz","RDMA","GPUDirect"]},{"location":"applications/deltacast_receiver/#python-application","title":"Python Application","text":"<p>From the build directory, run the command:</p> <pre><code>python3 applications/deltacast_receiver/python/deltacast_receiver.py\n</code></pre>","tags":["Video","Deltacast","Receiver","Display","Holoviz","RDMA","GPUDirect"]},{"location":"applications/deltacast_transmitter/","title":"Deltacast Videomaster Transmitter","text":"<p>     \u25b6 Run Locally  Authors: Laurent Radoux (DELTACAST), Pierre PERICK (DELTACAST) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: January 12, 2026 Latest version: 1.1 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0, 2.9.0, 3.0.0, 3.6.0 Contribution metric: Level 2 - Trusted</p> <p>This application demonstrates the use of videomaster_transmitter to transmit a video stream through a dedicated IO device.</p>","tags":["Streaming","Healthcare AI","Video","Deltacast","Endoscopy","RDMA","GPUDirect"]},{"location":"applications/deltacast_transmitter/#requirements","title":"Requirements","text":"<p>This application uses the DELTACAST.TV capture card for input stream. Contact DELTACAST.TV for more details on how access the SDK and to setup your environment.</p>","tags":["Streaming","Healthcare AI","Video","Deltacast","Endoscopy","RDMA","GPUDirect"]},{"location":"applications/deltacast_transmitter/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>See instructions from the top level README on how to build this application. Note that this application requires to provide the VideoMaster_SDK_DIR if it is not located in a default location on the system. This can be done with the following command, from the top level Holohub source directory:</p> <pre><code>./holohub build --local deltacast_transmitter --configure-args=\"-DVideoMaster_SDK_DIR=&lt;Path to VideoMasterSDK&gt;\"\n</code></pre>","tags":["Streaming","Healthcare AI","Video","Deltacast","Endoscopy","RDMA","GPUDirect"]},{"location":"applications/deltacast_transmitter/#run-instructions","title":"Run Instructions","text":"<p>From the build directory, run the command:</p> <pre><code>./applications/deltacast_transmitter/deltacast_transmitter --data &lt;holohub_data_dir&gt;/endoscopy\n</code></pre>","tags":["Streaming","Healthcare AI","Video","Deltacast","Endoscopy","RDMA","GPUDirect"]},{"location":"applications/depth_anything_v2/","title":"Depth Anything V2","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.5.0 Tested Holoscan SDK versions: 2.8.0 Contribution metric: Level 2 - Trusted</p> <p>This application uses the Depth Anything V2 model for monocular depth estimation.  Monocular Depth Estimation refers to the task of predicting the distance of objects in a scene from a single 2D image captured by a standard camera.</p>","tags":["Computer Vision and Perception","Visualization","Edge Accelerated Inference","Monocular Depth Estimation","Holoviz","Video"]},{"location":"applications/depth_anything_v2/#model","title":"Model","text":"<p>This application uses the Depth Anything V2 model from DepthAnythingV2 for monocular depth estimation. The model is downloaded when building the Docker image.</p> <p>NOTE: The user is responsible for checking if the model license is suitable for the intended purpose.</p>","tags":["Computer Vision and Perception","Visualization","Edge Accelerated Inference","Monocular Depth Estimation","Holoviz","Video"]},{"location":"applications/depth_anything_v2/#data","title":"Data","text":"<p>This application downloads a pre-recorded video from Pexels when the application is built.  Please review the license terms from Pexels.</p> <p>NOTE: The user is responsible for ensuring the dataset license is suitable for the intended purpose.</p>","tags":["Computer Vision and Perception","Visualization","Edge Accelerated Inference","Monocular Depth Estimation","Holoviz","Video"]},{"location":"applications/depth_anything_v2/#input","title":"Input","text":"<p>This app supports two different input options.  If you have a v4l2 compatible device plugged into your machine such as a webcam, you can run this application with option 1.  Otherwise you can run this application using a pre-recorded video with option 2.</p> <ol> <li>v4l2 compatible input device (default, see V4L2 Support below)</li> <li>pre-recorded video (see Video Replayer Support below)</li> </ol> <p>To see the list of v4l2 devices connected to your machine, install <code>v4l-utils</code> if it's not already installed:</p> <pre><code>sudo apt-get install v4l-utils\n</code></pre> <p>Then run:</p> <pre><code>v4l2-ctl --list-devices\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Edge Accelerated Inference","Monocular Depth Estimation","Holoviz","Video"]},{"location":"applications/depth_anything_v2/#run-instructions","title":"Run Instructions","text":"","tags":["Computer Vision and Perception","Visualization","Edge Accelerated Inference","Monocular Depth Estimation","Holoviz","Video"]},{"location":"applications/depth_anything_v2/#v4l2-support","title":"V4L2 Support","text":"<p>This application supports v4l2 compatible devices as input.  To run this application with your v4l2 compatible device, please plug in your input device and run: <pre><code>./holohub run depth_anything_v2\n</code></pre></p> <p>By default, this application expects the input device to be mounted at <code>/dev/video0</code>.  If this is not the case, update <code>applications/depth_anything_v2/depth_anything_v2.yaml</code> file to set the corresponding input device before running the application.  You can also override the default input device on the command line by running: <pre><code>./holohub run depth_anything_v2 --run-args=\"--video_device /dev/video0\"\n</code></pre></p>","tags":["Computer Vision and Perception","Visualization","Edge Accelerated Inference","Monocular Depth Estimation","Holoviz","Video"]},{"location":"applications/depth_anything_v2/#video-replayer-support","title":"Video Replayer Support","text":"<p>If you don't have a v4l2 compatible device plugged in, you can also run this application on a pre-recorded video. To launch the application using the Video Stream Replayer as the input source, run:</p> <pre><code>./holohub run depth_anything_v2 --run-args=\"--source replayer\"\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Edge Accelerated Inference","Monocular Depth Estimation","Holoviz","Video"]},{"location":"applications/depth_anything_v2/#display-modes","title":"Display Modes","text":"<p>This application has multiple display modes which you can toggle through using the left mouse button.</p> <ul> <li>original: output the original image from input source</li> <li>depth: output the color depthmap based on the depthmap returned from Depth Anything V2 model</li> <li>side-by-side: output a side-by-side view of the original image next to the color depthmap</li> <li>interactive: allow user </li> </ul> <p>In interactive mode, the middle or right mouse button can be used to modify the ratio of original image vs color depthmap is shown.</p>","tags":["Computer Vision and Perception","Visualization","Edge Accelerated Inference","Monocular Depth Estimation","Holoviz","Video"]},{"location":"applications/depth_anything_v2/#acknowledgement","title":"Acknowledgement","text":"<p>This project is based on the following projects: - Depth-Anything-V2 - Depth Anything V2 - depth-anything-tensorrt - Depth Anything TensorRT CLI</p>","tags":["Computer Vision and Perception","Visualization","Edge Accelerated Inference","Monocular Depth Estimation","Holoviz","Video"]},{"location":"applications/depth_anything_v2/#known-issues","title":"Known Issues","text":"<p>There is a known issue running this application on IGX w/ iGPU and on Jetson AGX (see #500). The workaround is to update the device to avoid picking up the libnvv4l2.so library.</p> <pre><code>cd /usr/lib/aarch64-linux-gnu/\nls -l libv4l2.so.0.0.999999\nsudo rm libv4l2.so.0.0.999999\nsudo ln -s libv4l2.so.0.0.0.0  libv4l2.so.0.0.999999\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Edge Accelerated Inference","Monocular Depth Estimation","Holoviz","Video"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/","title":"Distributed Endoscopy Tool Tracking with gRPC Streaming","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: October 29, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.7.0 Tested Holoscan SDK versions: 2.7.0 Contribution metric: Level 0 - Core Stable</p> <p>This application demonstrates how to offload heavy workloads to a remote Holoscan application using gRPC.</p>","tags":["Streaming","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#overview","title":"Overview","text":"<p>In this sample application, we divided the Endoscopy Tool Tracking application into a server and client application where the two communicate via gRPC.</p> <p>The client application inputs a video file and streams the video frames to the server application. The server application handles the heavy workloads of inferencing and post-processing of the video frames. It receives the video frames, processes each frame through the endoscopy tool tracking pipeline, and then streams the results to the client.</p> <p> Endoscopy Tool Tracking Application with gRPC</p> <p>From the diagram above, we can see that both the App Cloud (the server) and the App Edge (the client) are very similar to the standalone Endoscopy Tool Tracking application. This section will only describe the differences; for details on inference and post-processing, please refer to the link above.</p> <p>On the client side, we provided two examples, one using a single fragment and another one using two fragments. When comparing the client side to the standalone Endoscopy Tool Tracking application, the differences are the queues and the gRPC client. We added the following: - Outgoing Requests operator (<code>GrpcClientRequestOp</code>): It converts the video frames (GXF entities) received from the Video Stream Replayer operator into <code>EntityRequest</code> protobuf messages and queues each frame in the Request Queue. - gRPC Service &amp; Client (<code>EntityClientService</code> &amp; <code>EntityClient</code>): The gRPC Service is responsible for controlling the life cycle of the gRPC client. The client connects to the remote gRPC server and then sends the requests found in the Request Queue. When it receives a response, it converts it into a GXF entity and queues it in the Response Queue. - Incoming Responses operator (<code>GrpcClientResponseOp</code>): This operator is configured with an <code>AsynchronousCondition</code> condition to check the availability of the Response Queue. When notified of available responses in the queue, it dequeues each item and emits each to the output port.</p> <p>The App Cloud (the server) application consists of a gRPC server and a few components for managing Holoscan applications. When the server receives a new remote procedure call in this sample application, it launches a new instance of the Endoscopy Tool Tracking application. This is facilitated by the <code>ApplicationFactory</code> used for application registration.</p> <p>Under the hood, the Endoscopy Tool Tracking application here inherits a custom base class (<code>HoloscanGrpcApplication</code>) which manages the <code>Request Queue</code> and the <code>Response Queue</code> as well as the <code>GrpcServerRequestOp</code> and <code>GrpcServerResponseOp</code> operators for receiving requests and serving results, respectively. When the RPC is complete, the instance of the Endoscopy Tool Tracking application is destroyed and ready to serve the subsequent request.</p>","tags":["Streaming","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#requirements","title":"Requirements","text":"","tags":["Streaming","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded when building the application.</p>","tags":["Streaming","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#building-and-running-grpc-endoscopy-tool-tracking-application","title":"Building and Running gRPC Endoscopy Tool Tracking Application","text":"","tags":["Streaming","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#c","title":"C++","text":"<pre><code># Start the gRPC Server\n./holohub run grpc_endoscopy_tool_tracking --run-args=\"cloud\" [--language=cpp]\n\n# Start the gRPC Client\n./holohub run grpc_endoscopy_tool_tracking --run-args=\"edge\" [--language=cpp]\n</code></pre>","tags":["Streaming","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#python","title":"Python","text":"<pre><code># Start the gRPC Server\n./holohub run grpc_endoscopy_tool_tracking --language python --run-args=\"cloud\"\n\n# Start the gRPC Client\n./holohub run grpc_endoscopy_tool_tracking --language python --run-args=\"edge\"\n</code></pre>","tags":["Streaming","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#configurations","title":"Configurations","text":"<p>The Edge application runs in a single-fragment mode by default. However, it can be configured to run in a multi-fragment mode, as in the picture above.</p> <p>To switch to multi-fragment mode, edit the endoscopy_tool_tracking.yaml YAML file and change <code>multifragment</code> to <code>true</code>:</p> <pre><code>application:\n  multifragment: false\n  benchmarking: false\n</code></pre> <p>[!NOTE] The Python version of this application is only available in single-fragment mode with benchmarking turned on.</p> <p>Data Flow Tracking can also be enabled by editing the endoscopy_tool_tracking.yaml YAML file and changing <code>benchmarking</code> to <code>true</code>. This enables the built-in mechanism to profile the application and analyze the fine-grained timing properties and data flow between operators.</p> <p>For example, on the server side, when a client disconnects, it will output the results for that session:</p> <pre><code>Data Flow Tracking Results:\nTotal paths: 1\n\nPath 1: grpc_request_op,format_converter,lstm_inferer,tool_tracking_postprocessor,grpc_response_op\nNumber of messages: 663\nMin Latency Message No: 249\nMin end-to-end Latency (ms): 1.868\nAvg end-to-end Latency (ms): 2.15161\nMax Latency Message No: 371\nMax end-to-end Latency (ms): 4.19\n\nNumber of source messages [format: source operator-&gt;transmitter name: number of messages]:\ngrpc_request_op-&gt;output: 683\n</code></pre> <p>Similarly, on the client side, when it completes playing the video, it will print the results:</p> <pre><code>Data Flow Tracking Results:\nTotal paths: 3\n\nPath 1: incoming_responses,visualizer_op\nNumber of messages: 663\nMin Latency Message No: 249\nMin end-to-end Latency (ms): 0.214\nAvg end-to-end Latency (ms): 0.374005\nMax Latency Message No: 378\nMax end-to-end Latency (ms): 2.751\n\nPath 2: replayer,outgoing_requests\nNumber of messages: 663\nMin Latency Message No: 379\nMin end-to-end Latency (ms): 24.854\nAvg end-to-end Latency (ms): 27.1886\nMax Latency Message No: 142\nMax end-to-end Latency (ms): 28.003\n\nPath 3: replayer,visualizer_op\nNumber of messages: 663\nMin Latency Message No: 372\nMin end-to-end Latency (ms): 30.966\nAvg end-to-end Latency (ms): 33.325\nMax Latency Message No: 397\nMax end-to-end Latency (ms): 35.479\n\nNumber of source messages [format: source operator-&gt;transmitter name: number of messages]:\nincoming_responses-&gt;output: 683\nreplayer-&gt;output: 683\n</code></pre>","tags":["Streaming","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#development-environment","title":"Development Environment","text":"","tags":["Streaming","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#dev-container","title":"Dev Container","text":"<p>To start the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode\n</code></pre>","tags":["Streaming","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Streaming","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#c_1","title":"C++","text":"<p>The following launch profiles are available:</p> <ul> <li>(compound) grpc_endoscopy_tool_tracking/cpp (cloud &amp; edge): Launch both the gRPC server and the client.</li> <li>(gdb) grpc_endoscopy_tool_tracking/cpp (cloud): Launch the gRPC server.</li> <li>(gdb) grpc_endoscopy_tool_tracking/cpp (edge): Launch the gRPC client.</li> </ul>","tags":["Streaming","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#python_1","title":"Python","text":"<p>The following launch profiles are available:</p> <ul> <li>(compound) grpc_endoscopy_tool_tracking/python (cloud &amp; edge): Launch both the gRPC server and the client.</li> <li>(pythoncpp) grpc_endoscopy_tool_tracking/python (cloud): Launch the gRPC server with <code>pythoncpp</code>.</li> <li>(pythoncpp) grpc_endoscopy_tool_tracking/python (edge): Launch the gRPC client with <code>pythoncpp</code>.</li> <li>(debugpy) grpc_endoscopy_tool_tracking/python (cloud): Launch the gRPC server with <code>debugpy</code>.</li> <li>(debugpy) grpc_endoscopy_tool_tracking/python (edge):Launch the gRPC client with <code>debugpy</code>.</li> </ul> <p>[!NOTE] The <code>compound</code> profile uses the <code>debugpy</code> extension due to a limitation that prevents launching the cloud and the edge apps together using <code>pythoncpp</code>.</p>","tags":["Streaming","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#limitations-known-issues","title":"Limitations &amp; Known Issues","text":"","tags":["Streaming","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#c_2","title":"C++","text":"<ol> <li>Connection Timeout:</li> <li>The connection between the server and client is controlled by <code>rpc_timeout</code></li> <li>Default timeout is 5 seconds, configurable in endoscopy_tool_tracking.yaml</li> <li> <p>Consider increasing this value on slower networks</p> </li> <li> <p>Server Limitations:</p> </li> <li>Can only serve one request at a time</li> <li> <p>Subsequent calls receive <code>grpc::StatusCode::RESOURCE_EXHAUSTED</code> status</p> </li> <li> <p>Debugging Issues:</p> </li> <li>When using the compound profile, the server may need additional startup time</li> <li> <p>If needed, adjust the sleep value in tasks.json under <code>Build grpc_endoscopy_tool_tracking (delay 3s)</code></p> </li> <li> <p>Expected Exit Behavior:</p> </li> <li>The client will exit with the following expected error when the video completes:      <pre><code>[error] [program.cpp:614] Event notification 2 for entity [video_in__outgoing_requests] with id [33] received in an unexpected state [Origin]\n</code></pre></li> </ol>","tags":["Streaming","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#python_2","title":"Python","text":"<ul> <li>The client may require manual termination (CTRL+C) if errors occur during execution</li> </ul>","tags":["Streaming","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_endoscopy_tool_tracking/#containerization","title":"Containerization","text":"<p>To containerize the application:</p> <ol> <li>Install Holoscan CLI</li> <li>Build the application:    <pre><code>./holohub install grpc_endoscopy_tool_tracking\n</code></pre></li> <li>Run the appropriate packaging script:</li> <li>C++: cpp/package-app.sh</li> <li>Python: python/package-app.sh</li> <li>Follow the generated output instructions to package and run the application</li> </ol> <p>For more information about packaging Holoscan applications, refer to the Packaging Holoscan Applications section in the Holoscan User Guide.</p>","tags":["Streaming","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/","title":"Distributed H.264 Endoscopy Tool Tracking with gRPC Streaming","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: October 30, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.6.0 Tested Holoscan SDK versions: 2.6.0 Contribution metric: Level 0 - Core Stable</p> <p>This application demonstrates how to offload heavy workloads to a remote Holoscan application using gRPC.</p>","tags":["Healthcare AI","Distributed","Asynchronous Queues","gRPC","Video"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#overview","title":"Overview","text":"<p>In this sample application, we divided the h.264 Endoscopy Tool Tracking application into a server and client application where the two communicate via gRPC.</p> <p>The client application reads a pre-recorded h.264 video file and streams the encoded video frames to the server application. The server application handles the heavy workloads of inferencing and post-processing of the video frames. It receives the video frames, processes each frame through the endoscopy tool tracking pipeline, and then streams the results to the client.</p> <p> h.264 Endoscopy Tool Tracking Application with gRPC</p> <p>From the diagram above, we can see that both the App Cloud (the server) and the App Edge (the client) are very similar to the standalone Endoscopy Tool Tracking application. This section will only describe the differences; for details on inference and post-processing, please refer to the link above.</p> <p>On the client side, the differences are the queues and the gRPC client. In the Video Input Fragment, we added the following: - Outgoing Requests operator (<code>GrpcClientRequestOp</code>): It converts the video frames (GXF entities) received from the Video Read Stream operator into <code>EntityRequest</code> protobuf messages and queues each frame in the Request Queue. - gRPC Service &amp; Client (<code>EntityClientService</code> &amp; <code>EntityClient</code>): The gRPC Service is responsible for controlling the life cycle of the gRPC client. The client connects to the remote gRPC server and then sends the requests found in the Request Queue. When it receives a response, it converts it into a GXF entity and queues it in the Response Queue. - Incoming Responses operator (<code>GrpcClientResponseOp</code>): This operator is configured with an <code>AsynchronousCondition</code> condition to check the availability of the Response Queue. When notified of available responses in the queue, it dequeues each item and emits each to the output port.</p> <p> Details of App Cloud</p> <p>The App Cloud (the server) application consists of a gRPC server and a few components for managing Holoscan applications. When the server receives a new remote procedure call in this sample application, it launches a new instance of the Endoscopy Tool Tracking application. This is facilitated by the <code>ApplicationFactory</code> used for application registration.</p> <p>Under the hood, the Endoscopy Tool Tracking application here inherits a custom base class (<code>HoloscanGrpcApplication</code>) which manages the <code>Request Queue</code> and the <code>Response Queue</code> as well as the <code>GrpcServerRequestOp</code> and <code>GrpcServerResponseOp</code> operators for receiving requests and serving results, respectively. When the RPC is complete, the instance of the Endoscopy Tool Tracking application is destroyed and ready to serve the subsequent request.</p>","tags":["Healthcare AI","Distributed","Asynchronous Queues","gRPC","Video"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#requirements","title":"Requirements","text":"<p>This application is configured to use H.264 elementary stream from endoscopy sample data as input.</p>","tags":["Healthcare AI","Distributed","Asynchronous Queues","gRPC","Video"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded when building the application.</p>","tags":["Healthcare AI","Distributed","Asynchronous Queues","gRPC","Video"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#building-and-running-grpc-h264-endoscopy-tool-tracking-application","title":"Building and Running gRPC H.264 Endoscopy Tool Tracking Application","text":"<ul> <li>Building and running the application from the top level Holohub directory:</li> </ul>","tags":["Healthcare AI","Distributed","Asynchronous Queues","gRPC","Video"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#c","title":"C++","text":"<pre><code># Start the gRPC Server\n./holohub run grpc_h264_endoscopy_tool_tracking --run-args=\"cloud\" [--language cpp]\n\n# Start the gRPC Client\n./holohub run grpc_h264_endoscopy_tool_tracking --run-args=\"edge\" [--language cpp]\n</code></pre> <p>Important: on aarch64, applications also need tegra folder mounted inside the container and the <code>LD_LIBRARY_PATH</code> environment variable should be updated to include tegra folder path.</p> <p>Open and edit the Dockerfile and uncomment line 66:</p> <pre><code># Uncomment the following line for aarch64 support\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/aarch64-linux-gnu/tegra/\n</code></pre>","tags":["Healthcare AI","Distributed","Asynchronous Queues","gRPC","Video"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode h264\n</code></pre>","tags":["Healthcare AI","Distributed","Asynchronous Queues","gRPC","Video"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Healthcare AI","Distributed","Asynchronous Queues","gRPC","Video"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#c_1","title":"C++","text":"<p>The following launch profiles are available:</p> <ul> <li>(compound) grpc_h264_endoscopy_tool_tracking/cpp (cloud &amp; edge): Launch both the gRPC server and the client.</li> <li>(gdb) grpc_h264_endoscopy_tool_tracking/cpp (cloud): Launch the gRPC server.</li> <li>(gdb) grpc_h264_endoscopy_tool_tracking/cpp (edge): Launch the gRPC client.</li> </ul>","tags":["Healthcare AI","Distributed","Asynchronous Queues","gRPC","Video"]},{"location":"applications/distributed/grpc/grpc_h264_endoscopy_tool_tracking/#limitations-known-issues","title":"Limitations &amp; Known Issues","text":"<ul> <li>The connection between the server and the client is controlled by <code>rpc_timeout</code>. If no data is received or sent within the configured time, it assumes the call has been completed and hangs up. The <code>rpc_timeout</code> value can be configured in the endoscopy_tool_tracking.yaml file with a default of 5 seconds. Increasing this value may help on a slow network.</li> <li>The server can serve one request at any given time. Any subsequent call receives a <code>grpc::StatusCode::RESOURCE_EXHAUSTED</code> status.</li> <li>When debugging using the compound profile, the server may not be ready to serve, resulting in errors with the client application. When this happens, open tasks.json, find <code>Build grpc_h264_endoscopy_tool_tracking (delay 3s)</code>, and adjust the <code>command</code> field with a higher sleep value.</li> <li>The client is expected to exit with the following error. It is how the client application terminates when it completes streaming and displays the entire video.   <pre><code>[error] [program.cpp:614] Event notification 2 for entity [video_in__outgoing_requests] with id [33] received in an unexpected state [Origin]\n</code></pre></li> </ul>","tags":["Healthcare AI","Distributed","Asynchronous Queues","gRPC","Video"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/","title":"UCX-based Distributed Endoscopy Tool Tracking","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p>Digital endoscopy is a key technology for medical screenings and minimally invasive surgeries. Using real-time AI workflows to process and analyze the video signal produced by the endoscopic camera, this technology helps medical professionals with anomaly detection and measurements, image enhancements, alerts, and analytics.</p> <p> Fig. 1 Endoscopy (laparoscopy) image from a cholecystectomy (gallbladder removal surgery) showing AI-powered frame-by-frame tool identification and tracking. Image courtesy of Research Group Camma, IHU Strasbourg and the University of Strasbourg (NGC Resource)</p> <p>The Distributed Endoscopy Tool Tracking application provides an example of how an endoscopy data stream can be captured and processed using the C++ or Python APIs on multiple hardware platforms.</p> <p>The Distributed Endoscopy Tool Tracking application is very similar to the Endoscopy Tool Tracking application but divides all operators into three fragments as depicted below. This allows fragments to run on different systems. For example, one can run the Video Input Fragment and the Visualization Fragment on a radiology workstation while the Inference Fragment runs on a more powerful system with multiple GPUs. Refer to the Holoscan SDK User Guide for more information on distributed applications.</p>","tags":["Healthcare AI","Distributed","Surgical AI","Endoscopy","Video","Computer Vision and Perception","Segmentation"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/#video-stream-replayer-input","title":"Video Stream Replayer Input","text":"<p> Fig. 2 Tool tracking application workflow with replay from file</p> <p>The pipeline uses a recorded endoscopy video file (generated by <code>convert_video_to_gxf_entities</code> script) for input frames. Each input frame in the file is loaded by Video Stream Replayer and passed to the following two pipeline branches:</p> <ul> <li>In the first branch, the input frames are directly passed to Holoviz in the Visualization Fragment for rendering in the background.</li> <li>In the second branch, the frames go through the Format Converter in the Inference Fragment to convert the data type of the image from <code>uint8</code> to <code>float32</code> before it is fed to the tool tracking model (with Custom TensorRT Inference). The result is then ingested by the Tool Tracking Postprocessor which extracts the masks, points, and text from the inference output, before Holoviz renders them as overlays in the Visualization Fragment .</li> </ul>","tags":["Healthcare AI","Distributed","Surgical AI","Endoscopy","Video","Computer Vision and Perception","Segmentation"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode\n</code></pre>","tags":["Healthcare AI","Distributed","Surgical AI","Endoscopy","Video","Computer Vision and Perception","Segmentation"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Healthcare AI","Distributed","Surgical AI","Endoscopy","Video","Computer Vision and Perception","Segmentation"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/#c","title":"C++","text":"<p>There are several launch profiles configured for the C++ version of this application:</p> <ol> <li>(gdb) ucx_endoscopy_tool_tracking/cpp (all fragments): Launches all fragments in a single process in debug mode.</li> <li>(gdb) ucx_endoscopy_tool_tracking/cpp - video_in fragment: Starts the <code>video_in</code> fragment in debug mode.</li> <li>(gdb) ucx_endoscopy_tool_tracking/cpp - inference fragment: Starts the <code>inference</code> fragment in debug mode.</li> <li>(gdb) ucx_endoscopy_tool_tracking/cpp - viz fragment: Starts the <code>viz</code> fragment in debug mode.</li> <li>(compound) ucx_endoscopy_tool_tracking/cpp: Starts #2, #3, #4 in sequence.</li> </ol>","tags":["Healthcare AI","Distributed","Surgical AI","Endoscopy","Video","Computer Vision and Perception","Segmentation"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/#python","title":"Python","text":"<p>There are several launch profiles configured for this application:</p> <ol> <li>(debugpy) ucx_endoscopy_tool_tracking/python (all fragments): Launches all fragments in a single process in debug mode.</li> <li>(debugpy) ucx_endoscopy_tool_tracking/python - video_in fragment): Launches all fragments in a single process in debug mode.</li> <li>(pythoncpp) ucx_endoscopy_tool_tracking/python - video_in fragment: Starts the <code>video_in</code> fragment in debug mode.</li> <li>(debugpy) ucx_endoscopy_tool_tracking/python - inference fragment: Starts the <code>video_in</code> fragment in debug mode.</li> <li>(pythoncpp) ucx_endoscopy_tool_tracking/python - inference fragment: Starts the <code>inference</code> fragment in debug mode.</li> <li>(debugpy) ucx_endoscopy_tool_tracking/python - viz fragment: Starts the <code>inference</code> fragment in debug mode.</li> <li>(pythoncpp) ucx_endoscopy_tool_tracking/python - viz fragment: Starts the <code>viz</code> fragment in debug mode.</li> <li>(compound) ucx_endoscopy_tool_tracking/python: Starts #2, #4, #6 in sequence.</li> </ol> <p>Note: Launch profiles prefixed with <code>debugpy</code> enables debugging of Python code only. Use <code>pythoncpp</code> to debug both C++ and Python code.</p>","tags":["Healthcare AI","Distributed","Surgical AI","Endoscopy","Video","Computer Vision and Perception","Segmentation"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/cpp/","title":"UCX-based Distributed Endoscopy Tool Tracking","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ (see Python version) Last modified: February 4, 2026 Latest version: 1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.1.0, 3.10.0, 3.11.0 Contribution metric: Level 0 - Core Stable</p> <p>This application is similar to the Endoscopy Tool Tracking application, but the distributed version divides the application into three fragments:</p> <ol> <li>Video Input: get video input from a pre-recorded video file.</li> <li>Inference: run the inference using LSTM and run the post-processing script.</li> <li>Visualization: display input video and inference results.</li> </ol> <p>Based on an LSTM (long-short term memory) stateful model, these applications demonstrate the use of custom components for tool tracking, including composition and rendering of text, tool position, and mask (as heatmap) combined with the original video stream.</p>","tags":["Healthcare AI","Distributed","Surgical AI","Endoscopy","Video","Computer Vision and Perception","Segmentation"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/cpp/#requirements","title":"Requirements","text":"<p>The provided applications are configured to use a pre-recorded endoscopy video (replayer).</p>","tags":["Healthcare AI","Distributed","Surgical AI","Endoscopy","Video","Computer Vision and Perception","Segmentation"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/cpp/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Healthcare AI","Distributed","Surgical AI","Endoscopy","Video","Computer Vision and Perception","Segmentation"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/cpp/#run-instructions","title":"Run Instructions","text":"<pre><code># Build the Holohub container for the Distributed Endoscopy Tool Tracking application\n./holohub build-container ucx_endoscopy_tool_tracking --img holohub:ucx_endoscopy_tool_tracking\n\n# Launch the container\n./holohub run-container ucx_endoscopy_tool_tracking --no-docker-build --img holohub:ucx_endoscopy_tool_tracking\n\n# Build the Distributed Endoscopy Tool Tracking application\n./holohub build ucx_endoscopy_tool_tracking --local\n\n# Generate the TRT engine file from onnx\npython3 utilities/generate_trt_engine.py --input data/endoscopy/tool_loc_convlstm.onnx --output data/endoscopy/engines/ --fp16\n\n# Start the application with all three fragments\n./holohub run ucx_endoscopy_tool_tracking --language=cpp --local --no-local-build\n\n# Once you have completed the step to generate the TRT engine file, you may exit the container and\n#  use the following commands to run the application in distributed mode:\n\n# Start the application with the video_in fragment\n./holohub run ucx_endoscopy_tool_tracking --language=cpp --run-args=\"--driver --worker --fragments video_in --address :9999\"\n# Start the application with the inference fragment\n./holohub run ucx_endoscopy_tool_tracking --language=cpp --run-args=\"--worker --fragments inference --address :9999\"\n# Start the application with the visualization fragment\n./holohub run ucx_endoscopy_tool_tracking --language=cpp --run-args=\"--worker --fragments viz --address :9999\"\n</code></pre>","tags":["Healthcare AI","Distributed","Surgical AI","Endoscopy","Video","Computer Vision and Perception","Segmentation"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/python/","title":"UCX-based Distributed Endoscopy Tool Tracking","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ (see C++ version) Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application is similar to the Endoscopy Tool Tracking application, but the distributed version divides the application into three fragments:</p> <ol> <li>Video Input: get video input from a pre-recorded video file.</li> <li>Inference: run the inference using LSTM and run the post-processing script.</li> <li>Visualization: display input video and inference results.</li> </ol> <p>Based on an LSTM (long-short term memory) stateful model, these applications demonstrate the use of custom components for tool tracking, including composition and rendering of text, tool position, and mask (as heatmap) combined with the original video stream.</p>","tags":["Healthcare AI","Distributed","Surgical AI","Endoscopy","Video","Computer Vision and Perception","Segmentation"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/python/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8+</li> <li>The provided applications are configured to use a pre-recorded endoscopy video (replayer).</li> </ul>","tags":["Healthcare AI","Distributed","Surgical AI","Endoscopy","Video","Computer Vision and Perception","Segmentation"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/python/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Healthcare AI","Distributed","Surgical AI","Endoscopy","Video","Computer Vision and Perception","Segmentation"]},{"location":"applications/distributed/ucx/ucx_endoscopy_tool_tracking/python/#run-instructions","title":"Run Instructions","text":"<pre><code># Start the application with all three fragments\n./holohub run ucx_endoscopy_tool_tracking --language=python\n\n# Use the following commands to run the same application three processes:\n# Start the application with the video_in fragment\n./holohub run ucx_endoscopy_tool_tracking --language=python --run-args=\"--driver --worker --fragments video_in --address :10000\"\n# Start the application with the inference fragment\n./holohub run ucx_endoscopy_tool_tracking --language=python --run-args=\"--worker --fragments inference --address :10000\"\n# Start the application with the visualization fragment\n./holohub run ucx_endoscopy_tool_tracking --language=python --run-args=\"--worker --fragments viz --address :10000\"\n</code></pre>","tags":["Healthcare AI","Distributed","Surgical AI","Endoscopy","Video","Computer Vision and Perception","Segmentation"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/","title":"Distributed H.264 Endoscopy Tool Tracking","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: October 30, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.6.0 Tested Holoscan SDK versions: 2.6.0 Contribution metric: Level 0 - Core Stable</p> <p>This application is similar to the H.264 Endoscopy Tool Tracking application, but this distributed version divides the application into three fragments:</p> <ol> <li>Video Input: get video input from a pre-recorded video file.</li> <li>Inference: run the inference using LSTM and run the post-processing script.</li> <li>Visualization: display input video and inference results.</li> </ol>","tags":["Healthcare AI","Video","Surgical AI","Distributed","Holoviz","LSTM","TensorRT","Endoscopy"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/#requirements","title":"Requirements","text":"<p>This application is configured to use H.264 elementary stream from endoscopy sample data as input.</p>","tags":["Healthcare AI","Video","Surgical AI","Distributed","Holoviz","LSTM","TensorRT","Endoscopy"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded when building the application.</p>","tags":["Healthcare AI","Video","Surgical AI","Distributed","Holoviz","LSTM","TensorRT","Endoscopy"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/#building-and-running-h264-endoscopy-tool-tracking-application","title":"Building and Running H.264 Endoscopy Tool Tracking Application","text":"<ul> <li>Building and running the application from the top level Holohub directory:</li> </ul>","tags":["Healthcare AI","Video","Surgical AI","Distributed","Holoviz","LSTM","TensorRT","Endoscopy"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/#c","title":"C++","text":"<pre><code># Start the application with all three fragments\n./holohub run ucx_h264_endoscopy_tool_tracking --language=cpp\n\n# Use the following commands to run the same application three processes:\n# Start the application with the video_in fragment\n./holohub run ucx_h264_endoscopy_tool_tracking --language=cpp --run-args=\"--driver --worker --fragments video_in --address :10000 --worker-address :10001\"\n# Start the application with the inference fragment\n./holohub run ucx_h264_endoscopy_tool_tracking --language=cpp --run-args=\"--worker --fragments inference --address :10000 --worker-address :10002\"\n# Start the application with the visualization fragment\n./holohub run ucx_h264_endoscopy_tool_tracking --language=cpp --run-args=\"--worker --fragments viz --address :10000 --worker-address :10003\"\n</code></pre>","tags":["Healthcare AI","Video","Surgical AI","Distributed","Holoviz","LSTM","TensorRT","Endoscopy"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/#python","title":"Python","text":"<pre><code># Start the application with all three fragments\n./holohub run ucx_h264_endoscopy_tool_tracking --language=python\n\n# Use the following commands to run the same application three processes:\n# Start the application with the video_in fragment\n./holohub run ucx_h264_endoscopy_tool_tracking --language=python --run-args=\"--driver --worker --fragments video_in --address :10000 --worker-address :10001\"\n# Start the application with the inference fragment\n./holohub run ucx_h264_endoscopy_tool_tracking --language=python --run-args=\"--worker --fragments inference --address :10000 --worker-address :10002\"\n# Start the application with the visualization fragment\n./holohub run ucx_h264_endoscopy_tool_tracking --language=python --run-args=\"--worker --fragments viz --address :10000 --worker-address :10003\"\n</code></pre> <p>Important: on aarch64, applications also need tegra folder mounted inside the container and the <code>LD_LIBRARY_PATH</code> environment variable should be updated to include tegra folder path.</p> <p>Open and edit the Dockerfile and uncomment line 66:</p> <pre><code># Uncomment the following line for aarch64 support\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/aarch64-linux-gnu/tegra/\n</code></pre>","tags":["Healthcare AI","Video","Surgical AI","Distributed","Holoviz","LSTM","TensorRT","Endoscopy"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/#dev-container","title":"Dev Container","text":"<p>To start the VS Code Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode h264\n</code></pre>","tags":["Healthcare AI","Video","Surgical AI","Distributed","Holoviz","LSTM","TensorRT","Endoscopy"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Healthcare AI","Video","Surgical AI","Distributed","Holoviz","LSTM","TensorRT","Endoscopy"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/#c_1","title":"C++","text":"<p>Use the (gdb) ucx_h264_endoscopy_tool_tracking/cpp (all fragments) launch profile to run and debug the C++ application.</p>","tags":["Healthcare AI","Video","Surgical AI","Distributed","Holoviz","LSTM","TensorRT","Endoscopy"]},{"location":"applications/distributed/ucx/ucx_h264_endoscopy_tool_tracking/#python_1","title":"Python","text":"<p>Use the (pythoncpp) ucx_h264_endoscopy_tool_tracking/python (all fragments) launch profile to run and debug the Python application.</p>","tags":["Healthcare AI","Video","Surgical AI","Distributed","Holoviz","LSTM","TensorRT","Endoscopy"]},{"location":"applications/ehr_query_llm/fhir/","title":"FHIR Client for Retrieving and Posting FHIR Resources","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: October 9, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.5.0 Tested Holoscan SDK versions: 2.5.0, 2.7.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This is an application to interface with a FHIR server to retrieve or post FHIR resources.</p> <p>It requires that the FHIR Server endpoint URL is provided on the command line as well as client authentication credentials if required. Currently, authentication and authorization is limited to OAuth2.0 server-to-server workflow. When authorization is required by the server, its OAuth2.0 token service URL along with client ID and secret must be provided to the application.</p> <p>This application also uses ZeroMQ to communicate with its own clients, listening on a well-known port on localhost for messages to retrieve resources of a patient, as well as publishing the retrieved resources on another well-known port. For simplicity, the listening port is defined in the code to be <code>5600</code>, and the publishing port <code>5601</code>. Messaging security, at transport or message level, is not implemented in this example.</p> <p>The message schema is simple, with a well-known topic string and topic-specific content schema in JSON format.</p> <p>The default set of FHIR resource types to retrieve are listed below, which can be overridden by the request message:</p> <ul> <li>Observation</li> <li>ImagingStudy</li> <li>FamilyMemberHistory</li> <li>Condition</li> <li>DiagnosticReport</li> <li>DocumentReference</li> </ul>","tags":["Healthcare AI","Distributed","ZeroMQ","Auth and API","Healthcare Interop","LLM"]},{"location":"applications/ehr_query_llm/fhir/#requirements","title":"Requirements","text":"<ul> <li>On a Holohub supported platform</li> <li>Python 3.10+</li> <li>Python packages from PyPI, including holoscan, fhir.resources, pyzmq, requests and their dependencies</li> </ul>","tags":["Healthcare AI","Distributed","ZeroMQ","Auth and API","Healthcare Interop","LLM"]},{"location":"applications/ehr_query_llm/fhir/#run-instructions","title":"Run Instructions","text":"<p>There are several ways to build and run this application and package it as a Holoscan Application Package, an Open Container Initiative compliant image. The following sections describe each in detail.</p> <p>It is further expected that you have read the HoloHub README, have cloned the HoloHub repository to your local system, and the current working directory is the HoloHub root, <code>holohub</code>.</p> <p>Note: The application listens for request messages to start retrieving resources from the server and then publishes the results, so another application is needed to drive this workflow (e.g., the LLM application). To help with simple testing, a Python script is provided as part of this application, and its usage is described below in this section.</p>","tags":["Healthcare AI","Distributed","ZeroMQ","Auth and API","Healthcare Interop","LLM"]},{"location":"applications/ehr_query_llm/fhir/#quick-start-using-holohub-container","title":"Quick Start Using Holohub Container","text":"<p>This is the simplest and fastest way to start the application in a Holohub dev container and get it ready to listen to request messages.</p> <p>Note: Please use your own FHIR server endpoint, as well as the OAuth2.0 authorization endpoint and client credentials as needed.</p> <pre><code>./holohub run fhir --run-args \"--fhir_url &lt;f_url&gt; --auth_url &lt;a_url&gt; --uid &lt;id&gt; --secret &lt;token&gt;\"\n</code></pre>","tags":["Healthcare AI","Distributed","ZeroMQ","Auth and API","Healthcare Interop","LLM"]},{"location":"applications/ehr_query_llm/fhir/#run-the-application-in-holohub-dev-container","title":"Run the Application in Holohub Dev Container","text":"<p>Launch the container:</p> <pre><code>./holohub run-container fhir\n</code></pre> <p>This command builds the <code>holohub:fhir</code> container based on the application-specific Dockerfile.</p> <p>Build and run the application:</p> <p>Now in the container, build and run the application:</p> <pre><code>~$ pwd\n/workspace/holohub\n\n~$ ./holohub clear-cache\n~$ ./holohub run fhir --run-args \"--fhir_url &lt;f_url&gt; --auth_url &lt;a_url&gt; --uid &lt;id&gt; --secret &lt;token&gt;\"\n</code></pre> <p>Once done, <code>exit</code> the container.</p>","tags":["Healthcare AI","Distributed","ZeroMQ","Auth and API","Healthcare Interop","LLM"]},{"location":"applications/ehr_query_llm/fhir/#run-the-application-in-the-host-dev-environment-bare-metal","title":"Run the Application in the Host Dev Environment (Bare Metal)","text":"<p>First, create and activate a Python virtual environment, followed by installing the dependencies:</p> <pre><code>python3 -m venv .testenv\nsource .testenv/bin/activate\npip install -r applications/ehr_query_llm/fhir/requirements.txt\n</code></pre> <p>Then, Set up the Holohub environment:</p> <pre><code>./holohub setup  # sudo privileges may be required\n</code></pre> <p>Note: Although this application is implemented entirely in Python and relies on standard PyPI packages, you still may want to set up Holohub environment and use <code>./holohub</code> commandline .</p> <p>Next, build and install the application with <code>./holohub</code>:</p> <pre><code>./holohub install fhir --local\n</code></pre> <p>Now, run the application which is installed in the <code>install</code> folder, with server URLs and credentials of your own:</p> <pre><code>python install/bin/fhir/python/fhir_client.py --fhir_url &lt;f_url&gt; --auth_url &lt;a_url&gt; --uid &lt;id&gt; --secret &lt;token&gt;\n</code></pre>","tags":["Healthcare AI","Distributed","ZeroMQ","Auth and API","Healthcare Interop","LLM"]},{"location":"applications/ehr_query_llm/fhir/#test-the-running-application","title":"Test the Running Application","text":"<p>Once the FHIR application has been started with one of the above methods, a test application can be used to request and receive FHIR resources, namely <code>applications/ehr_query_llm/fhir/test_fhir_client.py</code>.</p> <p>The test application contains hard-coded patient name, patient FHIR resource ID, etc., corresponding to a specific test dataset, though it can be easily modified for another dataset.</p> <p>It is strongly recommended to run this test application in a Python virtual environment, which can be the same as that used for running the FHIR application. The following describes running it in its own environment:</p> <pre><code>echo \"Assuming venv already created with \\`python3 -m venv .testenv\\`\"\nsource .testenv/bin/activate\npip install -r applications/ehr_query_llm/fhir/requirements.txt\nexport PYTHONPATH=${PWD}\npython applications/ehr_query_llm/fhir/test_fhir_client.py\n</code></pre> <p>From the menu, pick one of the choices for the resources of interest.</p>","tags":["Healthcare AI","Distributed","ZeroMQ","Auth and API","Healthcare Interop","LLM"]},{"location":"applications/ehr_query_llm/fhir/#packaging-the-application-for-distribution-and-deployment","title":"Packaging the Application for Distribution and Deployment","text":"<p>With Holoscan CLI, applications built with Holoscan SDK can be packaged into a Holoscan Application Package (HAP), which is an Open Container Initiative compliant image. An HAP is well suited to be distributed for deployment on hosting platforms, be it Docker Compose, Kubernetes, or otherwise. Please refer to Packaging Holoscan Applications in the User Guide for more information.</p> <p>This example application provides all the necessary contents for HAP packaging. It is required to perform the packaging in a Python virtual environment, with the application's dependencies installed, before running the following script to reveal specific packaging commands.</p> <pre><code>applications/ehr_query_llm/fhir/packageHAP.sh\n</code></pre> <p>Once the HAP is created, it can then be saved and restored on the target deployment host, and run with the <code>docker run</code> command, shown below with user-specific parameters to be substituted.</p> <pre><code>docker run -it --rm --net host holohub-fhir-x64-workstation-dgpu-linux-amd64:1.0 \\\n--fhir_url &lt;f_url&gt; \\\n--auth_url &lt;a_url&gt; \\\n--uid &lt;id&gt; \\\n--secret &lt;token&gt;\n</code></pre> <p>Note: Packaging this application requires <code>holoscan-cli</code>, which can be installed using <code>pip</code>. If you are using the same Python environment for packaging as your development environment, there may be a version conflict for the <code>pydantic</code> package, as it is required by both <code>holoscan-cli</code> and <code>fhir.resources</code>. To ensure your development environment can still run the application after packaging, reinstall <code>fhir.resources</code>:</p> <pre><code>pip install fhir.resources\n</code></pre>","tags":["Healthcare AI","Distributed","ZeroMQ","Auth and API","Healthcare Interop","LLM"]},{"location":"applications/ehr_query_llm/lmm/","title":"EHR Agent Framework","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: December 10, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 1.0.0 Tested Holoscan SDK versions: 1.0.0, 2.5, 2.7.0 Contribution metric: Level 5 - Obsolete</p> <p>The EHR Agent Framework is designed to handle and interact with EHR (Electronic Health Records) and it provides a modular and extensible system for handling various types of queries through specialized agents, with robust error handling and performance optimization features.</p>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Agent Framework overview</li> <li>Setup Instructions</li> <li>Run Instructions</li> <li>Offline Mode</li> </ul>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#agent-framework-overview","title":"Agent Framework overview","text":"<p>The <code>AgentFrameworkOp</code> orchestrates multiple specialized agents to handle different types of queries and responses, it maintains a streaming queue for responses and it handles response states through <code>ResponseHandler</code>. It tracks conversation history using ChatHistory class and updates history based on agent responses and ASR transcripts.</p> <p>Agent Lifecycle:</p> <ul> <li>Processes requests asynchronously using threads</li> <li>Controls response streaming and muting during speech</li> </ul>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#the-base-agent-class","title":"The Base Agent Class","text":"<p>This is an abstract base class implementing common agent functionality:</p> <ul> <li>Uses threading.Lock for LLM and , if needed, LMM access</li> <li>Prevents concurrent requests to language models</li> </ul> <p>It loads configuration from YAML files and it handles prompt templates, tokens limits, and model URLs. The Base Agent is designed to stream responses from LLM server and it supports both text and image-based prompts while enforcing maximum prompt token limits. Throughout the agent lifecycle it maintains conversation context and it creates conversation strings within token limits.</p>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#the-selector-agent","title":"The Selector Agent","text":"<p>The Selector Agent routes user queries to the appropriate specialized agent by analyzing user input to determine the appropriate agent and return the selected agent name and corrected input text. For response parsing, it handles JSON response format. If there are parsing failures, it logs them and it returns <code>None</code> for invalid selection.</p>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#the-ehr-builder-agent","title":"The EHR Builder Agent","text":"<p>The EHR Builder Agent handles EHR database construction on demand and it tracks and reports build time performance in the process. For response generation, it uses custom prompt templates for EHR tasks and it returns structured JSON responses. It also verifies build capability before execution and it reports success/failure status.</p>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#the-ehr-agent","title":"The EHR Agent","text":"<p>The EHR Agent handles EHR queries and data retrieval. It uses Chroma for document storage while implementing HuggingFaceBgeEmbeddings for embeddings. For the RAG (Retrieval-Augmented Generation) pipeline, it performs MMR (Maximal Marginal Relevance) search with configurable search parameters (<code>k</code>, <code>lambda_mult</code>, <code>fetch_k</code>). For prompt generation, it incorporates retrieved documents into prompts and it supports both standard and RAG-specific prompts. The EHR Agent is using CUDA for embedding computation and optimizes for cosine similarity calculations.</p>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#common-features-across-agents","title":"Common features across agents","text":"","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#configuration-management","title":"Configuration Management","text":"<ul> <li>YAML-based settings</li> <li>Configurable prompts and rules</li> <li>Extensible tool support</li> </ul>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#response-handling","title":"Response Handling","text":"<ul> <li>Streaming response support</li> <li>Mutable response states</li> <li>Structured output formatting</li> </ul>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#error-management","title":"Error Management","text":"<ul> <li>Connection retry logic</li> <li>Comprehensive error logging</li> <li>Graceful failure handling</li> </ul>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Thread-safe operations</li> <li>Token usage optimization</li> <li>Efficient resource management</li> </ul>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#setup-instructions","title":"Setup Instructions","text":"","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#speech-pipeline","title":"Speech pipeline","text":"<p>Note</p> <p>NVIDIA Riva provides speech and translation services for user interaction with the LLM. We recommend running Riva in the bare metal host environment outside of the development container to minimize demands on container resources. During test run, it was observed that Riva could take up around 8 GB of GPU memory, while the rest of the application around 12 GB of GPU memory.</p> <p>NVIDIA Riva Version Compatibility : tested with v2.13.0 / v2.14.0.</p> <p>Please adhere to the \"Data Center\" configuration specifications in the Riva Quick Start guide.</p> <p>To optimize Riva installation footprint:</p> <ul> <li>Locate the <code>config.sh</code> file in the riva_quickstart_vX.XX.X directory.</li> <li>Modify the <code>service_enabled_*</code> variables as follows:</li> </ul> <pre><code>service_enabled_asr=true\nservice_enabled_nlp=false\nservice_enabled_tts=true\nservice_enabled_nmt=false\n</code></pre>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#model-acquisition","title":"Model acquisition","text":"<p>It is recommended to create a directory called <code>/models</code> on your machine to download the LLM.</p> <p>Download the quantized Mistral 7B finetuned LLM from HugginFace.co:</p> <pre><code>wget -nc -P &lt;your_model_dir&gt; https://huggingface.co/TheBloke/openchat-3.5-0106-GGUF/resolve/main/openchat-3.5-0106.Q8_0.gguf\n</code></pre> <p>Download the BGE-large finetuned embedding model from NGC:</p> <p><code>bash  wget https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/models/bge-large-ehr-finetune</code></p> <p>Execute the following command from the Holohub root directory:</p> <pre><code>./holohub build lmm\n</code></pre>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#run-instructions","title":"Run Instructions","text":"","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#step-1-enabling-httpsssl-only-required-once","title":"Step 1: Enabling HTTPS/SSL (only required once)","text":"<p>\u26a0\ufe0f Note: This has only been tested with Chrome and Chromium</p> <p>Browsers require HTTPS to be used in order to access the client's microphone.  Hence, you'll need to create a self-signed SSL certificate and key.</p> <p>This key must be placed in <code>/applications/ehr_query_llm/lmm/ssl</code></p> <pre><code>cd &lt;holohub root&gt;/applications/ehr_query_llm/lmm/\nmkdir ssl\ncd ssl\nopenssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -sha256 -days 365 -nodes -subj '/CN=localhost'\n</code></pre> <p>When you first navigate your browser to a page that uses these self-signed certificates, it will issue you a warning since they don't originate from a trusted authority. Ignore this and proceed to the web app:</p> <p></p> <p>ehr_query_llm will use your default speaker and microphone. To change this go to your Ubuntu sound settings and choose the correct devices:</p> <p></p>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#step-2-ensure-riva-server-is-running","title":"Step 2: Ensure Riva server is running","text":"<p>The Riva server must be running to use the LLM pipeline. If it is already running you can skip this step.</p> <pre><code>cd &lt;riva install dir&gt;\nbash riva_start.sh\n</code></pre>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#step-3-launch-and-run-the-app","title":"Step 3: Launch and Run the App","text":"","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#step-31","title":"Step 3.1","text":"<p>Launch the <code>holohub:lmm</code> container:</p> <pre><code>sudo ./holohub run-container lmm --add-volume &lt;your_model_dir&gt;\n</code></pre> <ul> <li>Note, if the parent directory of  is not <code>/models</code> you must update the asr_llm_tts.yaml and ehr.yaml files with the complete path to your model inside the container. You will also need to update the run_lmm.sh so the correct directory is exported in the <code>set_transformer_cache()</code> function. (You can determine these paths by looking in <code>/workspace/volumes</code> inside the launched container) or you can use the following <code>sed</code> commands: <p><code>sed -i -e 's#^model_path:.*#model_path: /workspace/volumes/&lt;your_model_dir&gt;#' asr_llm_tts.yaml</code></p> <p><code>sed -i -e 's#^model_path:.*#model_path: /workspace/volumes/&lt;your_model_dir&gt;#' agents_configs/ehr.yaml</code></p> <p><code>sed -i -e 's#^export TRANSFORMERS_CACHE=.*#export TRANSFORMERS_CACHE=\"/workspace/volumes/&lt;your_model_dir&gt;\"#' run_lmm.sh</code></p>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#step-32","title":"Step 3.2","text":"<p>Then run the application:</p> <pre><code>./applications/ehr_query_llm/lmm/run_lmm.sh\n</code></pre> <p>This command builds ehr_query_llm/lmm, starts an LLM api server, then launches the ehr_query_llm app. Access the web interface at <code>https://127.0.0.1:8080</code>. Llama.cpp LLM server output is redirected to <code>./applications/ehr_query_llm/lmm/llama_cpp.log/</code>.</p> <p>To interact with ehr_query_llm using voice input:</p> <ul> <li>Press and hold the space bar to activate the voice recognition feature.</li> <li>Speak your query or command clearly while maintaining pressure on the space bar.</li> <li>Release the space bar when you've finished speaking to signal the end of your input.</li> <li>ehr_query_llm will then process your speech and generate a response.</li> </ul> <p>\u26a0\ufe0f Note: When running via VNC, you must have your keyboard focus on the VNC terminal that you are using to run ehr_query_llm in order to use the push-to-talk feature.</p>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#stopping-instructions","title":"Stopping Instructions","text":"<p>To stop the main app, simply use <code>ctrl+c</code></p> <p>To stop Riva server:</p> <pre><code>bash &lt;Riva_install_dir&gt;riva_stop.sh\n</code></pre>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#asr_to_llm-application-arguments","title":"ASR_To_LLM Application arguments","text":"<p>The <code>asr_llm_tts.py</code> can receive the following optional cli argument:</p> <p><code>--sample-rate-hz</code>: The number of frames per second in audio streamed from the selected microphone.</p>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#offline-mode","title":"Offline mode","text":"<p>To enable offline use (no internet connection required):</p> <ol> <li>First run the complete application as-is (This ensures all relevant models are downloaded)</li> <li>Uncomment <code>set_offline_flags</code> at line 52 of run_lmm.sh</li> </ol>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#troubleshooting","title":"Troubleshooting","text":"","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#adding-agents","title":"Adding Agents","text":"<p>An Agent is an LLM (or LMM) with a task specific \"persona\" - such as a EHRAgent, etc., each with their own specific task. They also have a specific prompt tailored to complete that task, pre-fix prompts specific to the model used, grammar to constrain output, as well as context length.</p> <p>The AgentFrameworkOp works by using a SelectorAgent to select which Agent should be called upon based on user input.</p> <p>Adding a new \"agent\" for ehr_query_llm involves creating a new agent .py and YAML file in the <code>agents</code> directory, and in the new .py inheriting the Agent base class <code>agents/base_agent.py</code>.</p> <p>When creating a new agent .py file, you will need to define:</p> <p>Agent name: A class name which will also need to be added to the selector agent YAML, so it knows the agent is available to be called. process_request: A runtime method describing the logic of how an agent should carry out its task and send a response.</p> <p>For the YAML file, the fields needed are:</p> <p>name: This is the name of the agent, as well as what is used as the ZeroMQ topic when the agent  publishes its output. So you must make sure your listener is using this as the topic.</p> <p>user_prefix, bot_prefix, bot_rule_prefix, end_token:: These are dependent on the particular llm or lmm being used, and help to set the correct template for the model to interact with.</p> <p>agent_prompt: This gives the agent its \"persona\" - how it should behave, and for what purpose. It should have as much context as possible.</p> <p>ctx_length: Context length for the model. This determines how much output the agent is capable of generating. Smaller values lead to faster to first token time, but can be at the sacrifice of detail and verbosity.</p> <p>grammar: This is the BNF grammar used to constrain the models output. It can be a bit tricky to write. ChatGPT is great at writing these grammars for you if you give an example JSON of what you want. Also helpful, is the Llama.cpp BNF grammar guide.</p> <p>publish: The only important part of this field is the \"ags\" sub-field. This should be a list of your arg names. This is important as this is used as the list of keys to pull the relevant args from the LMM's response, and thus ensure the relevant fields are complete for a given tool use.</p> <p>For a specific example, please refer to the EHR Agent YAML file below:</p> <pre><code>description: This tool is used to search the patient's EHR in order to answer questions about the patient, or general questions about the patient's medical history.\nuser_prefix: \"&lt;|im_start|&gt;user\"\nbot_prefix: \"&lt;|im_start|&gt;assistant\"\nbot_rule_prefix: \"&lt;|im_start|&gt;system\"\nend_token: \"&lt;|im_end|&gt;\"\nagent_prompt: |\n  You are NVIDIA's EHR Agent,your job is to assist surgeons as they prepare for surgery.\n  You are an expert when it comes to surgery and medical topics - and answer all questions to the best of your abilities.\n  The patient has signed consent for you to access and discuss all of their electronic records.\n  Be as concise as you can be with your answers.\n\n  You NEVER make-up information that isn't grounded in the provided medical documents.\n\n  If applicable, include the relevant date (use sparingly)\n  The following medical documents may be helpful to answer the surgeon's question:\n  {documents}\n  Use your expert knowledge and the above context to answer the surgeon.\n# This is the request that the LLM replies to, where '{text}' indicates where the transcribed\n# text is inserted into the prompt\nrequest: \"{text}\"\n\nctx_length: 256\n\ngrammar: |\n  space ::= \" \"?\n  string ::= \"\\\"\" ([^\"\\\\])* \"\\\"\" space \n  root ::= \"{\" space \"\\\"name\\\"\" space \":\" space \"\\\"EHRAgent\\\"\" space \",\" space \"\\\"response\\\"\" space \":\" space string \"}\" space\n\npublish:\n  ags:\n    - \"response\"\n</code></pre> <p>With a complete YAML file, an agent should be able to use any new tool effectively. The only remaining step is ensure you have a ZeroMQ listener in the primary app with a topic that is the same as the tool's name.</p> <p>The <code>AgentFrameworkOp</code> is based on a ZeroMQ publish/subscribe pattern to send and receive messages from the Message Bus. It uses the <code>MessageReceiver</code> and <code>MessageSender</code> classes implemented in the <code>message_handling.py</code> Python script. The <code>MessageSender</code> creates a ZeroMQ PUB socket that binds to port 5555, accepts connections from any interface, and is used to broadcast messages and commands from the agent framework. It uses <code>send_json()</code> to send JSON-encoded messages with topics. A 0.1-second sleep on initialization prevents the \"slow joiner\" problem where early messages might be lost. The `MessageReceiver`` creates a ZeroMQ SUB socket connecting to port 5560 and uses receive_json() to get messages, with configurable blocking behavior.</p>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#ehr-rag","title":"EHR RAG","text":"<p>To test new document formats for the database use test_db.py This will start the current Vector database in <code>./rag/ehr/db</code> and allow you to test different queries via the CLI to see what documents are returned.</p> <p>When changing the Vector DB, remove the previous database first:</p> <pre><code>rm -rf ./rag/ehr/db\n</code></pre>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/ehr_query_llm/lmm/#riva-cant-find-speaker-to-use","title":"Riva - Can't find speaker to use","text":"<p>This usually means that some process is using the speaker you wish to use. This could be a Riva process that didn't exit correctly, or even Outlook loaded in your browser using your speakers to play notification sounds.</p> <p>First see what processes are using your speakers:</p> <pre><code>pactl list sink-inputs | grep -E 'Sink Input|application.name|client|media.name|sink: '\n</code></pre> <p>Sometimes that will give you all the information you need to kill the process responsible. If not, and the process has unfamiliar name such as \"speech-dispatcher-espeak-ng\" then find the responsible process ID:</p> <pre><code>pgrep -l -f &lt;grep expression here (ex: 'speech')&gt;\n</code></pre> <p>Once you know the PID's of the responsible process, kill them :)</p> <pre><code>kill &lt;PID&gt;\n</code></pre>","tags":["Healthcare AI","Audio","ZeroMQ","ASR","Healthcare Interop","RAG","Vector Database"]},{"location":"applications/endoscopy_depth_estimation/","title":"Endoscopy Depth Estimation","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 3.3.0 Tested Holoscan SDK versions: 3.5.0 Contribution metric: Level 2 - Trusted</p> <p>This application demonstrates the use of custom components for depth estimation and its rendering using Holoviz with triangle interpolation.</p> <p></p>","tags":["Healthcare AI","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_depth_estimation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8+</li> <li>OpenCV 4.8+</li> </ul>","tags":["Healthcare AI","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_depth_estimation/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for Endoscopy</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Healthcare AI","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_depth_estimation/#model","title":"Model","text":"<p>\ud83d\udce6\ufe0f (NGC) App Model for AI-based Endoscopy Depth Estimation</p> <p>The model is automatically downloaded to the same folder as the data in ONNX format.</p>","tags":["Healthcare AI","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_depth_estimation/#opencv-gpu","title":"OpenCV-GPU","text":"<p>This application uses OpenCV with GPU acceleration during the preprocessing stage when it runs with Histogram Equalization (flag <code>--clahe</code> or <code>-c</code>). Histogram equalization reduces the effect of specular reflections and improves the visual performance of the depth estimation overall. However, using regular OpenCV datatypes leads to unnecessary I/O operations to transfer data from Holoscan Tensors to the CPU and back. We show in this application how to blend together Holoscan Tensors and OpenCV's <code>GPUMat</code> datatype to get rid of this issue in the <code>CUDACLAHEOp</code> operator.  Compare it to <code>CPUCLAHEOp</code> for reference.</p> <p>To achieve an end-to-end GPU accelerated pipeline / application, the pre-processing operators shall support accessing the GPU memory (Holoscan Tensor)  directly without memory copy / movement in Holoscan SDK. This means that only libraries which implement the <code>__cuda_array_interface__</code>  and DLPack standards allow conversion from/to Holoscan Tensor, such as cuCIM. OpenCV, however, does not implement neither the <code>__cuda_array_interface__</code> nor the standard DLPack, and a little work is needed yet to use this library.</p> <p>First, we convert CuPy arrays to GPUMat using a fix in OpenCV only available from 4.8.0 on. More information here. This is done in the <code>gpumat_from_cp_array</code> function. With a <code>GPUMat</code>, we can now use any OpenCV-CUDA operations. Once the <code>GPUMat</code> processing has finished, we have to convert it back to a CuPy tensor with <code>gpumat_to_cupy</code>. </p> <p>Important: In order to run this application with CUDA acceleration, one must compile OpenCV with CUDA support. We provide a sample Dockerfile to build a container based on Holoscan v2.1.0 with the latest version of OpenCV and CUDA support. In case you use it, note that the variable <code>CUDA_ARCH_BIN</code>  must be modified according to your specific GPU configuration. Refer to this site to find out your NVIDIA GPU architecture.</p>","tags":["Healthcare AI","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_depth_estimation/#workflows","title":"Workflows","text":"<p>This application can be run with or without Histogram Equalization (CLAHE) by toggling the label <code>--clahe</code>.</p>","tags":["Healthcare AI","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_depth_estimation/#with-clahe","title":"With CLAHE","text":"<p> Fig. 1 Depth Estimation Application with CLAHE enabled</p> <p>The pipeline uses a recorded endoscopy video file (generated by <code>convert_video_to_gxf_entities</code> script) for input frames. Each input frame in the file is loaded by Video Stream Replayer and passed to the following two branches: - In the first branch (top), the input frames are passed to the <code>CUDACLAHEOp</code>,  then fed to the Format Converter to convert their data type from <code>uint8</code> to <code>float32</code>, and finally fed to the <code>InferenceOp</code>. The result is then ingested by the <code>DepthPostProcessingOp</code>, which converts the depth map to <code>uint8</code> and reorders its dimensions for rendering with Holoviz. - In the second branch (bottom), the input frames are passed to a Format Converter that resizes them. Its output is finally fed to the <code>DepthPostProcessingOp</code> for  rendering with Holoviz.</p>","tags":["Healthcare AI","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_depth_estimation/#without-clahe","title":"Without CLAHE","text":"<p> Fig. 2 Depth Estimation Application with CLAHE disabled</p> <p>The pipeline uses a recorded endoscopy video file (generated by <code>convert_video_to_gxf_entities</code> script) for input frames. Each input frame in the file is loaded by Video Stream Replayer and passed to a branch that firstly converts its data type to <code>float32</code> and resizes it with a Format Converter. Then, the preprocessed frames are fed to the <code>InferenceOp</code> and mixed with the original video in the custom <code>DepthPostProcessingOp</code> for rendering with Holoviz.</p>","tags":["Healthcare AI","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_depth_estimation/#run-instructions","title":"Run Instructions","text":"<p>To run this application, you'll need to configure your PYTHONPATH environment variable to locate the necessary python libraries based on your Holoscan SDK installation type.</p> <p>You should refer to the glossary for the terms defining specific locations within HoloHub.</p> <p>If your Holoscan SDK installation type is:</p> <ul> <li>python wheels:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <ul> <li>otherwise:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOSCAN_INSTALL_DIR&gt;/python/lib:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <p>This application should be run in the build directory of Holohub in order to load the GXF extensions. Alternatively, the relative path of the extensions in the corresponding yaml file can be modified to match path of the working directory.</p> <p>Next, run the command to run the application:</p> <pre><code>cd &lt;HOLOHUB_BUILD_DIR&gt;\npython3 &lt;HOLOHUB_SOURCE_DIR&gt;/applications/endoscopy_depth_estimation/endoscopy_depth_estimation.py --data=&lt;DATA_DIR&gt; --model=&lt;MODEL_DIR&gt; --clahe\n</code></pre>","tags":["Healthcare AI","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_depth_estimation/#container-build-run-instructions","title":"Container Build &amp; Run Instructions","text":"<p>Build container using Holoscan 2.0.0 NGC container as base image and built OpenCV with CUDA ARCH 8.6, 8.7 and 8.9 support for IGX Orin and Ampere and Ada Lovelace Architecture dGPUs. This application is currently not supported on iGPU.</p>","tags":["Healthcare AI","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_depth_estimation/#change-directory-to-holohub-source-directory","title":"Change directory to Holohub source directory","text":"<pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;\n</code></pre>","tags":["Healthcare AI","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_depth_estimation/#build-and-run-the-application-using-the-development-container","title":"Build and run the application using the development container","text":"<pre><code>./holohub run endoscopy_depth_estimation\n</code></pre>","tags":["Healthcare AI","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_depth_estimation/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode endoscopy_depth_estimation\n</code></pre> <p>This command will build and configure a Dev Container using a Dockerfile that is ready to run the application.</p>","tags":["Healthcare AI","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_depth_estimation/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"<p>There are two launch profiles configured for this application:</p> <ol> <li>(debugpy) endoscopy_depth_estimation/python: Launch endoscopy_depth_estimation using a launch profile that enables debugging of Python code.</li> <li>(pythoncpp) endoscopy_depth_estimation/python: Launch endoscopy_depth_estimation using a launch profile that enables debugging of Python and C++ code.</li> </ol>","tags":["Healthcare AI","Endoscopy","Monocular Depth Estimation","CV CUDA","Video","Rendering"]},{"location":"applications/endoscopy_out_of_body_detection/","title":"Endoscopy Out of Body Detection","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: February 4, 2026 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.9.0, 3.10.0, 3.11.0 Contribution metric: Level 1 - Highly Reliable</p> <p></p>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/#overview","title":"Overview","text":"<p>This application performs real-time detection of whether an endoscope is inside or outside the body during endoscopic procedures. For each input frame, the application:</p> <ul> <li>Classifies the frame as either \"in-body\" or \"out-of-body\"</li> <li>Provides a confidence score for the classification</li> <li>Outputs either to the console or to a CSV file (when analytics is enabled)</li> </ul> <p>Note: This application does not include visualization components.</p>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/#quick-start","title":"Quick Start","text":"<p>Run the following command to build and launch the application on a supported Holoscan platform:</p> <pre><code>./holohub run endoscopy_out_of_body_detection --language=&lt;cpp/python&gt;\n</code></pre>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/#prerequisites","title":"Prerequisites","text":"<ul> <li>NVIDIA Holoscan SDK (version 0.5 or higher)</li> <li>A supported Holoscan platform or workstation with a CUDA-capable NVIDIA GPU</li> <li>CMake build system</li> <li>FFmpeg (for data conversion)</li> </ul>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/#data-requirements","title":"Data Requirements","text":"","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/#model-and-sample-data","title":"Model and Sample Data","text":"<p>The endoscopy detection model and sample datasets are available on NGC. The package includes:</p> <ul> <li>Pre-trained ONNX model for out-of-body detection: <code>out_of_body_detection.onnx</code></li> <li>Sample endoscopy video clips (MP4 format): <code>sample_clip_out_of_body_detection.mp4</code></li> </ul>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/#data-preparation-optional","title":"Data Preparation (optional)","text":"<p>The application requires the input videos to be converted to GXF tensor format. This conversion happens automatically during building, but manual conversion can be done following these steps:</p> <ol> <li> <p>Download and extract the data:</p> <pre><code>unzip [NGC_DOWNLOAD].zip -d &lt;data_dir&gt;\n</code></pre> </li> <li> <p>Convert the video to GXF tensor format using the provided script:</p> <pre><code>ffmpeg -i &lt;INPUT_VIDEO_FILE&gt; -fs 900M -pix_fmt rgb24 -f rawvideo pipe:1 | \\\npython convert_video_to_gxf_entities.py --width 256 --height 256 --channels 3 --framerate 30\n</code></pre> <p>Note: The conversion script (<code>convert_video_to_gxf_entities.py</code>) is available in the Holoscan SDK repository.</p> </li> <li> <p>Organize the data directory as follows:</p> <pre><code>data/\n\u2514\u2500\u2500 endoscopy_out_of_body_detection/\n  \u251c\u2500\u2500 LICENSE.md\n  \u251c\u2500\u2500 out_of_body_detection.onnx\n  \u251c\u2500\u2500 sample_clip_out_of_body_detection.gxf_entities\n  \u251c\u2500\u2500 sample_clip_out_of_body_detection.gxf_index\n  \u2514\u2500\u2500 sample_clip_out_of_body_detection.mp4\n</code></pre> </li> </ol>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/#configuration","title":"Configuration","text":"<p>The application uses <code>endoscopy_out_of_body_detection.yaml</code> for configuration. Key settings include:</p> <ul> <li>Input video parameters in the <code>replayer</code> section</li> <li>Model parameters in the <code>inference</code> section</li> <li>Analytics settings for data export</li> </ul>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/#building","title":"Building","text":"<pre><code>./holohub build-container endoscopy_out_of_body_detection\n./holohub run-container endoscopy_out_of_body_detection\n./holohub build endoscopy_out_of_body_detection --language=&lt;cpp/python&gt;\n./holohub run endoscopy_out_of_body_detection --language=&lt;cpp/python&gt;\n</code></pre> <p>For more information, see the Holohub README.md.</p>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/#running-the-application","title":"Running the Application","text":"","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/#basic-usage","title":"Basic Usage","text":"<p>From your build directory:</p> <p>C++:</p> <pre><code>applications/endoscopy_out_of_body_detection/endoscopy_out_of_body_detection \\\n  --config endoscopy_out_of_body_detection.yaml \\\n  --data ../data/endoscopy_out_of_body_detection\n</code></pre> <p>Python:</p> <pre><code>applications/endoscopy_out_of_body_detection/endoscopy_out_of_body_detection.py \\\n  --config endoscopy_out_of_body_detection.yaml \\\n  --data ../data/endoscopy_out_of_body_detection\n</code></pre>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/#analytics-mode","title":"Analytics Mode","text":"<p>To enable analytics and export results to CSV:</p> <ol> <li>Set <code>enable_analytics: true</code> in the configuration file:</li> </ol> <pre><code># endoscopy_out_of_body_detection.yaml\nenable_analytics: true\n</code></pre> <ol> <li>Configure analytics output (optional):</li> </ol> <pre><code># Set output directory (default: current directory)\nexport HOLOSCAN_ANALYTICS_DATA_DIRECTORY=\"/path/to/output\"\n\n# Set output filename (default: data.csv)\nexport HOLOSCAN_ANALYTICS_DATA_FILE_NAME=\"results.csv\"\n</code></pre> <p>The application will create:</p> <ul> <li>A directory named after the application</li> <li>Subdirectories with timestamps for each run</li> <li>CSV files containing frame-by-frame classification results</li> </ul>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/#output-format","title":"Output Format","text":"<ul> <li> <p>Console Mode: Displays \"Likely in-body\" or \"Likely out-of-body\" along with confidence scores for each frame.</p> </li> <li> <p>Analytics Mode: Outputs a CSV file with frame-by-frame classification results in the following format:</p> </li> </ul> <pre><code>In-body,Out-of-body,Confidence Score\n1,0,0.972432\n1,0,0.902066\n</code></pre>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/","title":"Endoscopy Out of Body Detection","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ (see Python version) Last modified: February 4, 2026 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.9.0, 3.10.0, 3.11.0 Contribution metric: Level 1 - Highly Reliable</p> <p></p>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#overview","title":"Overview","text":"<p>This application performs real-time detection of whether an endoscope is inside or outside the body during endoscopic procedures. For each input frame, the application:</p> <ul> <li>Classifies the frame as either \"in-body\" or \"out-of-body\"</li> <li>Provides a confidence score for the classification</li> <li>Outputs either to the console or to a CSV file (when analytics is enabled)</li> </ul> <p>Note: This application does not include visualization components.</p>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#quick-start","title":"Quick Start","text":"<p>Run the following command to build and launch the application on a supported Holoscan platform:</p> <pre><code>./holohub run endoscopy_out_of_body_detection --language cpp\n</code></pre>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#prerequisites","title":"Prerequisites","text":"<ul> <li>NVIDIA Holoscan SDK (version 0.5 or higher)</li> <li>A supported Holoscan platform or workstation with a CUDA-capable NVIDIA GPU</li> <li>CMake build system</li> <li>FFmpeg (for data conversion)</li> </ul>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#data-requirements","title":"Data Requirements","text":"","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#model-and-sample-data","title":"Model and Sample Data","text":"<p>The endoscopy detection model and sample datasets are available on NGC. The package includes:</p> <ul> <li>Pre-trained ONNX model for out-of-body detection: <code>out_of_body_detection.onnx</code></li> <li>Sample endoscopy video clips (MP4 format): <code>sample_clip_out_of_body_detection.mp4</code></li> </ul>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#data-preparation-optional","title":"Data Preparation (optional)","text":"<p>The application requires the input videos to be converted to GXF tensor format. This conversion happens automatically during building, but manual conversion can be done following these steps:</p> <ol> <li> <p>Download and extract the data:</p> <pre><code>unzip [NGC_DOWNLOAD].zip -d &lt;data_dir&gt;\n</code></pre> </li> <li> <p>Convert the video to GXF tensor format using the provided script:</p> <pre><code>ffmpeg -i &lt;INPUT_VIDEO_FILE&gt; -fs 900M -pix_fmt rgb24 -f rawvideo pipe:1 | \\\npython convert_video_to_gxf_entities.py --width 256 --height 256 --channels 3 --framerate 30\n</code></pre> <p>Note: The conversion script (<code>convert_video_to_gxf_entities.py</code>) is available in the Holoscan SDK repository.</p> </li> <li> <p>Organize the data directory as follows:</p> <pre><code>data/\n\u2514\u2500\u2500 endoscopy_out_of_body_detection/\n  \u251c\u2500\u2500 LICENSE.md\n  \u251c\u2500\u2500 out_of_body_detection.onnx\n  \u251c\u2500\u2500 sample_clip_out_of_body_detection.gxf_entities\n  \u251c\u2500\u2500 sample_clip_out_of_body_detection.gxf_index\n  \u2514\u2500\u2500 sample_clip_out_of_body_detection.mp4\n</code></pre> </li> </ol>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#configuration","title":"Configuration","text":"<p>The application uses <code>endoscopy_out_of_body_detection.yaml</code> for configuration. Key settings include:</p> <ul> <li>Input video parameters in the <code>replayer</code> section</li> <li>Model parameters in the <code>inference</code> section</li> <li>Analytics settings for data export</li> </ul>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#building-and-running-the-application","title":"Building and running the application","text":"<pre><code>./holohub run endoscopy_out_of_body_detection --language cpp\n</code></pre> <p>It builds and starts a Docker container, and then builds and runs the application inside the container.</p> <p>For more information, see the Holohub README.md.</p>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#running-in-development-mode","title":"Running in development mode","text":"<p>You can also run the application with customized arguments, you can use Holohub CLI for creating and starting the Holohub container, and then building and running the application inside the container as follows:</p> <ol> <li> <p>Create and start the Holohub container:</p> <pre><code>./holohub run-container endoscopy_out_of_body_detection\n</code></pre> </li> <li> <p>Build the application:</p> <p>Once in the docker container, you can build the application by running the following command:</p> <pre><code>./holohub build endoscopy_out_of_body_detection --language python\n</code></pre> </li> <li> <p>Run the application:</p> <p>After building the application, you can run it from your build directory with the following command for the basic usage and can modify the arguments as needed:</p> <pre><code>applications/endoscopy_out_of_body_detection/endoscopy_out_of_body_detection \\\n  --config endoscopy_out_of_body_detection.yaml \\\n  --data ../data/endoscopy_out_of_body_detection\n</code></pre> </li> </ol>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#analytics-mode","title":"Analytics Mode","text":"<p>To enable analytics and export results to CSV:</p> <ol> <li>Set <code>enable_analytics: true</code> in the configuration file:</li> </ol> <pre><code># endoscopy_out_of_body_detection.yaml\nenable_analytics: true\n</code></pre> <ol> <li>Configure analytics output (optional):</li> </ol> <pre><code># Set output directory (default: current directory)\nexport HOLOSCAN_ANALYTICS_DATA_DIRECTORY=\"/path/to/output\"\n\n# Set output filename (default: data.csv)\nexport HOLOSCAN_ANALYTICS_DATA_FILE_NAME=\"results.csv\"\n</code></pre> <p>The application will create:</p> <ul> <li>A directory named after the application</li> <li>Subdirectories with timestamps for each run</li> <li>CSV files containing frame-by-frame classification results</li> </ul>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/cpp/#output-format","title":"Output Format","text":"<ul> <li> <p>Console Mode: Displays \"Likely in-body\" or \"Likely out-of-body\" along with confidence scores for each frame.</p> </li> <li> <p>Analytics Mode: Outputs a CSV file with frame-by-frame classification results in the following format:</p> </li> </ul> <pre><code>In-body,Out-of-body,Confidence Score\n1,0,0.972432\n1,0,0.902066\n</code></pre>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/python/","title":"Endoscopy Out of Body Detection","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ (see C++ version) Last modified: February 4, 2026 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.9.0, 3.10.0, 3.11.0 Contribution metric: Level 1 - Highly Reliable</p> <p></p>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/python/#overview","title":"Overview","text":"<p>This application performs real-time detection of whether an endoscope is inside or outside the body during endoscopic procedures. For each input frame, the application:</p> <ul> <li>Classifies the frame as either \"in-body\" or \"out-of-body\"</li> <li>Provides a confidence score for the classification</li> <li>Outputs either to the console or to a CSV file (when analytics is enabled)</li> </ul> <p>Note: This application does not include visualization components.</p>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/python/#quick-start","title":"Quick Start","text":"<p>Run the following command to build and launch the application on a supported Holoscan platform:</p> <pre><code>./holohub run endoscopy_out_of_body_detection --language python\n</code></pre>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/python/#prerequisites","title":"Prerequisites","text":"<ul> <li>NVIDIA Holoscan SDK (version 0.5 or higher)</li> <li>A supported Holoscan platform or workstation with a CUDA-capable NVIDIA GPU</li> <li>CMake build system</li> <li>FFmpeg (for data conversion)</li> </ul>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/python/#data-requirements","title":"Data Requirements","text":"","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/python/#model-and-sample-data","title":"Model and Sample Data","text":"<p>The endoscopy detection model and sample datasets are available on NGC. The package includes:</p> <ul> <li>Pre-trained ONNX model for out-of-body detection: <code>out_of_body_detection.onnx</code></li> <li>Sample endoscopy video clips (MP4 format): <code>sample_clip_out_of_body_detection.mp4</code></li> </ul>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/python/#data-preparation-optional","title":"Data Preparation (optional)","text":"<p>The application requires the input videos to be converted to GXF tensor format. This conversion happens automatically during building, but manual conversion can be done following these steps:</p> <ol> <li> <p>Download and extract the data:</p> <pre><code>unzip [NGC_DOWNLOAD].zip -d &lt;data_dir&gt;\n</code></pre> </li> <li> <p>Convert the video to GXF tensor format using the provided script:</p> <pre><code>ffmpeg -i &lt;INPUT_VIDEO_FILE&gt; -fs 900M -pix_fmt rgb24 -f rawvideo pipe:1 | \\\npython convert_video_to_gxf_entities.py --width 256 --height 256 --channels 3 --framerate 30\n</code></pre> <p>Note: The conversion script (<code>convert_video_to_gxf_entities.py</code>) is available in the Holoscan SDK repository.</p> </li> <li> <p>Organize the data directory as follows:</p> <pre><code>data/\n\u2514\u2500\u2500 endoscopy_out_of_body_detection/\n  \u251c\u2500\u2500 LICENSE.md\n  \u251c\u2500\u2500 out_of_body_detection.onnx\n  \u251c\u2500\u2500 sample_clip_out_of_body_detection.gxf_entities\n  \u251c\u2500\u2500 sample_clip_out_of_body_detection.gxf_index\n  \u2514\u2500\u2500 sample_clip_out_of_body_detection.mp4\n</code></pre> </li> </ol>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/python/#configuration","title":"Configuration","text":"<p>The application uses <code>endoscopy_out_of_body_detection.yaml</code> for configuration. Key settings include:</p> <ul> <li>Input video parameters in the <code>replayer</code> section</li> <li>Model parameters in the <code>inference</code> section</li> <li>Analytics settings for data export</li> </ul>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/python/#building-and-running-the-application","title":"Building and running the application","text":"<p>You can simply run the application with the following command:</p> <pre><code>./holohub run endoscopy_out_of_body_detection --language python\n</code></pre> <p>It builds and starts a Docker container, and then builds and runs the application inside the container.</p> <p>For more information, see the Holohub README.md.</p>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/python/#running-in-development-mode","title":"Running in development mode","text":"<p>You can also run the application with customized arguments, you can use Holohub CLI for creating and starting the Holohub container, and then building and running the application inside the container as follows:</p> <ol> <li> <p>Create and start the Holohub container:</p> <pre><code>./holohub run-container endoscopy_out_of_body_detection\n</code></pre> </li> <li> <p>Build the application:</p> <p>Once in the docker container, you can build the application by running the following command:</p> <pre><code>./holohub build endoscopy_out_of_body_detection --language python\n</code></pre> </li> <li> <p>Run the application:</p> <p>After building the application, you can run it from your build directory with the following command for the basic usage and can modify the arguments as needed:</p> <pre><code>applications/endoscopy_out_of_body_detection/endoscopy_out_of_body_detection.py \\\n  --config endoscopy_out_of_body_detection.yaml \\\n  --data ../data/endoscopy_out_of_body_detection\n</code></pre> </li> </ol>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/python/#analytics-mode","title":"Analytics Mode","text":"<p>To enable analytics and export results to CSV:</p> <ol> <li>Set <code>enable_analytics: true</code> in the configuration file:</li> </ol> <pre><code># endoscopy_out_of_body_detection.yaml\nenable_analytics: true\n</code></pre> <ol> <li>Configure analytics output (optional):</li> </ol> <pre><code># Set output directory (default: current directory)\nexport HOLOSCAN_ANALYTICS_DATA_DIRECTORY=\"/path/to/output\"\n\n# Set output filename (default: data.csv)\nexport HOLOSCAN_ANALYTICS_DATA_FILE_NAME=\"results.csv\"\n</code></pre> <p>The application will create:</p> <ul> <li>A directory named after the application</li> <li>Subdirectories with timestamps for each run</li> <li>CSV files containing frame-by-frame classification results</li> </ul>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_out_of_body_detection/python/#output-format","title":"Output Format","text":"<ul> <li> <p>Console Mode: Displays \"Likely in-body\" or \"Likely out-of-body\" along with confidence scores for each frame.</p> </li> <li> <p>Analytics Mode: Outputs a CSV file with frame-by-frame classification results in the following format:</p> </li> </ul> <pre><code>In-body,Out-of-body,Confidence Score\n1,0,0.972432\n1,0,0.902066\n</code></pre>","tags":["Healthcare AI","Video","AJA","Endoscopy","Classification","Computer Vision and Perception"]},{"location":"applications/endoscopy_tool_tracking/","title":"Endoscopy Tool Tracking","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: February 4, 2026 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3, 2.0.0, 2.1.0, 2.2.0, 3.10.0, 3.11.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates real-time AI-powered tool detection and tracking in endoscopic video streams.</p>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/#overview","title":"Overview","text":"<p>Digital endoscopy is a key technology for medical screenings and minimally invasive surgeries. Using real-time AI workflows to process and analyze the video signal produced by the endoscopic camera, this technology helps medical professionals with anomaly detection and measurements, image enhancements, alerts, and analytics.</p> <p> Fig. 1 Endoscopy (laparoscopy) image from a cholecystectomy (gallbladder removal surgery) showing AI-powered frame-by-frame tool identification and tracking. Image courtesy of Research Group Camma, IHU Strasbourg and the University of Strasbourg (NGC Resource)</p> <p>The Endoscopy tool tracking application provides an example of how an endoscopy data stream can be captured and processed using the C++ or Python APIs on multiple hardware platforms.</p>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/#video-stream-replayer-input","title":"Video Stream Replayer Input","text":"<p> Fig. 2 Tool tracking application workflow with replay from file</p> <p>The pipeline uses a recorded endoscopy video file (generated by <code>convert_video_to_gxf_entities</code> script) for input frames. Each input frame in the file is loaded by Video Stream Replayer and passed to the following two branches:</p> <ul> <li>In the first branch, the input frames are directly passed to Holoviz for rendering in the background.</li> <li>In the second branch, the frames go through the Format Converter to convert the data type of the image from <code>uint8</code> to <code>float32</code> before it is fed to the tool tracking model (with Custom TensorRT Inference). The result is then ingested by the Tool Tracking Postprocessor which extracts the masks, points, and text from the inference output, before Holoviz renders them as overlays.</li> </ul> <p>The pipeline graph also defines an optional Video Stream Recorder that can be enabled to record the original video stream to disk (<code>record_type: 'input'</code>), or the final render by Holoviz (<code>record_type: 'visualizer'</code>) after going from <code>RGBA8888</code> to <code>RGB888</code> using a Format Converter. Recording is disabled by default (<code>record_type: 'none'</code>) in order to maximize performance.</p>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/#aja-card-input","title":"AJA Card input","text":"<p> Fig. 3 Tool tracking application workflow with input from AJA video source</p> <p>The pipeline is similar to the one using the recorded video, with the exceptions below:</p> <ul> <li>the input source is replaced with AJA Source (pixel format is <code>RGBA8888</code> with a resolution of 1920x1080)</li> <li>the Format Converter in the inference pipeline is configured to also resize the image, and convert to <code>float32</code> from <code>RGBA8888</code></li> <li>the Format Converter in the recording pipeline is used for <code>record_type: INPUT</code> also</li> </ul>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/#building-with-aja-support","title":"Building with AJA support","text":"<pre><code>./holohub build --local endoscopy_tool_tracking --build-with=\"aja_source\"\n</code></pre>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/#hardware-keying","title":"Hardware keying","text":"<p>For AJA cards that support Hardware Keying, you can use the <code>endoscopy_tool_tracking_aja_overlay.yaml</code> config file to overlay the segmentation results on the input video on the AJA card FPGA. The overlay layer is sent from Holoviz back to the AJA Source operator which handles the alpha blending and outputs it to a port of the the AJA card. The blended image is also sent back to the Holoviz operator (instead of the input video only) for rendering the same image buffer.</p>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/#using-vtk-for-rendering","title":"Using VTK for rendering","text":"<p>The tool tracking application can use the VTK library to render the tool tracking results on top of the endoscopy video frames. The VTK library is a powerful open-source software system for 3D computer graphics, image processing, and visualization. The VTK library provides a wide range of functionalities for rendering, including 2D and 3D graphics, image processing, and visualization. The tool tracking application uses VTK to render the tool tracking results on top of the endoscopy video frames.</p>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/#how-to-build-and-run-the-endoscopy-tool-tracking-application-with-vtk","title":"How to build and run the Endoscopy Tool Tracking application with VTK","text":"<p>The following command builds and runs the Endoscopy Tool Tracking application with VTK:</p> <pre><code># change the configuration to use VTK (vtk_renderer) as the default renderer\nsed -i -e 's#^visualizer:.*#visualizer: \"vtk\"#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking.yaml\n\n# build and launch the application\n# C++\n./holohub run endoscopy_tool_tracking --build-with=\"vtk_renderer\" --docker-file=\"operators/vtk_renderer/Dockerfile\" --img holohub:endoscopy_tool_tracking_vtk --language=\"cpp\"\n\n# Python (see below for additional steps)\n./holohub run endoscopy_tool_tracking --build-with=\"vtk_renderer\" --docker-file=\"operators/vtk_renderer/Dockerfile\" --img holohub:endoscopy_tool_tracking_vtk --language=\"python\"\n</code></pre> <p>Arguments:</p> <ul> <li><code>--build-with</code> : instructs the script to build the application with the <code>vtk_renderer</code> operator</li> <li><code>--docker-file</code>: instructs the script to use the <code>operators/vtk_renderer/Dockerfile</code> that includes VTK libraries</li> </ul>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode\n</code></pre>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/#c","title":"C++","text":"<p>Use the <code>(gdb) endoscopy_tool_tracking/cpp</code> launch profile to start and debug the application.</p>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/#python","title":"Python","text":"<p>There are a two launch profiles configured for this application:</p> <ol> <li>(debugpy) endoscopy_tool_tracking/python: This launch profile enables debugging of Python code.</li> <li>(pythoncpp) endoscopy_tool_tracking/python: This launch profile enables debugging of Python and C++ code.</li> </ol>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/#containerize-the-application","title":"Containerize the application","text":"<p>To containerize the application using Holoscan CLI, first build the application using <code>./holohub install endoscopy_tool_tracking</code>, run the <code>package-app.sh</code> script in the cpp or the python directory and then follow the generated output to package and run the application.</p> <p>Refer to the Packaging Holoscan Applications section of the Holoscan User Guide to learn more about installing the Holoscan CLI or packaging your application using Holoscan CLI.</p>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/cpp/","title":"Endoscopy Tool Tracking","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ (see Python version) Last modified: February 4, 2026 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3, 2.0.0, 2.1.0, 2.2.0, 3.10.0, 3.11.0 Contribution metric: Level 0 - Core Stable</p> <p>Based on a LSTM (long-short term memory) stateful model, these applications demonstrate the use of custom components for tool tracking, including composition and rendering of text, tool position, and mask (as heatmap) combined with the original video stream.</p>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/cpp/#requirements","title":"Requirements","text":"<p>The provided applications are configured to either use capture cards for input stream, or a pre-recorded endoscopy video (replayer).</p> <p>Follow the setup instructions from the user guide to use the AJA capture card.</p> <p>Refer to the Deltacast documentation to use the Deltacast VideoMaster capture card.</p> <p>Refer to the Yuan documentation to use the Yuan QCap capture card.</p>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/cpp/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/cpp/#build-instructions","title":"Build Instructions","text":"<p>Please refer to the top level Holohub README.md file for information on how to build this application. In order to build with the Deltacast VideoMaster operator use <code>./holohub build endoscopy_tool_tracking --build-with deltacast_videomaster</code></p>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/cpp/#run-instructions","title":"Run Instructions","text":"<p>In your <code>build</code> directory, run the commands of your choice:</p> <ul> <li> <p>Using a pre-recorded video     <pre><code>sed -i -e 's#^source:.*#source: replayer#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\napplications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking --data &lt;data_dir&gt;/endoscopy\n</code></pre></p> </li> <li> <p>Using a vtk_renderer instead of holoviz     <pre><code>sed -i -e 's#^visualizer:.*#visualizer: \"vtk\"#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\napplications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking --data &lt;data_dir&gt;/endoscopy\n</code></pre></p> </li> <li> <p>Using a holoviz instead of vtk_renderer     <pre><code>sed -i -e 's#^visualizer:.*#visualizer: \"holoviz\"#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\napplications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking --data &lt;data_dir&gt;/endoscopy\n</code></pre></p> </li> <li> <p>Using an AJA card     <pre><code>sed -i -e 's#^source:.*#source: aja#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\napplications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking\n</code></pre></p> </li> <li> <p>Using a Deltacast card     <pre><code>sed -i -e '/^#.*deltacast_videomaster/s/^#//' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\nsed -i -e 's#^source:.*#source: deltacast#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\napplications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking\n</code></pre></p> </li> <li> <p>Using a Yuan card     <pre><code>sed -i -e '/^#.*yuan_qcap/s/^#//' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\nsed -i -e 's#^source:.*#source: yuan#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\napplications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking\n</code></pre></p> </li> <li> <p>Using an AJA card with hardware keying overlay (Only specific cards support this feature)     <pre><code>./holohub run endoscopy_tool_tracking --language=cpp --run-args=-capplications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking_aja_overlay.yaml\n</code></pre></p> </li> <li> <p>Using the Slang shader operator for post-processing     <pre><code>sed -i -e 's#^postprocessor:.*#postprocessor: slang_shader#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\napplications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking\n</code></pre></p> </li> </ul>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/python/","title":"Endoscopy Tool Tracking","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ (see C++ version) Last modified: February 4, 2026 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3, 2.0.0, 2.1.0, 2.2.0, 3.10.0, 3.11.0 Contribution metric: Level 1 - Highly Reliable</p> <p>Based on a LSTM (long-short term memory) stateful model, these applications demonstrate the use of custom components for tool tracking, including composition and rendering of text, tool position, and mask (as heatmap) combined with the original video stream.</p>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/python/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8+</li> <li>The provided applications are configured to either use the AJA, DELTACAST or Yuan capture cards for input stream, or a pre-recorded endoscopy video (replayer). Follow the setup instructions from the user guide to use the AJA capture card.</li> </ul>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/python/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/endoscopy_tool_tracking/python/#run-instructions","title":"Run Instructions","text":"<p>To run this application, you'll need to configure your PYTHONPATH environment variable to locate the necessary python libraries based on your Holoscan SDK installation type.</p> <p>You should refer to the glossary for the terms defining specific locations within HoloHub.</p> <p>If your Holoscan SDK installation type is:</p> <ul> <li>python wheels:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <ul> <li>otherwise:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOSCAN_INSTALL_DIR&gt;/python/lib:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <p>Next, run the commands of your choice:</p> <p>This application should be run in the build directory of Holohub in order to load the GXF extensions. Alternatively, the relative path of the extensions in the corresponding yaml file can be modified to match path of the working directory.</p> <ul> <li> <p>Using a pre-recorded video     <pre><code>cd &lt;HOLOHUB_BUILD_DIR&gt;\npython3 &lt;HOLOHUB_SOURCE_DIR&gt;/applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking.py --source=replayer --data=&lt;DATA_DIR&gt;/endoscopy\n</code></pre></p> </li> <li> <p>Using an AJA card     <pre><code>./holohub run endoscopy_tool_tracking python --run-args=-s=aja\n</code></pre></p> </li> <li> <p>Using a YUAN card     <pre><code>cd &lt;HOLOHUB_BUILD_DIR&gt;\npython3  &lt;HOLOHUB_SOURCE_DIR&gt;/applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking.py --source=yuan\n</code></pre></p> </li> <li> <p>Using an AJA card with hardware keying overlay (Only specific cards support this feature)     <pre><code>./holohub run endoscopy_tool_tracking --language python --run-args=\"-c=applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking_aja_overlay.yaml -s=aja\"\n</code></pre></p> </li> <li> <p>Using the Slang shader operator for post-processing     <pre><code>./holohub run endoscopy_tool_tracking --language python --run-args=\"-p=slang_shader\"\n</code></pre></p> </li> </ul>","tags":["Healthcare AI","Distributed","LSTM","Asynchronous Queues","gRPC","Video","Computer Vision and Perception","Visualization"]},{"location":"applications/florence-2-vision/","title":"Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: October 9, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates how to run the Florence-2 models on a live video feed with the possibility of changing the task and optional prompt via a QT UI.</p> <p> </p> <p>Note: This demo currently uses Florence-2-large-ft, but any of the Florence-2 models should work as long as the correct URLs and names are used in Dockerfile and config.yaml: - Florence-2-large-ft - Florence-2-large - Florence-2-base-ft - Florence-2-base</p>","tags":["Computer Vision and Perception","Video","CUDA Holoviz Integration","Qt","Multimodal Model","Detection","Segmentation"]},{"location":"applications/florence-2-vision/#setup-instructions","title":"\u2699\ufe0f Setup Instructions","text":"<p>The app defaults to using the video device at <code>/dev/video0</code></p> <p>Note: You can use a USB webcam as the video source, or an MP4 video by following the instructions for the V4L2_Camera example app.</p> <p>To debug if this is the correct device download <code>v4l2-ctl</code>: <pre><code>sudo apt-get install v4l-utils\n</code></pre> To check for your devices run: <pre><code>v4l2-ctl --list-devices\n</code></pre> This command will output something similar to this: <pre><code>NVIDIA Tegra Video Input Device (platform:tegra-camrtc-ca):\n        /dev/media0\n\nvi-output, lt6911uxc 2-0056 (platform:tegra-capture-vi:0):\n        /dev/video0\n\nDummy video device (0x0000) (platform:v4l2loopback-000):\n        /dev/video3\n</code></pre> Determine your desired video device and edit the source device in config.yaml</p>","tags":["Computer Vision and Perception","Video","CUDA Holoviz Integration","Qt","Multimodal Model","Detection","Segmentation"]},{"location":"applications/florence-2-vision/#build-and-run-instructions","title":"\ud83d\ude80 Build and Run Instructions","text":"<p>From the Holohub main directory run the following command: <pre><code>./holohub run florence-2-vision\n</code></pre> Note: The first build will take ~1.5 hours if you're on ARM64. This is largely due to building Flash Attention 2 since pre-built wheels are not distributed for ARM64 platforms.</p>","tags":["Computer Vision and Perception","Video","CUDA Holoviz Integration","Qt","Multimodal Model","Detection","Segmentation"]},{"location":"applications/florence-2-vision/#supported-hardware","title":"\ud83d\udcbb Supported Hardware","text":"<ul> <li>IGX w/ dGPU</li> <li>x86 w/ dGPU</li> <li>IGX w/ iGPU and Jetson AGX supported with workaround   There is a known issue running this application on IGX w/ iGPU and on Jetson AGX (see #500).   The workaround is to update the device to avoid picking up the libnvv4l2.so library.</li> </ul> <pre><code>cd /usr/lib/aarch64-linux-gnu/\nls -l libv4l2.so.0.0.999999\nsudo rm libv4l2.so.0.0.999999\nsudo ln -s libv4l2.so.0.0.0.0  libv4l2.so.0.0.999999\n</code></pre>","tags":["Computer Vision and Perception","Video","CUDA Holoviz Integration","Qt","Multimodal Model","Detection","Segmentation"]},{"location":"applications/florence-2-vision/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode florence-2-vision\n</code></pre> <p>This command will build and configure a Dev Container using a Dockerfile that is ready to run the application.</p>","tags":["Computer Vision and Perception","Video","CUDA Holoviz Integration","Qt","Multimodal Model","Detection","Segmentation"]},{"location":"applications/florence-2-vision/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"<p>There are two launch profiles configured for this application:</p> <ol> <li>(debugpy) florence-2-vision/python: Launch florence-2-vision using a launch profile that enables debugging of Python code.</li> <li>(pythoncpp) florence-2-vision/python: Launch florence-2-vision using a launch profile that enables debugging of Python and C++ code.</li> </ol>","tags":["Computer Vision and Perception","Video","CUDA Holoviz Integration","Qt","Multimodal Model","Detection","Segmentation"]},{"location":"applications/fm_asr/","title":"FM Radio Automatic Speech Recognition","text":"<p>     \u25b6 Run Locally  Authors: Joshua Martinez (NVIDIA) Supported platforms: x86_64 Language: Python Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.4.1 Tested Holoscan SDK versions: 0.4.1, 0.5.0 Contribution metric: Level 3 - Developmental</p> <p>This project is proof-of-concept demo featuring the combination of real-time, low-level signal processing and deep learning inference. It currently supports the RTL-SDR. Specifically, this project demonstrates the demodulation, downsampling, and automatic transcription of live, civilian FM radio broadcasts. The pipeline architecture is shown in the figure below. </p> <p></p> <p>The primary pipeline segments are written in Python. Future improvements will introduce a fully C++ system.</p> <p>This project leverages NVIDIA's Holoscan SDK for performant GPU pipelines, cuSignal package for GPU-accelerated signal processing, and the RIVA SDK for high accuracy automatic speech recognition (ASR).</p>","tags":["Signal Processing","Audio","FM demodulation","Polyphase Resampling","ASR","gRPC"]},{"location":"applications/fm_asr/#table-of-contents","title":"Table of Contents","text":"<ul> <li>FM ASR</li> <li>Table of Contents</li> <li>Install<ul> <li>Local Sensor - Basic Configuration</li> <li>Local Jetson Container</li> <li>Remote Sensor - Network in the Loop</li> <li>Bare Metal Install</li> </ul> </li> <li>Startup<ul> <li>Scripted Launch</li> <li>Manual Launch</li> <li>Initialize and Start the Riva Service</li> </ul> </li> <li>Configuration Parameters<ul> <li>Known Issues</li> </ul> </li> </ul>","tags":["Signal Processing","Audio","FM demodulation","Polyphase Resampling","ASR","gRPC"]},{"location":"applications/fm_asr/#install","title":"Install","text":"<p>To begin installation, clone this repository using the following: <pre><code>git clone https://github.com/nvidia-holoscan/holohub.git\n</code></pre> NVIDIA Riva is required to perform the automated transcriptions. You will need to install and configure the NGC-CLI tool, if you have not done so already, to obtain the Riva container and API. The Riva installation steps may be found at this link: Riva-Install. Note that Riva performs a TensorRT build during setup and requires access to the targeted GPU.  This project has been tested with RIVA 2.10.0.</p> <p>Container-based development and deployment is supported. The supported configurations are explained in the sections that follow. </p>","tags":["Signal Processing","Audio","FM demodulation","Polyphase Resampling","ASR","gRPC"]},{"location":"applications/fm_asr/#local-sensor-basic-configuration","title":"Local Sensor - Basic Configuration","text":"<p>The Local Sensor configuration assumes that the RTL-SDR is connected directly to the GPU-enabled system via USB. I/Q samples are collected from the RTL-SDR directly, using the SoapySDR library. Specialized containers are provided for Jetson devices.</p> <p>Only two containers are used in this configuration:  - The Application Container which includes all the necessary low level libraries, radio drivers, Holoscan SDK for the core application pipeline, and the Riva client API; and - The Riva SDK container that houses the ASR transcription service.</p> <p></p> <p>For convenience, container build scripts are provided to automatically build the application containers for Jetson and x86 systems. The Dockerfiles can be readily modified for ARM based systems with a discrete GPU. To build the container for this configuration, run the following: <pre><code># Starting from FM-ASR root directory\ncd scripts\n./build_application_container.sh # builds Application Container\n</code></pre> Note that this script does not build the Riva container.</p> <p>A script for running the application container is also provided. The run scripts will start the containers and leave the user at a bash terminal for development. Separate launch scripts are provided to automatically run the application. <pre><code># Starting from FM-ASR root directory\n./scripts/run_application_container.sh\n</code></pre></p>","tags":["Signal Processing","Audio","FM demodulation","Polyphase Resampling","ASR","gRPC"]},{"location":"applications/fm_asr/#local-jetson-container","title":"Local Jetson Container","text":"<p>Helper scripts will be provided in a future release.</p>","tags":["Signal Processing","Audio","FM demodulation","Polyphase Resampling","ASR","gRPC"]},{"location":"applications/fm_asr/#remote-sensor-network-in-the-loop","title":"Remote Sensor - Network in the Loop","text":"<p>This configuration is currently in work and will be provided in a future release. Developers can modify this code base to support this configuration if desired.</p>","tags":["Signal Processing","Audio","FM demodulation","Polyphase Resampling","ASR","gRPC"]},{"location":"applications/fm_asr/#bare-metal-install","title":"Bare Metal Install","text":"<p>Will be added in the future. Not currently supported.</p>","tags":["Signal Processing","Audio","FM demodulation","Polyphase Resampling","ASR","gRPC"]},{"location":"applications/fm_asr/#startup","title":"Startup","text":"<p>After installation, the following steps are needed to launch the application: 1. Start the Riva ASR service 2. Launch the Application Container</p>","tags":["Signal Processing","Audio","FM demodulation","Polyphase Resampling","ASR","gRPC"]},{"location":"applications/fm_asr/#scripted-launch","title":"Scripted Launch","text":"<p>The above steps are automated by some helper scripts. <pre><code># Starting from FM-ASR root directory\n./scripts/lauch_application.sh # Starts Application Container and launches app using the config file defined in the script\n</code></pre></p>","tags":["Signal Processing","Audio","FM demodulation","Polyphase Resampling","ASR","gRPC"]},{"location":"applications/fm_asr/#manual-launch","title":"Manual Launch","text":"<p>As an alternative to <code>launch_application.sh</code>, the FM-ASR pipeline can be run from inside the Application Container using the following commands: <pre><code>cd /workspace\nexport CONFIG_FILE=/workspace/params/holoscan.yml # can be edited by user\npython fm_asr_app.py $CONFIG_FILE\n</code></pre></p>","tags":["Signal Processing","Audio","FM demodulation","Polyphase Resampling","ASR","gRPC"]},{"location":"applications/fm_asr/#initialize-and-start-the-riva-service","title":"Initialize and Start the Riva Service","text":"<p>Riva can be setup following the Quickstart guide (version 2.10.0 currently supported). To summarize it, run the following: <pre><code>cd &lt;riva_quickstart_download_directory&gt;\nbash riva_init.sh\nbash riva_start.sh\n</code></pre> The initialization step will take a while to complete but only needs to be done once. Riva requires a capable GPU to setup and run properly. If your system has insufficient resources, the initialization script may hang. </p> <p>When starting the service, Riva may output a few \"retrying\" messages. This is normal and not an indication that the service is frozen. You should see a message saying <code>Riva server is ready...</code> once successful. </p> <p>Note for users with multiple GPUs:</p> <p>If you want to specify which GPU Riva uses (defaults to device 0), open and edit <code>&lt;riva_quickstart_download_directory&gt;/config.sh</code>, then change line <pre><code>gpus_to_use=\"device=0\"\n</code></pre> to <pre><code>gpus_to_use=\"device=&lt;your-device-number&gt;\"\n# or, to guarantee a specific device\ngpus_to_use=\"device=&lt;your-GPU-UUID&gt;\"\n</code></pre> You can determine your GPUs' UUIDs by running <code>nvidia-smi -L</code>.</p>","tags":["Signal Processing","Audio","FM demodulation","Polyphase Resampling","ASR","gRPC"]},{"location":"applications/fm_asr/#configuration-parameters","title":"Configuration Parameters","text":"<p>A table of the configuration parameters used in this project is shown below, organized by application operator.</p> Parameter Type Description run_time int Number of seconds that pipeline will execute RtlSdrGeneratorOp sample_rate float Reception sample rate used by the radio. RTL-SDR max stable sample rate without dropping is 2.56e6. tune_frequency float Tuning frequency for the radio in Hz. gain float 40.0 PlayAudioOp play_audio bool Flag used to enable simultaneous audio playback of signal. RivaAsrOp sample_rate int Audio sample rate expected by the Riva ASR model. Riva default is to 16000, other values will incurr an additional resample operation within Riva. max_alternatives int Riva - Maximum number of alternative transcripts to return (up to limit configured on server). Setting to 1 returns only the best response. word-time-offsets bool Riva - Option to output word timestamps in transcript. automatic-punctuation bool Riva - Flag that controls if transcript should be automatically punctuated. uri str localhost:50051 no-verbatim-transcripts bool Riva - If specified, text inverse normalization will be applied boosted_lm_words str Riva - words to boost when decoding. Useful for handling jargon and acronyms. boosted_lm_score float Value by which to boost words when decoding language-code str Riva - Language code of the model to be used. US English is en-US. Check Riva docs for more options interim_transcriptions bool Riva - Flag to include interim transcriptions in the output file. ssl_cert str Path to SSL client certificates file. Not currently utilized use_ssl bool Boolean to control if SSL/TLS encryption should be used. Not currently utilized. recognize_interval int Specifies the amount of data RIVA processes per request, in time (s). TranscriptSinkOp output_file str File path to store a transcript. Existing files will be overwritten.","tags":["Signal Processing","Audio","FM demodulation","Polyphase Resampling","ASR","gRPC"]},{"location":"applications/fm_asr/#known-issues","title":"Known Issues","text":"<p>This table will be populated as issues are identified.</p> Issue Description Status","tags":["Signal Processing","Audio","FM demodulation","Polyphase Resampling","ASR","gRPC"]},{"location":"applications/gstreamer/gst_video_recorder/","title":"GStreamer Video Recorder","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: aarch64, x86_64 Language: C++ Last modified: December 4, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 3.8.0 Tested Holoscan SDK versions: 3.8.0 Contribution metric: Level 2 - Trusted</p> <p>A Holoscan application that demonstrates video recording using the GStreamer encoding pipeline.</p> <p> Fig. 1: Application architecture showing the integration of Holoscan operators with GStreamer's encoding pipeline</p> <p>This application showcases how to:</p> <ul> <li>Capture video from V4L2 cameras (USB webcams, MIPI cameras) or generate test patterns</li> <li>Feed video frames to GStreamer for encoding</li> <li>Record encoded video to files in various formats (MP4, MKV)</li> <li>Use different video codecs (H.264, H.265, and other GStreamer-supported codecs)</li> <li>Support both host and CUDA device memory for zero-copy operation</li> </ul> <p>The application supports these video sources:</p> <ul> <li>V4L2 Camera - Capture from any V4L2-compatible camera (USB webcams, MIPI cameras)</li> <li>Pattern Generator - Generate animated test patterns (gradient, checkerboard, color bars)</li> </ul> <p>For more information about this application, refer to:</p> <ul> <li>Requirements - Lists requirements for the application.</li> <li>Quick Start - Run with default settings.</li> <li>Installation Guide - Requirements and building.</li> <li>Testing - Integration tests to validate the pipeline execution.</li> <li>Examples - Examples for common use cases.</li> <li>Architecture - Technical details and design.</li> </ul>","tags":["Video","GStreamer","Recording","Encoding","File"]},{"location":"applications/gstreamer/gst_video_recorder/#requirements","title":"Requirements","text":"<ul> <li>NVIDIA Holoscan SDK</li> <li>GStreamer 1.0 with the following plugins:</li> <li><code>gstreamer1.0-plugins-base</code> use <code>videoconvert</code> for host memory support</li> <li><code>gstreamer1.0-plugins-bad</code> use <code>cudaconvert</code>, <code>nvh264enc</code>, or <code>nvh265enc</code> for NVIDIA hardware encoding</li> <li><code>gstreamer1.0-plugins-good</code> use <code>mp4mux</code> and <code>matroskamux</code> for container formats</li> <li><code>gstreamer1.0-plugins-ugly</code> use <code>x264enc</code> for CPU-based H.264 encoding</li> <li><code>gstreamer1.0-libav</code> for additional codecs, if needed</li> <li>V4L2-compatible camera, optional, for camera capture mode</li> <li>USB webcams, MIPI CSI cameras, or any V4L2 video device</li> <li>Use <code>v4l2-ctl --list-devices</code> to see available cameras</li> </ul>","tags":["Video","GStreamer","Recording","Encoding","File"]},{"location":"applications/gstreamer/gst_video_recorder/#quick-start","title":"Quick Start","text":"<p>To run the application with the default settings, run one of the following commands:</p> Using the V4L2 Camera Generating Test Patterns <code>./holohub run gst_video_recorder v4l2</code> <code>./holohub run gst_video_recorder pattern</code> <p>These commands build and run the customized container for this application with all the dependencies installed (defined by <code>Dockerfile</code>), and then build and start the application using the default settings. The output video will be saved in the build directory as <code>output.mp4</code>.</p> <p>Run <code>gst-video-recorder --help</code> for command-line options.</p>","tags":["Video","GStreamer","Recording","Encoding","File"]},{"location":"applications/gstreamer/gst_video_recorder/#installation-guide","title":"Installation Guide","text":"<p>This guide covers requirements, building, and testing the GStreamer Video Recorder application.</p>","tags":["Video","GStreamer","Recording","Encoding","File"]},{"location":"applications/gstreamer/gst_video_recorder/#building-the-application","title":"Building the Application","text":"<p>There are two options that you can choose from to build the application. The recommended option is the containerized build, because all dependencies are included in the container and there is no setup. The other option is a local build, which can be best for faster builds and easier debugging. Building locally, requires the installation of some dependencies.</p> <p>The <code>install_deps.sh</code> script installs:</p> <ul> <li><code>pkg-config</code> (required for CMake)</li> <li>GStreamer development libraries</li> <li>All necessary GStreamer plugins for encoding</li> </ul> <p>Choose one of the following options to build the application:</p> Containerized Build (Recommended) Local Build Install the application: Install dependencies, from the <code>gst_video_recorder</code> directory: <code>./holohub build gst_video_recorder</code> <code>./install_deps.sh</code> Then build locally: <code>./holohub build --local gst_video_recorder</code>","tags":["Video","GStreamer","Recording","Encoding","File"]},{"location":"applications/gstreamer/gst_video_recorder/#usage-reference","title":"Usage Reference","text":"<p>Reference for running <code>gst_video_recorder</code> that includes:</p> <ul> <li>Running the Application</li> <li>Command-Line Options</li> </ul>","tags":["Video","GStreamer","Recording","Encoding","File"]},{"location":"applications/gstreamer/gst_video_recorder/#running-the-application","title":"Running the Application","text":"<p>The recommended way to run the application is through the <code>holohub</code> launcher:</p> <pre><code>./holohub run gst_video_recorder --run-args=\"[OPTIONS]\"\n</code></pre> <p>Alternatively, if you know the binary location, you can run it directly:</p> <pre><code>gst-video-recorder [OPTIONS]\n</code></pre>","tags":["Video","GStreamer","Recording","Encoding","File"]},{"location":"applications/gstreamer/gst_video_recorder/#command-line-options","title":"Command-Line Options","text":"<p>The command line options include the following main categories:</p> <ul> <li>General Options</li> <li>Resolution Options</li> <li>V4L2 Camera Options</li> <li>Pattern Generator Options</li> </ul>","tags":["Video","GStreamer","Recording","Encoding","File"]},{"location":"applications/gstreamer/gst_video_recorder/#general-options","title":"General Options","text":"Option Description Default <code>--source &lt;type&gt;</code> Video source: <code>pattern</code> or <code>v4l2</code> <code>pattern</code> <code>-o, --output &lt;filename&gt;</code> Output video filename. Supported formats: <code>.mp4</code>, <code>.mkv</code>. If no extension, defaults to <code>.mp4</code> <code>output.mp4</code> <code>-e, --encoder &lt;name&gt;</code> Encoder base name (for example, <code>nvh264</code>, <code>nvh265</code>, <code>x264</code>, <code>x265</code>). Note: 'enc' suffix is automatically appended <code>nvh264</code> <code>-c, --count &lt;number&gt;</code> Number of frames to capture or generate unlimited <code>-f, --framerate &lt;rate&gt;</code> Frame rate as fraction or decimal (for example, <code>30/1</code>, <code>30000/1001</code>, <code>29.97</code>, <code>60</code>). Use <code>0/1</code> for live mode (no throttling, real-time timestamps) <code>30/1</code> <code>--property &lt;key=value&gt;</code> Set encoder property (can be used multiple times, for example, <code>--property bitrate=8000 --property preset=1</code>). Property types are automatically detected and converted - <code>--help</code> Show help message -","tags":["Video","GStreamer","Recording","Encoding","File"]},{"location":"applications/gstreamer/gst_video_recorder/#resolution-options","title":"Resolution Options","text":"Option Description Default <code>-w, --width &lt;pixels&gt;</code> Frame width. For V4L2: Must match a supported camera resolution. For the pattern: any reasonable resolution (64-8192 pixels) <code>1920</code> <code>-h, --height &lt;pixels&gt;</code> Frame height. For V4L2: Must match a supported camera resolution. For the pattern: any reasonable resolution (64-8192 pixels) <code>1080</code>","tags":["Video","GStreamer","Recording","Encoding","File"]},{"location":"applications/gstreamer/gst_video_recorder/#v4l2-camera-options","title":"V4L2 Camera Options","text":"Option Description Default <code>--device &lt;path&gt;</code> V4L2 device path <code>/dev/video0</code> <code>--pixel-format &lt;format&gt;</code> V4L2 pixel format (<code>YUYV</code>, <code>MJPEG</code>, <code>auto</code>) <code>auto</code>","tags":["Video","GStreamer","Recording","Encoding","File"]},{"location":"applications/gstreamer/gst_video_recorder/#pattern-generator-options","title":"Pattern Generator Options","text":"Option Description Default <code>--pattern &lt;type&gt;</code> Pattern type: <code>0</code> = animated gradient, <code>1</code> = animated checkerboard, <code>2</code> = color bars (SMPTE style) <code>0</code> <code>--storage &lt;type&gt;</code> Memory storage type: <code>0</code> = host memory, <code>1</code> = device or CUDA memory <code>1</code>","tags":["Video","GStreamer","Recording","Encoding","File"]},{"location":"applications/gstreamer/gst_video_recorder/#testing","title":"Testing","text":"<p>The application includes integration tests to validate the pipeline execution and recording file creation.</p> <p>To run the tests, use the following command:</p> <pre><code>./holohub test gst_video_recorder --verbose\n</code></pre>","tags":["Video","GStreamer","Recording","Encoding","File"]},{"location":"applications/gstreamer/gst_video_recorder/#usage-examples","title":"Usage Examples","text":"<p>Practical, copy-paste examples for common use cases.</p>","tags":["Video","GStreamer","Recording","Encoding","File"]},{"location":"applications/gstreamer/gst_video_recorder/#tips","title":"Tips","text":"<ul> <li>Use <code>v4l2-ctl --list-formats-ext</code> to see supported camera resolutions</li> <li>Use absolute paths (for example, <code>/workspace/holohub/video.mp4</code>) for predictable output locations</li> <li>Higher bitrates produce better quality but larger files</li> <li>NVIDIA hardware encoders (<code>nvh264</code>, <code>nvh265</code>) require CUDA and provide better performance</li> <li>CPU encoders (<code>x264</code>, <code>x265</code>) work with <code>--storage 0</code> (host memory)</li> </ul>","tags":["Video","GStreamer","Recording","Encoding","File"]},{"location":"applications/gstreamer/gst_video_recorder/#v4l2-camera-examples","title":"V4L2 Camera Examples","text":"<ul> <li>Record from the default V4L2 camera at 1920x1080 (30 seconds at 30 FPS):</li> </ul> <pre><code>./holohub run gst_video_recorder --run-args=\"--source v4l2 --width 1920 --height 1080 --count 900 -o camera.mp4\"\n</code></pre> <ul> <li>Record from a specific V4L2 device with H.265:</li> </ul> <pre><code>./holohub run gst_video_recorder --run-args=\"--source v4l2 --device /dev/video1 --width 1920 --height 1080 --encoder nvh265 --count 600 -o camera_h265.mp4\"\n</code></pre> <ul> <li>Record at 720p resolution:</li> </ul> <pre><code>./holohub run gst_video_recorder --run-args=\"--source v4l2 --width 1280 --height 720 --count 300 -o camera_720p.mp4\"\n</code></pre> <ul> <li>Record with a specific pixel format (YUYV):</li> </ul> <pre><code>./holohub run gst_video_recorder --run-args=\"--source v4l2 --width 640 --height 480 --pixel-format YUYV --count 300 -o camera_yuyv.mp4\"\n</code></pre> <ul> <li>High-quality recording with custom encoder settings:</li> </ul> <pre><code>./holohub run gst_video_recorder --run-args=\"--source v4l2 --property bitrate=12000 --property preset=1 --count 900 -o high_quality.mp4\"\n</code></pre>","tags":["Video","GStreamer","Recording","Encoding","File"]},{"location":"applications/gstreamer/gst_video_recorder/#pattern-generator-examples","title":"Pattern Generator Examples","text":"<ul> <li>Basic recording of ten seconds of animated gradient (300 frames at 30 FPS):</li> </ul> <pre><code>./holohub run gst_video_recorder --run-args=\"--source pattern --count 300 -o video.mp4\"\n</code></pre> <ul> <li>Record high-quality H.265 video:</li> </ul> <pre><code>./holohub run gst_video_recorder --run-args=\"--source pattern --count 300 --encoder nvh265 --property bitrate=10000 -o video.mp4\"\n</code></pre> <ul> <li>Record a 720p resolution video:</li> </ul> <pre><code>./holohub run gst_video_recorder --run-args=\"--source pattern --count 300 --width 1280 --height 720 -o video_720p.mp4\"\n</code></pre> <ul> <li>Record the checkerboard pattern:</li> </ul> <pre><code>./holohub run gst_video_recorder --run-args=\"--source pattern --count 300 --pattern 1 -o checkerboard.mp4\"\n</code></pre> <ul> <li>Record color bars:</li> </ul> <pre><code>./holohub run gst_video_recorder --run-args=\"--source pattern --count 300 --pattern 2 -o colorbars.mp4\"\n</code></pre> <ul> <li>Record using host memory (CPU encoding):</li> </ul> <pre><code>./holohub run gst_video_recorder --run-args=\"--source pattern --count 300 --storage 0 --encoder x264 -o video.mp4\"\n</code></pre>","tags":["Video","GStreamer","Recording","Encoding","File"]},{"location":"applications/gstreamer/gst_video_recorder/#architecture","title":"Architecture","text":"<p>The application supports two video sources and uses a common encoding backend:</p>","tags":["Video","GStreamer","Recording","Encoding","File"]},{"location":"applications/gstreamer/gst_video_recorder/#video-sources","title":"Video Sources","text":"<ul> <li>V4L2VideoCaptureOp: Captures video from V4L2-compatible cameras (USB webcams, MIPI CSI cameras)</li> <li>PatternGenOperator: Generates animated test patterns as Holoscan entities with tensors</li> </ul>","tags":["Video","GStreamer","Recording","Encoding","File"]},{"location":"applications/gstreamer/gst_video_recorder/#encoding-backend","title":"Encoding Backend","text":"<ul> <li>GstVideoRecorderOp: Receives video frames, manages the GStreamer pipeline, and handles encoding</li> </ul>","tags":["Video","GStreamer","Recording","Encoding","File"]},{"location":"applications/gstreamer/gst_video_recorder/#pipeline-flow","title":"Pipeline Flow","text":"<ul> <li>V4L2 Camera Pipeline:</li> </ul> <pre><code>V4L2VideoCaptureOp \u2192 FormatConverterOp \u2192 GstVideoRecorderOp \u2192 GStreamer Pipeline \u2192 File\n</code></pre> <ul> <li>Pattern Generator Pipeline:</li> </ul> <pre><code>PatternGenOperator \u2192 GstVideoRecorderOp \u2192 GStreamer Pipeline \u2192 File\n</code></pre> <p>The GStreamer encoding pipeline is automatically constructed based on the encoder and file format:</p> <ul> <li>Pipeline structure: <code>[converter] ! [encoder]enc ! [parser] ! [muxer] ! filesink</code></li> <li>Converter: Automatically selected based on memory type (videoconvert for host, cudaconvert for device)</li> <li>Encoder: Specified via <code>--encoder</code> option (nvh264, nvh265, x264, x265)</li> <li>Parser: Automatically determined from encoder (h264parse, h265parse)</li> <li>Muxer: Automatically determined from file extension (mp4mux for .mp4, matroskamux for .mkv)</li> </ul> <p>Example pipelines:</p> <ul> <li>NVIDIA H.264 to MP4: <code>cudaconvert ! nvh264enc ! h264parse ! mp4mux ! filesink</code></li> <li>NVIDIA H.265 to MKV: <code>cudaconvert ! nvh265enc ! h265parse ! matroskamux ! filesink</code></li> <li>CPU x264 to MP4: <code>videoconvert ! x264enc ! h264parse ! mp4mux ! filesink</code></li> </ul>","tags":["Video","GStreamer","Recording","Encoding","File"]},{"location":"applications/gstreamer/gst_video_recorder/#performance","title":"Performance","text":"<p>The application supports both host and device (CUDA) memory:</p> <ul> <li>Device memory (<code>--storage 1</code>, default): Zero-copy operation for better performance when using NVIDIA hardware encoders (nvh264enc, nvh265enc)</li> <li>Host memory (<code>--storage 0</code>): Required for CPU encoders (x264, x265) but involves memory copies</li> </ul>","tags":["Video","GStreamer","Recording","Encoding","File"]},{"location":"applications/gstreamer/gst_video_recorder/#notes","title":"Notes","text":"","tags":["Video","GStreamer","Recording","Encoding","File"]},{"location":"applications/gstreamer/gst_video_recorder/#video-sources_1","title":"Video Sources","text":"<ul> <li> <p>V4L2 Camera:</p> </li> <li> <p>Supports any V4L2-compatible camera (USB webcams, MIPI CSI cameras)</p> </li> <li>Camera resolution must be explicitly specified with <code>--width</code> and <code>--height</code></li> <li>Use <code>v4l2-ctl --list-formats-ext</code> to see supported resolutions and formats</li> <li> <p>FormatConverterOp automatically converts camera output to the format expected by the recorder</p> </li> <li> <p>Pattern Generator:</p> </li> <li> <p>Supports three test patterns:</p> <ul> <li>Animated gradient (default): Colorful sine wave patterns</li> <li>Animated checkerboard: Moving checkerboard with variable square size</li> <li>Color bars: SMPTE-style color bars (seven colors)</li> </ul> </li> <li>Useful for testing the encoding pipeline without hardware dependencies</li> </ul>","tags":["Video","GStreamer","Recording","Encoding","File"]},{"location":"applications/gstreamer/gst_video_recorder/#recording","title":"Recording","text":"<ul> <li>The application waits for encoding to complete before exiting to ensure proper file finalization.</li> <li>EOS (End-Of-Stream) signal is sent automatically when recording completes.</li> <li>Video parameters (width, height, format, storage) are automatically detected from incoming frames.</li> <li>Frame count can be limited with <code>--count</code> or runs indefinitely if not specified.</li> <li>Output files are written to the Holohub workspace directory when using <code>./holohub run</code>. Use absolute paths like <code>/workspace/holohub/video.mp4</code> for predictable output locations.</li> </ul>","tags":["Video","GStreamer","Recording","Encoding","File"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/","title":"H.264 Endoscopy Tool Tracking","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: September 26, 2025 Latest version: 2.1 Minimum Holoscan SDK version: 3.6.1 Tested Holoscan SDK versions: 3.6.1 Contribution metric: Level 1 - Highly Reliable</p> <p>The application showcases how to use H.264 video source as input to and output from the Holoscan pipeline. This application is a modified version of Endoscopy Tool Tracking reference application in Holoscan SDK that supports H.264 elementary streams as the input and output.</p> <p>The H.264 video decode operators do not adjust framerate as it reads the elementary stream input. As a result the video stream can be displayed as quickly as the decoding can be performed. This application uses <code>PeriodicCondition</code> to play video at the same speed as the source video.</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/#requirements","title":"Requirements","text":"<ul> <li>NVIDIA GPU with display driver &gt;= 580 for CUDA 13 support. See the Holoscan SDK User Guide for a list of all Holoscan SDK supported platforms.</li> <li>Orin platforms (Jetpack 6.x, IGX OS 1.x) are not supported at this time.</li> <li>This application is configured to use H.264 elementary stream from endoscopy sample data as input. The recording of the output can be enabled by setting <code>record_output</code> flag in the config file to <code>true</code>. If the <code>record_output</code> flag in the config file is set to <code>true</code>, the output of the pipeline is again recorded to a H.264 elementary stream on the disk, file name / path for this can be specified in the 'h264_endoscopy_tool_tracking.yaml' file.</li> </ul>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded when building the application.</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/#build-and-run-the-h264-endoscopy-tool-tracking-application","title":"Build and Run the H.264 Endoscopy Tool Tracking Application","text":"","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/#c","title":"C++","text":"<pre><code>./holohub run h264_endoscopy_tool_tracking --language cpp\n</code></pre>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/#python","title":"Python","text":"<p>Separate build and run commands are required to address the known symbol loading issue.</p> <pre><code>./holohub build h264_endoscopy_tool_tracking --language python\n\n# Python version\n# Note: LD_PRELOAD required to address symbol issue\n./holohub run h264_endoscopy_tool_tracking --language python \\\n    --docker-opts=\"-e LD_PRELOAD=/opt/nvidia/holoscan/lib/libgxf_core.so\"\n</code></pre> <p>Important: on aarch64, applications also need tegra folder mounted inside the container and the <code>LD_LIBRARY_PATH</code> environment variable should be updated to include tegra folder path.</p> <p>Open and edit the Dockerfile and uncomment line 66:</p> <pre><code># Uncomment the following line for aarch64 support\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/aarch64-linux-gnu/tegra/\n</code></pre>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/#enable-recording-of-the-output","title":"Enable recording of the output","text":"<p>The recording of the output can be enabled by setting <code>record_output</code> flag in the config file <code>&lt;build_dir&gt;/applications/h264/endoscopy_tool_tracking/h264_endoscopy_tool_tracking.yaml</code> to <code>true</code>.</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode h264\n</code></pre>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/#c_1","title":"C++","text":"<p>Use the (gdb) h264_endoscopy_tool_tracking/cpp launch profile to run and debug the C++ application.</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/h264/h264_endoscopy_tool_tracking/#python_1","title":"Python","text":"<p>There are a couple of launch profiles configured for this application:</p> <ol> <li>(debugpy) h264_endoscopy_tool_tracking/python: Launch the h.264 Endoscopy Tool Tracking application with the ability to debug Python code.</li> <li>(pythoncpp) h264_endoscopy_tool_tracking/python: Launch the h.264 Endoscopy Tool Tracking application with the ability to debug both Python and C++ code.</li> </ol>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/h264/h264_video_decode/","title":"H.264 Video Decode","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: September 26, 2025 Latest version: 2.0 Minimum Holoscan SDK version: 3.6.1 Tested Holoscan SDK versions: 3.6.1 Contribution metric: Level 1 - Highly Reliable</p> <p>This is a minimal reference application demonstrating usage of H.264 video decode operators. This application makes use of H.264 elementary stream reader operator for reading H.264 elementary stream input and uses Holoviz operator for rendering decoded data to the native window.</p> <p>The H.264 video decode operators do not adjust framerate as it reads the elementary stream input. As a result the video stream can be displayed as quickly as the decoding can be performed. This application uses <code>PeriodicCondition</code> to play video at the same speed as the source video.</p>","tags":["Healthcare AI","Video","Hardware Accelerated Decode","Endoscopy"]},{"location":"applications/h264/h264_video_decode/#requirements","title":"Requirements","text":"<ul> <li>NVIDIA GPU with display driver &gt;= 580 for CUDA 13 support. See the Holoscan SDK User Guide for a list of all Holoscan SDK supported platforms.</li> <li>Orin platforms (Jetpack 6.x, IGX OS 1.x) are not supported at this time.</li> <li>This application is configured to use H.264 elementary stream from endoscopy sample data as input. To use any other stream, the filename / path for the input file can be specified in the 'h264_video_decode.yaml' file.</li> </ul>","tags":["Healthcare AI","Video","Hardware Accelerated Decode","Endoscopy"]},{"location":"applications/h264/h264_video_decode/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded when building the application.</p>","tags":["Healthcare AI","Video","Hardware Accelerated Decode","Endoscopy"]},{"location":"applications/h264/h264_video_decode/#build-and-run-the-h264-endoscopy-tool-tracking-application","title":"Build and Run the H.264 Endoscopy Tool Tracking Application","text":"","tags":["Healthcare AI","Video","Hardware Accelerated Decode","Endoscopy"]},{"location":"applications/h264/h264_video_decode/#c","title":"C++","text":"<pre><code>./holohub run h264_video_decode --language cpp\n</code></pre>","tags":["Healthcare AI","Video","Hardware Accelerated Decode","Endoscopy"]},{"location":"applications/h264/h264_video_decode/#python","title":"Python","text":"<p>Separate build and run commands are required to address the known symbol loading issue.</p> <pre><code># C++ version\n./holohub build h264_video_decode --language python\n\n# Python version\n# Note: LD_PRELOAD required to address symbol issue\n./holohub run h264_video_decode --language python \\\n    --docker-opts=\"-e LD_PRELOAD=/opt/nvidia/holoscan/lib/libgxf_core.so\"\n</code></pre> <p>Important: on aarch64, applications also need tegra folder mounted inside the container and the <code>LD_LIBRARY_PATH</code> environment variable should be updated to include tegra folder path.</p> <p>Open and edit the Dockerfile and uncomment line 66:</p> <pre><code># Uncomment the following line for aarch64 support\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/aarch64-linux-gnu/tegra/\n</code></pre>","tags":["Healthcare AI","Video","Hardware Accelerated Decode","Endoscopy"]},{"location":"applications/h264/h264_video_decode/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode h264\n</code></pre>","tags":["Healthcare AI","Video","Hardware Accelerated Decode","Endoscopy"]},{"location":"applications/h264/h264_video_decode/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Healthcare AI","Video","Hardware Accelerated Decode","Endoscopy"]},{"location":"applications/h264/h264_video_decode/#c_1","title":"C++","text":"<p>Use the (gdb) h264_video_decode/cpp launch profile to run and debug the C++ application.</p>","tags":["Healthcare AI","Video","Hardware Accelerated Decode","Endoscopy"]},{"location":"applications/h264/h264_video_decode/#python_1","title":"Python","text":"<p>There are a couple of launch profiles configured for this application:</p> <ol> <li>(debugpy) h264_video_decode/python: Launch the h.264 Video Decode application with the ability to debug Python code.</li> <li>(pythoncpp) h264_video_decode/python: Launch the h.264 Video Decode application with the ability to debug both Python and C++ code.</li> </ol>","tags":["Healthcare AI","Video","Hardware Accelerated Decode","Endoscopy"]},{"location":"applications/high_speed_endoscopy/","title":"High Speed Endoscopy","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: February 4, 2026 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0, 3.10.0, 3.11.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The high speed endoscopy application showcases how high resolution cameras can be used to capture the scene, post-processed on GPU, and displayed at high frame rate.</p> <p>This application requires: 1. an Emergent Vision Technologies camera (see {ref}<code>setup instructions&lt;emergent-vision-tech&gt;</code>) 2. an NVIDIA ConnectX SmartNIC with Rivermax SDK and drivers installed 3. a display with high refresh rate to keep up with the camera's framerate 4. {ref}<code>additional setups&lt;additional_setup&gt;</code> to reduce latency</p> <p>Tip Tested on the Holoscan DevKits (ConnectX included) with: - EVT HB-9000-G-C: 25GigE camera with Gpixel GMAX2509 - SFP28 cable and QSFP28 to SFP28 adaptor - Asus ROG Swift PG279QM and Asus ROG Swift 360 Hz PG259QNR monitors with NVIDIA G-SYNC technology</p> <p> Fig. 1 Hi-Speed Endoscopy App</p> <p>The data acquisition happens using <code>emergent-source</code>, by default it is set to 4200x2160 at 240Hz. The acquired data is then demosaiced in GPU using CUDA via <code>bayer-demosaic</code> and displayed through <code>holoviz-viewer</code>.</p> <p>The peak performance that can be obtained by running these applications with the recommended hardware, GSYNC and RDMA enabled on exclusive display mode is 10ms on Clara AGX Devkit and 8ms on NVIDIA IGX Orin DevKit ES. This is the photon-to-glass latency of a frame from scene acquisition to display on monitor.</p>","tags":["Healthcare AI","Video","Bayer RGB Pipeline Optimization","Endoscopy","RDMA","Holoviz"]},{"location":"applications/high_speed_endoscopy/#building-and-running-the-application","title":"Building and Running the Application","text":"<p>This application cannot be compiled within the Holohub container. It needs to be compiled on the host machine.</p>","tags":["Healthcare AI","Video","Bayer RGB Pipeline Optimization","Endoscopy","RDMA","Holoviz"]},{"location":"applications/high_speed_endoscopy/#set-up-the-environment","title":"Set up the environment","text":"<pre><code>export LD_LIBRARY_PATH=/opt/EVT/eSDK:$LD_LIBRARY_PATH\n</code></pre>","tags":["Healthcare AI","Video","Bayer RGB Pipeline Optimization","Endoscopy","RDMA","Holoviz"]},{"location":"applications/high_speed_endoscopy/#compile-the-application","title":"Compile the application","text":"","tags":["Healthcare AI","Video","Bayer RGB Pipeline Optimization","Endoscopy","RDMA","Holoviz"]},{"location":"applications/high_speed_endoscopy/#using-holohub-cli","title":"Using HoloHub CLI","text":"<p>HoloHub CLI command can also be used to build the application.</p> <pre><code>./holohub build high_speed_endoscopy --local\n</code></pre> <p>If the above command does not work, please file an issue and try the steps below.</p>","tags":["Healthcare AI","Video","Bayer RGB Pipeline Optimization","Endoscopy","RDMA","Holoviz"]},{"location":"applications/high_speed_endoscopy/#explicit-commands-on-the-host-machine","title":"Explicit commands on the host machine","text":"<pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;\nrm -rf build/high_speed_endoscopy\nmkdir -p build/high_speed_endoscopy\ncd build/high_speed_endoscopy\ncmake ../.. -DAPP_high_speed_endoscopy=ON\nmake -j\n</code></pre>","tags":["Healthcare AI","Video","Bayer RGB Pipeline Optimization","Endoscopy","RDMA","Holoviz"]},{"location":"applications/high_speed_endoscopy/#run-the-application","title":"Run the application","text":"<pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;\nsudo ./holohub run high_speed_endoscopy --local --language cpp\n</code></pre> <p>For python version:</p> <pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;\nsudo ./holohub run high_speed_endoscopy --local --language python\n</code></pre>","tags":["Healthcare AI","Video","Bayer RGB Pipeline Optimization","Endoscopy","RDMA","Holoviz"]},{"location":"applications/high_speed_endoscopy/#troubleshooting","title":"Troubleshooting","text":"<ol> <li> <p>Problem: The application fails to find the EVT camera.</p> <ul> <li>Make sure that the MLNX ConnectX SmartNIC is configured with the correct IP address. Follow section Post EVT Software Installation Steps</li> </ul> </li> <li> <p>Problem: The application fails to open the EVT camera.</p> <ul> <li>Make sure that the application was run with <code>sudo</code> privileges.</li> <li>Make sure a valid Rivermax license file is located at <code>/opt/mellanox/rivermax/rivermax.lic</code>.</li> <li>Make sure that a previously run <code>high_speed_endoscopy</code> application has been terminated successfully. One might need to run <code>sudo kill -9 &lt;PID&gt;</code> to terminate the application.</li> </ul> </li> <li> <p>Problem: The application fails to connect to the EVT camera with error message \u201cGVCP ack error\u201d.</p> <ul> <li>It could be an issue with the HR12 power connection to the camera. Disconnect the HR12 power connector from the camera and try reconnecting it.</li> </ul> </li> <li> <p>Problem: The applications fails with a segmentation fault at runtime, in the HoloViz visualization operator.</p> <ul> <li>It could be an issue with the configured BAR size on the GPU. Use the display mode selector tool to increase the BAR size.</li> <li>If you are using a lower resolution monitor, then you might need to decrease the resolution of the vistualization in the <code>high_speed_endoscopy.yaml</code> file.</li> </ul> </li> </ol>","tags":["Healthcare AI","Video","Bayer RGB Pipeline Optimization","Endoscopy","RDMA","Holoviz"]},{"location":"applications/high_speed_endoscopy/#known-issues","title":"Known Issues","text":"<ul> <li>When the application is closed from the 'X' button in the visualization window, the application might not terminate gracefully and throw an error. In this case, one might need to power-cycle the camera before running the application again.</li> </ul>","tags":["Healthcare AI","Video","Bayer RGB Pipeline Optimization","Endoscopy","RDMA","Holoviz"]},{"location":"applications/high_speed_endoscopy/cpp/","title":"High-Speed Endoscopy","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ (see Python version) Last modified: February 4, 2026 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0, 3.10.0, 3.11.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The application showcases how high resolution cameras can be used to capture the scene, post-processed on GPU and displayed at high frame rate.</p>","tags":["Healthcare AI","Video","Bayer RGB Pipeline Optimization","Endoscopy","RDMA","Holoviz"]},{"location":"applications/high_speed_endoscopy/cpp/#requirements","title":"Requirements","text":"<p>This application requires: 1. an Emergent Vision Technologies camera (see setup instructions 2. a NVIDIA ConnectX SmartNIC with Rivermax SDK and drivers installed (see prerequisites) 3. a display with high refresh rate to keep up with the camera's framerate 4. additional setups to reduce latency</p>","tags":["Healthcare AI","Video","Bayer RGB Pipeline Optimization","Endoscopy","RDMA","Holoviz"]},{"location":"applications/high_speed_endoscopy/cpp/#build-instructions","title":"Build Instructions","text":"<p>Please refer to the top level Holohub README.md file for information on how to build this application.</p> <p>\u26a0\ufe0f At this time, camera controls are hardcoded within the <code>gxf_emergent_source</code> extension. To update them at the application level, the GXF extension, and the application need to be rebuilt. For more information on the controls, refer to the EVT Camera Attributes Manual</p>","tags":["Healthcare AI","Video","Bayer RGB Pipeline Optimization","Endoscopy","RDMA","Holoviz"]},{"location":"applications/high_speed_endoscopy/cpp/#run-instructions","title":"Run Instructions","text":"<p>First, go in your <code>build</code> or <code>install</code> directory. Then, run the commands of your choice:</p> <ul> <li> <p>RDMA disabled     <pre><code># C++\nsed -i -e 's#rdma:.*#rdma: false#' ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy.yaml \\\n    &amp;&amp; sudo ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy\n</code></pre></p> </li> <li> <p>RDMA enabled     <pre><code># C++\nsed -i -e 's#rdma:.*#rdma: true#' ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy.yaml \\\n    &amp;&amp; sudo MELLANOX_RINGBUFF_FACTOR=14 ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy\n</code></pre></p> </li> </ul> <p>\u2139\ufe0f The <code>MELLANOX_RINGBUFF_FACTOR</code> is used by the EVT driver to decide how much BAR1 size memory would be used on the dGPU. It can be changed to different number based on different use cases.</p>","tags":["Healthcare AI","Video","Bayer RGB Pipeline Optimization","Endoscopy","RDMA","Holoviz"]},{"location":"applications/high_speed_endoscopy/python/","title":"High-Speed Endoscopy","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ (see C++ version) Last modified: February 4, 2026 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0, 3.10.0, 3.11.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The application showcases how high resolution cameras can be used to capture the scene, post-processed on GPU and displayed at high frame rate.</p>","tags":["Healthcare AI","Video","Bayer RGB Pipeline Optimization","Endoscopy","RDMA","Holoviz"]},{"location":"applications/high_speed_endoscopy/python/#requirements","title":"Requirements","text":"<p>This application requires: 1. an Emergent Vision Technologies camera (see setup instructions 2. a NVIDIA ConnectX SmartNIC with Rivermax SDK and drivers installed (see prerequisites) 3. a display with high refresh rate to keep up with the camera's framerate 4. additional setups to reduce latency</p>","tags":["Healthcare AI","Video","Bayer RGB Pipeline Optimization","Endoscopy","RDMA","Holoviz"]},{"location":"applications/high_speed_endoscopy/python/#run-instructions","title":"Run Instructions","text":"<p>TODO</p>","tags":["Healthcare AI","Video","Bayer RGB Pipeline Optimization","Endoscopy","RDMA","Holoviz"]},{"location":"applications/holochat/","title":"HoloChat","text":"<p>     \u25b6 Run Locally  Authors: Nigel Nelson (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 7, 2025 Latest version: 0.2.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 4 - Experimental</p> <p>HoloChat is an AI-driven chatbot, built on top of a locally hosted Code-Llama model OR a remote NIM API for Llama-3-70b, which acts as developer's copilot in Holoscan development. The LLM leverages a vector database comprised of the Holoscan SDK repository and user guide, enabling HoloChat to answer general questions about Holoscan, as well act as a Holoscan SDK coding assistant.</p> <p> </p>","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#quick-start-guide","title":"Quick Start Guide","text":"<p>Choose one of the following modes to run HoloChat:</p> <ul> <li>NVIDIA NIM API (cloud):  Uses the NVIDIA NIM API with Llama-3.3-70b-instruct model. <p>[!IMPORTANT] For cloud mode, you must create a <code>.env</code> file in the <code>holochat</code> directory with your NVIDIA API key. If you don't have an API key, you can create one on build.nvidia.com using your NVIDIA developer account.</p> <pre><code>echo \"NVIDIA_API_KEY=&lt;api_key_here&gt;\" &gt; ./applications/holochat/.env\n</code></pre> </li> </ul> <pre><code>./holohub run holochat\n</code></pre> <ul> <li>Local LLM (standalone): Runs the LLM locally (GPU required), using Phind-CodeLlama-34B-v2 model and Llama.cpp for inference.</li> </ul> <pre><code>./holohub run holochat standalone\n</code></pre> <p>[!NOTE] First local run will likely take ~45 minutes (container + model downloads), and expect ~28 GB of model data under <code>holochat/models</code>, so use a disk with plenty of free space.</p> <ul> <li>MCP server (mcp): Runs as a Model Context Protocol server that provides Holoscan documentation and code context to upstream LLMs like Claude</li> </ul> <pre><code>./holohub run holochat mcp\n</code></pre> <p>See MCP_MODE.md for more details on using MCP mode.</p>","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#hardware-requirements-running-locally","title":"Hardware Requirements (Running Locally)","text":"<ul> <li>GPU: NVIDIA dGPU w/ &gt;= 28 GB VRAM</li> <li>Memory: &gt;= 28 GB of available disk memory, needed to download fine-tuned Code Llama 34B and BGE-Large embedding model</li> </ul> <p>Tested using NVIDIA IGX Orin w/ RTX A6000 and Dell Precision 5820 Workstation w/ RTX A6000</p>","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#running-instructions","title":"Running Instructions","text":"<p>If connecting to your machine via SSH, be sure to forward the appropriate ports:</p> <ul> <li>For chatbot UI: 7860</li> <li>For local LLM: 8080</li> <li>For MCP server: 8090</li> </ul> <pre><code>ssh &lt;user_name&gt;@&lt;IP address&gt; -L 7860:localhost:7860 -L 8080:localhost:8080 -L 8090:localhost:8090\n</code></pre>","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#running-w-local-llm","title":"Running w/ Local LLM","text":"<p>To build and start the app:</p> <pre><code>./holohub run holochat standalone\n</code></pre> <p>Once the LLM is loaded on the GPU and the Gradio app is running, HoloChat should be available at http://127.0.0.1:7860/.</p>","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#running-w-nim-api","title":"Running w/ NIM API","text":"<p>To use the NIM API you must create a .env file at:</p> <pre><code>./applications/holochat/.env\n</code></pre> <p>This is where you should place your NVIDIA API key.</p> <pre><code>NVIDIA_API_KEY=&lt;api_key_here&gt;\n</code></pre> <p>To build and run the app:</p> <pre><code>./holohub run holochat\n</code></pre> <p>Once the Gradio app is running, HoloChat should be available at http://127.0.0.1:7860/.</p>","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#usage-notes","title":"Usage Notes","text":"","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#intended-use","title":"Intended use","text":"<p>HoloChat is developed to accelerate and assist Holoscan developers\u2019 learning and development. HoloChat serves as an intuitive chat interface, enabling users to pose natural language queries related to the Holoscan SDK. Whether seeking general information about the SDK or specific coding insights, users can obtain immediate responses thanks to the underlying Large Language Model (LLM) and vector database.</p> <p>HoloChat is given access to the Holoscan SDK repository, the HoloHub repository, and the Holoscan SDK user guide. This essentially allows users to engage in natural language conversations with these documents, gaining instant access to the information they need, thus sparing them the task of sifting through vast amounts of documentation themselves.</p>","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#known-limitations","title":"Known Limitations","text":"<p>Before diving into how to make the most of HoloChat, it's crucial to understand and acknowledge its known limitations. These limitations can guide you in adopting the best practices below, which will help you navigate and mitigate these issues effectively.</p> <ul> <li>Hallucinations: Occasionally, HoloChat may provide responses that are not entirely accurate. It's advisable to approach answers with a healthy degree of skepticism.</li> <li>Memory Loss: LLM's limited attention window may lead to the loss of previous conversation history. To mitigate this, consider restarting the application to clear the chat history when necessary.</li> <li>Limited Support for Stack Traces: HoloChat's knowledge is based on the Holoscan repository and the user guide, which lack large collections of stack trace data. Consequently, HoloChat may face challenges when assisting with stack traces.</li> </ul>","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#best-practices","title":"Best Practices","text":"<p>While users should be aware of the above limitations, following the recommended tips will drastically minimize these possible shortcomings. In general, the more detailed and precise a question is, the better the results will be. Some best practices when asking questions are:</p> <ul> <li>Be Verbose: If you want to create an application, specify which operators should be used if possible (HolovizOp, V4L2VideoCaptureOp, InferenceOp, etc.).</li> <li>Be Specific: The less open-ended a question is the less likely the model will hallucinate.</li> <li>Specify Programming Language: If asking for code, include the desired language (Python or C++).</li> <li>Provide Code Snippets: If debugging errors include as much relevant information as possible. Copy and paste the code snippet that produces the error, the abbreviated stack trace, and describe any changes that may have introduced the error.</li> </ul> <p>In order to demonstrate how to get the most out of HoloChat two example questions are posed below. These examples illustrate how a user can refine their questions and as a result, improve the responses they receive:</p> <p>Worst: \u201cCreate an app that predicts the labels associated with a video\u201d</p> <p>Better: \u201cCreate a Python app that takes video input and sends it through a model for inference.\u201d</p> <p>Best: \u201cCreate a Python Holoscan application that receives streaming video input, and passes that video input into a pytorch classification model for inference. Then, collect the model\u2019s predicted class and use Holoviz to display the class label on each video frame.\u201d</p> <p>Worst: \u201cWhat os can I use?\u201d</p> <p>Better: \u201cWhat operating system can I use with Holoscan?\u201d</p> <p>Best: \u201cCan I use MacOS with the Holoscan SDK?\u201d</p>","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#appendix","title":"Appendix","text":"","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#meta-terms-of-use","title":"Meta Terms of Use","text":"<p>By using the Code-Llama model, you are agreeing to the terms and conditions of the license, acceptable use policy and Meta\u2019s privacy policy.</p>","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holochat/#implementation-details","title":"Implementation Details","text":"<p>HoloChat operates by taking user input and comparing it to the text stored within the vector database, which is comprised of Holoscan SDK information. The most relevant text segments from SDK code and the user guide are then appended to the user's query. This approach allows the chosen LLM to answer questions about the Holoscan SDK, without being explicitly trained on SDK data.</p> <p>However, there is a drawback to this method - the most relevant documentation is not always found within the vector database. Since the user's question serves as the search query, queries that are too simplistic or abbreviated may fail to extract the most relevant documents from the vector database. As a consequence, the LLM will then lack the necessary context, leading to poor and potentially inaccurate responses. This occurs because LLMs strive to provide the most probable response to a question, and without adequate context, they hallucinate to fill in these knowledge gaps.</p>","tags":["Natural Language and Conversational AI","RAG","Vector Database","LLM"]},{"location":"applications/holoscan_ros2/pubsub/","title":"Holoscan ROS2 Publisher/Subscriber Examples","text":"<p>     \u25b6 Run Locally  Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: October 9, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 3.0 Tested Holoscan SDK versions: 3.0 Contribution metric: Level 1 - Highly Reliable</p>","tags":["Robotics","ROS2","Publisher","Subscriber"]},{"location":"applications/holoscan_ros2/pubsub/#overview","title":"Overview","text":"<p>This application demonstrates basic communication between Holoscan and ROS2, containing both publisher and subscriber examples in a single combined structure. The examples show how to: - Send simple string messages from Holoscan to ROS2 (Publisher) - Receive messages from ROS2 in Holoscan operators (Subscriber) - Bridge data between the two frameworks</p> <p>Both C++ and Python implementations are included, with the publisher and subscriber components organized under shared <code>cpp/</code> and <code>python/</code> directories.</p>","tags":["Robotics","ROS2","Publisher","Subscriber"]},{"location":"applications/holoscan_ros2/pubsub/#file-structure","title":"File Structure","text":"<pre><code>applications/holoscan_ros2/pubsub/\n\u251c\u2500\u2500 CMakeLists.txt          # Top-level build configuration\n\u251c\u2500\u2500 README.md               # This file\n\u251c\u2500\u2500 cpp/                    # C++ implementations\n\u2502   \u251c\u2500\u2500 CMakeLists.txt      # C++ build configuration\n\u2502   \u251c\u2500\u2500 metadata.json       # C++ application metadata\n\u2502   \u251c\u2500\u2500 talker.cpp          # Publisher implementation\n\u2502   \u2514\u2500\u2500 listener.cpp        # Subscriber implementation\n\u2514\u2500\u2500 python/                 # Python implementations\n    \u251c\u2500\u2500 CMakeLists.txt      # Python build configuration\n    \u251c\u2500\u2500 metadata.json       # Python application metadata\n    \u251c\u2500\u2500 talker.py           # Publisher implementation\n    \u2514\u2500\u2500 listener.py         # Subscriber implementation\n</code></pre> <p>Note: This structure combines the previously separate <code>holoscan_ros2_simple_publisher</code> and <code>holoscan_ros2_simple_subscriber</code> applications into a single, more organized layout. All functionality remains the same, but both components are now built and managed together.</p>","tags":["Robotics","ROS2","Publisher","Subscriber"]},{"location":"applications/holoscan_ros2/pubsub/#prerequisites","title":"Prerequisites","text":"<ul> <li>NVIDIA Holoscan SDK v3.0 or later</li> <li>ROS2 Jazzy (all examples and Dockerfiles are tested with Jazzy; other distributions may work but are not tested)</li> <li>Docker (with NVIDIA Container Toolkit and a recent version)</li> <li>NVIDIA GPU drivers (suitable for your hardware and Holoscan SDK)</li> </ul>","tags":["Robotics","ROS2","Publisher","Subscriber"]},{"location":"applications/holoscan_ros2/pubsub/#building-the-application","title":"Building the Application","text":"<p>Build the combined publisher/subscriber application: <pre><code>./holohub build pubsub\n</code></pre></p> <p>This single command builds both the publisher and subscriber components for both C++ and Python.</p>","tags":["Robotics","ROS2","Publisher","Subscriber"]},{"location":"applications/holoscan_ros2/pubsub/#application-modes","title":"Application Modes","text":"<p>The pubsub application uses modes to handle the publisher and subscriber components. You can list available modes using: <pre><code>./holohub modes pubsub --language cpp\n./holohub modes pubsub --language python\n</code></pre></p> <p>Available modes: - publisher: Sends simple string messages to ROS2 topic (default mode) - subscriber: Receives messages from ROS2 topic</p>","tags":["Robotics","ROS2","Publisher","Subscriber"]},{"location":"applications/holoscan_ros2/pubsub/#running-c-publisher-and-subscriber","title":"Running C++ Publisher and Subscriber","text":"<p>Run the Publisher and the Subscriber components in different consoles. Use the <code>modes</code> feature to specify which component to run.</p> <p>Publisher component: <pre><code>./holohub run pubsub publisher --language cpp\n# Or simply (since publisher is the default mode):\n./holohub run pubsub --language cpp\n</code></pre></p> <p>Expected output: <pre><code>Publishing: 'Hello, world! 12'\nPublishing: 'Hello, world! 13'\nPublishing: 'Hello, world! 14'\n...\n</code></pre></p> <p>Subscriber component: <pre><code>./holohub run pubsub subscriber --language cpp\n</code></pre></p> <p>Expected output: <pre><code>I heard: 'Hello, world! 12'\nI heard: 'Hello, world! 13'\nI heard: 'Hello, world! 14'\n...\n</code></pre></p> <p>Validation: - The publisher should output periodic \"Publishing\" logs with incrementing numbers - The subscriber should output corresponding \"I heard\" logs with matching messages - Messages should match between publisher and subscriber</p>","tags":["Robotics","ROS2","Publisher","Subscriber"]},{"location":"applications/holoscan_ros2/pubsub/#running-python-publisher-and-subscriber","title":"Running Python Publisher and Subscriber","text":"<p>Run the Publisher and the Subscriber components in different consoles. Use the same <code>modes</code> as the C++ version.</p> <p>Publisher component: <pre><code>./holohub run pubsub publisher --language python\n# Or simply (since publisher is the default mode):\n./holohub run pubsub --language python\n</code></pre></p> <p>Subscriber component: <pre><code>./holohub run pubsub subscriber --language python\n</code></pre></p> <p>Validation: - Similar to C++ examples, you should see matching publish/receive logs - Python and C++ versions are interoperable (you can mix languages)</p>","tags":["Robotics","ROS2","Publisher","Subscriber"]},{"location":"applications/holoscan_ros2/pubsub/#additional-resources","title":"Additional Resources","text":"","tags":["Robotics","ROS2","Publisher","Subscriber"]},{"location":"applications/holoscan_ros2/pubsub/#documentation","title":"Documentation","text":"<ul> <li>Holoscan SDK Documentation</li> <li>ROS2 Humble Documentation</li> <li>ROS2 Tutorials</li> </ul>","tags":["Robotics","ROS2","Publisher","Subscriber"]},{"location":"applications/holoscan_ros2/pubsub/#related-examples","title":"Related Examples","text":"<ul> <li>Applications Overview: <code>../</code> - Background on ROS2 and Holoscan integration</li> <li>Camera Examples: <code>../vb1940/</code> - Advanced camera integration with VB1940 hardware</li> <li>Bridge Library: <code>../../../operators/holoscan_ros2/</code> - Bridge implementation and headers</li> </ul>","tags":["Robotics","ROS2","Publisher","Subscriber"]},{"location":"applications/holoscan_ros2/pubsub/#community-and-support","title":"Community and Support","text":"<ul> <li>Holoscan SDK GitHub</li> <li>ROS2 Community</li> <li>NVIDIA Developer Forums</li> </ul>","tags":["Robotics","ROS2","Publisher","Subscriber"]},{"location":"applications/holoscan_ros2/vb1940/","title":"Holoscan ROS2 VB1940 (Eagle) Camera","text":"<p>     \u25b6 Run Locally  Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: October 9, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 3.0 Tested Holoscan SDK versions: 3.0 Contribution metric: Level 1 - Highly Reliable</p>","tags":["Robotics","ROS2","VB1940","Publisher","Subscriber"]},{"location":"applications/holoscan_ros2/vb1940/#overview","title":"Overview","text":"<p>This unified VB1940 (Eagle) Camera application demonstrates advanced usage with real camera hardware, containing both publisher and subscriber components in a single structure. The application shows how to: - Capture images from a VB1940 (Eagle) camera using Holoscan (Publisher mode) - Process camera data through a complete pipeline - Publish processed images to ROS2 topics for visualization or further processing - Subscribe to and visualize camera streams (Subscriber mode)</p> <p>The application uses modes to run either the publisher or subscriber component, with both C++ implementations organized under a unified <code>cpp/</code> directory.</p> <p>Important: This application requires access to NVIDIA's internal hololink repository and VB1940 camera hardware.</p>","tags":["Robotics","ROS2","VB1940","Publisher","Subscriber"]},{"location":"applications/holoscan_ros2/vb1940/#file-structure","title":"File Structure","text":"<pre><code>applications/holoscan_ros2/vb1940/\n\u251c\u2500\u2500 CMakeLists.txt          # Top-level build configuration\n\u251c\u2500\u2500 README.md               # This file\n\u2514\u2500\u2500 cpp/                    # C++ implementation\n    \u251c\u2500\u2500 CMakeLists.txt      # C++ build configuration\n    \u251c\u2500\u2500 metadata.json       # Application metadata with modes\n    \u251c\u2500\u2500 vb1940_publisher.cpp    # Publisher implementation\n    \u2514\u2500\u2500 vb1940_subscriber.cpp   # Subscriber implementation\n</code></pre> <p>Note: This structure combines the previously separate <code>holoscan_ros2_vb1940_publisher</code> and <code>holoscan_ros2_vb1940_subscriber</code> applications.</p>","tags":["Robotics","ROS2","VB1940","Publisher","Subscriber"]},{"location":"applications/holoscan_ros2/vb1940/#prerequisites","title":"Prerequisites","text":"<ul> <li>NVIDIA Holoscan SDK v3.0 or later</li> <li>ROS2 Jazzy (all examples and Dockerfiles are tested with Jazzy; other distributions may work but are not tested)</li> <li>Docker (with NVIDIA Container Toolkit)</li> <li>NVIDIA GPU drivers (suitable for your hardware and Holoscan SDK)</li> <li>Git and SSH access (for VB1940 examples requiring hololink repository)</li> </ul> <p>For VB1940 camera examples, you'll also need: - Access to NVIDIA's internal hololink repository - VB1940 (Eagle) camera hardware - Proper network configuration for camera communication</p>","tags":["Robotics","ROS2","VB1940","Publisher","Subscriber"]},{"location":"applications/holoscan_ros2/vb1940/#network-setup","title":"Network Setup","text":"<p>Before running VB1940 examples, ensure proper network configuration: - Default Hololink board IP: <code>192.168.0.2</code> - Ensure your host can reach the Hololink board - Verify IBV (InfiniBand Verbs) device configuration if using custom hardware</p>","tags":["Robotics","ROS2","VB1940","Publisher","Subscriber"]},{"location":"applications/holoscan_ros2/vb1940/#building-the-application","title":"Building the Application","text":"<p>Build the unified VB1940 camera application: <pre><code>./holohub build vb1940 --build-args=\"--ssh default\"\n</code></pre></p> <p>Note: The <code>--build-args=\"--ssh default\"</code> is required for accessing the internal hololink repository during build.</p>","tags":["Robotics","ROS2","VB1940","Publisher","Subscriber"]},{"location":"applications/holoscan_ros2/vb1940/#application-modes","title":"Application Modes","text":"<p>The vb1940 application uses modes to handle the publisher and subscriber components. You can list available modes using: <pre><code>./holohub modes vb1940 --language cpp\n</code></pre></p> <p>Available modes: - publisher: Captures and publishes VB1940 camera images (default mode) - subscriber: Receives and visualizes camera images</p>","tags":["Robotics","ROS2","VB1940","Publisher","Subscriber"]},{"location":"applications/holoscan_ros2/vb1940/#running-the-application","title":"Running the Application","text":"","tags":["Robotics","ROS2","VB1940","Publisher","Subscriber"]},{"location":"applications/holoscan_ros2/vb1940/#publisher-mode-vb1940_publishercpp","title":"Publisher Mode (<code>vb1940_publisher.cpp</code>)","text":"<p>Captures images from a VB1940 (Eagle) camera using Holoscan and publishes to ROS2: - Processes images through a complete pipeline:   - CSI to Bayer conversion   - Image processing   - Bayer demosaicing - Publishes processed images to ROS2 topic <code>vb1940/image</code> - Supports various camera modes and configurations</p> <p>Usage: <pre><code># Run publisher (default mode)\n./holohub run vb1940 [options]\n# Or explicitly specify publisher mode:\n./holohub run vb1940 publisher [options]\n</code></pre></p> <p>Publisher Options: - <code>--camera-mode</code>: VB1940 (Eagle) mode (default: 2560x1984 30FPS) - <code>--frame-limit</code>: Exit after publishing specified number of frames - <code>--hololink</code>: IP address of Hololink board (default: 192.168.0.2) - <code>--ibv-name</code>: IBV device to use - <code>--ibv-port</code>: Port number of IBV device (default: 1)</p>","tags":["Robotics","ROS2","VB1940","Publisher","Subscriber"]},{"location":"applications/holoscan_ros2/vb1940/#subscriber-mode-vb1940_subscribercpp","title":"Subscriber Mode (<code>vb1940_subscriber.cpp</code>)","text":"<p>Subscribes to camera images from ROS2 and visualizes them: - Subscribes to the <code>vb1940/image</code> ROS2 topic - Receives and processes the images - Visualizes images using Holoviz - Supports headless and fullscreen modes</p> <p>Usage: <pre><code>./holohub run vb1940 subscriber [options]\n</code></pre></p> <p>Subscriber Options: - <code>--headless</code>: Run in headless mode - <code>--fullscreen</code>: Run in fullscreen mode</p>","tags":["Robotics","ROS2","VB1940","Publisher","Subscriber"]},{"location":"applications/holoscan_ros2/vb1940/#additional-resources","title":"Additional Resources","text":"","tags":["Robotics","ROS2","VB1940","Publisher","Subscriber"]},{"location":"applications/holoscan_ros2/vb1940/#documentation","title":"Documentation","text":"<ul> <li>Holoscan SDK Documentation</li> <li>ROS2 Humble Documentation</li> <li>ROS2 Tutorials</li> </ul>","tags":["Robotics","ROS2","VB1940","Publisher","Subscriber"]},{"location":"applications/holoscan_ros2/vb1940/#hardware","title":"Hardware","text":"<ul> <li>VB1940 Eagle Camera - Leopard Imaging</li> </ul>","tags":["Robotics","ROS2","VB1940","Publisher","Subscriber"]},{"location":"applications/holoscan_ros2/vb1940/#related-examples","title":"Related Examples","text":"<ul> <li>Applications Overview: <code>../</code> - Background on ROS2 and Holoscan integration</li> <li>Simple Examples: <code>../pubsub/</code> - Basic publisher/subscriber communication</li> <li>Bridge Library: <code>../../../operators/holoscan_ros2/</code> - Bridge implementation and headers</li> </ul>","tags":["Robotics","ROS2","VB1940","Publisher","Subscriber"]},{"location":"applications/holoscan_ros2/vb1940/#community-and-support","title":"Community and Support","text":"<ul> <li>Holoscan SDK GitHub</li> <li>ROS2 Community</li> <li>NVIDIA Developer Forums</li> </ul>","tags":["Robotics","ROS2","VB1940","Publisher","Subscriber"]},{"location":"applications/holoviz/holoviz_hdr/","title":"Holoviz HDR","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: August 5, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.5 Tested Holoscan SDK versions: 2.5 Contribution metric: Level 1 - Highly Reliable</p> <p> This application demonstrates displaying HDR images using the Holoviz operator. The application creates image data in HDR10 (BT2020 color space) with SMPTE ST2084 Perceptual Quantizer (PQ) EOTF and displays the image on the screen.</p> <p>Note that the screenshot above does not show the real HDR image on the display since it's not possible to take screenshots of HDR images.</p> <p>The Holoviz operator parameter <code>display_color_space</code> is used to set the color space. This allows HDR output on Linux distributions and displays supporting that feature. See https://docs.nvidia.com/holoscan/sdk-user-guide/visualization.html#hdr for more information.</p> <pre><code>    auto holoviz = make_operator&lt;ops::HolovizOp&gt;(\n        \"holoviz\",\n        // select the HDR10 ST2084 display color space\n        Arg(\"display_color_space\", ops::HolovizOp::ColorSpace::HDR10_ST2084));\n</code></pre>","tags":["Computer Vision and Perception","Visualization","color space conversion","ST2084","Holoviz"]},{"location":"applications/holoviz/holoviz_hdr/#run-instructions","title":"Run Instructions","text":"<p>To build and start the application:</p> <pre><code>./holohub run holoviz_hdr\n</code></pre>","tags":["Computer Vision and Perception","Visualization","color space conversion","ST2084","Holoviz"]},{"location":"applications/holoviz/holoviz_srgb/","title":"Holoviz sRGB","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: August 5, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.3 Tested Holoscan SDK versions: 2.3 Contribution metric: Level 1 - Highly Reliable</p> <p> This application demonstrates the handling of the sRGB color space supported by the Holoviz operator.</p> <p>The Holoviz operator can convert sRGB input images to linear color space before rendering and also can convert from linear color space to sRGB before writing to the frame buffer.</p> <p>sRGB color space can be enabled for input images and for the frame buffer independently. By default, the sRGB color space is disabled for both.</p> <p>By default, the Holoviz operator is auto detecting the input image format. Auto detection always assumes linear color space for input images. To change this to sRGB color space explicitly set the <code>image_format_</code> member of the input spec for that input image to a format ending with <code>SRGB</code>:</p> <pre><code>    // By default the image format is auto detected. Auto detection assumes linear color space,\n    // but we provide an sRGB encoded image. Create an input spec and change the image format to\n    // sRGB.\n    ops::HolovizOp::InputSpec input_spec(\"image\", ops::HolovizOp::InputType::COLOR);\n    input_spec.image_format_ = ops::HolovizOp::ImageFormat::R8G8B8_SRGB;\n\n    auto holoviz = make_operator&lt;ops::HolovizOp&gt;(\n        \"holoviz\",\n        Arg(\"tensors\", std::vector&lt;ops::HolovizOp::InputSpec&gt;{input_spec}));\n</code></pre> <p>By default, the frame buffer is using linear color space. To use the sRGB color space, set the <code>framebuffer_srbg</code> argument of the Holoviz operator to <code>true</code>:</p> <pre><code>    auto holoviz = make_operator&lt;ops::HolovizOp&gt;(\n        \"holoviz\",\n        // enable the sRGB frame buffer\n        Arg(\"framebuffer_srbg\", true));\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Benchmarking","Holoviz","Rendering"]},{"location":"applications/holoviz/holoviz_srgb/#run-instructions","title":"Run Instructions","text":"<p>To build and start the application:</p> <pre><code>./holohub run holoviz_srgb\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Benchmarking","Holoviz","Rendering"]},{"location":"applications/holoviz/holoviz_ui/","title":"Holoviz UI","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: October 9, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.5 Tested Holoscan SDK versions: 2.5 Contribution metric: Level 1 - Highly Reliable</p> <p> This application uses the layer callback provided by the Holoviz operator and leverages the Holoviz module API to add an UI layer with <code>Dear ImGui</code> elements and a geometry layer dynamically changing based on user input.</p>","tags":["Rendering","Visualization","Holoviz"]},{"location":"applications/holoviz/holoviz_ui/#run-instructions","title":"Run Instructions","text":"<p>To build and start the application:</p> <pre><code>./holohub run holoviz_ui\n</code></pre>","tags":["Rendering","Visualization","Holoviz"]},{"location":"applications/holoviz/holoviz_vsync/","title":"Holoviz vsync","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: August 5, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.3 Tested Holoscan SDK versions: 2.3 Contribution metric: Level 1 - Highly Reliable</p> <p> This application demonstrates the capability of the Holoviz operator to wait for the vertical blank of the display before updating the current image. It prints the displayed frames per second to the console, if sync to vertical blank is enabled the frames per second are capped to the display refresh rate.</p> <p>To enable syncing to vertical blank set the <code>vsync</code> parameter of the Holoviz operator to <code>true</code>:</p> <pre><code>    auto holoviz = make_operator&lt;ops::HolovizOp&gt;(\n        \"holoviz\",\n        // enable synchronization to vertical blank\n        Arg(\"vsync\", true));\n</code></pre> <p>By default, the Holoviz operator is not syncing to the vertical blank of the display.</p>","tags":["Computer Vision and Perception","Visualization","frame rate synchronization","Image Processing","Holoviz","Rendering"]},{"location":"applications/holoviz/holoviz_vsync/#run-instructions","title":"Run Instructions","text":"<p>To build and start the application:</p> <pre><code>./holohub run holoviz_vsync\n</code></pre>","tags":["Computer Vision and Perception","Visualization","frame rate synchronization","Image Processing","Holoviz","Rendering"]},{"location":"applications/holoviz/holoviz_yuv/","title":"Holoviz YUV","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: August 5, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.4 Tested Holoscan SDK versions: 2.4 Contribution metric: Level 1 - Highly Reliable</p> <p> This application demonstrates the capability of the Holoviz operator to display images in YUV (aka YCbCr) format.</p> <p>Holoviz supports multiple YUV formats including 420 and 422, 8 and 16 bit, single plane and multi plane. It supports BT.601, BT.709 and BT.2020 color conversions, narrow and full range and cosited even and midpoint chroma downsample positions.</p> <p>The application creates a GXF video buffer containing YUV 420 BT.601 extended range data.</p> <p>The YUV image properties are specified using a input spec structure:</p> <pre><code>    ops::HolovizOp::InputSpec input_spec(\"image\", ops::HolovizOp::InputType::COLOR);\n\n    // Set the YUV image format, model conversion and range for the input tensor.\n    input_spec.image_format_ = ops::HolovizOp::ImageFormat::Y8_U8V8_2PLANE_420_UNORM;\n    input_spec.yuv_model_conversion_ = ops::HolovizOp::YuvModelConversion::YUV_601;\n    input_spec.yuv_range_ = ops::HolovizOp::YuvRange::ITU_FULL;\n\n    auto holoviz = make_operator&lt;ops::HolovizOp&gt;(\n        \"holoviz\",\n        Arg(\"tensors\", std::vector&lt;ops::HolovizOp::InputSpec&gt;{input_spec}));\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Video","Holoviz","Image Processing"]},{"location":"applications/holoviz/holoviz_yuv/#run-instructions","title":"Run Instructions","text":"<p>To build and start the application:</p> <pre><code>./holohub run holoviz_yuv\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Video","Holoviz","Image Processing"]},{"location":"applications/hyperspectral_segmentation/","title":"Hyperspectral Image Segmentation","text":"<p>     \u25b6 Run Locally  Authors: Lars Doorenbos (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 2 - Trusted</p> <p></p> <p>This application segments endoscopic hyperspectral cubes into 20 organ classes. It visualizes the result together with the RGB image corresponding to the cube.</p>","tags":["Healthcare AI","Visualization","Hyperspectral Imaging","Segmentation","Endoscopy","Image Processing"]},{"location":"applications/hyperspectral_segmentation/#data-and-models","title":"Data and Models","text":"<p>The data is a subset of the HeiPorSPECTRAL dataset. The application loops over the 84 cubes selected. The model is the <code>2022-02-03_22-58-44_generated_default_model_comparison</code> checkpoint from this repository, converted to ONNX with the script in <code>utils/convert_to_onnx.py</code>.</p> <p>\ud83d\udce6\ufe0f (NGC) App Data and Model for Hyperspectral Segmentation.  This resource is automatically downloaded when building the application.</p>","tags":["Healthcare AI","Visualization","Hyperspectral Imaging","Segmentation","Endoscopy","Image Processing"]},{"location":"applications/hyperspectral_segmentation/#run-instructions","title":"Run Instructions","text":"<p>This application requires some python modules to be installed. You can simply use Holohub CLI to build and run the application.</p> <pre><code>./holohub run hyperspectral_segmentation\n</code></pre> <p>This single command builds and runs a Docker container, then inside that container, it builds and runs the application.</p> <p>To build and run the container without building the application, you can use the following command:</p> <pre><code>./holohub run-container hyperspectral_segmentation\n</code></pre>","tags":["Healthcare AI","Visualization","Hyperspectral Imaging","Segmentation","Endoscopy","Image Processing"]},{"location":"applications/hyperspectral_segmentation/#viewing-results","title":"Viewing Results","text":"<p>With the default settings, the results of this application are saved to <code>result.png</code> file in the hyperspectral segmentation app directory. Each time a new image is processed, it overwrites <code>result.png</code>.  By opening this image while the application is running, you can see the results as the updates are made (may depend on your image viewer).</p>","tags":["Healthcare AI","Visualization","Hyperspectral Imaging","Segmentation","Endoscopy","Image Processing"]},{"location":"applications/iio/","title":"Industrial I/O (IIO) - ADALM-Pluto SDR Integration","text":"<p>     \u25b6 Run Locally  Authors: Andrei-Fabian Pop (Analog Devices Inc.) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: October 9, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 3 - Developmental</p>","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#overview","title":"Overview","text":"<p>This application demonstrates how to use the IIO (Industrial I/O) operators to interface with Software Defined Radio (SDR) devices, specifically the ADALM-Pluto SDR from Analog Devices. The application showcases real-time signal generation, transmission, and reception capabilities using the Holoscan SDK framework.</p>","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#what-is-adalm-pluto","title":"What is ADALM-Pluto?","text":"<p>The ADALM-Pluto (Active Learning Module) is an affordable, portable SDR platform that can transmit and receive RF signals from 325 MHz to 3.8 GHz. It features: - AD9363 transceiver chip - USB-powered operation - 20 MHz bandwidth (upgradeable to 56 MHz) - Support for various modulation schemes - Multiple input/output channels for data streaming</p>","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#application-capabilities","title":"Application Capabilities","text":"<p>This application provides examples of: 1. Attribute Reading/Writing: Configure SDR parameters like sampling rate, gain, and frequency 2. Buffer Operations: Stream raw buffer data through device channels for signal transmission and reception 3. Device Configuration: Apply complex device settings from YAML configuration files 4. Signal Generation: Create and transmit test signals through buffer channels</p>","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#use-cases","title":"Use Cases","text":"<ul> <li>RF Signal Analysis: Capture and analyze radio frequency signals in real-time</li> <li>Communications Research: Prototype and test wireless communication systems</li> <li>Educational Labs: Learn about SDR concepts and digital signal processing</li> <li>IoT Development: Test and debug wireless IoT protocols</li> <li>Amateur Radio: Digital mode experimentation and signal monitoring</li> <li>Data Acquisition: High-speed streaming of ADC/DAC samples</li> </ul>","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#architecture","title":"Architecture","text":"<p>The application uses the IIO (Industrial I/O) Linux subsystem to communicate with the ADALM-Pluto device. The IIO framework provides a standardized interface for: - Reading/writing device attributes (frequency, gain, sample rate) - Streaming raw data buffers through device channels - Managing device states and configurations</p>","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#requirements","title":"Requirements","text":"","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#hardware","title":"Hardware","text":"<ul> <li>ADALM-Pluto SDR device</li> <li>USB connection to host computer</li> <li>Optional: Antennas for RF transmission/reception</li> </ul>","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#software","title":"Software","text":"<ul> <li>Holoscan SDK</li> <li>libiio library (version 0.x)</li> <li>Network connection to Pluto (default IP: 192.168.2.1), connecting through usb requires minimal code modifications</li> </ul>","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#quick-start","title":"Quick Start","text":"","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#1-connect-adalm-pluto","title":"1. Connect ADALM-Pluto","text":"<p>Connect your ADALM-Pluto to your computer via USB. The device will appear as a network interface with IP address 192.168.2.1.</p>","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#2-build-the-application","title":"2. Build the Application","text":"<pre><code># From HoloHub root directory\n./dev_container build_and_run iio\n</code></pre>","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#3-run-examples","title":"3. Run Examples","text":"<p>The application includes several example modes:</p>","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#transmit-a-test-signal","title":"Transmit a Test Signal","text":"<pre><code># Transmits a sine wave on the configured frequency\n./run launch iio python\n</code></pre>","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#read-device-attributes","title":"Read Device Attributes","text":"<p>Uncomment the <code>attr_read_example()</code> line in the compose() method to read device parameters.</p>","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#configure-device-from-yaml","title":"Configure Device from YAML","text":"<p>Uncomment the <code>configurator_example()</code> line to apply settings from iio_config.yaml.</p>","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#run-the-pluto-fft-example","title":"Run the Pluto FFT example","text":"<p>The C++ FFT example (<code>cpp/pluto_fft_example</code>) demonstrates real-time FFT visualization of RF signals captured from the ADALM-Pluto:</p> <pre><code># Build and run the FFT example\n./run build iio cpp\n./run launch iio cpp\n</code></pre> <p>This example captures IQ samples from the Pluto SDR and displays a real-time FFT spectrum:</p> <p></p>","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#configuration","title":"Configuration","text":"","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#network-settings","title":"Network Settings","text":"<p>The default URI for ADALM-Pluto is <code>ip:192.168.2.1</code>. Modify the <code>G_URI</code> variable in the Python script if your device has a different IP address.</p>","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#signal-parameters","title":"Signal Parameters","text":"<p>In the <code>BasicIIOBufferEmitterOp</code> class: - <code>frequency</code>: Sine wave frequency (Hz) - <code>amplitude</code>: Signal amplitude - <code>sample_rate</code>: Samples per second - <code>total_samples</code>: Number of samples per buffer</p>","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#channel-configuration","title":"Channel Configuration","text":"<ul> <li><code>G_NUM_CHANNELS</code>: Set to 1 or 2 for single/dual channel operation</li> </ul>","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#example-workflows","title":"Example Workflows","text":"","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#1-signal-generation-and-transmission","title":"1. Signal Generation and Transmission","text":"<p>The buffer write example generates a sine wave and transmits it through the SDR: <pre><code># Generates 8kHz sine wave with amplitude 408\ndata_vector = self.generate_sinewave(total_samples, frequency=8, amplitude=408, sample_rate=400)\n</code></pre></p>","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#2-signal-reception","title":"2. Signal Reception","text":"<p>The buffer read example captures IQ samples from the RF input: <pre><code># Captures 8192 samples from the receiver\niio_buf_read_op = IIOBufferRead(\n    samples_count=8192,\n    enabled_channel_names=[\"voltage0\", \"voltage1\"]\n)\n</code></pre></p>","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#3-device-configuration","title":"3. Device Configuration","text":"<p>The YAML configuration allows complex device setup: <pre><code>devices:\n  - ad9361-phy:\n      attrs:\n        - ensm_mode: \"fdd\"\n        - calib_mode: \"manual\"\n</code></pre></p>","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#understanding-channel-data","title":"Understanding Channel Data","text":"<p>The ADALM-Pluto provides multiple channels for data streaming: - voltage0, voltage1: Independent channels that can be used for different data streams - Channel Types: Each channel can be configured as input or output - Data Format: Raw 16-bit signed integer samples - Buffer Organization: Samples from enabled channels are interleaved in the buffer</p> <p>For example, with two channels enabled: - Single channel: [S0, S1, S2, S3, ...] where S represents samples from that channel - Dual channel: [Ch0_S0, Ch1_S0, Ch0_S1, Ch1_S1, ...] where samples alternate between channels</p> <p>Note: The actual interpretation of the data (whether it represents I/Q components, real signals, or other data types) depends on your application and how you process the raw samples. The IIO operators simply provide raw buffer access without any automatic data conversion.</p>","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>Connection Issues: Ensure ADALM-Pluto is recognized as a network device (check with <code>ping 192.168.2.1</code> or using <code>iio_info -S</code> from <code>libiio</code>)</li> <li>Buffer Overruns: Reduce sample count or increase processing speed</li> <li>Signal Quality: Check antenna connections and gain settings</li> <li>Permission Errors: May need to run with appropriate permissions for USB device access</li> </ol>","tags":["Signal Processing","iio","libiio"]},{"location":"applications/iio/#references","title":"References","text":"<ul> <li>ADALM-Pluto Documentation</li> <li>libiio Documentation</li> <li>AD9361 Datasheet</li> <li>Scopy Application</li> </ul>","tags":["Signal Processing","iio","libiio"]},{"location":"applications/imaging_ai_segmentator/","title":"Imaging AI Whole Body Segmentation","text":"<p>     \u25b6 Run Locally  Authors: Ming Qin (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.3.0 Tested Holoscan SDK versions: 2.3.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates the use of medical imaging operators to build and package an application that parses DICOM images and performs inference using a MONAI model (TotalSegmentator).</p> <p> Fig. 1: 3D volume rendering of segmentation results in NIfTI format</p>","tags":["Healthcare AI","Imaging","Volumetric Reconstruction","Healthcare Interop","TotalSegmentator","Medical Imaging"]},{"location":"applications/imaging_ai_segmentator/#overview","title":"Overview","text":"<p>This application uses a MONAI re-trained TotalSegmentator model to segment 104 body parts from a DICOM CT series. It is implemented using Holohub DICOM processing operators and PyTorch inference operators.</p> <p>The input is a DICOM CT series, and the segmentation results are saved as both DICOM Segmentation (Part 10 storage format) and NIfTI format. The workflow includes:</p> <ul> <li>Loading DICOM studies</li> <li>Selecting series with application-defined rules</li> <li>Converting DICOM pixel data to a 3D volume image</li> <li>Using the MONAI SDK to transform input/output and perform inference</li> <li>Writing results as a DICOM Segmentation OID instance, reusing study-level metadata from the original DICOM study</li> </ul> <p> Fig. 2: A slice of the segmentation saved in a DICOM segmentation instance (without color coding the segments)</p>","tags":["Healthcare AI","Imaging","Volumetric Reconstruction","Healthcare Interop","TotalSegmentator","Medical Imaging"]},{"location":"applications/imaging_ai_segmentator/#requirements","title":"Requirements","text":"<ul> <li>On a Holohub supported platform</li> <li>Python 3.8+</li> <li>Python packages from PyPI, including:</li> <li>torch</li> <li>monai</li> <li>nibabel</li> <li>pydicom</li> <li>highdicom</li> <li>Other dependencies as specified in requirements.txt</li> <li>NVIDIA GPU with at least 14GB memory (for a 200-slice CT series)</li> </ul>","tags":["Healthcare AI","Imaging","Volumetric Reconstruction","Healthcare Interop","TotalSegmentator","Medical Imaging"]},{"location":"applications/imaging_ai_segmentator/#data","title":"Data","text":"<p>The input for this application is a folder of DICOM image files from a CT series. For testing, CT scan images can be downloaded from The Cancer Imaging Archive, subject to Data Usage Policies and Restrictions.</p> <p>One such dataset, a CT Abdomen series described as <code>ABD/PANC_3.0_B31f</code>, was used in testing the application. Other DICOM CT Abdomen series can be downloaded from TCIA as test inputs.</p>","tags":["Healthcare AI","Imaging","Volumetric Reconstruction","Healthcare Interop","TotalSegmentator","Medical Imaging"]},{"location":"applications/imaging_ai_segmentator/#data-citation","title":"Data Citation","text":"<p>National Cancer Institute Clinical Proteomic Tumor Analysis Consortium (CPTAC). (2018). The Clinical Proteomic Tumor Analysis Consortium Cutaneous Melanoma Collection (CPTAC-CM) (Version 11) [Dataset]. The Cancer Imaging Archive. https://doi.org/10.7937/K9/TCIA.2018.ODU24GZE</p>","tags":["Healthcare AI","Imaging","Volumetric Reconstruction","Healthcare Interop","TotalSegmentator","Medical Imaging"]},{"location":"applications/imaging_ai_segmentator/#model","title":"Model","text":"<p>This application uses the MONAI whole-body segmentation model, which can segment 104 body parts from CT scans.</p>","tags":["Healthcare AI","Imaging","Volumetric Reconstruction","Healthcare Interop","TotalSegmentator","Medical Imaging"]},{"location":"applications/imaging_ai_segmentator/#build-and-run-instructions","title":"Build and Run Instructions","text":"","tags":["Healthcare AI","Imaging","Volumetric Reconstruction","Healthcare Interop","TotalSegmentator","Medical Imaging"]},{"location":"applications/imaging_ai_segmentator/#quick-start-using-holohub-container","title":"Quick Start Using Holohub Container","text":"<p>This is the simplest and fastest way to run the application:</p> <pre><code>./holohub run imaging_ai_segmentator\n</code></pre> <p>Note: It takes quite a few minutes when this command is run the first time. This command pulls the latest Holoscan SDK docker image, create Holohub docker image and set up requirement for this application, run the container, and finally build and run the application.</p> <p>The output will be available in the <code>\"&lt;LOCAL_HOLOHUB_PATH&gt;/build/imaging_ai_segmentator/output\"</code> directory, where <code>&lt;LOCAL_HOLOHUB_PATH&gt;</code> refers to where you have cloned your Holohub repository and running the <code>./holohub</code> command.</p> <pre><code>output\n\u251c\u2500\u2500 1.2.826.0.1.3680043.10.511.3.57591117750107235783166330094310669.dcm\n\u2514\u2500\u2500 saved_images_folder\n    \u2514\u2500\u2500 1.3.6.1.4.1.14519.5.2.1.7085.2626\n        \u251c\u2500\u2500 1.3.6.1.4.1.14519.5.2.1.7085.2626.nii\n        \u2514\u2500\u2500 1.3.6.1.4.1.14519.5.2.1.7085.2626_seg.nii\n</code></pre>","tags":["Healthcare AI","Imaging","Volumetric Reconstruction","Healthcare Interop","TotalSegmentator","Medical Imaging"]},{"location":"applications/imaging_ai_segmentator/#development-environment-setup","title":"Development Environment Setup","text":"<p>You can run the application either in your local development environment or inside the Holohub development container.</p> <ol> <li>Set up the environment:</li> </ol> <p>A. Holohub Container:</p> <ul> <li> <p>Build and launch the Holohub Container:</p> <pre><code>./holohub run-container imaging_ai_segmentator\n</code></pre> </li> </ul> <p>B. Bare Metal (not using Holohub/Holoscan container):</p> <pre><code>- Install Python dependencies:  \n  It is strongly recommended a Python virtual environment is used for running the application in dev environment.\n\n   ```bash\n   pip install -r applications/imaging_ai_segmentator/requirements.txt\n   ```\n\n- Set up the Holohub environment:  \n  Although this application is implemented entirely in Python and relies on standard PyPI packages, you still may want to set up Holohub environment and use `./holohub build` to help organize the Python code and automatically download the required segmentation model.\n\n   ```bash\n   ./holohub setup  # sudo privileges may be required\n   ```\n\n- Set environment variables for the application:\n\n   ```bash\n   source applications/imaging_ai_segmentator/env_settings.sh\n   ```\n</code></pre> <ol> <li>Download test data (if not already done):</li> <li>Download CT series from TCIA</li> <li> <p>Save DICOM files under <code>$HOLOSCAN_INPUT_PATH</code></p> </li> <li> <p>Build the application:</p> </li> </ol> <pre><code>./holohub build imaging_ai_segmentator\n</code></pre> <ol> <li>Run the application:</li> </ol> <pre><code>rm -fr $HOLOSCAN_OUTPUT_PATH  # Optional\nexport PYTHONPATH=$PYTHONPATH:/workspace/holohub/build/imaging_ai_segmentator/python/lib:/workspace/holohub\npython applications/imaging_ai_segmentator/app.py\n</code></pre> <p>Tip:    You can override the default input, output, and model directories by specifying them as command-line arguments. For example:</p> <pre><code>python applications/imaging_ai_segmentator/app.py -m /path/to/model -i /path/to/input -o /path/to/output\n</code></pre> <ol> <li> <p>Check output:</p> <pre><code>ls $HOLOSCAN_OUTPUT_PATH\n</code></pre> </li> </ol>","tags":["Healthcare AI","Imaging","Volumetric Reconstruction","Healthcare Interop","TotalSegmentator","Medical Imaging"]},{"location":"applications/imaging_ai_segmentator/#output","title":"Output","text":"<p>The application generates two types of outputs:</p> <ol> <li>DICOM Segmentation file (Part10 storage format)</li> <li>NIfTI format files in the <code>saved_images_folder</code>:</li> <li>Original CT scan in NIfTI format</li> <li>Segmentation results in NIfTI format</li> </ol>","tags":["Healthcare AI","Imaging","Volumetric Reconstruction","Healthcare Interop","TotalSegmentator","Medical Imaging"]},{"location":"applications/imaging_ai_segmentator/#packaging-the-application-for-distribution","title":"Packaging the Application for Distribution","text":"<p>With Holoscan CLI, an applications built with Holoscan SDK can be packaged into a Holoscan Application Package (HAP), which is essentially a Open Container Initiative compliant container image. An HAP is well suited to be distributed for deployment on hosting platforms, be a Docker Compose, Kubernetes, or else. Please refer to Packaging Holoscan Applications in the User Guide for more information.</p> <p>This example application includes all the necessary files for HAP packaging. First, you should install the application:</p> <pre><code>./holohub install imaging_ai_segmentator\n</code></pre> <p>Then, run the following command to see and use the specific packaging commands.</p> <pre><code>source applications/imaging_ai_segmentator/packageHAP.sh\n</code></pre>","tags":["Healthcare AI","Imaging","Volumetric Reconstruction","Healthcare Interop","TotalSegmentator","Medical Imaging"]},{"location":"applications/isaac_sim_holoscan_bridge/","title":"Isaac Sim Holoscan Bridge","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64 Language: Python Last modified: October 9, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 3.3.0 Tested Holoscan SDK versions: 3.3.0 Contribution metric: Level 3 - Developmental</p> <p>The Isaac Sim Holoscan Bridge application demonstrates how to conduct software-in-the-loop testing of a Holoscan robotics application using NVIDIA Isaac Sim. This allows developers to test and validate the functionality of the robot application in a simulated environment before it is deployed to the physical robot.</p> <p></p>","tags":["Robotics","Processing","Isaac Sim","Bridge"]},{"location":"applications/isaac_sim_holoscan_bridge/#overview","title":"Overview","text":"<p>The application is comprised of two components:</p> <ul> <li>Isaac Sim simulates the robot, sensors, and physical environment.</li> <li>A Holoscan application runs the robot logic.</li> </ul> <p>On each step of the simulation:</p> <ul> <li>Isaac Sim generates and sends physically-accurate robot sensor data (camera images, robot joint positions) to the Holoscan application.</li> <li>The Holoscan application receives the sensor data, processes it, and sends the resulting robot actuation commands (target joint positions) back to Isaac Sim.</li> <li>Isaac Sim then applies these commands to the virtual robot.</li> </ul> <pre><code>graph LR\n    A[\"Isaac Sim&lt;br&gt;(Simulation)\"] -- Sensor Data --&gt; B[\"Holoscan application&lt;br&gt;(Data Processing)\"]\n    B -- Robot Commands --&gt; A</code></pre>","tags":["Robotics","Processing","Isaac Sim","Bridge"]},{"location":"applications/isaac_sim_holoscan_bridge/#architecture","title":"Architecture","text":"<p>The Holoscan application is comprised of the following operators, which enable pushing data to and from Isaac Sim in real time:</p> <ul> <li><code>AsyncDataPushOp</code> streams data from Isaac Sim to Holoscan</li> <li><code>CallbackOp</code> transfers data from Holoscan to Isaac Sim</li> <li><code>SobelOp</code> processes the simulated robot camera images as an example of image processing</li> <li><code>ControlOp</code> generates target joint positions for the robot</li> <li><code>HolovizOp</code> visualizes the robot camera images before and after processing with <code>SobelOp</code></li> </ul> <pre><code>flowchart LR\n    subgraph Holoscan application\n        AsyncDataPushOp --&gt;|camera_image| HolovizOp\n        AsyncDataPushOp --&gt;|camera_image| SobelOp\n        AsyncDataPushOp --&gt;|arm_joint_positions| ControlOp\n        SobelOp --&gt;|camera_image_sobel| CallbackOp\n        SobelOp --&gt;|camera_image_sobel| HolovizOp\n        ControlOp --&gt;|arm_joint_positions| CallbackOp\n    end\n    subgraph Isaac Sim\n        Simulation --&gt; push_data_callback\n        data_ready_callback --&gt; Simulation\n        CallbackOp -.-&gt;|callback| data_ready_callback\n        push_data_callback -.-&gt;|callback| AsyncDataPushOp\n    end</code></pre>","tags":["Robotics","Processing","Isaac Sim","Bridge"]},{"location":"applications/isaac_sim_holoscan_bridge/#requirements","title":"Requirements","text":"<ul> <li>On a Holohub supported platform</li> <li>Python 3.10+</li> <li>Nvidia GPU with at least 8GB memory</li> </ul>","tags":["Robotics","Processing","Isaac Sim","Bridge"]},{"location":"applications/isaac_sim_holoscan_bridge/#run-instructions","title":"Run Instructions","text":"<pre><code>./holohub run isaac_sim_holoscan_bridge\n</code></pre> <p>To keep Isaac Sim configuration and data persistent when running in a container, various directories are mounted into the container.</p> <p>Note It takes quite a few minutes when this command is run the first time since shaders need to be compiled.</p>","tags":["Robotics","Processing","Isaac Sim","Bridge"]},{"location":"applications/laser_detection_latency/evt_cam_calibration/","title":"EVT Camera Calibration","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.2.0 Contribution metric: Level 1 - Highly Reliable</p>","tags":["Computer Vision and Perception","Visualization","Image Processing","Camera","Detection"]},{"location":"applications/laser_detection_latency/evt_cam_calibration/#overview","title":"Overview","text":"<p>This application performs monitor registration using an Emergent Vision Technologies (EVT) camera. It detects April tags placed at the four corners of a monitor to establish the monitor's position and orientation in 3D space.</p>","tags":["Computer Vision and Perception","Visualization","Image Processing","Camera","Detection"]},{"location":"applications/laser_detection_latency/evt_cam_calibration/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>EVT HB-9000-G 25GE camera</li> <li>Monitor with April tags at all four corners</li> <li>Proper lighting conditions (well-lit environment without backlight)</li> </ul>","tags":["Computer Vision and Perception","Visualization","Image Processing","Camera","Detection"]},{"location":"applications/laser_detection_latency/evt_cam_calibration/#setup-instructions","title":"Setup Instructions","text":"<ol> <li>Follow the Holoscan SDK user guide to set up the EVT camera</li> <li>Place the calibration image with April tags on the monitor</li> <li>Position the camera so it can see all four corners of the monitor</li> <li>Verify camera visibility using the high_speed_endoscopy app</li> </ol>","tags":["Computer Vision and Perception","Visualization","Image Processing","Camera","Detection"]},{"location":"applications/laser_detection_latency/evt_cam_calibration/#running-the-application","title":"Running the Application","text":"<pre><code>./holohub build evt_cam_calibration --local\n./holohub run evt_cam_calibration --local --no-local-build\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Image Processing","Camera","Detection"]},{"location":"applications/laser_detection_latency/evt_cam_calibration/#output","title":"Output","text":"<p>The application generates a calibration file <code>evt-cali.npy</code> in the build directory, which contains the monitor's corner coordinates.</p>","tags":["Computer Vision and Perception","Visualization","Image Processing","Camera","Detection"]},{"location":"applications/laser_detection_latency/evt_cam_calibration/#notes","title":"Notes","text":"<ul> <li>The camera must have a clear view of all four April tags</li> <li>Avoid backlighting or glare on the monitor</li> <li>If using a different camera model, update the camera settings in the Python app or YAML configuration file</li> <li>The application requires sudo privileges to run</li> </ul>","tags":["Computer Vision and Perception","Visualization","Image Processing","Camera","Detection"]},{"location":"applications/laser_detection_latency/laser_detection/","title":"Laser Detection","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.2.0 Contribution metric: Level 1 - Highly Reliable</p>","tags":["Computer Vision and Perception","Video","CV CUDA","Camera","Detection"]},{"location":"applications/laser_detection_latency/laser_detection/#overview","title":"Overview","text":"<p>This application demonstrates the latency differences between USB and EVT cameras by detecting laser pointer positions on a monitor. It uses two camera sources to track laser positions and displays the results in real-time with different colored icons.</p>","tags":["Computer Vision and Perception","Video","CV CUDA","Camera","Detection"]},{"location":"applications/laser_detection_latency/laser_detection/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>USB camera (Logitech 4k Pro Webcam or compatible)</li> <li>EVT camera (HB-9000-G 25GE or compatible)</li> <li>Monitor with matte screen (120fps or higher refresh rate recommended)</li> <li>Safe laser pointer for viewing purposes</li> <li>Completed calibration files from both USB and EVT calibration apps</li> </ul>","tags":["Computer Vision and Perception","Video","CV CUDA","Camera","Detection"]},{"location":"applications/laser_detection_latency/laser_detection/#setup-instructions","title":"Setup Instructions","text":"<ol> <li>Complete the calibration process for both USB and EVT cameras</li> <li>Ensure both cameras are properly connected and configured</li> <li>Position the cameras to have a clear view of the monitor</li> <li>Verify the calibration files (<code>usb-cali.npy</code> and <code>evt-cali.npy</code>) are present in the build directory</li> </ol>","tags":["Computer Vision and Perception","Video","CV CUDA","Camera","Detection"]},{"location":"applications/laser_detection_latency/laser_detection/#running-the-application","title":"Running the Application","text":"<pre><code>[sudo] LD_PRELOAD=/usr/lib/aarch64-linux-gnu/nvidia/libnvjpeg.so ./holohub run laser_detection\n</code></pre>","tags":["Computer Vision and Perception","Video","CV CUDA","Camera","Detection"]},{"location":"applications/laser_detection_latency/laser_detection/#usage","title":"Usage","text":"<ul> <li>A white icon represents the USB camera's laser detection</li> <li>A green icon represents the EVT camera's laser detection</li> <li>Point the laser at the monitor to see the latency difference between the two cameras</li> <li>The icons will move to the coordinates where the laser is detected</li> </ul>","tags":["Computer Vision and Perception","Video","CV CUDA","Camera","Detection"]},{"location":"applications/laser_detection_latency/laser_detection/#notes","title":"Notes","text":"<ul> <li>Use only a matte screen monitor to avoid specular reflections</li> <li>Ensure proper lighting conditions</li> <li>Use only safe laser pointers designed for viewing purposes</li> <li>If detection is inaccurate, recalibrate both cameras</li> <li>The application requires sudo privileges to run</li> </ul>","tags":["Computer Vision and Perception","Video","CV CUDA","Camera","Detection"]},{"location":"applications/laser_detection_latency/usb_cam_calibration/","title":"USB Camera Calibration","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.2.0 Contribution metric: Level 1 - Highly Reliable</p>","tags":["Computer Vision and Perception","Visualization","Camera","Detection","Image Processing","Holoviz"]},{"location":"applications/laser_detection_latency/usb_cam_calibration/#overview","title":"Overview","text":"<p>This application performs monitor registration using a USB camera. It detects April tags placed at the four corners of a monitor to establish the monitor's position and orientation in 3D space.</p>","tags":["Computer Vision and Perception","Visualization","Camera","Detection","Image Processing","Holoviz"]},{"location":"applications/laser_detection_latency/usb_cam_calibration/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>Logitech 4k Pro Webcam or compatible USB camera</li> <li>Monitor with April tags at all four corners</li> <li>Proper lighting conditions (well-lit environment without backlight)</li> </ul>","tags":["Computer Vision and Perception","Visualization","Camera","Detection","Image Processing","Holoviz"]},{"location":"applications/laser_detection_latency/usb_cam_calibration/#setup-instructions","title":"Setup Instructions","text":"<ol> <li>Ensure the USB camera is properly connected</li> <li>Place the calibration image with April tags on the monitor</li> <li>Position the camera so it can see all four corners of the monitor</li> <li>Verify camera visibility using the v4l2_camera app</li> </ol>","tags":["Computer Vision and Perception","Visualization","Camera","Detection","Image Processing","Holoviz"]},{"location":"applications/laser_detection_latency/usb_cam_calibration/#running-the-application","title":"Running the Application","text":"<pre><code>[sudo] LD_PRELOAD=/usr/lib/aarch64-linux-gnu/nvidia/libnvjpeg.so ./holohub run usb_cam_calibration\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Camera","Detection","Image Processing","Holoviz"]},{"location":"applications/laser_detection_latency/usb_cam_calibration/#output","title":"Output","text":"<p>The application generates a calibration file <code>usb-cali.npy</code> in the build directory, which contains the monitor's corner coordinates.</p>","tags":["Computer Vision and Perception","Visualization","Camera","Detection","Image Processing","Holoviz"]},{"location":"applications/laser_detection_latency/usb_cam_calibration/#notes","title":"Notes","text":"<ul> <li>The camera must have a clear view of all four April tags</li> <li>Avoid backlighting or glare on the monitor</li> <li>If using a different camera model, update the camera settings in the Python app or YAML configuration file</li> </ul>","tags":["Computer Vision and Perception","Visualization","Camera","Detection","Image Processing","Holoviz"]},{"location":"applications/matlab_gpu_coder/matlab_beamform/","title":"Ultrasound Beamforming with MATLAB GPU Coder","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA), MathWorks Team (MathWorks) Supported platforms: x86_64, aarch64 Language: C++ Last modified: October 9, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 4 - Experimental</p> <p>This application does real-time ultrasound beamforming of simulated data. The beamforming algorithm is implemented in MATLAB and converted to CUDA using MATLAB GPU Coder. When the application is run, Holoviz will display the beamformed data in real time.</p>","tags":["Healthcare AI","Visualization","Synthetic Aperture Beamforming","Signal Processing","Beamforming","Ultrasound"]},{"location":"applications/matlab_gpu_coder/matlab_beamform/#folder-structure","title":"Folder Structure","text":"<pre><code>matlab_beamform\n\u251c\u2500\u2500 data  # Data is generated with generate_data.mlx\n\u2502   \u2514\u2500\u2500 ultrasound_beamforming.bin  # Simulated ultrasound data\n\u251c\u2500\u2500 matlab  # MATLAB files\n\u2502   \u251c\u2500\u2500 generate_beamform_jetson.m  # MATLAB script to generate CUDA DLLs on Jetson\n\u2502   \u251c\u2500\u2500 generate_beamform_x86.m  # MATLAB script to generate CUDA DLLs on x86\n\u2502   \u251c\u2500\u2500 generate_data.mlx  # MATLAB script to generate simulated data\n\u2502   \u2514\u2500\u2500 matlab_beamform.m  # MATLAB function that CUDA code is generated from\n\u251c\u2500\u2500 CMakeLists.txt  # CMake build file\n\u251c\u2500\u2500 main.cpp  # Ultrasound beamforming app\n\u2514\u2500\u2500 matlab_beamform.yaml  # Ultrasound beamforming config\n</code></pre>","tags":["Healthcare AI","Visualization","Synthetic Aperture Beamforming","Signal Processing","Beamforming","Ultrasound"]},{"location":"applications/matlab_gpu_coder/matlab_beamform/#generate-simulated-data","title":"Generate Simulated Data","text":"<p>The required MATLAB Toolboxes are:</p> <ul> <li>Phased Array System Toolbox</li> <li>Communications Toolbox</li> </ul> <p>Simply run the script <code>matlab/generate_data.mlx</code> from MATLAB and a binary file <code>ultrasound_beamforming.bin</code> will be written to a top-level <code>data</code> folder. The binary file contains the simulated ultrasound data, prior to beamforming.</p>","tags":["Healthcare AI","Visualization","Synthetic Aperture Beamforming","Signal Processing","Beamforming","Ultrasound"]},{"location":"applications/matlab_gpu_coder/matlab_beamform/#generate-cuda-code-with-matlab-gpu-coder","title":"Generate CUDA Code with MATLAB GPU Coder","text":"","tags":["Healthcare AI","Visualization","Synthetic Aperture Beamforming","Signal Processing","Beamforming","Ultrasound"]},{"location":"applications/matlab_gpu_coder/matlab_beamform/#x86-ubuntu","title":"x86: Ubuntu","text":"<p>In order to generate the CUDA Code, start MATLAB and <code>cd</code> to the <code>matlab</code> folder and open the <code>generate_beamform_x86.m</code> script. Run the script and a folder <code>codegen/dll/matlab_beamform</code> will be generated in the <code>matlab_beamform</code> folder.</p>","tags":["Healthcare AI","Visualization","Synthetic Aperture Beamforming","Signal Processing","Beamforming","Ultrasound"]},{"location":"applications/matlab_gpu_coder/matlab_beamform/#arm64-jetson","title":"arm64: Jetson","text":"<p>On an x86 computer with MATLAB installed, <code>cd</code> to the <code>matlab</code> folder and open the <code>generate_beamform_jetson.m</code> script. Having an <code>ssh</code> connection to the Jetson device you want to build the CUDA DLLs on, specify the parameters of that connection in the <code>hwobj</code> on line 7, also replace <code>&lt;ABSOLUTE_PATH&gt;</code> of <code>cfg.Hardware.BuildDir</code> on line 39, as the absolute path (on the Jetson device) to <code>holohub</code> folder. Run the script and a folder <code>MATLAB_ws</code> will be created in the <code>matlab_beamform</code> folder.</p>","tags":["Healthcare AI","Visualization","Synthetic Aperture Beamforming","Signal Processing","Beamforming","Ultrasound"]},{"location":"applications/matlab_gpu_coder/matlab_beamform/#configure-holoscan-for-matlab","title":"Configure Holoscan for MATLAB","text":"","tags":["Healthcare AI","Visualization","Synthetic Aperture Beamforming","Signal Processing","Beamforming","Ultrasound"]},{"location":"applications/matlab_gpu_coder/matlab_beamform/#x86-ubuntu_1","title":"x86: Ubuntu","text":"<p>Define the environment variable:</p> <pre><code>export MATLAB_ROOT=\"/usr/local/MATLAB\"\nexport MATLAB_VERSION=\"R2023b\"\n</code></pre> <p>where you, if need be, replace <code>MATLAB_ROOT</code> with the location of your MATLAB install and <code>MATLAB_VERSION</code> with the correct version.</p> <p>Next, run the HoloHub Docker container:</p> <pre><code>./holohub run-container \\\n    --add-volume ${MATLAB_ROOT}/${MATLAB_VERSION} \\\n    --docker-opts=\"-e MATLAB_ROOT=/workspace/volumes/${MATLAB_VERSION}\"\n</code></pre>","tags":["Healthcare AI","Visualization","Synthetic Aperture Beamforming","Signal Processing","Beamforming","Ultrasound"]},{"location":"applications/matlab_gpu_coder/matlab_beamform/#arm64-jetson_1","title":"arm64: Jetson","text":"<p>The folder <code>MATLAB_ws</code>, created by MATLAB, mirrors the folder structure of the host machine and is therefore different from one user to another; hence, we need to specify the path to the <code>codegen</code> folder in the <code>CMakeLists.txt</code>, in order for the build to find the required libraries. Set the variable <code>REL_PTH_MATLAB_CODEGEN</code> to the relative path where the <code>codegen</code> folder is located in the <code>MATLAB_ws</code> folder. For example, if GPU Coder created the following folder structure on the Jetson device:</p> <pre><code>matlab_beamform\n\u2514\u2500\u2500 MATLAB_ws\n    \u2514\u2500\u2500 R2023b\n        \u2514\u2500\u2500 C\n            \u2514\u2500\u2500 Users\n                \u2514\u2500\u2500 Jensen\n                    \u2514\u2500\u2500 holohub\n                        \u2514\u2500\u2500 applications\n                            \u2514\u2500\u2500 matlab_gpu_coder\n                                \u2514\u2500\u2500 matlab_beamform\n                                    \u2514\u2500\u2500 matlab\n                                        \u2514\u2500\u2500 codegen\n</code></pre> <p>the variable should be set as:</p> <pre><code>REL_PTH_MATLAB_CODEGEN=MATLAB_ws/R2023b/C/Users/Jensen/holohub/applications/matlab_gpu_coder/matlab_beamform/matlab/codegen\n</code></pre> <p>Next, run the HoloHub Docker container:</p> <pre><code>./holohub run-container\n</code></pre>","tags":["Healthcare AI","Visualization","Synthetic Aperture Beamforming","Signal Processing","Beamforming","Ultrasound"]},{"location":"applications/matlab_gpu_coder/matlab_image_processing/","title":"Image Processing with MATLAB GPU Coder","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA), MathWorks Team (MathWorks) Supported platforms: x86_64, aarch64 Language: C++ Last modified: August 5, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 4 - Experimental</p> <p>This application does real-time image processing of Holoscan sample data. The image processing is implemented in MATLAB and converted to CUDA using GPU Coder. When the application is run, Holoviz will display the processed data in real time.</p> <p></p>","tags":["Computer Vision and Perception","Visualization","Optimization","Image Processing","Video","Holoviz"]},{"location":"applications/matlab_gpu_coder/matlab_image_processing/#folder-structure","title":"Folder Structure","text":"<pre><code>matlab_image_processing\n\u251c\u2500\u2500 matlab  # MATLAB files\n\u2502   \u251c\u2500\u2500 generate_image_processing_jetson.m  # MATLAB script to generate CUDA DLLs on Jetson\n\u2502   \u251c\u2500\u2500 generate_image_processing_x86.m  # MATLAB script to generate CUDA DLLs on x86\n\u2502   \u251c\u2500\u2500 matlab_image_processing.m  # MATLAB function that CUDA code is generated from\n\u2502   \u2514\u2500\u2500 test_image_processing.m  # MATLAB script to test MATLAB function\n\u251c\u2500\u2500 CMakeLists.txt  # CMake build file\n\u251c\u2500\u2500 main.cpp  # Ultrasound beamforming app\n\u2514\u2500\u2500 matlab_image_processing.yaml  # Ultrasound beamforming config\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Optimization","Image Processing","Video","Holoviz"]},{"location":"applications/matlab_gpu_coder/matlab_image_processing/#generate-cuda-code-with-matlab-gpu-coder","title":"Generate CUDA Code with MATLAB GPU Coder","text":"","tags":["Computer Vision and Perception","Visualization","Optimization","Image Processing","Video","Holoviz"]},{"location":"applications/matlab_gpu_coder/matlab_image_processing/#x86-ubuntu","title":"x86: Ubuntu","text":"<p>In order to generate the CUDA Code, start MATLAB and <code>cd</code> to the <code>matlab</code> folder and open the <code>generate_image_processing_x86.m</code> script. Run the script and a folder <code>codegen/dll/matlab_image_processing</code> will be generated in the <code>matlab_image_processing</code> folder.</p>","tags":["Computer Vision and Perception","Visualization","Optimization","Image Processing","Video","Holoviz"]},{"location":"applications/matlab_gpu_coder/matlab_image_processing/#arm64-jetson","title":"arm64: Jetson","text":"<p>On an x86 computer with MATLAB installed, <code>cd</code> to the <code>matlab</code> folder and open the <code>generate_image_processing_jetson.m</code> script. Having an <code>ssh</code> connection to the Jetson device you want to build the CUDA DLLs on, specify the parameters of that connection in the <code>hwobj</code> on line 7, also replace <code>&lt;ABSOLUTE_PATH&gt;</code> of <code>cfg.Hardware.BuildDir</code> on line 39, as the absolute path (on the Jetson device) to <code>holohub</code> folder. Run the script and a folder <code>MATLAB_ws</code> will be created in the <code>matlab_image_processing</code> folder.</p>","tags":["Computer Vision and Perception","Visualization","Optimization","Image Processing","Video","Holoviz"]},{"location":"applications/matlab_gpu_coder/matlab_image_processing/#configure-holoscan-for-matlab","title":"Configure Holoscan for MATLAB","text":"","tags":["Computer Vision and Perception","Visualization","Optimization","Image Processing","Video","Holoviz"]},{"location":"applications/matlab_gpu_coder/matlab_image_processing/#x86-ubuntu_1","title":"x86: Ubuntu","text":"<p>Define the environment variable:</p> <pre><code>export MATLAB_ROOT=\"/usr/local/MATLAB\"\nexport MATLAB_VERSION=\"R2023b\"\n</code></pre> <p>where you, if need be, replace <code>MATLAB_ROOT</code> with the location of your MATLAB install and <code>MATLAB_VERSION</code> with the correct version.</p> <p>Next, run the HoloHub Docker container:</p> <pre><code>./holohub run-container \\\n    --add-volume ${MATLAB_ROOT}/${MATLAB_VERSION} \\\n    --docker-opts=\"-e MATLAB_ROOT=/workspace/volumes/${MATLAB_VERSION}\"\n</code></pre> <p>and build the endoscopy tool tracking application to download the necessary data:</p> <pre><code>./holohub build endoscopy_tool_tracking\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Optimization","Image Processing","Video","Holoviz"]},{"location":"applications/matlab_gpu_coder/matlab_image_processing/#arm64-jetson_1","title":"arm64: Jetson","text":"<p>The folder <code>MATLAB_ws</code>, created by MATLAB, mirrors the folder structure of the host machine and is therefore different from one user to another; hence, we need to specify the path to the <code>codegen</code> folder in the <code>CMakeLists.txt</code>, in order for the build to find the required libraries. Set the variable <code>REL_PTH_MATLAB_CODEGEN</code> to the relative path where the <code>codegen</code> folder is located in the <code>MATLAB_ws</code> folder. For example, if GPU Coder created the following folder structure on the Jetson device:</p> <pre><code>matlab_gpu_coder\n\u2514\u2500\u2500 MATLAB_ws\n    \u2514\u2500\u2500 R2023b\n        \u2514\u2500\u2500 C\n            \u2514\u2500\u2500 Users\n                \u2514\u2500\u2500 Jensen\n                    \u2514\u2500\u2500 holohub\n                        \u2514\u2500\u2500 applications\n                            \u2514\u2500\u2500 matlab_gpu_coder\n                                \u2514\u2500\u2500 matlab_image_processing\n                                    \u2514\u2500\u2500 matlab\n                                        \u2514\u2500\u2500 codegen\n</code></pre> <p>the variable should be set as:</p> <pre><code>REL_PTH_MATLAB_CODEGEN=MATLAB_ws/R2023b/C/Users/Jensen/holohub/applications/matlab_gpu_coder/matlab_image_processing/matlab/codegen\n</code></pre> <p>Next, run the HoloHub Docker container:</p> <pre><code>./holohub run-container\n</code></pre> <p>and build the endoscopy tool tracking application **inside the container to download the necessary data:</p> <pre><code>./holohub build endoscopy_tool_tracking\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Optimization","Image Processing","Video","Holoviz"]},{"location":"applications/monai_endoscopic_tool_seg/","title":"Endoscopy Tool Segmentation from MONAI Model Zoo","text":"<p>     \u25b6 Run Locally  Authors: Jin Li (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: January 15, 2026 Latest version: 1.1 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 2 - Trusted</p> <p>This endoscopy tool segmentation application runs the MONAI Endoscopic Tool Segmentation from MONAI Model Zoo.</p> <p>This HoloHub application has been verified on the GI Genius sandbox and is currently deployable to GI Genius Intelligent Endoscopy Modules. GI Genius is Cosmo Intelligent Medical Devices\u2019 AI-powered endoscopy system. This implementation by Cosmo Intelligent Medical Devices showcases the fast and seamless deployment of HoloHub applications on products/platforms running on NVIDIA Holoscan.</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Segmentation"]},{"location":"applications/monai_endoscopic_tool_seg/#model","title":"Model","text":"<p>We will be deploying the endoscopic tool segmentation model from MONAI Model Zoo.  Note that you could also use the MONAI model zoo repo for training your own semantic segmentation model with your own data, but here we are directly deploying the downloaded MONAI model checkpoint into Holoscan. </p> <p>You can choose to  - download the MONAI Endoscopic Tool Segmentation Model on NGC directly and skip the rest of this Model section, or  - go through the following conversion steps yourself.</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Segmentation"]},{"location":"applications/monai_endoscopic_tool_seg/#model-conversion-to-onnx-optional","title":"Model conversion to ONNX (optional)","text":"<p>Before deploying the MONAI Model Zoo's trained model checkpoint in Holoscan SDK, we convert the model checkpoint into ONNX. </p> <ol> <li>Download the PyTorch model checkpoint linked in the README of endoscopic tool segmentation. We will assume its name to be <code>model.pt</code>.</li> <li> <p>Clone the MONAI Model Zoo repo.  <pre><code>cd [your-workspace]\ngit clone https://github.com/Project-MONAI/model-zoo.git\n</code></pre> and place the downloaded PyTorch model into <code>model-zoo/models/endoscopic_tool_segmentation/</code>.</p> </li> <li> <p>Pull and run the docker image for MONAI. We will use this docker image for converting the PyTorch model to ONNX.  <pre><code>docker pull projectmonai/monai\ndocker run -it --rm --gpus all -v [your-workspace]/model-zoo:/workspace/model-zoo -w /workspace/model-zoo/models/endoscopic_tool_segmentation/ projectmonai/monai\n</code></pre></p> </li> <li>Install onnxruntime within the container  <code>pip install onnxruntime onnx-graphsurgeon</code></li> <li>Convert model</li> </ol> <p>We will first export the model.pt file to ONNX by using the export_to_onnx.py file. Modify the backbone in line 122 to be efficientnet-b2: <pre><code>model = load_model_and_export(modelname, outname, out_channels, height, width, multigpu, backbone=\"efficientnet-b2\")\n</code></pre> Note that the model in the Model Zoo here was trained to have only two output channels: label 1 = tools, label 0 = everything else, but the same Model Zoo repo can be repurposed to train a model with a different dataset that has more than two classes. <pre><code>python scripts/export_to_onnx.py --model model.pt --outpath model_endoscopic_tool_seg.onnx --width 736 --height 480 --out_channels 2\n</code></pre> Fold constants in the ONNX model. <pre><code>polygraphy surgeon sanitize --fold-constants model_endoscopic_tool_seg.onnx -o model_endoscopic_tool_seg_sanitized.onnx\n</code></pre> Finally, modify the input and output channels to have shape [n, height, width, channels], [n, channels, height, width].  <pre><code>python scripts/graph_surgeon_tool_seg.py --orig_model model_endoscopic_tool_seg_sanitized.onnx --new_model model_endoscopic_tool_seg_sanitized_nhwc_in_nchw_out.onnx\n</code></pre></p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Segmentation"]},{"location":"applications/monai_endoscopic_tool_seg/#data","title":"Data","text":"<p>For this application we will use the same Endoscopy Sample Data as the Holoscan SDK reference applications.</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Segmentation"]},{"location":"applications/monai_endoscopic_tool_seg/#requirements","title":"Requirements","text":"<p>The only requirement is to make sure the model and data are accessible by the application. At runtime we will need to specify via the <code>--data</code> arg, assuming the directory specified contains two subdirectories <code>endoscopy/</code> (endoscopy video data directory) and <code>monai_tool_seg_model/</code> (model directory).</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Segmentation"]},{"location":"applications/monai_endoscopic_tool_seg/#running-the-application","title":"Running the application","text":"","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Segmentation"]},{"location":"applications/monai_endoscopic_tool_seg/#quick-start","title":"Quick start","text":"<p>The easiest way to test this application is to use Holohub CLI from the top level of Holohub</p> <pre><code>./holohub run monai_endoscopic_tool_seg\n</code></pre>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Segmentation"]},{"location":"applications/monai_endoscopic_tool_seg/#running-the-application-manually","title":"Running the application manually","text":"<p>To run this application, you'll need to configure your PYTHONPATH environment variable to locate the necessary python libraries based on your Holoscan SDK installation type.</p> <p>You should refer to the glossary for the terms defining specific locations within HoloHub.</p> <p>If your Holoscan SDK installation type is:</p> <ul> <li>python wheels:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <ul> <li>otherwise:</li> </ul> <p><pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOSCAN_INSTALL_DIR&gt;/python/lib:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> Next, run the application, where  is a directory that contains two subdirectories <code>endoscopy/</code> and <code>monai_tool_seg_model/</code>.: <p><pre><code>python3 tool_segmentation.py --data &lt;DATA_DIR&gt;\n</code></pre> If you'd like the application to run at the input framerate, change the <code>replayer</code> config in the yaml file to <code>realtime: true</code>.</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","Segmentation"]},{"location":"applications/multiai_endoscopy/","title":"Multi AI SSD Detection and MONAI Endoscopic Tool Segmentation","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 2 - Trusted</p> <p>In this application, we show how to build a Multi AI application with detection and segmentation models, write postprocessing operators using CuPy and NumPy in Python tensor interop and MatX library (An efficient C++17 GPU numerical computing library with Python-like syntax) in C++ tensor interop, and pass multiple tensors from postprocessing to Holoviz.</p> <p> Fig. 1 Endoscopy (laparoscopy) image from a cholecystectomy (gallbladder removal surgery) showing tool detection and segmentation results from two concurrently executed AI models. Image courtesy of Research Group Camma, IHU Strasbourg and the University of Strasbourg (NGC Resource)</p> <p>Please refer to the README under ./app_dev_process to see the process of developing the applications.</p> <p>The application graph looks like: </p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","SSD","Segmentation"]},{"location":"applications/multiai_endoscopy/#model","title":"Model","text":"<p>We combine two models from the single model applications SSD Tool Detection and MONAI Endoscopic Tool Segmentation:</p> <ul> <li>SSD model from NGC with additional NMS op: <code>epoch24_nms.onnx</code></li> <li>MONAI tool segmentation model from NGC: <code>model_endoscopic_tool_seg_sanitized_nhwc_in_nchw_out.onnx</code></li> </ul>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","SSD","Segmentation"]},{"location":"applications/multiai_endoscopy/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","SSD","Segmentation"]},{"location":"applications/multiai_endoscopy/#requirements","title":"Requirements","text":"<p>Ensure you have installed the Holoscan SDK via one of the methods specified in the SDK user guide.</p> <p>The directory specified by <code>--data</code> at app runtime is assumed to contain three subdirectories, corresponding to the three NGC resources specified in Model and Data: <code>endoscopy</code>, <code>monai_tool_seg_model</code> and <code>ssd_model</code>.  These resources will be automatically downloaded to the holohub data directory when building the application.</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","SSD","Segmentation"]},{"location":"applications/multiai_endoscopy/#building-and-running-the-application","title":"Building and Running the Application","text":"","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","SSD","Segmentation"]},{"location":"applications/multiai_endoscopy/#python-apps","title":"Python Apps","text":"<p>To run the Python application, you can make use of the run script</p> <pre><code>./holohub run multiai_endoscopy --language python\n</code></pre> <p>Alternatively, to run this application, you'll need to configure your PYTHONPATH environment variable to locate the necessary python libraries based on your Holoscan SDK installation type.</p> <p>You should refer to the glossary for the terms defining specific locations within HoloHub.</p> <p>If your Holoscan SDK installation type is:</p> <ul> <li>python wheels:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <ul> <li>otherwise:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOSCAN_INSTALL_DIR&gt;/python/lib:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <p>Next, run the application:</p> <pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;/applications/multiai_endoscopy/python\npython3 multi_ai.py --data &lt;DATA_DIR&gt;\n</code></pre>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","SSD","Segmentation"]},{"location":"applications/multiai_endoscopy/#c-apps","title":"C++ Apps","text":"<p>There are three versions of C++ apps, with the only difference being that they implement the inference post-processing operator <code>DetectionPostprocessorOp</code> in different ways:</p> <ul> <li><code>post-proc-cpu</code>: Multi-AI app running the inference post-processing operator on the CPU using <code>std</code> features only.</li> <li><code>post-proc-matx-cpu</code>: Multi-AI app running the inference post-processing operator on the CPU using the MatX library).</li> <li><code>post-proc-matx-gpu</code>: Multi-AI app running  the inference post-processing operator on the GPU using MatX (CUDA).</li> </ul> <p>To run <code>post-proc-cpu</code>, you can simply run:</p> <pre><code>./holohub run multiai_endoscopy --language cpp\n</code></pre> <p>For the other two C++ applications, you'll need to build these without the Holohub CLI as follows.</p> <p>To run <code>post-proc-matx-cpu</code> or <code>post-proc-matx-gpu</code>, first navigate to the app directory.</p> <pre><code>cd applications/multiai_endoscopy/cpp/post-proc-matx-cpu\n</code></pre> <p>Next we need to configure and build the app.</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","SSD","Segmentation"]},{"location":"applications/multiai_endoscopy/#configuring","title":"Configuring","text":"<p>First, create a build folder:</p> <pre><code>mkdir -p build\n</code></pre> <p>then run CMake configure with:</p> <pre><code>cmake -S . -B build\n</code></pre> <p>Unless you make changes to <code>CMakeLists.txt</code>, this step only needs to be done once.</p>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","SSD","Segmentation"]},{"location":"applications/multiai_endoscopy/#building","title":"Building","text":"<p>The app can be built with:</p> <pre><code>cmake --build build\n</code></pre> <p>or equally:</p> <pre><code>cd build\nmake\n</code></pre>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","SSD","Segmentation"]},{"location":"applications/multiai_endoscopy/#running","title":"Running","text":"<p>You can run the app with:</p> <pre><code>./build/multi_ai --data &lt;DATA_DIR&gt;\n</code></pre>","tags":["Healthcare AI","Video","Surgical AI","Endoscopy","SSD","Segmentation"]},{"location":"applications/multiai_ultrasound/","title":"Multi AI Ultrasound","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: February 4, 2026 Latest version: 1.1 Minimum Holoscan SDK version: 2.9.0 Tested Holoscan SDK versions: 3.7.0, 3.10.0, 3.11.0 Contribution metric: Level 1 - Highly Reliable</p> <p>To run multiple inference pipelines in a single application, the Multi AI operators (inference and postprocessor) use APIs from the Holoscan Inference module to extract data, initialize and execute the inference workflow, process, and transmit data for visualization.</p> <p>This application uses models and echocardiogram data from iCardio.ai. The models include: - a Plax chamber model, that identifies four critical linear measurements of the heart - a B-Mode Perspective Classifier model, that determines confidence of each frame to known 28 cardiac anatomical view as defined by the guidelines of the American Society of Echocardiography - an Aortic Stenosis Classification model, that provides a score which determines likeability for the presence of aortic stenosis</p> <p> Fig. 1 Multi AI sample application workflow</p> <p>The pipeline uses a recorded ultrasound video file (generated by <code>convert_video_to_gxf_entities</code> script) for input frames. The data is loaded by Video Stream Replayer and forwarded to the following operators for pre-processing: - B-mode Perspective Preprocessor: Entity uses Format Converter to convert the data type of the image to <code>float32</code> and resize the data to 320x240 per frame. - Plax Chamber Resized: Entity uses Format Converter to resize the input image to 320x320x3 with <code>RGB888</code> image format for visualization. - Plax Chamber Preprocessor: Entity uses Format Converter to convert the data type of the image to <code>float32</code> and resize the data to 320x320 per frame. - Aortic Stenosis Preprocessor: Entity uses Format Converter to convert the data type of the image to <code>float32</code> and resize the data to 300x300 per frame.</p> <p>Then: - the Multi AI Inference uses outputs from three preprocessors to execute the inference. - Multi AI Postprocessor uses the inferred output to process as per specifications. - The Visualizer iCardio extension is used to generate visualization components for the plax chamber output. - Visualization components are finally fed into HoloViz to generate the visualization.</p> <p>The sample application outputs demonstrates 5 keypoints identified by the Plax Chamber model. Keypoints are connected in the output frame as shown in the image below.</p> <p> Fig. 2 Multi AI sample application. Data courtesy of iCardio.ai (NGC Resource)</p> <p>Aortic Stenosis and B-mode Perspective models are the Classification models. Classification results can be printed using <code>print</code> keyword against the output tensors from the Classification models in the Multi AI Postprocessor settings. Printing of the results is optional and can be ignored by removing relevant entries in the post processor settings.</p> <p>Note: The Holoscan SDK provides capability to process all models in ONNX, TensorRT FP32 and TensorRT FP16 format. Classification models (Aortic Stenosis and B-mode Perspective), do not support TensorRT FP16 conversion. Plax Chamber model is supported for all available formats (ONNX, TensorRT FP32 and TensorRT FP16).</p>","tags":["Healthcare AI","Visualization","Cardiac Keypoints Detection","Ultrasound","Video","Classification"]},{"location":"applications/multiai_ultrasound/#holoscan-sdk-version","title":"Holoscan SDK version","text":"<p>Multi AI application in HoloHub requires version 0.6+ of the Holoscan SDK. If the Holoscan SDK version is 0.5 or lower, following code changes must be made in the application:</p> <ul> <li>In cpp/main.cpp: <code>#include &lt;holoscan/operators/inference/inference.hpp&gt;</code> is replaced with <code>#include &lt;holoscan/operators/multiai_inference/multiai_inference.hpp&gt;</code></li> <li>In cpp/main.cpp: <code>#include &lt;holoscan/operators/inference_processor/inference_processor.hpp&gt;</code> is replaced with <code>#include &lt;holoscan/operators/multiai_postprocessor/multiai_postprocessor.hpp&gt;</code></li> <li>In cpp/main.cpp: <code>ops::InferenceOp</code> is replaced with <code>ops::MultiAIInferenceOp</code></li> <li>In cpp/main.cpp: <code>ops::InferenceProcessorOp</code> is replaced with <code>ops::MultiAIPostprocessorOp</code></li> <li>In cpp/CMakeLists.txt: update the holoscan SDK version from <code>0.6</code> to <code>0.5</code></li> <li>In cpp/CMakeLists.txt: <code>holoscan::ops::inference</code> is replaced with <code>holoscan::ops::multiai_inference</code></li> <li>In cpp/CMakeLists.txt: <code>holoscan::ops::inference_processor</code> is replaced with <code>holoscan::ops::multiai_postprocessor</code></li> <li>In python/CMakeLists.txt: update the holoscan SDK version from <code>0.6</code> to <code>0.5</code></li> <li>In python/multiai_ultrasound.py: <code>InferenceOp</code> is replaced with <code>MultiAIInferenceOp</code></li> <li>In python/multiai_ultrasound.py: <code>InferenceProcessorOp</code> is replaced with <code>MultiAIPostprocessorOp</code></li> </ul>","tags":["Healthcare AI","Visualization","Cardiac Keypoints Detection","Ultrasound","Video","Classification"]},{"location":"applications/multiai_ultrasound/cpp/","title":"Multi-AI Ultrasound","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ (see Python version) Last modified: February 4, 2026 Latest version: 1.1 Minimum Holoscan SDK version: 2.9.0 Tested Holoscan SDK versions: 3.7.0, 3.10.0, 3.11.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates how to run multiple inference pipelines in a single application by leveraging the Holoscan Inference module, a framework that facilitates designing and executing inference applications in the Holoscan SDK.</p> <p>The Inference and the Processing operators use APIs from the Holoscan Inference module to extract data, initialize and execute the inference workflow, process, and transmit data for visualization.</p> <p>The applications uses models and echocardiogram data from iCardio.ai. The models include: - a Plax chamber model, that identifies four critical linear measurements of the heart - a Viewpoint Classifier model, that determines confidence of each frame to known 28 cardiac anatomical view as defined by the guidelines of the American Society of Echocardiography - an Aortic Stenosis Classification model, that provides a score which determines likeability for the presence of aortic stenosis</p> <p>The default configuration (<code>multiai_ultrasound.yaml</code>) runs on default GPU (GPU-0). Multi-AI Ultrasound application can be executed on multiple GPUs with the Holoscan SDK version 0.6 onwards. A sample configuration file for multi GPU configuration for multi-AI ultrasound application (<code>mgpu_multiai_ultrasound.yaml</code>) is present in both <code>cpp</code> and <code>python</code> applications. The multi-GPU configuration file is designed for a system with at least 2 GPUs connected to the same PCIE network.</p>","tags":["Healthcare AI","Visualization","Cardiac Keypoints Detection","Ultrasound","Video","Classification"]},{"location":"applications/multiai_ultrasound/cpp/#requirements","title":"Requirements","text":"<p>The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the echocardiogram (replayer). Follow the setup instructions from the user guide to use the AJA capture card.</p>","tags":["Healthcare AI","Visualization","Cardiac Keypoints Detection","Ultrasound","Video","Classification"]},{"location":"applications/multiai_ultrasound/cpp/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for Multi-AI Ultrasound Pipeline</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Healthcare AI","Visualization","Cardiac Keypoints Detection","Ultrasound","Video","Classification"]},{"location":"applications/multiai_ultrasound/cpp/#build-and-run-instructions","title":"Build and Run Instructions","text":"<p>Please refer to the top level Holohub README.md file for more information on the HoloHub CLI</p>","tags":["Healthcare AI","Visualization","Cardiac Keypoints Detection","Ultrasound","Video","Classification"]},{"location":"applications/multiai_ultrasound/cpp/#pre-recorded-video-replay","title":"Pre-Recorded Video Replay","text":"<pre><code>sed -i -e 's#^source:.*#source: replayer#' applications/multiai_ultrasound/cpp/multiai_ultrasound.yaml\n./holohub run multiai_ultrasound --language=cpp [--local]\n</code></pre>","tags":["Healthcare AI","Visualization","Cardiac Keypoints Detection","Ultrasound","Video","Classification"]},{"location":"applications/multiai_ultrasound/cpp/#using-a-pre-recorded-video-on-multi-gpu-system","title":"Using a pre-recorded video on multi-GPU system","text":"<pre><code>sed -i -e 's#^source:.*#source: replayer#' applications/multiai_ultrasound/cpp/mgpu_multiai_ultrasound.yaml\n./holohub run multiai_ultrasound --language=cpp [--local] \\\n    --run-args=\"--config applications/multiai_ultrasound/cpp/mgpu_multiai_ultrasound.yaml\"\n</code></pre>","tags":["Healthcare AI","Visualization","Cardiac Keypoints Detection","Ultrasound","Video","Classification"]},{"location":"applications/multiai_ultrasound/cpp/#using-an-aja-capture-card","title":"Using an AJA capture card","text":"<pre><code>sed -i -e 's#^source:.*#source: aja#' applications/multiai_ultrasound/cpp/multiai_ultrasound.yaml\n./holohub run multiai_ultrasound --language=cpp [--local] \\\n    --configure-args=\"-DOP_aja_source:BOOL=ON\"\n</code></pre>","tags":["Healthcare AI","Visualization","Cardiac Keypoints Detection","Ultrasound","Video","Classification"]},{"location":"applications/multiai_ultrasound/operators/visualizer_icardio/","title":"Visualizer iCardio","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Last modified: October 27, 2025 Latest version: 1.1 Minimum Holoscan SDK version: 2.9.0 Tested Holoscan SDK versions: 3.7.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>visualizer_icardio</code> extension generates the visualization components from the processed results of the plax chamber model.</p>","tags":["Video"]},{"location":"applications/multiai_ultrasound/operators/visualizer_icardio/#nvidiaholoscanmultiaivisualizericardio","title":"<code>nvidia::holoscan::multiai::VisualizerICardio</code>","text":"<p>Visualizer iCardio extension ingests the processed results of the plax chamber model and generates the key points, the key areas and the lines that are transmitted to the HoloViz codelet.</p>","tags":["Video"]},{"location":"applications/multiai_ultrasound/operators/visualizer_icardio/#parameters","title":"Parameters","text":"<ul> <li><code>in_tensor_names_</code>: Input tensor names</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>out_tensor_names_</code>: Output tensor names</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>allocator_</code>: Memory allocator</li> <li>type: <code>gxf::Handle&lt;gxf::Allocator&gt;</code></li> <li><code>receivers_</code>: Vector of input receivers. Multiple receivers supported.</li> <li>type: <code>HoloInfer::GXFReceivers</code></li> <li><code>transmitter_</code>: Output transmitter. Single transmitter supported.</li> <li>type: <code>HoloInfer::GXFTransmitters</code></li> </ul>","tags":["Video"]},{"location":"applications/multiai_ultrasound/python/","title":"Multi-AI Ultrasound","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ (see C++ version) Last modified: February 4, 2026 Latest version: 1.1 Minimum Holoscan SDK version: 2.9.0 Tested Holoscan SDK versions: 3.7.0, 3.10.0, 3.11.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates how to run multiple inference pipelines in a single application by leveraging the Holoscan Inference module, a framework that facilitates designing and executing inference applications in the Holoscan SDK.</p> <p>The Multi AI operators (inference and postprocessor) use APIs from the Holoscan Inference module to extract data, initialize and execute the inference workflow, process, and transmit data for visualization.</p> <p>The applications uses models and echocardiogram data from iCardio.ai. The models include: - a Plax chamber model, that identifies four critical linear measurements of the heart - a Viewpoint Classifier model, that determines confidence of each frame to known 28 cardiac anatomical view as defined by the guidelines of the American Society of Echocardiography - an Aortic Stenosis Classification model, that provides a score which determines likeability for the presence of aortic stenosis</p> <p>The default configuration (<code>multiai_ultrasound.yaml</code>) runs on default GPU (GPU-0). Multi-AI Ultrasound application can be executed on multiple GPUs with the Holoscan SDK version 0.6 onwards. A sample configuration file for multi GPU configuration for multi-AI ultrasound application (<code>mgpu_multiai_ultrasound.yaml</code>) is present in both <code>cpp</code> and <code>python</code> applications. The multi-GPU configuration file is designed for a system with at least 2 GPUs connected to the same PCIE network.</p>","tags":["Healthcare AI","Visualization","Cardiac Keypoints Detection","Ultrasound","Video","Classification"]},{"location":"applications/multiai_ultrasound/python/#requirements","title":"Requirements","text":"<ul> <li>Python 3.9+</li> <li>The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the echocardiogram (replayer). Follow the setup instructions from the user guide to use the AJA capture card.</li> </ul>","tags":["Healthcare AI","Visualization","Cardiac Keypoints Detection","Ultrasound","Video","Classification"]},{"location":"applications/multiai_ultrasound/python/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for Multi-AI Ultrasound Pipeline</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Healthcare AI","Visualization","Cardiac Keypoints Detection","Ultrasound","Video","Classification"]},{"location":"applications/multiai_ultrasound/python/#run-instructions","title":"Run Instructions","text":"<p>To run this application, you'll need to configure your PYTHONPATH environment variable to locate the necessary python libraries based on your Holoscan SDK installation type.</p> <p>You should refer to the glossary for the terms defining specific locations within HoloHub.</p> <p>If your Holoscan SDK installation type is:</p> <ul> <li>python wheels:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <ul> <li>otherwise:</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOSCAN_INSTALL_DIR&gt;/python/lib:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\n</code></pre> <p>Next, run the commands of your choice:</p> <ul> <li> <p>Using a pre-recorded video     <pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;/applications/multiai_ultrasound/python\npython3 multiai_ultrasound.py --source=replayer --data &lt;DATA_DIR&gt;/multiai_ultrasound\n</code></pre></p> </li> <li> <p>Using a pre-recorded video on multi-GPU system     <pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;/applications/multiai_ultrasound/python\npython3 multiai_ultrasound.py --config mgpu_multiai_ultrasound.yaml --source=replayer --data &lt;DATA_DIR&gt;/multiai_ultrasound\n</code></pre></p> </li> <li> <p>Using an AJA card     <pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;/applications/multiai_ultrasound/python\npython3 multiai_ultrasound.py --source=aja\n</code></pre></p> </li> </ul>","tags":["Healthcare AI","Visualization","Cardiac Keypoints Detection","Ultrasound","Video","Classification"]},{"location":"applications/network_radar_pipeline/","title":"Radar Signal Processing over Network","text":"<p>     \u25b6 Run Locally  Authors: Dylan Eustice (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: August 5, 2025 Latest version: 1.1 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 2 - Trusted</p> <p>The Network Radar application demonstrates signal processing on data streamed via packets over a network. It showcases the use of both the Advanced Network Operator and Basic Network Operator to send or receive data, combined with the signal processing operators implemented in the Simple Radar Pipeline application.</p> <p>Using the GPUDirect capabilities afforded by the Advanced Network Operator, this pipeline has been tested up to 100 Gbps (Tx/Rx) using a ConnectX-7 NIC and A30 GPU.</p> <p>The motivation for building this application is to demonstrate how data arrays can be assembled from packet data in real-time for low-latency, high-throughput sensor processing applications. The main components of this work are defining a message format and writing code connecting the network operators to the signal processing operators.</p> <p>This application supports the Advanced Network Operator DPDK and DOCA GPUNetIO transport layers.</p>","tags":["Signal Processing","Networking and Distributed Computing","GPUDirect","DPDK"]},{"location":"applications/network_radar_pipeline/#prerequisites","title":"Prerequisites","text":"<p>See the README for the Advanced Network Operator for requirements and system tuning needed to enable high-throughput GPUDirect capabilities.</p>","tags":["Signal Processing","Networking and Distributed Computing","GPUDirect","DPDK"]},{"location":"applications/network_radar_pipeline/#environment","title":"Environment","text":"<p>Note: Dockerfile should be cross-compatible, but has only been tested on x86. Needs to be edited if different versions / architectures are required.</p>","tags":["Signal Processing","Networking and Distributed Computing","GPUDirect","DPDK"]},{"location":"applications/network_radar_pipeline/#build","title":"Build","text":"<p>Please refer to the top level Holohub README.md file for information on how to build this application: <code>./holohub build network_radar_pipeline</code>.</p>","tags":["Signal Processing","Networking and Distributed Computing","GPUDirect","DPDK"]},{"location":"applications/network_radar_pipeline/#run","title":"Run","text":"<p>Note: must properly configure YAML files before running. To run with DPDK as ANO transport layer: - On Tx machine: <code>./build/applications/network_radar_pipeline/cpp/network_radar_pipeline source.yaml</code> - On Rx machine: <code>./build/applications/network_radar_pipeline/cpp/network_radar_pipeline process.yaml</code></p> <p>To run with DOCA GPUNetIO as ANO transport layer: - On Tx machine: <code>./build/applications/network_radar_pipeline/cpp/network_radar_pipeline source_doca.yaml</code> - On Rx machine: <code>./build/applications/network_radar_pipeline/cpp/network_radar_pipeline process_doca.yaml</code></p>","tags":["Signal Processing","Networking and Distributed Computing","GPUDirect","DPDK"]},{"location":"applications/network_radar_pipeline/#network-operator-connectors","title":"Network Operator Connectors","text":"<p>See each operators' README before using / for more detailed information.</p>","tags":["Signal Processing","Networking and Distributed Computing","GPUDirect","DPDK"]},{"location":"applications/network_radar_pipeline/#basic-network-operator-connector","title":"Basic Network Operator Connector","text":"<p>Implementation in <code>basic_network_connectors</code>. Only supports CPU packet receipt / transmit. Uses cudaMemcpy to move data between network operator and MatX tensors.</p>","tags":["Signal Processing","Networking and Distributed Computing","GPUDirect","DPDK"]},{"location":"applications/network_radar_pipeline/#advanced-network-operator-connector","title":"Advanced Network Operator Connector","text":"<p>Implementation in <code>advanced_network_connectors</code>. RX connector is only configured to run with GPUDirect enabled, in header-data split (HDS) mode. TX connector supports both GPUDirect/HDS or CPU-only.</p>","tags":["Signal Processing","Networking and Distributed Computing","GPUDirect","DPDK"]},{"location":"applications/network_radar_pipeline/#testing-rx-on-generic-packet-data","title":"Testing RX on generic packet data","text":"<p>When using the Advanced network operator, the application supports testing the radar processing component in a \"spoof packets\" mode. This functionality allows for easier benchmarking of the application by ingesting generic packet data and writing in header fields such that the full radar pipeline will still be exercised. When \"SPOOF_PACKET_DATA\" (adv_networking_rx.h) is set to \"true\", the index of the packet will be used to set fields appropriately. This functionality is currently unsupported using the basic network operator connectors.</p>","tags":["Signal Processing","Networking and Distributed Computing","GPUDirect","DPDK"]},{"location":"applications/network_radar_pipeline/#message-format","title":"Message format","text":"<p>The message format is defined by <code>RFPacket</code>. It is a byte array, represented by <code>RFPacket::payload</code>, where the first 16 bytes are reserved for metadata and the rest are used for representing complex I/Q samples. The metadata is: - Sample index: The starting index for a single pulse/channel of the transmitted samples (2 bytes) - Waveform ID: Index of the transmitted waveform (2 bytes) - Channel index: Index of the channel (2 bytes) - Pulse index: Index of the pulse (2 bytes) - Number samples: Number of I/Q samples transmitted (2 bytes) - End of array: Boolean - true if this is the last message for the waveform (2 bytes)</p>","tags":["Signal Processing","Networking and Distributed Computing","GPUDirect","DPDK"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/","title":"Chat with NVIDIA NIM","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3, 2.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This is a sample application that shows how to use the OpenAI SDK with NVIDIA Inference Microservice (NIM). Whether you are using a NIM from build.nvidia.com/ or a self-hosted NIM, this sample application will work for both.</p>","tags":["Natural Language and Conversational AI","Auth and API","LLM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#quick-start","title":"Quick Start","text":"<ol> <li>Add API key in <code>nvidia_nim.yaml</code></li> <li><code>./holohub run nvidia_nim_chat</code></li> </ol>","tags":["Natural Language and Conversational AI","Auth and API","LLM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#configuring-the-sample-application","title":"Configuring the sample application","text":"<p>Use the <code>nvidia_nim.yaml</code> configuration file to configure the sample application:</p>","tags":["Natural Language and Conversational AI","Auth and API","LLM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#connection-information","title":"Connection Information","text":"<pre><code>nim:\n  base_url: https://integrate.api.nvidia.com/v1\n  api_key:\n</code></pre> <p><code>base_url</code>: The URL of your NIM instance. Defaults to NVIDIA hosted NIMs. <code>api_key</code>: Your API key to access NVIDIA hosted NIMs.</p>","tags":["Natural Language and Conversational AI","Auth and API","LLM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#model-information","title":"Model Information","text":"<p>The <code>models</code> section in the YAML file is configured with multiple NVIDIA hosted models by default. This allows you to switch between different models easily within the application by sending the prompt <code>/m</code> to the application.</p> <p>Model parameters may be added or adjusted in the <code>models</code> section as well per model.</p>","tags":["Natural Language and Conversational AI","Auth and API","LLM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#run-the-sample-application","title":"Run the sample application","text":"<p>There are a couple of options to run the sample application:</p>","tags":["Natural Language and Conversational AI","Auth and API","LLM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#run-using-docker","title":"Run using Docker","text":"<p>To run the sample application with Docker, you must first build and run a Docker image that includes the sample application and its dependencies:</p> <pre><code># Build and run the Docker images from the root directory of Holohub\n./holohub run-container nvidia_nim\n</code></pre> <p>Continue to the Start the Application section once inside the Docker container.</p>","tags":["Natural Language and Conversational AI","Auth and API","LLM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#run-the-application-without-docker","title":"Run the Application without Docker","text":"<p>Install all dependencies from the <code>requirements.txt</code> file:</p> <pre><code># optionally create a virtual environment and activate it\npython3 -m venv .venv\nsource .venv/bin/activate\n\n# install the required packages\npip install -r applications/nvidia_nim/chat/requirements.txt\n</code></pre>","tags":["Natural Language and Conversational AI","Auth and API","LLM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#start-the-application","title":"Start the Application","text":"<p>To use the NIMs on build.nvidia.com/, configure your API key in the <code>nvidia_nim.yaml</code> configuration file and run the sample app as follows:</p> <p>note: you may also configure your api key using an environment variable. E.g., <code>export API_KEY=...</code></p> <pre><code># To use NVIDIA hosted NIMs available on build.nvidia.com, export your API key first\nexport API_KEY=[enter your api key here]\n\n./holohub run nvidia_nim_chat\n</code></pre> <p>Have fun!</p>","tags":["Natural Language and Conversational AI","Auth and API","LLM"]},{"location":"applications/nvidia_nim/nvidia_nim_chat/#connecting-with-locally-hosted-nims","title":"Connecting with Locally Hosted NIMs","text":"<p>To use a locally hosted NIM, first download and start the NIM. Then configure the <code>base_url</code> parameter in the <code>nvidia_nim.yaml</code> configuration file to point to your local NIM instance.</p> <p>The following example shows a NIM running locally and serving its APIs and the <code>meta-llama3-8b-instruct</code> model from <code>http://0.0.0.0:8000/v1</code>.</p> <pre><code>nim:\n  base_url: http://0.0.0.0:8000/v1/\n\nmodels:\n  llama3-8b-instruct:\n    model: meta-llama3-8b-instruct # name of the model serving by the NIM\n    # add/update/remove the following key/value pairs to configure the parameters for the model\n    top_p: 1\n    n: 1\n    max_tokens: 1024\n    frequency_penalty: 1.0\n</code></pre>","tags":["Natural Language and Conversational AI","Auth and API","LLM"]},{"location":"applications/nvidia_nim/nvidia_nim_imaging/","title":"Medical Imaging Segmentation with NVIDIA Vista-3D NIM","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.0 Tested Holoscan SDK versions: 1.0.0, 2.0.2 Contribution metric: Level 1 - Highly Reliable</p> <p>Vista-3D is a specialized interactive foundation model for segmenting and annotating human anatomies. This sample application demonstrates using the Vista-3D NVIDIA Inference Microservice (NIM) in a Holoscan pipeline.</p> <p>The application instructs the Vista-3D NIM API to process the given dataset and downloads and extracts the results of a segmentation NRRD file onto a local directory.</p> <p>Visit build.nvidia.com to learn more about Vista-3D and generate an API key to use with this application.</p>","tags":["Healthcare AI","Imaging","Segmentation","NRRD processing","3D Slicer","Medical Imaging"]},{"location":"applications/nvidia_nim/nvidia_nim_imaging/#quick-start","title":"Quick Start","text":"<ol> <li>Add API key in <code>nvidia_nim.yaml</code></li> <li><code>./holohub run nvidia_nim_imaging</code></li> </ol>","tags":["Healthcare AI","Imaging","Segmentation","NRRD processing","3D Slicer","Medical Imaging"]},{"location":"applications/nvidia_nim/nvidia_nim_imaging/#configuring-the-sample-application","title":"Configuring the sample application","text":"<p>Use the <code>nvidia_nim.yaml</code> configuration file to configure the sample application:</p>","tags":["Healthcare AI","Imaging","Segmentation","NRRD processing","3D Slicer","Medical Imaging"]},{"location":"applications/nvidia_nim/nvidia_nim_imaging/#connection-information","title":"Connection Information","text":"<pre><code>nim:\n base_url: https://integrate.api.nvidia.com/v1\n api_key:\n</code></pre> <ul> <li><code>base_url</code>: The URL of your NIM instance. Defaults to NVIDIA-hosted NIMs.</li> <li><code>api_key</code>: Your API key to access NVIDIA-hosted NIMs.</li> </ul>","tags":["Healthcare AI","Imaging","Segmentation","NRRD processing","3D Slicer","Medical Imaging"]},{"location":"applications/nvidia_nim/nvidia_nim_imaging/#build-and-run-the-sample-application","title":"Build and Run the sample application","text":"<pre><code># This first builds and runs the Docker images, then builds and runs the application.\n./holohub run nvidia_nim_imaging\n</code></pre>","tags":["Healthcare AI","Imaging","Segmentation","NRRD processing","3D Slicer","Medical Imaging"]},{"location":"applications/nvidia_nim/nvidia_nim_imaging/#display-the-results","title":"Display the Results","text":"<p>In this section, we will show how to view the sample data and segmentation results returned from Vista-3D.</p> <ol> <li>Download 3D Slicer: https://download.slicer.org/</li> <li>Decompress and launch 3D Slicer    <pre><code>tar -xvzf Slicer-5.6.2-linux-amd64.tar.gz\n</code></pre></li> <li>Locate the sample data volume and the segmentation results in <code>build/nvidia_nim_imaging/applications/nvidia_nim/nvidia_nim_imaging</code> <pre><code>drwxr-xr-x 3 user domain-users \u00a0 \u00a0 4096 Jul \u00a03 11:41 ./\ndrwxr-xr-x 4 user domain-users \u00a0 \u00a0 4096 Jul \u00a03 11:40 ../\n-rw-r--r-- 1 user user         27263336 Jul 23 14:22 example-1_seg.nrrd\n-rw-r--r-- 1 user user         33037057 Jul 23 14:21 sample.nii.gz\n</code></pre></li> <li>In 3D Slicer, click File, Add Data and click Choose File(s) to Add.    From the Add Data into the scene dialog, find and add the <code>sample.nii.gz</code> file and the <code>example-1_seg.nrrd</code> file.    For the <code>sample.nrrd</code> file, select Segmentation and click Ok.    </li> <li>3D Slicer shall display the volume and the segmentation results as shown below:    </li> </ol>","tags":["Healthcare AI","Imaging","Segmentation","NRRD processing","3D Slicer","Medical Imaging"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/","title":"NVIDIA NV-CLIP NIM","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3, 2.1.0, 2.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>NV-CLIP is a multimodal embeddings model for image and text, and this is a sample application that shows how to use the OpenAI SDK with NVIDIA Inference Microservice (NIM). Whether you are using a NIM from build.nvidia.com/ or a self-hosted NIM, this sample application will work for both.</p>","tags":["Computer Vision and Perception","Multimodal Model","Auth and API","Image Processing"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/#quick-start","title":"Quick Start","text":"<p>Get your API Key and start the sample application.</p> <ol> <li>Enter your API key in <code>nvidia_nim.yaml</code></li> <li><code>./holohub run nvidia_nim_nvclip</code></li> </ol>","tags":["Computer Vision and Perception","Multimodal Model","Auth and API","Image Processing"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/#advanced","title":"Advanced","text":"","tags":["Computer Vision and Perception","Multimodal Model","Auth and API","Image Processing"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/#configuring-the-sample-application","title":"Configuring the sample application","text":"<p>Use the <code>nvidia_nim.yaml</code> configuration file to configure the sample application:</p>","tags":["Computer Vision and Perception","Multimodal Model","Auth and API","Image Processing"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/#nvidia-hosted-nv-clip-nim","title":"NVIDIA-Hosted NV-CLIP NIM","text":"<p>By default, the application is configured to use NVIDIA-hosted NV-CLIP NIM.</p> <pre><code>nim:\n base_url: https://integrate.api.nvidia.com/v1\n api_key:\n</code></pre> <p><code>base_url</code>: The URL of your NIM instance. Defaults to NVIDIA-hosted NIMs. <code>api_key</code>: Your API key to access NVIDIA-hosted NIMs.</p> <p>Note: you may also configure your API key using an environment variable. E.g., <code>export API_KEY=...</code></p> <pre><code># To use NVIDIA hosted NIMs available on build.nvidia.com, export your API key first\nexport API_KEY=[enter your API key here]\n</code></pre>","tags":["Computer Vision and Perception","Multimodal Model","Auth and API","Image Processing"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/#self-hosted-nims","title":"Self-Hosted NIMs","text":"<p>To use a self-hosted NIM, refer to the NV-CLIP NIM documentation to configure and start the NIM.</p> <p>Then, comment out the NVIDIA-hosted section and uncomment the self-hosted configuration section in the <code>nvidia_nim.yaml</code> file.</p> <pre><code>nim:\n  base_url: http://0.0.0.0:8000/v1/\n  encoding_format: float\n  api_key: NA\n  model: nvidia/nvclip-vit-h-14\n</code></pre>","tags":["Computer Vision and Perception","Multimodal Model","Auth and API","Image Processing"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/#build-the-application","title":"Build The Application","text":"<p>To run the sample application, you must first build a Docker image that includes the sample application and its dependencies:</p> <pre><code># Build and run the Docker images from the root directory of Holohub\n./holohub run-container nvidia_nim_nvclip\n</code></pre>","tags":["Computer Vision and Perception","Multimodal Model","Auth and API","Image Processing"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/#run-the-application","title":"Run the Application","text":"<p>To use the NIMs on build.nvidia.com/, configure your API key in the <code>nvidia_nim.yaml</code> configuration file and run the sample app as follows:</p> <pre><code>./holohub run nvidia_nim_nvclip\n</code></pre>","tags":["Computer Vision and Perception","Multimodal Model","Auth and API","Image Processing"]},{"location":"applications/nvidia_nim/nvidia_nim_nvclip/#using-the-application","title":"Using the Application","text":"<p>Once the application is ready, it will prompt you to input URLs to the images you want to perform inference.</p> <pre><code>Enter a URL to an image: https://domain.to/my/image-cat.jpg\nDownloading image...\n\nEnter a URL to another image or hit ENTER to continue: https://domain.to/my/image-rabbit.jpg\nDownloading image...\n\nEnter a URL to another image or hit ENTER to continue: https://domain.to/my/image-dog.jpg\nDownloading image...\n</code></pre> <p>If there are no more images that you want to use, hit ENTER to continue and then enter a prompt:</p> <pre><code>Enter a URL to another image or hit ENTER to continue:\n\nEnter a prompt: Which image contains a rabbit?\n</code></pre> <p>The application will connect to the NIM to generate an answer and then calculate the cosine similarity between the images and the prompt:</p> <pre><code>\u2827 Generating...\nPrompt: Which image contains a rabbit?\nOutput:\nImage 1: 3.0%\nImage 2: 52.0%\nImage 3: 46.0%\n</code></pre>","tags":["Computer Vision and Perception","Multimodal Model","Auth and API","Image Processing"]},{"location":"applications/nvidia_video_codec/nvc_decode/","title":"NVIDIA Video Codec: H.264 File Decoder","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 3.3.0 Tested Holoscan SDK versions: 3.3.0, 3.4.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates decoding H.264 elementary stream files using the NVIDIA Video Codec SDK. The application reads H.264 files directly, decodes them with the NVIDIA Video Decoder, and displays the results with Holoviz.</p> <p>[!IMPORTANT] By using the NVIDIA Video Codec Demo application and its operators, you agree to the NVIDIA Software Developer License Agreement. If you disagree with the EULA, please do not run this application.</p>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_decode/#key-features","title":"Key Features","text":"<ul> <li>Direct Elementary Stream Processing: Reads H.264 elementary stream files without container conversion</li> <li>GPU-Accelerated Decoding: Uses NVIDIA Video Codec SDK for hardware-accelerated decoding</li> <li>Real-time Visualization: Displays decoded frames using Holoviz</li> <li>Performance Monitoring: Tracks decode latency, FPS, and jitter metrics</li> </ul>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_decode/#requirements","title":"Requirements","text":"<ul> <li>NVIDIA Driver Version &gt;= 570</li> <li>CUDA Version &gt;= 12.8</li> <li>H.264 elementary stream file (<code>.h264</code>, <code>.264</code> extension)</li> </ul> <p>\ud83d\udca1 Note: Tested on x86 + dGPU and NVIDIA IGX Orin + dGPU. NVIDIA IGX Orin with integrated GPU is not currently supported.</p>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_decode/#architecture","title":"Architecture","text":"<pre><code>graph TD;\n    H264_Elementary_Stream_File--&gt;H264FileReaderOp;\n    H264FileReaderOp--&gt;NvVideoDecoderOp;\n    NvVideoDecoderOp--&gt;HolovizOp;\n    NvVideoDecoderOp--&gt;StatsOp;</code></pre>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_decode/#key-components","title":"Key Components","text":"<ol> <li>H264FileReaderOp: Reads H.264 elementary stream files and emits raw bitstream data as tensors</li> <li>NvVideoDecoderOp: Hardware-accelerated decoder using NVIDIA Video Codec SDK</li> <li>HolovizOp: Visualization of decoded video frames</li> <li>StatsOp: Performance monitoring and metrics collection</li> </ol>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_decode/#preparing-h264-elementary-stream-files","title":"Preparing H.264 Elementary Stream Files","text":"<p>The application expects H.264 elementary stream files (pure H.264 bitstream without container). You can create these files using FFmpeg:</p>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_decode/#from-mp4container-format","title":"From MP4/Container Format","text":"<pre><code>ffmpeg -i input_video.mp4 -c:v copy -f h264 output.h264\n</code></pre>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_decode/#from-endoscopy-sample-data","title":"From Endoscopy Sample Data","text":"<pre><code>ffmpeg -i surgical_video.mp4 -c:v copy -f h264 surgical_video.264\n</code></pre>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_decode/#verification","title":"Verification","text":"<p>You can verify your H.264 file format in several ways:</p> <ul> <li>Run the application with a small test file to ensure it decodes and displays correctly.</li> <li>Play the file in VLC Media Player: VLC can open raw H.264 streams. Go to <code>Media</code> &gt; <code>Open File...</code> and select your <code>.h264</code> or <code>.264</code> file. If the video plays, the file is likely valid.</li> <li>Use FFmpeg to probe the file: Run <code>ffmpeg -i output.h264</code> to check if FFmpeg recognizes the stream and displays stream information.</li> <li>Use other video players that support raw H.264 streams, such as MPV or PotPlayer.</li> </ul> <p>If the file fails to play or is not recognized by these tools, double-check your FFmpeg command and source file.</p>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_decode/#building-and-running","title":"Building and Running","text":"","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_decode/#python","title":"Python","text":"<pre><code>./holohub run nvc_decode --language python\n</code></pre>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_decode/#c","title":"C++","text":"<pre><code>./holohub run nvc_decode --language cpp\n</code></pre>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_decode/#configuration","title":"Configuration","text":"<p>The application is configured via configuration files:</p> <ul> <li>Python: python/nvc_decode.yaml</li> <li>C++: cpp/nvc_decode.yaml</li> </ul> <pre><code>decoder:\n  cuda_device_ordinal: 0\n\nholoviz:\n  window_title: \"NVIDIA Video Codec - H.264 Decode\"\n  tensors:\n    - name: \"\"\n      type: color\n      opacity: 1.0\n      priority: 0\n</code></pre>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_decode/#technical-details","title":"Technical Details","text":"","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_decode/#elementary-stream-vs-containerized-formats","title":"Elementary Stream vs Containerized Formats","text":"<ul> <li>Elementary Stream: Raw H.264 encoded data containing NAL units with start codes (0x00000001 or 0x000001)</li> <li>Containerized Stream: Elementary stream wrapped in container format (MP4, AVI, MOV) with metadata and timing information</li> </ul> <p>This application works directly with elementary streams, which is more efficient for scenarios where you have raw H.264 data.</p>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_decode/#integration-with-nvvideodecoderop","title":"Integration with NvVideoDecoderOp","text":"<p>The <code>NvVideoDecoderOp</code> uses an internal <code>FFmpegDemuxer</code> with a <code>StreamDataProvider</code> interface that can handle elementary streams directly:</p> <ol> <li>H264FileReaderOp reads the entire H.264 file into memory</li> <li>Creates a tensor with the raw bitstream data</li> <li>NvVideoDecoderOp receives the tensor and feeds it to StreamDataProvider</li> <li>FFmpegDemuxer parses the elementary stream internally</li> <li>NVIDIA decoder processes the parsed data</li> </ol>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_decode/#performance-metrics","title":"Performance Metrics","text":"<p>The application tracks several performance metrics:</p> <ul> <li>Decode Latency: Time taken for hardware decoding</li> <li>FPS: Frames per second throughput</li> <li>Jitter Time: Frame timing variability</li> </ul> <p>Example output: <pre><code>Decode Latency (min, max, avg): 2.145, 5.234, 2.876\nFPS (min, max, avg): 95.234, 120.456, 108.765\nJitter Time (min, max, avg): 0.123, 0.456, 0.234\n</code></pre></p>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_decode/#troubleshooting","title":"Troubleshooting","text":"","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_decode/#common-issues","title":"Common Issues","text":"<ol> <li>File Not Found: Ensure your H.264 file exists and the path is correct</li> <li>Invalid Format: Run the application with your file to verify it contains valid H.264 elementary stream</li> <li>GPU Not Supported: Ensure your GPU supports NVIDIA Video Codec SDK</li> </ol>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_decode/#file-format-requirements","title":"File Format Requirements","text":"<ul> <li>File must be H.264 elementary stream (not containerized format)</li> <li>Should contain NAL units with proper start codes</li> <li>Supported extensions: <code>.h264</code>, <code>.264</code></li> </ul>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_decode/#creating-test-files","title":"Creating Test Files","text":"<p>If you don't have H.264 elementary stream files, create them from existing videos:</p> <pre><code># From any video file\nffmpeg -i input.mp4 -c:v libx264 -preset fast -crf 23 -f h264 test.h264\n\n# From webcam (for testing)\nffmpeg -f v4l2 -i /dev/video0 -t 10 -c:v libx264 -f h264 webcam_test.h264\n</code></pre>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_decode/#licensing","title":"Licensing","text":"<p>Holohub applications and operators are licensed under Apache-2.0.</p> <p>NVIDIA Video Codec is governed by the terms of the NVIDIA Software Developer License Agreement, which you accept by cloning, running, or using the NVIDIA Video Codec sample applications and operators.</p>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_encode_decode/","title":"NVIDIA Video Codec: Encode-Decode Video","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 3.3.0 Tested Holoscan SDK versions: 3.3.0, 3.4.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates the use of NVIDIA Video Codec SDK. The application loads a video file, encodes the video using either H.264 or HEVC (H.265), decodes the video, and displays it with Holoviz.</p> <p>[!IMPORTANT] By using the NVIDIA Video Codec Demo application and its operators, you agree to the NVIDIA Software Developer License Agreement. If you disagree with the EULA, please do not run this application.</p>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_encode_decode/#requirements","title":"Requirements","text":"<ul> <li>NVIDIA Driver Version &gt;= 570</li> <li>CUDA Version &gt;= 12.8</li> <li>x86 and SBSA platforms with dedicated GPU</li> </ul> <p>\ud83d\udca1 Note: NVIDIA IGX Orin with integrated GPU is not currently supported.</p>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_encode_decode/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_encode_decode/#building-and-running-the-nvidia-video-codec-application","title":"Building and Running the NVIDIA Video Codec Application","text":"","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_encode_decode/#python","title":"Python","text":"<pre><code>./holohub run nvc_encode_decode --language python\n</code></pre>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_encode_decode/#c","title":"C++","text":"<pre><code>./holohub run nvc_encode_decode --language cpp\n</code></pre>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_encode_decode/#configuration","title":"Configuration","text":"<p>The application is configured with H.264 codec by default. It may be modified in the configuration files:</p> <ul> <li>Python: python/nvc_encode_decode.yaml</li> <li>C++: cpp/nvc_encode_decode.yaml</li> </ul> <pre><code>encoder:\n  codec: \"H264\" # H265 or HEVC\n  preset: \"P3\" # P1, P2, P3, P4, P5, P6, P7\n  cuda_device_ordinal: 0\n  bitrate: 10000000\n  frame_rate: 60\n  rate_control_mode: 0 # 0: Constant QP, 1: Variable bitrate, 2: Constant bitrate\n  multi_pass_encoding: 1 # 0: Disabled, 1: Quarter resolution, 2: Full resolution\n</code></pre> <p>Refer to the NVIDIA Video Codec documentation for additional details.</p>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_encode_decode/#benchmarks","title":"Benchmarks","text":"<p>We collected latency benchmark results using Holoscan Data Flow Tracking tools on the NVIDIA Video Codec sample application. The benchmark is conducted on x86_64 with AMD Ryzen 9 7950X, 128 GB system memory and NVIDIA ADA6000 GPU.</p> <p>Encoder Configurations: - Bitrate: 10 Mbps - FPS: 60 - Rate Control Mode: 1 Variable Bitrate - Multi-pass Encoding: 1 Quarter Resolution</p> E2E Encoding Decoding FPS Codec Preset Min Max Avg Min Max Avg Min Max Avg Avg H.264 P3 6.242 8.423 6.743 0.536 0.865 0.593 5.258 7.307 5.699 145.270 P4 6.22 8.219 6.674 0.561 0.962 0.615 5.220 7.218 5.616 146.875 P5 6.508 8.441 7.044 0.921 1.403 0.971 5.229 7.097 5.658 139.433 P6 6.37 9.409 7.102 0.680 1.060 0.730 5.141 7.301 5.646 143.368 P7 6.529 8.531 7.107 0.740 1.155 0.795 5.104 8.231 5.650 142.727 HEVC P3 6.258 9.039 6.898 0.684 1.078 0.728 5.141 7.371 5.656 145.058 P4 6.146 9.351 6.788 0.684 1.088 0.731 5.130 7.301 5.642 143.481 P5 6.193 8.991 6.818 0.680 1.060 0.730 5.130 7.301 5.643 143.371 P6 6.337 9.113 6.924 0.682 1.109 0.733 5.254 8.043 5.729 141.633 P7 6.246 10.11 6.909 0.740 1.155 0.793 5.104 8.231 5.667 142.035 <p>Note: all reported latency values are in milliseconds.</p>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_encode_decode/#licensing","title":"Licensing","text":"<p>Holohub applications and operators are licensed under Apache-2.0.</p> <p>NVIDIA Video Codec is governed by the terms of the NVIDIA Software Developer License Agreement, which you accept by cloning, running, or using the NVIDIA Video Codec sample applications and operators.</p>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_encode_writer/","title":"NVIDIA Video Codec: Video Writer","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: October 9, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 3.3.0 Tested Holoscan SDK versions: 3.3.0, 3.4.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates the use of NVIDIA Video Codec SDK. The application loads a video file, encodes the video using either H.264 or HEVC (H.265), and then writes the encoded video frames to disk.</p>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_encode_writer/#requirements","title":"Requirements","text":"<ul> <li>NVIDIA Driver Version &gt;= 570</li> <li>CUDA Version &gt;= 12.8</li> <li>x86 and SBSA platforms with dedicated GPU</li> </ul> <p>\ud83d\udca1 Note: NVIDIA IGX Orin with integrated GPU is not currently supported.</p>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_encode_writer/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_encode_writer/#building-and-running-the-nvidia-video-codec-application","title":"Building and Running the NVIDIA Video Codec Application","text":"","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_encode_writer/#python","title":"Python","text":"<pre><code>./holohub run nvc_encode_writer --language python\n</code></pre>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_encode_writer/#c","title":"C++","text":"<pre><code>./holohub run nvc_encode_writer --language cpp\n</code></pre>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_encode_writer/#configuration","title":"Configuration","text":"<p>The application is configured with H.264 codec by default for lower power consumption. It may be modified in the configuration files:</p> <ul> <li>Python: python/nvc_encode_writer.yaml</li> <li>C++: cpp/nvc_encode_writer.yaml</li> </ul> <pre><code>encoder:\n  codec: \"H264\" # H265 or HEVC\n  preset: \"P3\" # P1, P2, P3, P4, P5, P6, P7\n  cuda_device_ordinal: 0\n  bitrate: 10000000\n  frame_rate: 60\n  rate_control_mode: 0 # 0: Constant QP, 1: Variable bitrate, 2: Constant bitrate\n  multi_pass_encoding: 1 # 0: Disabled, 1: Quarter resolution, 2: Full resolution\n</code></pre> <p>Refer to the NVIDIA Video Codec documentation for additional details.</p>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_encode_writer/#licensing","title":"Licensing","text":"<p>Holohub applications and operators are licensed under Apache-2.0.</p> <p>NVIDIA Video Codec is governed by the terms of the NVIDIA Software Developer License Agreement, which you accept by cloning, running, or using the NVIDIA Video Codec sample applications and operators.</p>","tags":["Streaming","NVIDIA Video Codec","H.264","H.265","HEVC","Video"]},{"location":"applications/nvidia_video_codec/nvc_endoscopy_tool_tracking/","title":"NVIDIA Video Codec: Endoscopy Tool Tracking","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: November 20, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 3.3.0 Tested Holoscan SDK versions: 3.7.0, 3.8.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates GPU-accelerated H.264 video decoding combined with LSTM-based AI inference for endoscopic surgical tool tracking using the NVIDIA Video Codec SDK. The application reads an H.264 elementary stream file, decodes it with hardware acceleration, runs real-time LSTM inference for tool detection and tracking, and optionally encodes the output back to H.264.</p> <p>This is a modified version of the Endoscopy Tool Tracking reference application in Holoscan SDK that uses NVIDIA Video Codec SDK for efficient video decode and encode operations.</p> <p>[!IMPORTANT] By using the NVIDIA Video Codec Demo application and its operators, you agree to the NVIDIA Software Developer License Agreement. If you disagree with the EULA, please do not run this application.</p>","tags":["NVIDIA Video Codec","H.264","H.265","HEVC","Video","Healthcare AI","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/nvidia_video_codec/nvc_endoscopy_tool_tracking/#key-features","title":"Key Features","text":"<ul> <li>Hardware-Accelerated Video Decode: Uses NVIDIA Video Codec SDK for efficient H.264 decoding</li> <li>AI-Based Tool Tracking: LSTM neural network for real-time surgical tool detection and tracking</li> <li>Optional Video Encode: Can encode the processed output back to H.264 format</li> <li>Real-time Visualization: Displays video with tool tracking overlays using Holoviz</li> <li>Multiple Tool Detection: Tracks up to 7 different surgical tools (Grasper, Bipolar, Hook, Scissors, Clipper, Irrigator, Specimen Bag)</li> </ul>","tags":["NVIDIA Video Codec","H.264","H.265","HEVC","Video","Healthcare AI","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/nvidia_video_codec/nvc_endoscopy_tool_tracking/#requirements","title":"Requirements","text":"<ul> <li>NVIDIA Driver Version &gt;= 570</li> <li>CUDA Version &gt;= 12.8</li> <li>x86 and SBSA platforms with dedicated GPU</li> </ul> <p>\ud83d\udca1 Note: NVIDIA IGX Orin with integrated GPU is not currently supported.</p>","tags":["NVIDIA Video Codec","H.264","H.265","HEVC","Video","Healthcare AI","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/nvidia_video_codec/nvc_endoscopy_tool_tracking/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking</p> <p>The data is automatically downloaded when building the application.</p>","tags":["NVIDIA Video Codec","H.264","H.265","HEVC","Video","Healthcare AI","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/nvidia_video_codec/nvc_endoscopy_tool_tracking/#building-and-running","title":"Building and Running","text":"","tags":["NVIDIA Video Codec","H.264","H.265","HEVC","Video","Healthcare AI","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/nvidia_video_codec/nvc_endoscopy_tool_tracking/#python","title":"Python","text":"<pre><code>./holohub run nvc_endoscopy_tool_tracking --language python\n</code></pre>","tags":["NVIDIA Video Codec","H.264","H.265","HEVC","Video","Healthcare AI","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/nvidia_video_codec/nvc_endoscopy_tool_tracking/#c","title":"C++","text":"<pre><code>./holohub run nvc_endoscopy_tool_tracking --language cpp\n</code></pre>","tags":["NVIDIA Video Codec","H.264","H.265","HEVC","Video","Healthcare AI","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/nvidia_video_codec/nvc_endoscopy_tool_tracking/#configuration","title":"Configuration","text":"<p>The application is configured via YAML configuration files:</p> <ul> <li>Python: python/nvc_endoscopy_tool_tracking.yaml</li> <li>C++: cpp/nvc_endoscopy_tool_tracking.yaml</li> </ul>","tags":["NVIDIA Video Codec","H.264","H.265","HEVC","Video","Healthcare AI","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/nvidia_video_codec/nvc_endoscopy_tool_tracking/#decoder-configuration","title":"Decoder Configuration","text":"<pre><code>decoder:\n  cuda_device_ordinal: 0\n  verbose: false\n</code></pre>","tags":["NVIDIA Video Codec","H.264","H.265","HEVC","Video","Healthcare AI","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/nvidia_video_codec/nvc_endoscopy_tool_tracking/#encoder-configuration-optional","title":"Encoder Configuration (Optional)","text":"<p>Set <code>record_output: true</code> to enable encoding of the processed output:</p> <pre><code>record_output: true  # Setting this to `false` disables H264 encoding\n\nencoder:\n  codec: \"H264\"  # H265 or HEVC\n  preset: \"P3\"   # P1, P2, P3, P4, P5, P6, P7\n  cuda_device_ordinal: 0\n  bitrate: 10000000\n  frame_rate: 60\n  rate_control_mode: 1  # 0: Constant QP, 1: Variable bitrate, 2: Constant bitrate\n  multi_pass_encoding: 1  # 0: Disabled, 1: Quarter resolution, 2: Full resolution\n</code></pre> <p>Refer to the NVIDIA Video Codec documentation for additional encoder configuration details.</p>","tags":["NVIDIA Video Codec","H.264","H.265","HEVC","Video","Healthcare AI","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/nvidia_video_codec/nvc_endoscopy_tool_tracking/#input-video-file","title":"Input Video File","text":"<p>The application expects H.264 elementary stream files (<code>.h264</code> or <code>.264</code> extension):</p> <pre><code>reader:\n  filename: \"surgical_video.264\"\n  verbose: false\n  loop: false\n</code></pre>","tags":["NVIDIA Video Codec","H.264","H.265","HEVC","Video","Healthcare AI","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/nvidia_video_codec/nvc_endoscopy_tool_tracking/#architecture","title":"Architecture","text":"<p>The application pipeline consists of:</p> <ol> <li>NvVideoReaderOp: Reads H.264 elementary stream files</li> <li>NvVideoDecoderOp: Hardware-accelerated H.264 decoder</li> <li>FormatConverterOp: Converts decoded frames (NV12 \u2192 RGB888 \u2192 Float32)</li> <li>LSTMTensorRTInferenceOp: LSTM-based AI inference for tool tracking</li> <li>ToolTrackingPostprocessorOp: Post-processes inference results</li> <li>HolovizOp: Visualizes video with tool tracking overlays</li> <li>NvVideoEncoderOp (optional): Encodes output to H.264 if <code>record_output</code> is enabled</li> </ol>","tags":["NVIDIA Video Codec","H.264","H.265","HEVC","Video","Healthcare AI","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/nvidia_video_codec/nvc_endoscopy_tool_tracking/#preparing-h264-elementary-stream-files","title":"Preparing H.264 Elementary Stream Files","text":"<p>The application expects H.264 elementary stream files (pure H.264 bitstream without container). You can create these files using FFmpeg:</p>","tags":["NVIDIA Video Codec","H.264","H.265","HEVC","Video","Healthcare AI","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/nvidia_video_codec/nvc_endoscopy_tool_tracking/#from-mp4container-format","title":"From MP4/Container Format","text":"<pre><code>ffmpeg -i input_video.mp4 -c:v copy -f h264 output.h264\n</code></pre>","tags":["NVIDIA Video Codec","H.264","H.265","HEVC","Video","Healthcare AI","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/nvidia_video_codec/nvc_endoscopy_tool_tracking/#from-endoscopy-sample-data","title":"From Endoscopy Sample Data","text":"<pre><code>ffmpeg -i surgical_video.mp4 -c:v copy -f h264 surgical_video.264\n</code></pre>","tags":["NVIDIA Video Codec","H.264","H.265","HEVC","Video","Healthcare AI","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/nvidia_video_codec/nvc_endoscopy_tool_tracking/#tool-tracking","title":"Tool Tracking","text":"<p>The application tracks seven different surgical tools in real-time:</p> <ol> <li>Grasper - Displayed with red cross overlay</li> <li>Bipolar - Forceps for coagulation</li> <li>Hook - Electrosurgical hook</li> <li>Scissors - Surgical scissors</li> <li>Clipper - Clip applier</li> <li>Irrigator - Irrigation/suction device</li> <li>Spec.Bag - Specimen retrieval bag</li> </ol> <p>Each detected tool is marked with: - Cross overlay showing tool position - Text label identifying the tool type - Colored mask indicating tool region</p>","tags":["NVIDIA Video Codec","H.264","H.265","HEVC","Video","Healthcare AI","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/nvidia_video_codec/nvc_endoscopy_tool_tracking/#performance-considerations","title":"Performance Considerations","text":"<p>The H.264 video decode operators do not adjust framerate as they read the elementary stream input. As a result, the video stream can be displayed as quickly as the decoding and inference can be performed. This application uses <code>PeriodicCondition</code> to play video at the same speed as the source video.</p> <p>The LSTM inference runs at each frame, providing real-time tool tracking with minimal latency.</p>","tags":["NVIDIA Video Codec","H.264","H.265","HEVC","Video","Healthcare AI","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/nvidia_video_codec/nvc_endoscopy_tool_tracking/#licensing","title":"Licensing","text":"<p>Holohub applications and operators are licensed under Apache-2.0.</p> <p>NVIDIA Video Codec is governed by the terms of the NVIDIA Software Developer License Agreement, which you accept by cloning, running, or using the NVIDIA Video Codec sample applications and operators.</p>","tags":["NVIDIA Video Codec","H.264","H.265","HEVC","Video","Healthcare AI","Surgical AI","Endoscopy","Visualization"]},{"location":"applications/object_detection_torch/","title":"Object Detection using PyTorch Faster R-CNN","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: February 4, 2026 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0, 3.10.0, 3.11.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application performs object detection using frcnn resnet50 model from torchvision. The inference is executed using <code>torch</code> backend in <code>holoinfer</code> module in Holoscan SDK.</p> <p><code>object_detection_torch.yaml</code> is the configuration file. Input video file is converted into GXF tensors and the name and location of the GXF tensors are updated in the <code>basename</code> and the <code>directory</code> field in <code>replayer</code>.</p> <p>This application need <code>Libtorch</code> for inferencing. Ensure that the Holoscan SDK is build with <code>build_libtorch</code> flag as true. If not, then rebuild the SDK with following: <code>./holohub build --build_libtorch true</code> before running this application.</p>","tags":["Computer Vision and Perception","Video","Detection","Visualization","Holoviz"]},{"location":"applications/object_detection_torch/#data","title":"Data","text":"<p>To run this application, you will need the following:</p> <ul> <li>Model name: frcnn_resnet50_t.pt<ul> <li>The model should be converted to torchscript format.  The original pytorch model can be downloaded from pytorch model. <code>frcnn_resnet50_t.pt</code> is used</li> </ul> </li> <li>Model configuration file: frcnn_resnet50_t.yaml<ul> <li>Model config documents input and output nodes, their dimensions and respective datatype.</li> </ul> </li> <li>Labels file: labels.txt<ul> <li>Labels for identified objects.</li> </ul> </li> <li>Postprocessor configuration file: postprocessing.yaml<ul> <li>This configuration stores the number and type of objects to be identified. By default, the application detects and generates bounding boxes for <code>car</code> (max 50), <code>person</code> (max 50), <code>motorcycle</code> (max 10) in the input frame. All remaining identified objects are tagged with label <code>object</code> (max 50).</li> <li>Additionally, color of the bounding box for each identified object can be set.</li> <li>Threshold of scores can be set in the <code>params</code>. Default value is 0.75.</li> </ul> </li> </ul> <p>Sample dataset can be any video file freely available for testing on the web. E.g. Traffic video</p> <p>Once the video is downloaded, it must be converted into GXF entities. As shown in the command below, width and height is set to 1920x1080 by default. To reduce the size of generated tensors a lower resolution can be used. Generated entities must be saved at /object_detection_torch folder. <pre><code>ffmpeg -i &lt;downloaded_video&gt; -pix_fmt rgb24 -f rawvideo pipe:1 | python utilities/convert_video_to_gxf_entities.py --width 1920 --height 1080 --channels 3 --framerate 30\n</code></pre> <p>If resolution is updated in entity generation, it must be updated in the following config files as well: /object_detection_torch/frcnn_resnet50_t.yaml /object_detection_torch/postprocessing.yaml","tags":["Computer Vision and Perception","Video","Detection","Visualization","Holoviz"]},{"location":"applications/object_detection_torch/#quick-start","title":"Quick start","text":"<p>If you want to quickly run this application, you can use the <code>./holohub run</code> command.</p> <pre><code>./holohub run object_detection_torch\n</code></pre> <p>Otherwise, you can build and run the application using the commands below.</p>","tags":["Computer Vision and Perception","Video","Detection","Visualization","Holoviz"]},{"location":"applications/object_detection_torch/#building-the-application","title":"Building the application","text":"<p>The best way to run this application is inside the container, as it would provide all the required third-party packages:</p> <pre><code># Create and launch the container image for this application\n./holohub run-container object_detection_torch\n# Build the application inside the container. Note that this downloads the video data as well\n./holohub build object_detection_torch\n# Generate the pytorch model\npython3 applications/object_detection_torch/generate_resnet_model.py  data/object_detection_torch/frcnn_resnet50_t.pt\n# Run the application\n./holohub run object_detection_torch\n</code></pre> <p>Please refer to the top level Holohub README.md file for more information on how to build this application.</p>","tags":["Computer Vision and Perception","Video","Detection","Visualization","Holoviz"]},{"location":"applications/object_detection_torch/#running-the-application","title":"Running the application","text":"<pre><code># ensure the current working directory contains the &lt;data_dir&gt;.\n&lt;build_dir&gt;/object_detection_torch\n</code></pre> <p>If application is executed from within the holoscan sdk container and is not able to find <code>libtorch.so</code>, update <code>LD_LIBRARY_PATH</code> as below:</p> <pre><code>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/libtorch/1.13.1/lib\n</code></pre> <p>On aarch64, if application is executed from within the holoscan sdk container and libtorch throws linking errors, update the <code>LD_LIBRARY_PATH</code> as below:</p> <pre><code>export LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:/opt/hpcx/ompi/lib\"\n</code></pre>","tags":["Computer Vision and Perception","Video","Detection","Visualization","Holoviz"]},{"location":"applications/object_detection_torch/#containerize-the-application","title":"Containerize the application","text":"<p>To containerize the application using Holoscan CLI, first build the application using <code>./holohub install object_detection_torch</code>, run the <code>package-app.sh</code> script and then follow the generated output to package and run the application.</p> <p>Refer to the Packaging Holoscan Applications section of the Holoscan User Guide to learn more about installing the Holoscan CLI or packaging your application using Holoscan CLI.</p>","tags":["Computer Vision and Perception","Video","Detection","Visualization","Holoviz"]},{"location":"applications/object_detection_torch/#known-issues","title":"Known Issues","text":"<p>Limited platform support: The Faster R-CNN PyTorch model in this example relies on symbols available only in <code>torchvision&lt;=0.23.0</code>. arm64 SBSA platforms and x86_64 platforms with CUDA Toolkit &gt;= 13.0 are currently not supported as no compatible combination of Holoscan SDK, PyTorch (CUDA), and torchvision (CUDA) are available for these platforms.</p> <p>Please refer to the docker file for information about the torchvision version on different platforms.</p> <p>The application container will fail to build on other platforms with the error: <pre><code>ERROR: No matching distribution found\n</code></pre></p>","tags":["Computer Vision and Perception","Video","Detection","Visualization","Holoviz"]},{"location":"applications/openigtlink_3dslicer/","title":"OpenIGTLink 3D Slicer: Bidirectional Video Streaming with AI Segmentation","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 2 - Trusted</p> <p>This application demonstrates how to interface Holoscan SDK with 3D Slicer, using the OpenIGTLink protocol. The application is shown in the application graph below.</p> <p></p> <p>In summary, the <code>openigtlink</code> transmit and receive operators are used in conjunction with an AI segmentation pipeline to:</p> <ol> <li>Send Holoscan sample video data from a node running Holoscan SDK, using <code>OpenIGTLinkTxOp</code>, to 3D Slicer running on a different node (simulating a video source connected to 3D Slicer):<ul> <li>For <code>cpp</code> application, the ultrasound sample data is sent.</li> <li>For <code>python</code> application, the colonoscopy sample data is sent.</li> </ul> </li> <li>Transmit the video data back to Holoscan SDK using OpenIGTLinkIF Module, and receive the data with the <code>OpenIGTLinkRxOp</code> operator.</li> <li>Perform an AI segmentation pipeline in Holoscan:<ul> <li>For <code>cpp</code> application, the ultrasound segmentation model is deployed.</li> <li>For <code>python</code> application, the colonoscopy segmentation model is deployed.</li> </ul> </li> <li>Use Holoviz in <code>headless</code> mode to render image and segmentation and then send the data back to 3D Slicer using the <code>OpenIGTLinkTxOp</code> operator.</li> </ol> <p>This workflow allows for sending image data from 3D Slicer over network to Holoscan SDK (running on either <code>x86</code> or <code>arm</code>), do some compute task (e.g., AI inference), and send the results back to 3D Slicer for visualization. Nodes can run distributed; for example, Holoscan SDK can run on an IGX Orin (Node A) sending the video data, 3D Slicer on a Windows laptop (Node B) and the AI inference pipeline on yet another machine (Node C). Also, note that the <code>openigtlink</code> operators can connect to any software/library that supports the OpenIGTLink protocol; here, 3D Slicer is used as it is a popular open source software package for image analysis and scientific visualization.</p> <p>For the <code>cpp</code> application, which does ultrasound segmentations the results look like</p> <p></p> <p>and for the <code>python</code> application, which does colonoscopy segmentation, the results look like</p> <p></p> <p>where the image data before Holoscan processing is shown in the left slice view, and the image data with segmentation overlay (after Holoscan processing) is shown in the right slice view.</p>","tags":["Healthcare AI","Networking and Distributed Computing","Video","Visualization","3D Slicer","Segmentation","Ultrasound","Endoscopy"]},{"location":"applications/openigtlink_3dslicer/#run-instructions","title":"Run Instructions","text":"","tags":["Healthcare AI","Networking and Distributed Computing","Video","Visualization","3D Slicer","Segmentation","Ultrasound","Endoscopy"]},{"location":"applications/openigtlink_3dslicer/#machine-running-3d-slicer","title":"Machine running 3D Slicer","text":"<p>On the machine running 3D Slicer:</p> <ol> <li>In 3D Slicer, open the Extensions Manager and install the <code>SlicerOpenIGTLink</code> extension.</li> <li>Next, load the scene <code>openigtlink_3dslicer/scene/openigtlink_3dslicer.mrb</code> into 3D Slicer.</li> <li>Go to the <code>OpenIGTLinkIF</code> module and make sure that the <code>SendToHoloscan</code> connector has the IP address of the machine running Holoscan SDK in the Hostname input box (under Properties).</li> <li>Then activate the two connectors <code>ReceiveFromHoloscan</code> and <code>SendToHoloscan</code> (click Active check box under Properties).</li> </ol>","tags":["Healthcare AI","Networking and Distributed Computing","Video","Visualization","3D Slicer","Segmentation","Ultrasound","Endoscopy"]},{"location":"applications/openigtlink_3dslicer/#machine-running-holoscan-sdk","title":"Machine running Holoscan SDK","text":"<p>On the machine running Holoscan SDK:</p> <ol> <li>Configure the connection: Update the <code>host_name</code> parameters in the configuration files for both <code>OpenIGTLinkRxOp</code> operators:</li> <li><code>openigtlink_tx_slicer_img</code></li> <li><code>openigtlink_tx_slicer_holoscan</code></li> </ol> <p>Set these to the IP address of the machine running 3D Slicer.</p> <pre><code>&gt; **Note**: This application requires [OpenIGTLink](http://openigtlink.org/) to be installed.\n</code></pre> <ol> <li> <p>Run the application: Use the Holohub CLI to launch the application.</p> </li> <li> <p>For the <code>python</code> application:</p> <pre><code>./holohub run openigtlink_3dslicer --language python\n</code></pre> </li> <li> <p>For the <code>cpp</code> application:</p> <pre><code>./holohub run openigtlink_3dslicer --language cpp\n</code></pre> </li> </ol>","tags":["Healthcare AI","Networking and Distributed Computing","Video","Visualization","3D Slicer","Segmentation","Ultrasound","Endoscopy"]},{"location":"applications/orsi/orsi_in_out_body/","title":"In-Out Body Detection and Surgical Video Anonymization","text":"<p>     \u25b6 Run Locally  Authors: Jasper Hofman (Orsi Academy) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 3 - Developmental</p>  Fig. 1: Example of anonymized result after inference  <p></p>","tags":["Healthcare AI","Video","Computer Vision and Perception","Detection","Visualization"]},{"location":"applications/orsi/orsi_in_out_body/#introduction","title":"Introduction","text":"<p>In robotic surgery, anonymization of video is necessary to ensure privacy and protect patient data. During e.g. swapping of robotic tools or lens cleaning the endoscope is removed from the body, possibly capturing faces, recognizable tattoos or written patient data. Therefore, going out of body is a suiting definition for an anonymization boundary.</p>","tags":["Healthcare AI","Video","Computer Vision and Perception","Detection","Visualization"]},{"location":"applications/orsi/orsi_in_out_body/#pipeline","title":"Pipeline","text":"Fig. 2: Schematic overview of Holoscan application  <p>Towards realtime anonymization, a binary out-of-body classifier was trained and deployed using Holoscan platform. Figure 2 shows a schematic overview of the application. After capturing the frame, the alpha channel is dropped by the Format Converter operator. Additionally, the black padding borders added by the robotic system are removed, the tensor is resized to 512x512 pixels and the tensor type is converted from int [0, 255] to float [0, 1]. In the Anonymization Preprocessor operator the tensor pixel values of every collor channel are normalized using the corresponding means and standard deviations of the anonymization dataset. After model inference with the Multi-AI inference operator, the result frame is anonymized in the Orsi Visualizer operator according to the model output. The blurring is applied using a glsl program.</p>","tags":["Healthcare AI","Video","Computer Vision and Perception","Detection","Visualization"]},{"location":"applications/orsi/orsi_in_out_body/#controls","title":"Controls","text":"Action Control Enable anonymization B","tags":["Healthcare AI","Video","Computer Vision and Perception","Detection","Visualization"]},{"location":"applications/orsi/orsi_in_out_body/#build-and-launch-app","title":"Build and Launch app","text":"<p>C++</p> <pre><code>./holohub run orsi_in_out_body --language cpp\n</code></pre> <p>Python</p> <pre><code>./holohub run orsi_in_out_body --language python\n</code></pre>","tags":["Healthcare AI","Video","Computer Vision and Perception","Detection","Visualization"]},{"location":"applications/orsi/orsi_multi_ai_ar/","title":"Multi AI and AR Visualization","text":"<p>     \u25b6 Run Locally  Authors: Jasper Hofman (Orsi Academy) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 3 - Developmental</p>  Fig. 1: Application screenshots   <p></p>","tags":["Healthcare AI","Extended Reality","Segmentation","VTK"]},{"location":"applications/orsi/orsi_multi_ai_ar/#introduction","title":"Introduction","text":"<p>3D models are used in surgery to improve patient outcomes. They provide information on patient specific anatomies that are not visible in the present surgical scene. Especially in robotic surgery these 3D models give great insights because they can be projected and aligned directly onto the endoscopic video. This augmented reality supports navigation for the surgeon in the console. The downside of the 3D model projection is that it occludes the surgical instruments, creating a possible hazardous situation for the patient. This application uses a deep learning segmentation model to identify non-organic items such as tools, clips and wires and projects them on top of the 3D model. This solves the occlusion problem and adds a sense of depth to the AR application. Figure 1 shows in-app screenshots where the 3D model is used to support navigation. The large tumor in green is visible as anatomical landmark.  In addition, the application features an out-of-body detector. Anonymization of video is necessary to ensure privacy and protect patient data. During e.g. swapping of robotic tools or lens cleaning the endoscope is removed from the body, possibly capturing faces, recognizable tattoos or written patient data. Therefore, going out of body is a suiting definition for an anonymization boundary.</p>  Fig. 2: 3D model of kidney tumor case  <p></p> <p>The application was successfully used to remove a kidney tumor during a partial nephrectomy. Figure 2 shows the kidney 3D model where the tumor is colored in yellow.</p>","tags":["Healthcare AI","Extended Reality","Segmentation","VTK"]},{"location":"applications/orsi/orsi_multi_ai_ar/#pipeline","title":"Pipeline","text":"Fig. 3: Schematic overview of Holoscan application  <p>Towards realtime segmentation, AR overlay and anonymization, a binary segmentation model for non-organic items and a binary out-of-body detector was trained and deployed using Holoscan platform. Figure 3 shows a schematic overview of the application. After capturing the frame, the alpha channel is dropped by the Format Converter operator. Additionally, the black padding borders added by the robotic system are removed, the tensor is resized to 512x512 pixels and the tensor type is converted from int [0, 255] to float [0, 1]. In the Segmentation Preprocessor and Anonymization Preprocessor operators the tensor pixel values of every color channel are normalized using the corresponding means and standard deviations of the segmentation and anonymization dataset respectively. After model inference with the Multi-AI inference operator, a sigmoid layer is applied to the model predictions by the Segmentation Postprocessor resulting in a binary segmentation mask. Additionally, the resizing and cropping operations are inverted by first resizing and then adding black padding borders as to not compromise resolution. In the Orsi Visualizer operator the 3D model is rendered using the VTK library and composited onto the endoscopic image using OpenGL. The pixels corresponding to non-organic items are passed through the 3D model layer using a glsl shader program. Finally, the frame is anonymized according to the out-of-body detector output.</p>","tags":["Healthcare AI","Extended Reality","Segmentation","VTK"]},{"location":"applications/orsi/orsi_multi_ai_ar/#controls","title":"Controls","text":"Action Control Enable/Disable anonymization B Enable/Disable manipulations T Load 3D model orientation preset CTRL + L Save current 3D model orientation as preset (will overwrite default preset) CTRL + S Rotate 3D model (3 degrees of freedom) Left Click + Drag Rotate 3D model (1 degree of freedom) CTRL + Left Click + Drag Zoom 3D model Right Click + Drag Translate 3D  model SHIFT + Left Click + Drag Enable/Disable 3D model E Enable/Disable segmentation overlay O Increase opacity 3D model + Decrease opacity 3D model - Toggle arterial tree 1 Toggle venous tree 2 Toggle ureter 4 Toggle parenchyma 5 Toggle tumor 6","tags":["Healthcare AI","Extended Reality","Segmentation","VTK"]},{"location":"applications/orsi/orsi_multi_ai_ar/#build-and-launch-app","title":"Build and Launch app","text":"<pre><code>./holohub run orsi_multi_ai_ar --language cpp\n</code></pre> <p>or</p> <pre><code>./holohub run orsi_multi_ai_ar --language python\n</code></pre>","tags":["Healthcare AI","Extended Reality","Segmentation","VTK"]},{"location":"applications/orsi/orsi_segmentation_ar/","title":"Surgical Tool Segmentation and AR Overlay","text":"<p>     \u25b6 Run Locally  Authors: Jasper Hofman (Orsi Academy) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 3 - Developmental</p>  Fig. 1: Application screenshot   <p></p>","tags":["Healthcare AI","Extended Reality","Surgical AI","VTK","Segmentation","Endoscopy"]},{"location":"applications/orsi/orsi_segmentation_ar/#introduction","title":"Introduction","text":"<p>3D models are used in surgery to improve patient outcomes. They provide information on patient specific anatomies that are not visible in the present surgical scene. Especially in robotic surgery these 3D models give great insights because they can be projected and aligned directly onto the endoscopic video. This augmented reality supports navigation for the surgeon in the console. The downside of the 3D model projection is that it occludes the surgical instruments, creating a possible hazardous situation for the patient. This application uses a deep learning segmentation model to identify non-organic items such as tools, clips and wires and projects them on top of the 3D model. This solves the occlusion problem and adds a sense of depth to the AR application. Figure 1 shows an in-app screenshot where the 3D model is aligned with the anatomy and the tools are segmented on top.</p>  Fig. 2: 3D model of nutcracker case  <p></p> <p>The application was successfully used to verify stent location during a nutcracker syndrome stent removal. Nutcracker syndrome is a rare vein compression disorder where the left renal vein is squeezed between the superior mesenteric artery and abdominal aorta, obstructing blood drainage and even risking blood to flow backwards, causing pain and blood in the urine. Typically, blood flow is restored through endovascular stenting. Although effective, over time the stent had migrated beyond the compression site, causing the symptoms to return and in addition posing an incidental obstruction for blood flow. The stent ofcourse had to be removed. Figure 2 shows a 3D model depicting the current situation.</p>","tags":["Healthcare AI","Extended Reality","Surgical AI","VTK","Segmentation","Endoscopy"]},{"location":"applications/orsi/orsi_segmentation_ar/#pipeline","title":"Pipeline","text":"Fig. 3: Schematic overview of Holoscan application  <p>Towards realtime segmentation and AR overlay, a binary segmentation model for non-organic items was trained and deployed using Holoscan platform. Figure 3 shows a schematic overview of the application. After capturing the frame, the alpha channel is dropped by the Format Converter operator. Additionally, the black padding borders added by the robotic system are removed, the tensor is resized to 512x512 pixels and the tensor type is converted from int [0, 255] to float [0, 1]. In the Segmentation Preprocessor operator the tensor pixel values of every color channel are normalized using the corresponding means and standard deviations of the segmentation dataset. After model inference with the Multi-AI inference operator, a sigmoid layer is applied to the model predictions by the Segmentation Postprocessor resulting in a binary segmentation mask. Additionally, the resizing and cropping operations are inverted by first resizing and then adding black padding borders as to not compromise resolution. In the Orsi Visualizer operator the 3D model is rendered using the VTK library and composited onto the endoscopic image using OpenGL. The pixels corresponding to non-organic items are passed through the 3D model layer using a glsl shader program.</p>","tags":["Healthcare AI","Extended Reality","Surgical AI","VTK","Segmentation","Endoscopy"]},{"location":"applications/orsi/orsi_segmentation_ar/#controls","title":"Controls","text":"Action Control Enable/Disable anonymization B Enable/Disable manipulations T Load 3D model orientation preset CTRL + L Save current 3D model orientation as preset (will overwrite default preset) CTRL + S Rotate 3D model (3 degrees of freedom) Left Click + Drag Rotate 3D model (1 degree of freedom) CTRL + Left Click + Drag Zoom 3D model Right Click + Drag Translate 3D  model SHIFT + Left Click + Drag Enable/Disable 3D model E Enable/Disable segmentation overlay O Increase opacity 3D model + Decrease opacity 3D model - Toggle venous tree 0 Toggle venous stent zone 1 Toggle stent 2","tags":["Healthcare AI","Extended Reality","Surgical AI","VTK","Segmentation","Endoscopy"]},{"location":"applications/orsi/orsi_segmentation_ar/#build-and-launch-app","title":"Build and Launch app","text":"<pre><code>./holohub run orsi_segmentation_ar --language cpp\n</code></pre> <p>or</p> <pre><code>./holohub run orsi_segmentation_ar --language python\n</code></pre>","tags":["Healthcare AI","Extended Reality","Surgical AI","VTK","Segmentation","Endoscopy"]},{"location":"applications/orthorectification_with_optix/","title":"GPU-Accelerated Orthorectification with NVIDIA OptiX","text":"<p>     \u25b6 Run Locally  Authors: Brent Bartlett (NVIDIA) Supported platforms: x86_64 Language: Python Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 4 - Experimental</p> <p>This application is an example of utilizing the nvidia OptiX SDK via the PyOptix bindings to create per-frame orthorectified imagery. In this example, one can create a visualization of mapping frames from a drone mapping mission processed with Open Drone Map. A typical output of a mapping mission is a single merged mosaic. While this product is useful for GIS applications, it is difficult to apply algorithms on a such a large single image without incurring additional steps like image chipping. Additionally, the mosaic process introduces image artifacts which can negativley impact algorithm performance. </p> <p>Since this holoscan pipeline processes each frame individually, it opens the door for one to apply an algorithm to the original un-modififed imagery and then map the result. If custom image processing is desired, it is recommended to insert custom operators before the Ray Trace Ortho operator in the application flow. </p> <p> Fig. 1 Orthorectification sample application workflow</p> <p>Steps for running the application:</p> <p>a) Download and Prep the ODM Dataset 1. Download the Lafayette Square Dataset and place into ~/Data.</p> <ol> <li>Process the dataset with ODM via docker command:  <code>docker run -ti --rm -v ~/Data/lafayette_square:/datasets/code opendronemap/odm --project-path /datasets --camera-lens perspective --dsm</code></li> </ol> <p>If you run out of memory add the following argument to preserve some memory: <code>--feature-quality medium</code></p> <p>b) Clone holohub and navigate to this application directory</p> <p>c) Download OptiX SDK 7.4.0 and extract the package in the same directory as the source code (i.e. applications/orthorectification_with_optix).</p> <p>d) Build development container  1. <code>DOCKER_BUILDKIT=1 docker build -t holohub-ortho-optix:latest .</code></p> <p>You can now run the docker container by:  1. <code>xhost +local:docker</code> 2. <code>nvidia_icd_json=$(find /usr/share /etc -path '*/vulkan/icd.d/nvidia_icd.json' -type f 2&gt;/dev/null | grep .) || (echo \"nvidia_icd.json not found\" &gt;&amp;2 &amp;&amp; false)</code> 3. <code>docker run -it --rm --net host --runtime=nvidia -v ~/Data:/root/Data  -v .:/work/ -v /tmp/.X11-unix:/tmp/.X11-unix  -v $nvidia_icd_json:$nvidia_icd_json:ro  -e NVIDIA_DRIVER_CAPABILITIES=graphics,video,compute,utility,display -e DISPLAY=$DISPLAY  holohub-ortho-optix</code></p> <p>Finish prepping the input data:  1. <code>gdal_translate -tr 0.25 0.25 -r cubic ~/Data/lafayette_square/odm_dem/dsm.tif ~/Data/lafayette_square/odm_dem/dsm_small.tif</code> 2. <code>gdal_fillnodata.py -md 0 ~/Data/lafayette_square/odm_dem/dsm_small.tif ~/Data/lafayette_square/odm_dem/dsm_small_filled.tif</code></p> <p>Finally run the application:  1. <code>python ./python/ortho_with_pyoptix.py</code></p> <p>You can modify the applications settings in the file \"ortho_with_pyoptix.py\" </p> <pre><code>sensor_resize = 0.25 # resizes the raw sensor pixels\nncpu = 8 # how many cores to use to load sensor simulation\ngsd = 0.25 # controls how many pixels are in the rendering\niterations = 425 # how many frames to render from the source images (in this case 425 is max)\nuse_mosaic_bbox = True # render to a static bounds on the ground as defined by the DEM\nwrite_geotiff = False \nnb=3 # how many bands to write to the GeoTiff\nrender_scale = 0.5 # scale the holoview window up or down\nfps = 8.0 # rate limit the simulated sensor feed to this many frames per second\n</code></pre> <p> Fig. 2 Running the orthorectification sample application</p>","tags":["Computer Vision and Perception","Visualization","Optix","Drone","Image Processing"]},{"location":"applications/pipeline_visualization/","title":"Live Streaming Data Web Dashboard with NATS","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: December 1, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 3.8.0 Tested Holoscan SDK versions: 3.8.0 Contribution metric: Level 2 - Trusted</p> <p>This example demonstrates real-time visualization of data from Holoscan applications using NATS messaging and web-based dashboards. It showcases how to stream tensor data from a Holoscan pipeline and visualize it dynamically in a browser.</p> <p></p>","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#overview","title":"Overview","text":"<p>The example consists of three main components:</p> <ul> <li>C++ Data Producer: A Holoscan application that generates data (sine waves) and publishes it to NATS</li> <li>NATS Server: A message broker that handles real-time data streaming</li> <li>Python Web Visualizers: Dash-based web applications that subscribe to NATS streams and display live plots</li> </ul>","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#quick-start","title":"Quick Start","text":"","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#step-1-start-the-nats-server","title":"Step 1: Start the NATS Server","text":"<p>In a terminal, start the NATS server using Docker:</p> <pre><code>cd applications/pipeline_visualization\n./start_nats_server.sh\n</code></pre> <p>This will start a NATS server listening on <code>0.0.0.0:4222</code>.</p>","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#step-2-run-the-visualizer","title":"Step 2: Run the Visualizer","text":"<p>In a second terminal, start the web dashboard visualizer. The dependencies are installed automatically when using the <code>holohub run</code> command inside the Holohub container. You can choose to run the static or dynamic visualizer by specifying the <code>web_static</code> or <code>web_dynamic</code> mode.</p> <p>Start the static web dashboard visualizer:</p> <pre><code>./holohub run pipeline_visualization web_static\n</code></pre> <p>The web interface will be available at: http://localhost:8050</p>","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#step-3-run-the-holoscan-application","title":"Step 3: Run the Holoscan Application","text":"<p>In a third terminal, run the application:</p> <ul> <li> <p>Run the Python version (default when <code>--language</code> is not specified):</p> <pre><code>./holohub run pipeline_visualization\n</code></pre> </li> <li> <p>Or explicitly specify the language:</p> <pre><code>./holohub run pipeline_visualization --language python\n./holohub run pipeline_visualization --language cpp\n</code></pre> </li> </ul> <p>Command-line options for <code>pipeline_visualization</code>:</p> <pre><code>Usage: ./pipeline_visualization [options]\nOptions:\n  -h, --help            Display help information\n  -d, --disable_logger  Disable NATS logger\n  -c, --config          Config file path\n  -u, --nats_url        NATS URL (default: nats://0.0.0.0:4222)\n  -p, --subject_prefix  NATS subject prefix (default: nats_demo)\n  -r, --publish_rate    Publish rate in Hz (default: 2.0)\n</code></pre> <p>Example with custom settings:</p> <pre><code>./holohub run pipeline_visualization --nats_url nats://0.0.0.0:4222 --subject_prefix my_demo --publish_rate 5.0\n</code></pre>","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#step-4-visualize-the-data","title":"Step 4: Visualize the Data","text":"<ol> <li>Open your web browser to http://localhost:8050.</li> <li>Enter the subject name (default: <code>nats_demo</code>).</li> <li>Click Connect.</li> <li>Watch the real-time data plots update.</li> </ol> <p>The visualizer will display:</p> <ul> <li>source.out: Original sine wave from the source operator</li> <li>modulate.in: Input to the modulate operator (same as source.out)</li> <li>modulate.out: Modulated signal with high-frequency component</li> <li>sink.in: Final processed signal (same as modulate.out)</li> </ul>","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#nats-logger-configuration-pipeline_visualizationyaml","title":"NATS Logger Configuration (<code>pipeline_visualization.yaml</code>)","text":"<p>The NATS logger behavior can be configured using YAML:</p> <pre><code>nats_logger:\n  # Filter which operators to log (regex patterns)\n  allowlist_patterns:\n    - \"*\"\n  denylist_patterns:\n    - \"*\"\n  log_inputs: true              # Log operator inputs\n  log_outputs: true             # Log operator outputs\n  log_metadata: true            # Include metadata in messages\n  log_tensor_data_content: true # Include actual tensor data\n</code></pre>","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#architecture","title":"Architecture","text":"<pre><code>%%{init: {'flowchart':{'subGraphTitleMargin':{'top':10, 'bottom':40}}}}%%\nflowchart TB\n    subgraph holoscan[\"Holoscan Application&lt;br/&gt;(C++ Producer)\"]\n        source[Source]\n        modulate[Modulate]\n        sink[Sink]\n        logger[NATS Logger]\n\n        source --&gt;|out| modulate\n        modulate --&gt;|out| sink\n        source -.-&gt;|logs| logger\n        modulate -.-&gt;|logs| logger\n        sink -.-&gt;|logs| logger\n    end\n\n    nats[\"NATS Server&lt;br/&gt;(Port 4222)\"]\n\n    subgraph viz[\"Web Visualizer&lt;br/&gt;(Python/Dash)&lt;br/&gt;http://localhost:8050\"]\n        plots[Data Plots]\n    end\n\n    holoscan --&gt;|NATS Messages&lt;br/&gt;#40;FlatBuffers#41;| nats\n    nats --&gt;|Subscribe| viz</code></pre>","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#visualizer-python-prerequisites","title":"Visualizer Python Prerequisites","text":"<p>All dependencies to run the application are installed automatically when using the <code>holohub run</code> command inside the Holohub container.</p> <p>If you are running the visualizer outside the Holohub container, its dependencies must be installed separately.</p> <p>Install the required Python packages:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>The packages that get installed are:</p> <ul> <li><code>numpy&gt;=1.24.0,&lt;3.0</code> - Numerical computing</li> <li><code>dash&gt;=3.0.0,&lt;4.0</code> - Web application framework</li> <li><code>plotly&gt;=6.0.0,&lt;7.0</code> - Interactive plotting</li> <li><code>nats-py&gt;=2.0.0,&lt;3.0</code> - NATS messaging client</li> <li><code>flatbuffers&gt;=25.9.23,&lt;26.0.0</code> - FlatBuffers</li> <li><code>pandas&gt;=2.3.3,&lt;3.0</code> - Data manipulation</li> </ul>","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#components","title":"Components","text":"","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#application-cpp-and-python","title":"Application (<code>cpp/</code> and <code>python/</code>)","text":"<p>The C++ and Python applications demonstrate a basic Holoscan pipeline with data logging:</p> <ul> <li>SourceOp: Generates sine waves with varying frequencies (10-20 Hz)</li> <li>ModulateOp: Adds high-frequency modulation (300 Hz) to the signal</li> <li>SinkOp: Receives the processed data</li> <li>NatsLogger: A custom data logger that publishes tensor data to NATS using FlatBuffers serialization</li> </ul> <p>The applications log both inputs and outputs of operators, allowing visualization of data at each stage of the pipeline.</p>","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#python-visualizers-visualizer","title":"Python Visualizers (<code>visualizer/</code>)","text":"<p>There are two Python visualizers. One is static and one is dynamic and they both display:</p> <ul> <li>Real-time line plots of tensor data</li> <li>Stream name (operator.port format)</li> <li>IO type (Input/Output)</li> <li>Acquisition timestamp (nanoseconds)</li> <li>Publish timestamp (nanoseconds)</li> </ul> <p>Use <code>start_visualizer.sh</code> to set the required Python path to the flatbuffers definitions and start the visualizer. The script takes one parameter, its values are:</p> <ul> <li><code>dynamic</code>, to start the dynamic visualizer.</li> <li><code>static</code>, to start the static visualizer.</li> </ul>","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#static-visualizer-visualizer_staticpy","title":"Static Visualizer (<code>visualizer_static.py</code>)","text":"<p>The static visualizer can be used when the output data and format of the Holoscan pipeline is known or some data needs special formatting.</p> <ul> <li>Displays predefined data streams: <code>source.out</code>, <code>modulate.in</code>, <code>modulate.out</code>, <code>sink.in</code></li> <li>Best for applications with known, fixed operator topology</li> <li>All graphs are created upfront and updated as data arrives</li> </ul>","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#dynamic-visualizer-visualizer_dynamicpy","title":"Dynamic Visualizer (<code>visualizer_dynamic.py</code>)","text":"<ul> <li>Automatically discovers and creates graphs for new data streams</li> <li>Ideal for applications with dynamic or unknown operator configurations</li> <li>Graphs are created on-the-fly as new unique IDs are detected</li> </ul>","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#flatbuffers-schemas-schemas","title":"FlatBuffers Schemas (<code>schemas/</code>)","text":"<p>The data format is defined using FlatBuffers for efficient serialization:</p> <ul> <li>message.fbs: Top-level message structure with metadata</li> <li>tensor.fbs: Tensor data structure based on DLPack</li> </ul> <p>FlatBuffers access the data directly without unpacking or parsing it and allow the schema to evolve over time while maintaining forward and backward compatibility.</p>","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#nats-message-structure-and-data-format","title":"NATS Message Structure and Data Format","text":"<p>Messages are published to the subject: <code>&lt;subject_prefix&gt;.data</code> (for example, <code>nats_demo.data</code>)</p> <p>Each message is a FlatBuffer-serialized <code>Message</code> containing:</p> <pre><code>Message {\n  unique_id: string          // Format: \"operator_name.port_name\"\n  io_type: IOType            // kInput (0) or kOutput (1)\n  acquisition_timestamp_ns: int64  // When data was acquired\n  timestamp_ns: int64        // When message was published\n  payload: Payload           // Union, currently always Tensor\n}\n</code></pre>","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#unique-id-format","title":"Unique ID Format","text":"<p>The <code>unique_id</code> field follows the format: <code>&lt;operator_name&gt;.&lt;port_name&gt;</code></p> <p>Examples:</p> <ul> <li><code>source.out</code> - Output port of the source operator</li> <li><code>modulate.in</code> - Input port of the modulate operator</li> <li><code>modulate.out</code> - Output port of the modulate operator</li> <li><code>sink.in</code> - Input port of the sink operator</li> </ul>","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#customization","title":"Customization","text":"","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#adding-custom-operators","title":"Adding Custom Operators","text":"<p>To visualize data from your own Holoscan operators:</p> <ol> <li>Add the NATS logger to your application:</li> </ol> <pre><code>auto nats_logger = make_resource&lt;NatsLogger&gt;(\n    \"nats_logger\",\n    Arg(\"nats_url\", \"nats://0.0.0.0:4222\"),\n    Arg(\"subject_prefix\", \"my_app\"));\nadd_data_logger(nats_logger);\n</code></pre> <ol> <li>For static visualizer, update the <code>_unique_ids</code> list:</li> </ol> <pre><code>self._unique_ids = [\"my_op.out\", \"my_other_op.in\"]\n</code></pre> <ol> <li>For dynamic visualizer, no changes needed - it will auto-discover.</li> </ol>","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#customizing-visualizations","title":"Customizing Visualizations","text":"<p>The Plotly graphs can be customized by modifying the <code>px.line()</code> calls in the visualizer code:</p> <pre><code>dcc.Graph(\n    figure=px.line(\n        x=np.arange(len(data)),\n        y=data,\n        labels={\"x\": \"Sample\", \"y\": \"Amplitude\"},\n        title=\"My Custom Title\",\n        # Add more Plotly options here\n    )\n)\n</code></pre>","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#changing-update-rate","title":"Changing Update Rate","text":"<ul> <li>C++ side: Use <code>--publish_rate</code> flag (default: 2 Hz)</li> <li>Visualizer side: Modify the <code>interval</code> parameter in milliseconds:</li> </ul> <pre><code>dcc.Interval(\n    id=\"interval-component\",\n    interval=500,  # Update every 500ms (2 Hz)\n)\n</code></pre>","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Publish Rate: Higher rates (&gt;10 Hz) may cause latency in the web interface</li> <li>Tensor Size: Large tensors (&gt;100K elements) may slow down serialization</li> <li>Number of Streams: The dynamic visualizer handles multiple streams, but too many (&gt;20) may impact browser performance</li> <li>NATS Queue: Messages are queued if the visualizer can't keep up; monitor queue depth</li> </ul>","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#advanced-usage","title":"Advanced Usage","text":"","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#remote-visualization","title":"Remote Visualization","text":"<p>To access the visualizer from another machine:</p> <ol> <li>Start the visualizer with host <code>0.0.0.0</code> (already configured)</li> <li>Ensure port 8050 is accessible through firewall</li> <li>Access using: <code>http://&lt;server-ip&gt;:8050</code></li> </ol>","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#multiple-applications","title":"Multiple Applications","text":"<p>To run multiple Holoscan apps simultaneously:</p> <ol> <li>Use different subject prefixes for each app.</li> <li>Start multiple visualizer instances on different ports:</li> </ol> <pre><code>self._app.run(debug=True, host=\"0.0.0.0\", port=8051)\n</code></pre>","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#troubleshooting","title":"Troubleshooting","text":"","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#nats-connection-issues","title":"NATS Connection Issues","text":"<p>Problem: <code>Cannot connect to NATS</code> error</p> <p>Solution:</p> <ul> <li>Ensure the NATS server is running: <code>docker ps | grep nats</code></li> <li>Check if port 4222 is available: <code>netstat -an | grep 4222</code></li> <li>Verify the NATS URL matches in both the C++ app and visualizer</li> </ul>","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#visualizer-not-updating","title":"Visualizer Not Updating","text":"<p>Problem: Web page loads but graphs don't update</p> <p>Solution:</p> <ol> <li>Check that the C++ application is running</li> <li>Verify the subject name matches (default: <code>nats_demo</code>)</li> <li>Click the Connect button in the web interface</li> <li>Check browser console for JavaScript errors</li> </ol>","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#flatbuffers-import-errors","title":"FlatBuffers Import Errors","text":"<p>Problem: <code>ModuleNotFoundError: No module named 'pipeline_visualization.flatbuffers'</code></p> <p>Solution:</p> <ol> <li>Ensure the FlatBuffers files were generated during build.</li> <li> <p>Set <code>PYTHONPATH</code> correctly:</p> <pre><code>export PYTHONPATH=$PYTHONPATH:/path/to/build/applications/pipeline_visualization/flatbuffers/\n</code></pre> </li> <li> <p>Verify that the files exist in the build directory.</p> </li> </ol>","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/pipeline_visualization/#no-data-displayed","title":"No Data Displayed","text":"<p>Problem: Graphs are empty or show no data</p> <p>Solution:</p> <ul> <li>Verify that <code>log_tensor_data_content: true</code> is in the YAML config</li> <li>Verify that the operator names match between the app and visualizer</li> <li>For static visualizer, ensure that the unique IDs in the code match your operators</li> <li>For dynamic visualizer, wait a few seconds for auto-discovery</li> </ul>","tags":["Visualization","NATS","Streaming","Real-time","Tensor","Signal Processing"]},{"location":"applications/polyp_detection/","title":"Polyp Detection","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 3.3.0 Tested Holoscan SDK versions: 3.3.0 Contribution metric: Level 2 - Trusted</p> <p>This application demonstrates how to run polyp detection models on live video in real-time.</p> <p></p> <p>The model: RT-DETR v2 is trained on the REAL-Colon dataset.</p> <p>Compared to the <code>SSD</code> object detection model described in the paper, <code>RT-DETR</code> demonstrates improvements. The table below shows metrics for SSD obtained from Table 3 of the paper, and metrics for RT-DETR calculated on the same test set (using all test images from the <code>REAL-Colon</code> dataset).</p> Method MAP@0.5 MAP@0.5:0.95 SSD 0.338 0.216 RT-DETR 0.452 0.301","tags":["Healthcare AI","Colonoscopy","Detection","RT-DETR","bounding box"]},{"location":"applications/polyp_detection/#run-instructions","title":"Run Instructions","text":"<p>For simplicity a DockerFile is available. To run this application:</p> <pre><code>./holohub run polyp_detection\n</code></pre> <p>The (NGC) Sample App Model for AI Polyp Detection and the (NGC) Sample App Data are automatically downloaded and converted to the correct format when first run the application.</p> <p>If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p> <p>If you want to run the <code>polyp_detection.py</code> script directly with the built docker container, please refer to the following instructions:</p> <ul> <li>Ensure that the <code>rtdetrv2_timm_r50_nvimagenet_pretrained_neg_finetune_bhwc.onnx</code> file is located in the directory specified by the <code>data</code> argument.</li> <li>Verify that the generated video files (<code>.gxf_index</code> and <code>.gxf_entities</code> files) are in the directory specified by the <code>video_dir</code> argument.</li> <li>Specify the correct video width and height using the <code>video_size</code> argument.</li> </ul> <p>For example: <pre><code>python polyp_detection.py --data /path-to-onnx-model/ --video_dir /path-to-video/ --video_size \"(width, height)\"\n</code></pre></p>","tags":["Healthcare AI","Colonoscopy","Detection","RT-DETR","bounding box"]},{"location":"applications/prohawk_video_replayer/","title":"ProHawk Video Replayer","text":"<p>     \u25b6 Run Locally  Authors: Tim Wooldridge (Prohawk Technology Group) Supported platforms: aarch64 Language: Python, C++ Last modified: October 29, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 0.5.1 Tested Holoscan SDK versions: 0.5.1, 0.6.0 Contribution metric: Level 4 - Experimental</p> <p>This application utilizes the ProHawk restoration operator along with Holoscan's Video Replayer and Holoviz operators to enhance and restore medical imagery in real-time, offering superior image quality. The user-friendly interface of the application provides a range of filter options, enabling users to dynamically select the most suitable filter for optimal results.</p> <p></p>","tags":["Healthcare AI","Video","filter presets","Endoscopy","Holoviz"]},{"location":"applications/prohawk_video_replayer/#prohawk-vision-restoration-operator","title":"ProHawk Vision Restoration Operator","text":"<p>The ProHawk Vision Operator is a groundbreaking solution that is transforming both healthcare and manufacturing industries by revolutionizing computer vision technology with its patented restoration capabilities. It seamlessly integrates into the NVIDIA Holoscan full-stack infrastructure, fundamentally altering the way healthcare professionals diagnose and treat patients, while also optimizing manufacturing processes. In healthcare, the ProHawk Vision Operator Plugin automatically interprets medical imaging frames, identifies real-world conditions, and employs precise control over ProHawk Vision Restoration algorithms, all driven by an objective mathematical model using quantitative measurements to enhance accuracy in diagnoses and treatments by restoring degraded frames. In manufacturing, the ProHawk Vision Operator Plugin algorithms reveal manufacturing line defects ensuring product quality.</p>","tags":["Healthcare AI","Video","filter presets","Endoscopy","Holoviz"]},{"location":"applications/prohawk_video_replayer/#application-controls","title":"Application Controls","text":"<p>The operator can be controlled with keyboard shortcuts:</p> <ul> <li>AFS (0) - Automatic Filter Selection (AFS) \u2013 Automatically and continuously adjust the image pixel values to maximize image detail and visibility.</li> <li>LowLight (1) - Lowlight preset filter that corrects lighting compromised imagery.</li> <li>Vascular Detail (2) - Vasculature Detail Preset Filter that reveals the fine details of vasculature structures without the need for dye or contrast agents.</li> <li>Vapor (3) - Vapor Preset Filter that removes vapor, smoke, and stream from the video.</li> <li>Disable Restoration (d) - Disable ProHawk Vision computer vision restoration.</li> <li>Side-by-Side View (v) - Display Side-by-Side (restored/non-restores) Video.</li> <li>Display Menu Items (m) - Display menus control items.</li> <li>Quit (q) - Exit the application</li> </ul>","tags":["Healthcare AI","Video","filter presets","Endoscopy","Holoviz"]},{"location":"applications/prohawk_video_replayer/#data","title":"Data","text":"<p>The following dataset is used by this application: \ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking.</p>","tags":["Healthcare AI","Video","filter presets","Endoscopy","Holoviz"]},{"location":"applications/prohawk_video_replayer/#quick-start","title":"Quick Start","text":"<p>To build this application within a container and run it, please use the following command:</p> <pre><code>./holohub run prohawk_video_replayer\n</code></pre> <p>For a separate build and run, please see the following instructions:</p>","tags":["Healthcare AI","Video","filter presets","Endoscopy","Holoviz"]},{"location":"applications/prohawk_video_replayer/#step-by-step-build-and-run","title":"Step by step build and run","text":"<p>From the Holohub main directory run the following command:</p> <pre><code>./holohub build-container prohawk_video_replayer\n</code></pre> <p>Then launch the container to build the application:</p> <pre><code>./holohub run-container prohawk_video_replayer --no-docker-build\n</code></pre> <p>Inside the container build the application:</p> <pre><code>./holohub build prohawk_video_replayer\n</code></pre> <p>Inside the container run the application:</p> <ul> <li>C++:     <pre><code>./holohub run prohawk_video_replayer --language=cpp --no-local-build\n</code></pre></li> <li>Python:     <pre><code>export PYTHONPATH=$PYTHONPATH:/workspace/holohub/build/python/lib/\npython &lt;prohawk_app_dir&gt;/python/prohawk_video_replayer.py\n</code></pre></li> </ul> <p>For more information about this application and operator please visit https://prohawk.ai/prohawk-vision-operator/#learn For technical support or other assistance, please don't hesitate to visit us at https://prohawk.ai/contact</p>","tags":["Healthcare AI","Video","filter presets","Endoscopy","Holoviz"]},{"location":"applications/psd_pipeline/","title":"VITA 49 Power Spectral Density (latest)","text":"","tags":["Signal Processing","Networking and Distributed Computing","PSD","GPUDirect","UDP"]},{"location":"applications/psd_pipeline/#vita-49-power-spectral-density-psd","title":"VITA 49 Power Spectral Density (PSD)","text":"<p>     \u25b6 Run Locally  Authors: John Moon john.moon@vts-i.com (Valley Tech Systems, Inc.), Eric Ferrara eric.ferrara@vts-i.com (Valley Tech Systems, Inc.), Matthew Luckenbihl matthew.luckenbihl@vts-i.com (Valley Tech Systems, Inc.) Supported platforms: x86_64 Language: C++ Last modified: October 9, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.5.0 Tested Holoscan SDK versions: 2.5.0, 2.6.0, 2.7.0, 2.8.0, 2.9.0, 3.0.0, 3.1.0 Contribution metric: Level 3 - Developmental</p>","tags":["Signal Processing","Networking and Distributed Computing","PSD","GPUDirect","UDP"]},{"location":"applications/psd_pipeline/#overview","title":"Overview","text":"<p>The VITA 49 Power Spectral Density (PSD) application takes in a VITA49 data stream from the advanced network operator, then performs an FFT, PSD, and averaging operation before generating a VITA 49.2 spectral data packet which gets sent to a destination UDP socket.</p> <p></p>","tags":["Signal Processing","Networking and Distributed Computing","PSD","GPUDirect","UDP"]},{"location":"applications/psd_pipeline/#acronyms","title":"Acronyms","text":"Acronym Meaning FFT Fast Fourier Transform NIC Network Interface Card PSD Power Spectral Display VITA 49 Standard for interoperability between RF (radio frequency) devices VRT VITA Radio Transport (transport-layer protocol)","tags":["Signal Processing","Networking and Distributed Computing","PSD","GPUDirect","UDP"]},{"location":"applications/psd_pipeline/#requirements","title":"Requirements","text":"<ul> <li>ConnectX 6 or 7 NIC for GPUDirect RDMA with packet size steering</li> <li>MatX (dependency - assumed to be installed on system)</li> <li>vita49-rs (dependency)</li> </ul>","tags":["Signal Processing","Networking and Distributed Computing","PSD","GPUDirect","UDP"]},{"location":"applications/psd_pipeline/#configuration","title":"Configuration","text":"<p>[!IMPORTANT] The settings in <code>config.yaml</code> need to be tailored to your system/radio.</p> <p>Each operator in the pipeline has its own configuration section. The specific options and their meaning are defined in each operator's own README:</p> <ol> <li><code>advanced_network</code></li> <li><code>vita_connector</code></li> <li><code>fft</code></li> <li><code>high_rate_psd</code></li> <li><code>low_rate_psd</code></li> <li><code>vita49_psd_packetizer</code></li> </ol> <p>There is also one option specific to this application:</p> <ol> <li><code>num_psds</code>: Number of PSDs to produce out of the pipeline before exiting.                Passing <code>-1</code> here will cause the pipeline to run indefinitely.</li> </ol>","tags":["Signal Processing","Networking and Distributed Computing","PSD","GPUDirect","UDP"]},{"location":"applications/psd_pipeline/#metadata","title":"Metadata","text":"<p>This pipeline leverages Holoscan's operator metadata dictionaries to pass VITA 49-adjacent metadata through the pipeline.</p> <p>Each operator in the pipeline adds its own metadata to the dictionary. At the end of the pipeline, the packetizer operator uses the metadata to construct VITA 49 context packets to send alongside the spectral data.</p>","tags":["Signal Processing","Networking and Distributed Computing","PSD","GPUDirect","UDP"]},{"location":"applications/psd_pipeline/#memory-layout","title":"Memory Layout","text":"<p>The ANO operates using memory regions that it directs data to. Since VITA49 is somewhat unusual in that signal data packets and context packets arrive at the same IP/port, we want to use the ANO's packet length steering feature to drop packets in the appropriate memory region.</p> <p>First, we want to define our memory regions:</p> <ol> <li>A region for any packets that don't match any of our flows [CPU]</li> <li>A region for frame headers (i.e. Ethernet + IP + UDP) [CPU]</li> <li>These headers are not currently used, so this memory region is      essentially acting as a <code>/dev/null</code> sink.</li> <li>A region for each channel's VRT headers [CPU]</li> <li>We need these headers to grab things like stream ID and      timestamp, but don't need that information in the GPU processing,      so make this a CPU region.</li> <li>A region for each channel's VRT signal data [GPU]</li> <li>These are the raw IQ samples from our radio - we want these to      land in GPU memory via GPUDirect RDMA.</li> <li>A region for all channels' VRT context data [CPU]</li> <li>We need the whole packet in the CPU to fill out our metadata      map for downstream processing/packet assembly.</li> </ol> <p>When an individual packet comes in, the ANO will try to match it against the defined flows. So, for our data packets, we want to define a queue like this:</p> <pre><code>            flows:\n              - name: \"Data packets\"\n                id: 0\n                action:\n                  type: queue\n                  id: 1\n                match:\n                  # Match with the port your SDR is sending to and the\n                  # length of the signal data packets\n                  udp_src: 4991\n                  udp_dst: 4991\n                  ipv4_len: 4148\n</code></pre> <p>This is saying \"if a UDP packet with IPv4 length 4,148 comes in on port 4991, send it to the queue with ID 1\". Now, if we look at our queue with ID 1, we see:</p> <pre><code>              - name: \"Data\"\n                id: 1\n                cpu_core: 5\n                batch_size: 12500\n                memory_regions:\n                  - \"Headers_RX_CPU\"\n                  - \"VRT_Headers_RX_CPU\"\n                  - \"Data_RX_GPU\"\n</code></pre> <p>When multiple <code>memory_regions</code> are specified like this, it means that each packet should be split based on the memory region size. In this case, <code>Headers_RX_CPU</code> has <code>buf_size: 42</code> (the size of the frame header), <code>VRT_Headers_RX_CPU</code> has <code>buf_size: 20</code> (the size of the VRT header), and <code>Data_RX_GPU</code> has <code>buf_size: 4100</code> (the remaining size of the data packet). These numbers may be different depending on the packet size of your radio!</p> <p><code>batch_size: 12500</code> tells the ANO to batch up 12,500 packets before sending the data to downstream operators. In our case, 12,500 packets represents 100ms worth of data and that's how much we want to process on each run of the pipeline.</p>","tags":["Signal Processing","Networking and Distributed Computing","PSD","GPUDirect","UDP"]},{"location":"applications/psd_pipeline/#multiple-channels","title":"Multiple Channels","text":"<p>When working with multiple channels, this pipeline expects all context packets (from every channel) to flow to one queue, but each data channel flows to its own queue.</p> <p>The connector operator also makes the following assumptions:</p> <ol> <li>All context packets flow to queue <code>id: 0</code>.</li> <li>All context packets flow ID matches its channel (e.g., flow ID <code>1</code>    is for context packets from channel <code>1</code>).</li> <li>All data packets arrive on a queue ID one greater than its channel    (e.g., queue ID <code>1</code> is for channel <code>0</code> data).</li> <li>The <code>batch_size</code> of the context queue is equal to the number of    channels.</li> </ol>","tags":["Signal Processing","Networking and Distributed Computing","PSD","GPUDirect","UDP"]},{"location":"applications/psd_pipeline/#ingest-nic","title":"Ingest NIC","text":"<p>The PCIe address of your ingest NIC needs to be specified in <code>config.yaml</code>.</p> <pre><code>    interfaces:\n      - name: sdr_data\n        address: 0000:17:00.0\n</code></pre> <p>You can find the addresses of your devices with: <code>lshw -c network -businfo</code>:</p> <pre><code># lshw -c network -businfo\nBus info          Device     Class          Description\n=======================================================\npci@0000:03:00.0  eth0       network        I210 Gigabit Network Connection\npci@0000:06:00.0  eno1       network        Aquantia Corp.\npci@0000:51:00.0  ens3f0np0  network        MT2910 Family [ConnectX-7]\npci@0000:51:00.1  ens3f1np1  network        MT2910 Family [ConnectX-7]\nusb@1:14.2        usb0       network        Ethernet interface\n</code></pre> <p>In this example, if you wanted to use the <code>ens3f1np1</code> interface, you'd pass <code>0000:51:00.1</code>.</p>","tags":["Signal Processing","Networking and Distributed Computing","PSD","GPUDirect","UDP"]},{"location":"applications/psd_pipeline/#build-run","title":"Build &amp; Run","text":"<ol> <li>Build the development container in two steps:    <pre><code># Build the ANO dev container\n./holohub build-container advanced_network --docker-file ./operators/advanced_network/Dockerfile\n\n# Add the psd-pipeline deps\n./holohub build-container psd_pipeline --base-img holohub:ngc-v3.1.0-dgpu --img holohub-psd-pipeline:ngc-v3.1.0-dgpu\n</code></pre></li> <li>Launch the development container with the command:    <pre><code>./holohub run-container psd_pipeline --no-docker-build --docker-opts=\"-u root --privileged\" --img holohub-psd-pipeline:ngc-v3.1.0-dgpu\n</code></pre></li> </ol> <p>Once you are in the dev container: 1. Build the application using:     <pre><code>./holohub build psd_pipeline\n</code></pre> 2. Run the application using:     <pre><code>./holohub run psd_pipeline --local --no-local-build --run-args=\"config.yaml\"\n</code></pre></p>","tags":["Signal Processing","Networking and Distributed Computing","PSD","GPUDirect","UDP"]},{"location":"applications/psd_pipeline/data_writer/","title":"data_writer (latest)","text":"","tags":["Signal Processing"]},{"location":"applications/psd_pipeline/data_writer/#data-writer-operator","title":"Data Writer Operator","text":"<p>     \u25b6 Run Locally  Authors: John Moon john.moon@vts-i.com (Valley Tech Systems, Inc.), Eric Ferrara eric.ferrara@vts-i.com (Valley Tech Systems, Inc.), Matthew Luckenbihl matthew.luckenbihl@vts-i.com (Valley Tech Systems, Inc.) Supported platforms: x86_64 Language: C++ Last modified: August 5, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.5.0 Tested Holoscan SDK versions: 2.5.0, 2.6.0, 2.7.0, 2.8.0, 2.9.0, 3.0.0, 3.1.0 Contribution metric: Level 4 - Experimental</p>","tags":["Signal Processing"]},{"location":"applications/psd_pipeline/data_writer/#overview","title":"Overview","text":"<p>Writes binary data from its input to an output file. This operator is intened for use as a debugging aid.</p>","tags":["Signal Processing"]},{"location":"applications/psd_pipeline/data_writer/#description","title":"Description","text":"<p>The data writer operator takes in a <code>std::tuple&lt;tensor_t&lt;complex, 2&gt;, cuda_stream_t&gt;</code>, copies the data to a host tensor, then writes the data out to a binary file.</p> <p>The file path is determined based on input metadata with the following keys:</p> <ol> <li><code>channel_number</code> (default <code>0</code>)</li> <li><code>bandwidth_hz</code> (default <code>0.0</code>)</li> <li><code>rf_ref_freq_hz</code> (default <code>0.0</code>)</li> </ol> <p>With this, it creates: <code>data_writer_out_ch{channel_number}_bw{bandwidth_hz}_freq{rf_ref_freq_hz}.dat</code>.</p>","tags":["Signal Processing"]},{"location":"applications/psd_pipeline/data_writer/#requirements","title":"Requirements","text":"<ul> <li>MatX (dependency - assumed to be installed on system)</li> </ul>","tags":["Signal Processing"]},{"location":"applications/psd_pipeline/data_writer/#configuration","title":"Configuration","text":"<p>The data writer operator takes in a few parameters:</p> <pre><code>data_writer:\n  burst_size: 1280\n  num_bursts: 625\n</code></pre> <ul> <li><code>burst_size</code>: Number of samples contained in each burst</li> <li><code>num_bursts</code>: Number of bursts to process at once</li> </ul>","tags":["Signal Processing"]},{"location":"applications/psd_pipeline/data_writer/#example-usage","title":"Example Usage","text":"<p>For an example of how to use this operator, see the <code>psd_pipeline</code> application.</p> <p>Usually, you'd just want to write one burst of data to a file. To do that, you could use a <code>CountCondition</code> to limit the number of times this operator runs:</p> <pre><code>auto dataWriterOp = make_operator&lt;ops::DataWriter&gt;(\n    \"dataWriterOp\",\n    make_condition&lt;CountCondition&gt;(1));\n</code></pre>","tags":["Signal Processing"]},{"location":"applications/pva_video_filter/","title":"PVA-Accelerated Image Sharpening","text":"<p>     \u25b6 Run Locally  Authors: Soham Sinha (NVIDIA), Mehmet Umut Demircin (NVIDIA), Wendell Hom (NVIDIA) Supported platforms: aarch64 Language: C++ Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates the usage of Programmable Vision Accelerator (PVA) within a Holoscan application. It reads a video stream, applies a 2D unsharp mask filter and renders it via the visualizer. The unsharp mask filtering operation is done in PVA. Since the PVA is used for this operation, the GPU workload is minimized. This example is a demonstration of how pre-processing, post-processing, and image processing tasks can be offloaded from a GPU, allowing it to concentrate on more compute-intensive machine learning and artificial intelligence tasks.</p> <p>This example application processes a video stream, displaying two visualizer windows: one for the original stream and another for the stream enhanced with image sharpening via PVA.</p>","tags":["Computer Vision and Perception","Video","Optimization","PVA","Endoscopy","Visualization"]},{"location":"applications/pva_video_filter/#about-pva","title":"About PVA","text":"<p>PVA is a highly power-efficient VLIW processor integrated into NVIDIA Tegra platforms, specifically designed for advanced image processing and computer vision algorithms. The Compute Unified Programmable Vision Accelerator (CUPVA) SDK offers a comprehensive and unified programming model for PVA, enabling developers to create and optimize their own algorithms. For access to the SDK and further development opportunities, please contact NVIDIA.</p>","tags":["Computer Vision and Perception","Video","Optimization","PVA","Endoscopy","Visualization"]},{"location":"applications/pva_video_filter/#content","title":"Content","text":"<ul> <li><code>main.cpp</code>: This file contains a C++ Holoscan application that demonstrates the use of an operator for loading and executing a pre-compiled PVA library dedicated to performing the unsharp masking algorithm on images. CUPVA SDK and license are not required to run this Holohub application.</li> <li><code>pva_unsharp_mask/</code>: This directory houses the <code>pva_unsharp_mask.hpp</code> header file, which declares the <code>PvaUnsharpMask</code> class. The <code>PvaUnsharpMask</code> class includes an <code>init</code> API, invoked for the initial tensor, and a <code>process</code> API, used for processing input tensors. Pre-compiled algorithm library file, <code>libpva_unsharp_mask.a</code>, and the corresponding allowlist file, <code>cupva_allowlist_pva_unsharp_mask</code>, are automatically downloaded by the CMake scripts. Please note that only PVA executables with signatures included in a secure allowlist database are permitted to execute on the PVA. This ensures that only verified and trusted executables are run, enhancing the security and integrity of the system.</li> </ul>","tags":["Computer Vision and Perception","Video","Optimization","PVA","Endoscopy","Visualization"]},{"location":"applications/pva_video_filter/#algorithm-overview","title":"Algorithm Overview","text":"<p>The PVAVideoFilterExecutor operator performs an image sharpening operation in three steps:</p> <ol> <li>Convert the input RGB image to the NV24 color format.</li> <li>Apply a 5x5 unsharp mask filter on the luminance color plane.</li> <li>Convert the enhanced image back to the RGB format.</li> </ol> <p>Numerous algorithm examples leveraging the PVA can be found in the Vision Programming Interface (VPI) library. VPI enables computer vision software developers to utilize multiple compute engines simultaneously\u2014including CPU, GPU, PVA, VIC, NVENC, and OFA\u2014through a unified interface. For comprehensive details, please refer to the VPI Documentation.</p>","tags":["Computer Vision and Perception","Video","Optimization","PVA","Endoscopy","Visualization"]},{"location":"applications/pva_video_filter/#compiling-the-application","title":"Compiling the application","text":"<p>Build the application inside docker</p> <pre><code>$ ./holohub build-container pva_video_filter --base-img nvcr.io/nvidia/clara-holoscan/holoscan:v2.1.0-dgpu \n# Check which version of CUPVA is installed on your platform at /opt/nvidia\n$ ./holohub run-container pva_video_filter --no-docker-build --docker_opts \"-v /opt/nvidia/cupva-&lt;version&gt;:/opt/nvidia/cupva-&lt;version&gt; --device /dev/nvhost-ctrl-pva0:/dev/nvhost-ctrl-pva0 --device /dev/nvmap:/dev/nvmap --device /dev/dri/renderD129:/dev/dri/renderD129\"\n</code></pre> <p>Inside docker, add to your environment variable the following directories: <pre><code># inside docker\n$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/aarch64-linux-gnu/tegra/:/opt/nvidia/cupva-2.5/lib/aarch64-linux-gnu/\n</code></pre></p> <p>Build the application inside docker: <pre><code>$ ./holohub build pva_video_filter --local\n</code></pre></p>","tags":["Computer Vision and Perception","Video","Optimization","PVA","Endoscopy","Visualization"]},{"location":"applications/pva_video_filter/#running-the-application","title":"Running the application","text":"<p>The application takes an endoscopy video stream as input, applies the unsharp mask filter, and shows it in HoloViz window.</p> <p>Before running the application, deploy VPU application signature allowlist on target in your host (outside a container): <pre><code>sudo cp &lt;HOLOHUB_BUILD_DIR&gt;/applications/pva_video_filter/cpp/pva_unsharp_mask/cupva_allowlist_pva_unsharp_mask /etc/pva/allow.d/cupva_allowlist_pva_unsharp_mask\nsudo pva_allow\n</code></pre></p> <p>Run the same docker container you used to build your application</p> <pre><code>$ ./holohub run-container pva_video_filter --no-docker-build --docker_opts \"-v /opt/nvidia/cupva-&lt;version&gt;:/opt/nvidia/cupva-&lt;version&gt; --device /dev/nvhost-ctrl-pva0:/dev/nvhost-ctrl-pva0 --device /dev/nvmap:/dev/nvmap --device /dev/dri/renderD129:/dev/dri/renderD129\"\n\n# inside docker\n# don't forget the line below to export the environment variables\n$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/aarch64-linux-gnu/tegra/:/opt/nvidia/cupva-2.5/lib/aarch64-linux-gnu/\n$ ./holohub run pva_video_filter --local --no-local-build\n</code></pre> <p></p>","tags":["Computer Vision and Perception","Video","Optimization","PVA","Endoscopy","Visualization"]},{"location":"applications/qt_video_replayer/","title":"Qt Video Replayer","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: August 5, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p></p> <p>This application demonstrates how to integrate Holoscan with a Qt application. It support displaying the video frames output by a Holoscan operator and changing operator properties using Qt UI elements.</p> <pre><code>flowchart LR\n    subgraph Holoscan application\n        A[(VideoFile)] --&gt; VideostreamReplayerOp\n        VideostreamReplayerOp --&gt; FormatConverterOp\n        FormatConverterOp --&gt; NppFilterOp\n        NppFilterOp --&gt; QtVideoOp\n    end\n    subgraph Qt Window\n        QtVideoOp &lt;-.-&gt; QtHoloscanVideo\n    end</code></pre> <p>The application uses the VideostreamReplayerOp to read from a file on disk, the FormatConverterOp to convert the frames from RGB to RGBA, the NppFilterOp to apply a filter to the frame and the QtVideoOp operator to display the video stream in a Qt window.</p> <p>The QtHoloscanApp class, which extends the <code>holoscan::Application</code> class, is used to expose parameters of Holoscan operators as Qt properties.</p> <p>For example the application uses a QML Checkbox is used the set the <code>realtime</code> property of the <code>VideostreamReplayerOp</code> operator.</p> <pre><code>    CheckBox {\n        id: realtime\n        text: \"Use Video Framerate\"\n        checked: holoscanApp.realtime\n        onCheckedChanged: {\n            holoscanApp.realtime = checked;\n        }\n    }\n</code></pre> <p>The QtHoloscanVideo is a QQuickItem which can be use in the QML file. Multiple <code>QtHoloscanVideo</code> items can be placed in a Qt window.</p> <pre><code>import QtHoloscanVideo\nItem {\n    QtHoloscanVideo {\n        objectName: \"video\"\n    }\n}\n</code></pre>","tags":["Computer Vision and Perception","UI","Qt","Video","Visualization"]},{"location":"applications/qt_video_replayer/#run-instructions","title":"Run Instructions","text":"<p>Note: This application container is pinned to Holoscan SDK 3.2 since it requires CUDA Driver 550+ for running with Holoscan SDK 3.3+.</p> <p>This application requires Qt.</p> <p>For simplicity a DockerFile is available. To run this application:</p> <pre><code>./holohub run qt_video_replayer\n</code></pre>","tags":["Computer Vision and Perception","UI","Qt","Video","Visualization"]},{"location":"applications/realsense_visualizer/","title":"Intel RealSense Camera Visualizer","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: October 28, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 0 - Core Stable</p> <p>Visualizes frames captured from an Intel RealSense camera. </p>","tags":["Computer Vision and Perception","Visualization","Camera","Depth","Holoviz"]},{"location":"applications/realsense_visualizer/#build-and-run","title":"Build and Run","text":"<p>This application requires an Intel RealSense camera.</p> <p>At the top level of the holohub run the following command:</p> <pre><code>./holohub run realsense_visualizer\n</code></pre>","tags":["Computer Vision and Perception","Visualization","Camera","Depth","Holoviz"]},{"location":"applications/sam2/","title":"SAM 2: Segment Anything in Images and Videos","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates how to run SAM2 models on live video feed with the possibility of changing query points in real-time.</p> <p> </p> <p>The application currently uses a single query point as a foreground point that moves on the perimeter of a circle with a configured angular speed. The models returns three masks, the best mask is selected based on the model scores. For visualization, two options exist. Select between \"logits\" or \"masks\". - \"logits\": predictions of the network, mapped onto a colorscale that matches matplotlib.pyplot's \"viridis\" - \"masks\": binarized predictions</p> <p>SAM2, recently announced by Meta, is the next iteration of the Segment Anything Model (SAM). This new version expands upon its predecessor by adding the capability to segment both videos and images. This sample application wraps the ImageInference class, and applies it on a live video feed.</p> <p>Note: This demo currently uses \"sam2_hiera_l.yaml\", but any of the sam2 models work. You only need to adjust segment_one_thing.yaml.</p>","tags":["Computer Vision and Perception","Video","Segmentation","Visualization","Camera"]},{"location":"applications/sam2/#setup-instructions","title":"\u2699\ufe0f Setup Instructions","text":"<p>The app defaults to using the video device at <code>/dev/video0</code></p> <p>To debug if this is the correct device download <code>v4l2-ctl</code>: <pre><code>sudo apt-get install v4l-utils\n</code></pre> To check for your devices run: <pre><code>v4l2-ctl --list-devices\n</code></pre> This command will output something similar to this: <pre><code>NVIDIA Tegra Video Input Device (platform:tegra-camrtc-ca):\n        /dev/media0\n\nvi-output, lt6911uxc 2-0056 (platform:tegra-capture-vi:0):\n        /dev/video0\n\nDummy video device (0x0000) (platform:v4l2loopback-000):\n        /dev/video3\n</code></pre> Determine your desired video device and edit the source device in segment_one_thing.yaml</p>","tags":["Computer Vision and Perception","Video","Segmentation","Visualization","Camera"]},{"location":"applications/sam2/#build-and-run-instructions","title":"\ud83d\ude80 Build and Run Instructions","text":"","tags":["Computer Vision and Perception","Video","Segmentation","Visualization","Camera"]},{"location":"applications/sam2/#arm64-and-x86","title":"ARM64 and x86","text":"<p>OBS: If you are building on a Clara AGX Dev Kit, replace the <code>Dockerfile</code> below with <code>./alternative_docker/Dockerfile_cagx</code>.</p> <p>This application uses a custom Dockerfile based on a pytorch container. Build and run the application using <pre><code> ./holohub run sam2\n</code></pre> Or first build the container, then launch it and run.</p> <p><pre><code> ./holohub build-container sam2\n</code></pre> <pre><code>./holohub run-container sam2 --no-docker-build\n</code></pre> <pre><code>./holohub run sam2 --local --no-local-build\n</code></pre></p>","tags":["Computer Vision and Perception","Video","Segmentation","Visualization","Camera"]},{"location":"applications/sam2/#x86-only","title":"x86 only","text":"<p>If you are only using an x86 system, you may use a Dockerfile based on the Holoscan container. Replace the Dockerfile with this alternative Dockerfile. Then, from the Holohub main directory run the following command: <pre><code>./holohub run sam2\n</code></pre></p> <p>Alternatively build and run: <pre><code>./holohub vscode sam2\n</code></pre> Run the application in debug mode from vscode, or execute it by <pre><code>python applications/sam2/segment_one_thing.py\n</code></pre></p> <p>You can choose to output \"logits\" or \"masks\" in the configuration of the postprocessor and holoviz operator segment_one_thing.yaml</p>","tags":["Computer Vision and Perception","Video","Segmentation","Visualization","Camera"]},{"location":"applications/sam2/#supported-hardware","title":"\ud83d\udcbb Supported Hardware","text":"<ul> <li>x86 w/ dGPU</li> <li>IGX Dev Kit w/ dGPU</li> <li>Clara AGX Dev Kit w/ dGPU</li> </ul>","tags":["Computer Vision and Perception","Video","Segmentation","Visualization","Camera"]},{"location":"applications/sam2/#acknowledgements","title":"\ud83d\ude4c Acknowledgements","text":"<ul> <li>Meta, SAM2: for providing these models and inference infrastructure</li> </ul>","tags":["Computer Vision and Perception","Video","Segmentation","Visualization","Camera"]},{"location":"applications/sdr_fm_demodulation/","title":"Software Defined Radio FM Demodulation","text":"<p>     \u25b6 Run Locally  Authors: Adam Thompson (NVIDIA) Supported platforms: x86_64 Language: Python Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.4.0 Tested Holoscan SDK versions: 0.4.0 Contribution metric: Level 2 - Trusted</p> <p>As the \"Hello World\" application of software defined radio developers, this demonstration highlights real-time FM demodulation, resampling, and playback on GPU with NVIDIA's Holoscan SDK. In this example, we are using an inexpensive USB-based RTL-SDR dongle to feed complex valued Radio Frequency (RF) samples into GPU memory and use cuSignal functions to perform the relevant signal processing. The main objectives of this demonstration are to: - Highlight developer productivity in building an end-to-end streaming application with Holoscan and existing GPU-Accelerated Python libraries - Demonstrate how to construct and connect isolated units of work via Holoscan operators - Emphasize that operators created for this application can be reused in other ones doing similar tasks</p>","tags":["Signal Processing","Audio"]},{"location":"applications/sdr_fm_demodulation/#running-the-application","title":"Running the Application","text":"<p>Prior to running the application, the user needs to install the necessary dependencies (and, of course, plug in a SDR into your computer). This is most easily done in an Anaconda environment.</p> <pre><code>conda create --name holoscan-sdr-demo python=3.8\nconda activate holoscan-sdr-demo\nconda install -c conda-forge -c rapidsai -c nvidia cusignal soapysdr soapysdr-module-rtlsdr pyaudio\npip install holoscan\n</code></pre> <p>The FM demodulation example can then be run via <pre><code>python applications/sdr_fm_demodulation/sdr_fm_demodulation.py\n</code></pre></p>","tags":["Signal Processing","Audio"]},{"location":"applications/simple_pdw_pipeline/","title":"Basic Pulse Description Word (PDW) Generator","text":"<p>     \u25b6 Run Locally  Authors: Joshua Anderson (GTRI), Christopher Jones (GTRI) Supported platforms: x86_64, aarch64 Language: C++ Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 4 - Experimental</p> <p>This is a Holoscan pipeline that shows the possibility of using Holoscan as a Pulse Description Word (PDW) generator. This is a process that takes in IQ samples (signals represented using time-series complex numbers) and picks out peaks in the signal that may be transmissions from another source. These PDW processors are used to see what is transmitting in your area, be they radio towers or radars.</p> <p>siggen.c a signal generator written in C that will transmit the input to this pipeline.</p>","tags":["Signal Processing","UDP"]},{"location":"applications/simple_pdw_pipeline/#basicnetworkoprx","title":"BasicNetworkOpRx","text":"<p>This uses the Basic Network Operator to read udp packets this operator is documented elsewhere.</p>","tags":["Signal Processing","UDP"]},{"location":"applications/simple_pdw_pipeline/#packettotensorop","title":"PacketToTensorOp","text":"<p>This converts the bytes from the Basic Network Operator into the packets used in the rest of the pipeline. The format of the incoming packets is a 16-bit id followed by 8192 IQ samples each sample has the following format: 16 bits (I) 16 bits (Q)</p>","tags":["Signal Processing","UDP"]},{"location":"applications/simple_pdw_pipeline/#fftop","title":"FFTOp","text":"<p>Does what it says on the tin. Takes an FFT of the input data. Also shifts data so that 0 Hz is centered.</p>","tags":["Signal Processing","UDP"]},{"location":"applications/simple_pdw_pipeline/#thresholdingop","title":"ThresholdingOp","text":"<p>Detects samples over a threshold and then packetizes the runs of samples that are above the threshold as a \"pulse\".</p>","tags":["Signal Processing","UDP"]},{"location":"applications/simple_pdw_pipeline/#pulsedescriptiorop","title":"PulseDescriptiorOp","text":"<p>Takes simple statistics of input pulses. This is where I am most excited for future work, but that is not the point of this particular project.</p>","tags":["Signal Processing","UDP"]},{"location":"applications/simple_pdw_pipeline/#pulseprinterop","title":"PulsePrinterOp","text":"<p>Prints the pulse to screen. Also optionally sends packets to a BasicNetworkOpTx. The transmitted network packets have the following format: Each of the following fields are 16bit unsigned integers   id   low bin   high bin   zero bin   sum power   max amplitude   average amplitude</p>","tags":["Signal Processing","UDP"]},{"location":"applications/simple_radar_pipeline/cpp/","title":"Simple Radar Pipeline","text":"<p>     \u25b6 Run Locally  Authors: Cliff Burdick (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ (see Python version) Last modified: August 5, 2025 Latest version: 1.1 Minimum Holoscan SDK version: 2.6.0 Tested Holoscan SDK versions: 2.6.0 Contribution metric: Level 2 - Trusted</p> <p>This demonstration walks the developer through building a simple radar signal processing pipeline, targeted towards detecting objects, with Holoscan. In this example, we generate random radar and waveform data, passing both through: 1. Pulse Compression 2. Moving Target Indication (MTI) Filtering 3. Range-Doppler Map 4. Constant False Alarm Rate (CFAR) Analysis</p> <p>While this example generates 'offline' complex-valued data, it could be extended to accept streaming data from a phased array system or simulation via modification of the <code>SignalGeneratorOperator</code>.</p> <p>The output of this demonstration is a measure of the number of pulses per second processed on GPU.</p> <p>The main objectives of this demonstration are to: - Highlight developer productivity in building an end-to-end streaming application with Holoscan and existing GPU-Accelerated Python libraries - Demonstrate how to construct and connect isolated units of work via Holoscan operators, particularly with handling multiple inputs and outputs into an Operator - Emphasize that operators created for this application can be reused in other ones doing similar tasks</p>","tags":["Signal Processing","Detection"]},{"location":"applications/simple_radar_pipeline/cpp/#building-the-application","title":"Building the application","text":"<p>Make sure CMake (https://www.cmake.org) is installed on your system (minimum version 3.20)</p> <ul> <li> <p>Holoscan Debian Package - Follow the instructions in the link to install the latest version of Holoscan Debian package from NGC.</p> </li> <li> <p>Create a build directory:   <pre><code>mkdir -p &lt;build_dir&gt; &amp;&amp; cd &lt;build_dir&gt;\n</code></pre></p> </li> <li>Configure with CMake:</li> </ul> <p>Make sure CMake can find your installation of the Holoscan SDK. For example, setting <code>holoscan_ROOT</code> to its install directory during configuration:</p> <pre><code>cmake -S &lt;source_dir&gt; -B &lt;build_dir&gt; -DAPP_simple_radar_pipeline=1 \n</code></pre> <p>Notes: If the error <code>No CMAKE_CUDA_COMPILER could be found</code> is encountered, make sure that the :code:<code>nvcc</code> executable can be found by adding the CUDA runtime location to your <code>PATH</code> variable:</p> <pre><code>export PATH=$PATH:/usr/local/cuda/bin\n</code></pre> <ul> <li>Build:</li> </ul> <pre><code>cmake --build &lt;build_dir&gt;\n</code></pre>","tags":["Signal Processing","Detection"]},{"location":"applications/simple_radar_pipeline/cpp/#running-the-application","title":"Running the application","text":"<pre><code>&lt;build_dir&gt;/simple_radar_pipeline\n</code></pre>","tags":["Signal Processing","Detection"]},{"location":"applications/simple_radar_pipeline/python/","title":"Simple Radar Pipeline","text":"<p>     \u25b6 Run Locally  Authors: Cliff Burdick (NVIDIA) Supported platforms: x86_64 Language: Python, C++ (see C++ version) Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.4.0 Tested Holoscan SDK versions: 0.4.0 Contribution metric: Level 2 - Trusted</p> <p>This demonstration walks the developer through building a simple radar signal processing pipeline, targeted towards detecting objects, with Holoscan. In this example, we generate random radar and waveform data, passing both through: 1. Pulse Compression 2. Moving Target Indication (MTI) Filtering 3. Range-Doppler Map 4. Constant False Alarm Rate (CFAR) Analysis</p> <p>While this example generates 'offline' complex-valued data, it could be extended to accept streaming data from a phased array system or simulation via modification of the <code>SignalGeneratorOperator</code>.</p> <p>The output of this demonstration is a measure of the number of pulses per second processed on GPU.</p> <p>The main objectives of this demonstration are to: - Highlight developer productivity in building an end-to-end streaming application with Holoscan and existing GPU-Accelerated Python libraries - Demonstrate how to construct and connect isolated units of work via Holoscan operators, particularly with handling multiple inputs and outputs into an Operator - Emphasize that operators created for this application can be reused in other ones doing similar tasks</p>","tags":["Signal Processing","Detection"]},{"location":"applications/simple_radar_pipeline/python/#running-the-application","title":"Running the Application","text":"<p>Prior to running the application, the user needs to install the necessary dependencies. This is most easily done in an Anaconda environment.</p> <pre><code>conda create --name holoscan-sdr-demo python=3.8\nconda activate holoscan-sdr-demo\nconda install -c conda-forge -c rapidsai -c nvidia cusignal\npip install holoscan\n</code></pre> <p>The simple radar signal processing pipeline example can then be run via <pre><code>python applications/simple_radar_pipeline/simple_radar_pipeline.py\n</code></pre></p>","tags":["Signal Processing","Detection"]},{"location":"applications/slang/slang_gamma_correction/","title":"Slang Gamma Correction Example","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: October 9, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 3.5.0 Tested Holoscan SDK versions: 3.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>A Holoscan application demonstrating the integration of Slang shading language for GPU-accelerated gamma correction on image data.</p>","tags":["Rendering","Slang","shading","shader","compute","cuda"]},{"location":"applications/slang/slang_gamma_correction/#overview","title":"Overview","text":"<p>This application showcases how to use the Slang shading language operator (<code>SlangShaderOp</code>) within Holoscan applications for GPU-accelerated image processing. The example implements a pipeline that generates a test image with smooth color transitions, applies gamma correction using a Slang compute shader, and displays the result with proper sRGB color space handling.</p> <p></p>","tags":["Rendering","Slang","shading","shader","compute","cuda"]},{"location":"applications/slang/slang_gamma_correction/#what-is-gamma-correction","title":"What is Gamma Correction?","text":"<p>Gamma correction is a non-linear operation used to encode and decode luminance or tristimulus values in video or still image systems. It's essential for: - Correcting the non-linear response of display devices - Ensuring proper color reproduction across different devices - Converting between linear and sRGB color spaces - Improving image quality and color accuracy</p>","tags":["Rendering","Slang","shading","shader","compute","cuda"]},{"location":"applications/slang/slang_gamma_correction/#what-is-slang","title":"What is Slang?","text":"<p>Slang is a shading language that provides a modern, high-level interface for GPU programming. It supports both traditional graphics shaders and compute kernels, making it ideal for GPU-accelerated image processing in Holoscan applications.</p>","tags":["Rendering","Slang","shading","shader","compute","cuda"]},{"location":"applications/slang/slang_gamma_correction/#application-structure","title":"Application Structure","text":"<p>The application consists of four main components:</p> <ol> <li>SourceOp: Generates a 64x64x3 RGB test image with smooth color transitions</li> <li>GammaCorrectionOp: Processes the image using a Slang compute shader for gamma correction</li> <li>HolovizOp: Displays the processed image with proper sRGB color space handling</li> <li>Slang Shader: GPU-accelerated gamma correction kernel</li> </ol>","tags":["Rendering","Slang","shading","shader","compute","cuda"]},{"location":"applications/slang/slang_gamma_correction/#pipeline-flow","title":"Pipeline Flow","text":"<pre><code>flowchart LR\n   SourceOp --&gt; GammaCorrectionOp\n   GammaCorrectionOp --&gt; HolovizOp</code></pre>","tags":["Rendering","Slang","shading","shader","compute","cuda"]},{"location":"applications/slang/slang_gamma_correction/#prerequisites","title":"Prerequisites","text":"<ul> <li>Holoscan SDK 3.5.0 or later</li> <li>CUDA-capable GPU (for GPU acceleration)</li> <li>Python 3.10+ (for Python implementation)</li> <li>C++ compiler with CUDA support (for C++ implementation)</li> </ul>","tags":["Rendering","Slang","shading","shader","compute","cuda"]},{"location":"applications/slang/slang_gamma_correction/#supported-platforms","title":"Supported Platforms","text":"<ul> <li>x86_64</li> <li>aarch64</li> </ul>","tags":["Rendering","Slang","shading","shader","compute","cuda"]},{"location":"applications/slang/slang_gamma_correction/#installation","title":"Installation","text":"<ol> <li>Clone the HoloHub repository</li> <li>Navigate to the slang_gamma_correction directory</li> </ol>","tags":["Rendering","Slang","shading","shader","compute","cuda"]},{"location":"applications/slang/slang_gamma_correction/#usage","title":"Usage","text":"","tags":["Rendering","Slang","shading","shader","compute","cuda"]},{"location":"applications/slang/slang_gamma_correction/#running-the-application","title":"Running the Application","text":"<pre><code># Run the Python version\n./holohub run slang_gamma_correction\n\n# Run the C++ version\n./holohub run slang_gamma_correction --language=cpp\n</code></pre>","tags":["Rendering","Slang","shading","shader","compute","cuda"]},{"location":"applications/slang/slang_gamma_correction/#customization","title":"Customization","text":"","tags":["Rendering","Slang","shading","shader","compute","cuda"]},{"location":"applications/slang/slang_gamma_correction/#modifying-the-gamma-value","title":"Modifying the Gamma Value","text":"<p>You can adjust the gamma correction parameter:</p>","tags":["Rendering","Slang","shading","shader","compute","cuda"]},{"location":"applications/slang/slang_gamma_correction/#c","title":"C++","text":"<pre><code>auto gamma_correction = make_operator&lt;ops::GammaCorrectionOp&gt;(\n    \"GammaCorrection\",\n    Arg(\"gamma\", 1.8f),  // Change this value\n    Arg(\"data_type\", std::string(\"uint8_t\")),\n    Arg(\"component_count\", 3));\n</code></pre>","tags":["Rendering","Slang","shading","shader","compute","cuda"]},{"location":"applications/slang/slang_gamma_correction/#python","title":"Python","text":"<pre><code>gamma_correction = GammaCorrectionOp(\n    self,\n    name=\"GammaCorrection\",\n    gamma=1.8,  # Change this value\n    data_type=\"uint8_t\",\n    component_count=3\n)\n</code></pre>","tags":["Rendering","Slang","shading","shader","compute","cuda"]},{"location":"applications/slang/slang_gamma_correction/#related-documentation","title":"Related Documentation","text":"<ul> <li>Slang Language Reference</li> <li>Gamma Correction Theory</li> <li>sRGB Color Space</li> </ul>","tags":["Rendering","Slang","shading","shader","compute","cuda"]},{"location":"applications/slang/slang_gamma_correction/#license","title":"License","text":"<p>This project is licensed under the Apache License 2.0. See the LICENSE file for details.</p>","tags":["Rendering","Slang","shading","shader","compute","cuda"]},{"location":"applications/slang/slang_gamma_correction/#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please refer to the HoloHub contribution guidelines for more information.</p>","tags":["Rendering","Slang","shading","shader","compute","cuda"]},{"location":"applications/slang/slang_simple/","title":"Slang Simple Compute Kernel Example","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: February 4, 2026 Latest version: 0.1.0 Minimum Holoscan SDK version: 3.3.0 Tested Holoscan SDK versions: 3.3.0, 3.10.0, 3.11.0 Contribution metric: Level 1 - Highly Reliable</p> <p>A Holoscan application demonstrating the integration of Slang shading language for GPU-accelerated compute kernels.</p>","tags":["Rendering","Slang","shading","shader","shadertoy","compute","raytracing","cuda"]},{"location":"applications/slang/slang_simple/#overview","title":"Overview","text":"<p>This application showcases how to use the Slang shading language operator (<code>SlangShaderOp</code>) within Holoscan applications for GPU-accelerated data processing. The example implements a simple pipeline that generates incrementing integer values, processes them through a Slang compute shader, and outputs the results.</p>","tags":["Rendering","Slang","shading","shader","shadertoy","compute","raytracing","cuda"]},{"location":"applications/slang/slang_simple/#what-is-slang","title":"What is Slang?","text":"<p>Slang is a shading language that provides a modern, high-level interface for GPU programming. It supports both traditional graphics shaders and compute kernels, making it ideal for GPU-accelerated data processing in Holoscan applications.</p>","tags":["Rendering","Slang","shading","shader","shadertoy","compute","raytracing","cuda"]},{"location":"applications/slang/slang_simple/#application-structure","title":"Application Structure","text":"<p>The application consists of three main components:</p> <ol> <li>SourceOp: Generates a single-element Tensor containing incrementing integer values (1, 2, 3, ...)</li> <li>SlangShaderOp: Processes data using a Slang compute shader</li> <li>SinkOp: Receives and prints the processed results</li> </ol>","tags":["Rendering","Slang","shading","shader","shadertoy","compute","raytracing","cuda"]},{"location":"applications/slang/slang_simple/#pipeline-flow","title":"Pipeline Flow","text":"<pre><code>flowchart LR\n   SourceOp --&gt; SlangShaderOp\n   SlangShaderOp --&gt; SinkOp</code></pre>","tags":["Rendering","Slang","shading","shader","shadertoy","compute","raytracing","cuda"]},{"location":"applications/slang/slang_simple/#prerequisites","title":"Prerequisites","text":"<ul> <li>Holoscan SDK 3.3.0 or later</li> <li>CUDA-capable GPU (for GPU acceleration)</li> <li>Python 3.10+ (for Python implementation)</li> <li>C++ compiler with CUDA support (for C++ implementation)</li> </ul>","tags":["Rendering","Slang","shading","shader","shadertoy","compute","raytracing","cuda"]},{"location":"applications/slang/slang_simple/#supported-platforms","title":"Supported Platforms","text":"<ul> <li>x86_64</li> <li>aarch64</li> </ul>","tags":["Rendering","Slang","shading","shader","shadertoy","compute","raytracing","cuda"]},{"location":"applications/slang/slang_simple/#installation","title":"Installation","text":"<ol> <li>Clone the HoloHub repository</li> <li>Navigate to the slang_simple directory</li> </ol>","tags":["Rendering","Slang","shading","shader","shadertoy","compute","raytracing","cuda"]},{"location":"applications/slang/slang_simple/#usage","title":"Usage","text":"","tags":["Rendering","Slang","shading","shader","shadertoy","compute","raytracing","cuda"]},{"location":"applications/slang/slang_simple/#running-the-application","title":"Running the Application","text":"<pre><code># Run the C++ version\n./holohub run slang_simple\n\n# Run the Python version\n./holohub run slang_simple --language=python\n</code></pre>","tags":["Rendering","Slang","shading","shader","shadertoy","compute","raytracing","cuda"]},{"location":"applications/slang/slang_simple/#slang-shader-code","title":"Slang Shader Code","text":"<p>The application uses a simple Slang compute shader (<code>simple.slang</code>) that adds a offset to each input element:</p> <pre><code>// Import the holoscan library\nimport holoscan;\n\n// Create an input port named \"input_buffer\"\n[holoscan::input(\"input_buffer\")]\nRWStructuredBuffer&lt;int&gt; input_buffer;\n\n// Create an output port named \"output_buffer\"\n[holoscan::output(\"output_buffer\")]\n// Use the size of the input buffer to allocate the output buffer\n[holoscan::alloc::size_of(\"input_buffer\")]\nRWStructuredBuffer&lt;int&gt; output_buffer;\n\n// Create a parameter named \"offset\" with a default value of \"1\"\n[holoscan::parameter(\"offset=1\")]\nuniform int offset;\n\n// Use the size of the input buffer to set the grid size\n[holoscan::invocations::size_of(\"input_buffer\")]\n[shader(\"compute\")]\nvoid add(uint3 gid : SV_DispatchThreadID)\n{\n   output_buffer[gid.x] = input_buffer[gid.x] + offset;\n}\n</code></pre>","tags":["Rendering","Slang","shading","shader","shadertoy","compute","raytracing","cuda"]},{"location":"applications/slang/slang_simple/#shader-features-demonstrated","title":"Shader Features Demonstrated","text":"<ul> <li>Input/Output Buffers: The shader defines input and output buffers using Holoscan annotations</li> <li>Parameters: A uniform parameter can be set from the host application</li> <li>Grid Size: The compute grid size is automatically set based on input buffer size</li> <li>Simple Computation: Each thread adds a offset to the corresponding input element, the offset is set by a parameter</li> </ul> <p>Note that the data sent to the input of <code>SlangShaderOp</code> must be a data buffer (currently <code>holoscan::Tensor</code> and <code>nvidia::gxf::VideoBuffer</code> types are supported). For Python, any array-like objects implementing the <code>__dlpack__</code>, <code>__array_interface__</code> or <code>__cuda_array_interface__</code> are also supported.</p>","tags":["Rendering","Slang","shading","shader","shadertoy","compute","raytracing","cuda"]},{"location":"applications/slang/slang_simple/#expected-output","title":"Expected Output","text":"<p>The application will output processed values showing the result of adding the offset (10) to each input value:</p> <pre><code>Received value: 11\nReceived value: 12\nReceived value: 13\n...\nReceived value: 20\n</code></pre>","tags":["Rendering","Slang","shading","shader","shadertoy","compute","raytracing","cuda"]},{"location":"applications/slang/slang_simple/#customization","title":"Customization","text":"","tags":["Rendering","Slang","shading","shader","shadertoy","compute","raytracing","cuda"]},{"location":"applications/slang/slang_simple/#modifying-the-shader","title":"Modifying the Shader","text":"<p>You can modify the <code>simple.slang</code> file to implement different computations:</p> <ol> <li>Change the computation in the <code>add</code> function</li> <li>Add more parameters using <code>[holoscan::parameter(\"name=default_value\")]</code></li> <li>Modify buffer types and sizes</li> <li>Add more complex GPU algorithms</li> </ol>","tags":["Rendering","Slang","shading","shader","shadertoy","compute","raytracing","cuda"]},{"location":"applications/slang/slang_simple/#changing-parameters","title":"Changing Parameters","text":"<p>In the application code, you can modify the parameter value:</p>","tags":["Rendering","Slang","shading","shader","shadertoy","compute","raytracing","cuda"]},{"location":"applications/slang/slang_simple/#c","title":"C++","text":"<pre><code>auto slang = make_operator&lt;ops::SlangShaderOp&gt;(\"Slang\",\n                                               Arg(\"shader_source_file\", \"simple.slang\"),\n                                               Arg(\"parameter\", 42),  // Change this value\n                                               make_condition&lt;CountCondition&gt;(10));\n</code></pre>","tags":["Rendering","Slang","shading","shader","shadertoy","compute","raytracing","cuda"]},{"location":"applications/slang/slang_simple/#python","title":"Python","text":"<pre><code>slang = SlangShaderOp(self, name=\"Slang\", shader_source_file=\"simple.slang\")\nslang.add_arg(parameter=42)  # Change this value\n</code></pre>","tags":["Rendering","Slang","shading","shader","shadertoy","compute","raytracing","cuda"]},{"location":"applications/slang/slang_simple/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>The Slang shader runs on the GPU, providing significant performance benefits for large datasets</li> <li>The compute grid is automatically sized based on input data dimensions</li> </ul>","tags":["Rendering","Slang","shading","shader","shadertoy","compute","raytracing","cuda"]},{"location":"applications/slang/slang_simple/#troubleshooting","title":"Troubleshooting","text":"","tags":["Rendering","Slang","shading","shader","shadertoy","compute","raytracing","cuda"]},{"location":"applications/slang/slang_simple/#common-issues","title":"Common Issues","text":"<ol> <li>Shader compilation errors: Check the Slang shader syntax and Holoscan annotations</li> </ol>","tags":["Rendering","Slang","shading","shader","shadertoy","compute","raytracing","cuda"]},{"location":"applications/slang/slang_simple/#related-documentation","title":"Related Documentation","text":"<ul> <li>Slang Language Reference</li> </ul>","tags":["Rendering","Slang","shading","shader","shadertoy","compute","raytracing","cuda"]},{"location":"applications/slang/slang_simple/#license","title":"License","text":"<p>This project is licensed under the Apache License 2.0. See the LICENSE file for details.</p>","tags":["Rendering","Slang","shading","shader","shadertoy","compute","raytracing","cuda"]},{"location":"applications/slang/slang_simple/#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please refer to the HoloHub contribution guidelines for more information.</p>","tags":["Rendering","Slang","shading","shader","shadertoy","compute","raytracing","cuda"]},{"location":"applications/speech_to_text_llm/","title":"Speech-to-text + Large Language Model","text":"<p>     \u25b6 Run Locally  Authors: Sean Huver (NVIDIA) Supported platforms: x86_64 Language: Python Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 2 - Trusted</p> <p>This application transcribes an audio file using a speech-to-text model (STT), then uses a large language model (LLM) to summarize and generate new relevant information.</p> <p>While this workflow in principle could be used for a number of domains, here we provide a healthcare specific example. A <code>sample.wav</code> file is provided which is an example of a radiology interpretation. An OpenAI whisper model is used to transform the audio into text, then an API call is made to either the GPT3.5-turbo or GPT4 LLM.</p>","tags":["Healthcare AI","Audio","ASR","LLM","Auth and API","Medical Imaging"]},{"location":"applications/speech_to_text_llm/#yaml-configuration","title":"YAML Configuration","text":"<p>The input (either audio or video file), specific Whisper model (tiny, small, medium, or large), LLM model, and directions for the LLM are all determined by the <code>stt_to_nlp.yaml</code> file. As you see from our example, the directions for the LLM are made via natural language, and can result in very different applications.</p> <p>For our purposes we specify the directions as:</p> <pre><code>  context: 'Make summary of the transcript (and correct any transcription errors in CAPS).\\n Create a Patient Summary with no medical jargon. \\n \n  Create a full radiological report write-up. \\n Give likely ICD-10 Codes \\n Suggested follow-up steps.'\n</code></pre> <p>This results in the following output from the LLM:</p> <pre><code>LLM Response: \n Summary of Transcript:\nThe patient has full thickness wear on the dorsal half of the second metatarsal head with reactive bone marrow edema and capsulitis. There is also second web space bursitis and a third web space neuroma. The 51-year-old male has multiple gallbladder polyps, with the largest measuring 1.9 x 2 cm, 1.7 x 1.7 cm in the mid portion, and 1.6 x 1.6 cm distally. Other smaller polyps are also present.\n\nPatient Summary (No Medical Jargon):\nThe patient has damage and inflammation in the foot, specifically in the second toe joint and surrounding areas. They also have multiple growths in their gallbladder, with the largest being about the size of a grape. The patient is a 51-year-old male with a family history of abdominal aortic aneurysm.\n\nFull Radiological Report Write-up:\nPatient: 51-year-old male\nFamily History: Abdominal aortic aneurysm\n\nFindings:\n1. Foot: Full thickness wear over the dorsal half of the second metatarsal head with reactive subchondral bone marrow edema and capsulitis. Second web space intermetatarsal bursitis and a third web space neuroma are also present.\n2. Gallbladder: Multiple gallbladder polyps are observed. The largest polyp measures 1.9 x 2 cm, with additional polyps measuring 1.7 x 1.7 cm in the mid portion and 1.6 x 1.6 cm distally. Two smaller polyps measure 0.5 x 0.4 x 0.4 cm and 0.5 x 0.3 x 0.5 cm.\n\nLikely ICD-10 Codes:\n1. M25.572 - Capsulitis, left ankle and foot\n2. M79.671 - Bursitis, right ankle and foot\n3. G57.60 - Lesion of plantar nerve, unspecified lower limb\n4. K82.8 - Other specified diseases of the gallbladder (gallbladder polyps)\n\nSuggested Follow-up Steps:\n1. For the foot issues, the patient may benefit from a consultation with a podiatrist or orthopedic specialist to discuss treatment options, which may include physical therapy, orthotics, or surgery.\n2. For the gallbladder polyps, the patient should consult with a gastroenterologist to determine the need for further evaluation, monitoring, or possible surgical intervention. Regular ultrasound examinations may be recommended to monitor the growth of the polyps.\n</code></pre>","tags":["Healthcare AI","Audio","ASR","LLM","Auth and API","Medical Imaging"]},{"location":"applications/speech_to_text_llm/#run-instructions","title":"Run Instructions","text":"<p>Note: To run this application you will need to create an OpenAI account and obtain your own API key with active credits.</p> <p>You should refer to the glossary for the terms defining specific locations within HoloHub.</p> <ul> <li>(Optional) Create and use a virtual environment:</li> </ul> <pre><code>python3 -m venv .venv\nsource .venv/bin/activate\n</code></pre> <ul> <li>Install the python packages</li> </ul> <pre><code>pip install -r applications/speech_to_text_llm/requirements.txt\n</code></pre> <ul> <li>Run the application</li> </ul> <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;HOLOSCAN_INSTALL_DIR&gt;/python/lib:&lt;HOLOHUB_BUILD_DIR&gt;/python/lib\ncd applications/speech_to_text_llm \npython3 stt_to_nlp.py\n</code></pre>","tags":["Healthcare AI","Audio","ASR","LLM","Auth and API","Medical Imaging"]},{"location":"applications/speech_to_text_llm/#sample-audio-file","title":"Sample Audio File","text":"<p>Please note the sample audio file included is licensed as CC-BY-4.0 International, copyright NVIDIA 2023.</p>","tags":["Healthcare AI","Audio","ASR","LLM","Auth and API","Medical Imaging"]},{"location":"applications/ssd_detection_endoscopy_tools/","title":"SSD Detection for Endoscopy Tools","text":"<p>     \u25b6 Run Locally  Authors: Jin Li (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.1 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 2 - Trusted</p>","tags":["Healthcare AI","Video","SSD","Detection","Visualization"]},{"location":"applications/ssd_detection_endoscopy_tools/#model","title":"Model","text":"<p>We can train the SSD model from NVIDIA DeepLearningExamples repo with any data of our choosing. Here for the purpose of demonstrating the deployment process, we will use a SSD model checkpoint that is only trained for the demo video clip.</p> <p>Please download the models at this NGC Resource for <code>epoch_24.pt</code>, <code>epoch24_nms.onnx</code> and <code>epoch24.onnx</code>. You can go through the next steps of Model Conversion to ONNX to convert <code>epoch_24.pt</code> into <code>epoch24_nms.onnx</code> and <code>epoch24.onnx</code>, or use the downloaded ONNX models directly.</p>","tags":["Healthcare AI","Video","SSD","Detection","Visualization"]},{"location":"applications/ssd_detection_endoscopy_tools/#model-conversion-to-onnx","title":"Model Conversion to ONNX","text":"<p>The scripts we need to export the model from .pt checkpoint to the ONNX format are all within this dir <code>./scripts</code>. It is a two step process.</p> <p>Step 1: Export the trained checkpoint to ONNX.  We use <code>export_to_onnx_ssd.py</code> if we want to use the model as is without NMS, or <code>export_to_onnx_ssd_nms.py</code> to prepare the model with NMS.  Let's assume the re-trained SSD model checkpoint from the repo is saved as <code>epoch_24.pt</code>.  The export process is <pre><code># For exporting the original ONNX model\n python export_to_onnx_ssd.py --model epoch_24.pt  --outpath epoch24_temp.onnx\n</code></pre> <pre><code># For preparing to add the NMS step to ONNX model\npython export_to_onnx_ssd_nms.py --model epoch_24.pt  --outpath epoch24_nms_temp.onnx\n</code></pre> Step 2: modify input shape.  Step 1 produces a onnx model with input shape <code>[1, 3, 300, 300]</code>, but we will want to modify the input node to have shape <code>[1, 300, 300, 3]</code> or in general <code>[batch_size, height, width, channels]</code> for compatibility and easy of deployment in the Holoscan SDK. If we want to incorporate the NMS operation in the the ONNX model, we could add a <code>EfficientNMS_TRT</code> op, which is documented in <code>graph_surgeon_ssd.py</code>'s nms related block. <pre><code># For exporting the original ONNX model\npython graph_surgeon_ssd.py --orig_model epoch24_temp.onnx --new_model epoch24.onnx\n</code></pre> <pre><code># For adding the NMS step to ONNX model, use --nms\npython graph_surgeon_ssd.py --orig_model epoch24_nms_temp.onnx --new_model epoch24_nms.onnx --nms\n</code></pre></p> <p>Note that  - <code>epoch24.onnx</code> is used in <code>ssd_step1.py</code> and <code>ssd_step2_route1.py</code>  - <code>epoch24_nms.onnx</code> is used in <code>ssd_step2_route2.py</code> and <code>ssd_step2_route2_render_labels.py</code></p>","tags":["Healthcare AI","Video","SSD","Detection","Visualization"]},{"location":"applications/ssd_detection_endoscopy_tools/#data","title":"Data","text":"<p>For this application we will use the same Endoscopy Sample Data as the Holoscan SDK reference applications.</p>","tags":["Healthcare AI","Video","SSD","Detection","Visualization"]},{"location":"applications/ssd_detection_endoscopy_tools/#requirements","title":"Requirements","text":"<p>There are two requirements 1. To run <code>ssd_step1.py</code> and <code>ssd_step2_route1.py</code> with the original exported model, we need the installation of PyTorch and CuPy.  To run <code>ssd_step2_route2.py</code> and <code>ssd_step2_route2_render_labels.py</code> with the exported model with additional NMS layer in ONNX, we need the installation of CuPy.  If you're using the dGPU on the devkit, since there are no prebuilt PyTorch wheels for aarch64 dGPU, the simplest way is to modify the Dockerfile and build from source; if you're on x86 or using the iGPU on the devkit, there should be existing prebuilt PyTorch wheels.  If you choose to build the SDK from source, you can find the modified Dockerfile here to replace the SDK repo Dockerfile to satisfy the installation requirements.  The main changes in Dockerfile for dGPU: the base image changed to <code>nvcr.io/nvidia/pytorch:22.03-py3</code> instead of the <code>nvcr.io/nvidia/tensorrt:22.03-py3</code> as dGPU's base image; adding the installation of NVTX for optional profiling. Build the SDK container following the README instructions.   Make sure the directory containing this application and the directory containing the NGC data and models are mounted in the container. Add the <code>-v</code> mount options to the <code>docker run</code> command launched by <code>./holohub run-container</code> in the SDK repo.</p> <ol> <li>Make sure the model and data are accessible by the application.  Make sure the yaml files <code>ssd_endo_model.yaml</code> and <code>ssd_endo_model_with_NMS.yaml</code> are pointing to the right locations for the ONNX model and data. The assumption in the yaml file is that the <code>epoch24_nms.onnx</code> and <code>epoch24.onnx</code> are located at: <pre><code>model_file_path: /byom/models/endo_ssd/epoch24_nms.onnx\nengine_cache_dir: /byom/models/endo_ssd/epoch24_nms_engines\n</code></pre> and / or <pre><code>model_file_path: /byom/models/endo_ssd/epoch24.onnx\nengine_cache_dir: /byom/models/endo_ssd/epoch24_engines\n</code></pre> The Endoscopy Sample Data is assumed to be at <pre><code>/workspace/holoscan-sdk/data/endoscopy\n</code></pre> Please check and modify the paths to model and data in the yaml file if needed.</li> </ol>","tags":["Healthcare AI","Video","SSD","Detection","Visualization"]},{"location":"applications/ssd_detection_endoscopy_tools/#building-the-application","title":"Building the application","text":"<p>Please refer to the README under ./app_dev_process to see the process of building the applications.</p>","tags":["Healthcare AI","Video","SSD","Detection","Visualization"]},{"location":"applications/ssd_detection_endoscopy_tools/#running-the-application","title":"Running the application","text":"<p>Run the incrementally improved Python applications by: <pre><code>python ssd_step1.py\n\npython ssd_step2_route1.py\n\npython ssd_step2_route2.py\n\npython ssd_step2_route2_render_labels.py --labelfile endo_ref_data_labels.csv\n</code></pre></p>","tags":["Healthcare AI","Video","SSD","Detection","Visualization"]},{"location":"applications/stereo_vision/","title":"Stereo Vision","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: October 30, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.4.0 Tested Holoscan SDK versions: 2.4.0 Contribution metric: Level 1 - Highly Reliable</p> <p> </p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Depth","Camera","Visualization","Image Processing"]},{"location":"applications/stereo_vision/#overview","title":"Overview","text":"<p>A demo pipeline showcasing stereo disparity estimation.</p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Depth","Camera","Visualization","Image Processing"]},{"location":"applications/stereo_vision/#description","title":"Description","text":"<p>This pipeline takes video from a stereo camera and estimates disparity using DNN ESS. The disparity map is displayed through Holoviz.</p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Depth","Camera","Visualization","Image Processing"]},{"location":"applications/stereo_vision/#requirements","title":"Requirements","text":"<p>This application requires a V4L2 stereo camera or recorded stereo video as input. A video acquired from a StereoLabs ZED camera is downloaded when running the <code>get_data_and_models.sh</code> script when building the application. A script for obtaining the calibration for StereoLabs cameras is also provided. Holoscan SDK &gt;=2.0,&lt;=2.5 is required for TensorRT 8.6 compatibility.</p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Depth","Camera","Visualization","Image Processing"]},{"location":"applications/stereo_vision/#camera-calibration","title":"Camera Calibration","text":"<p>The default calibration will work for the sample video. If using a stereolabs camera the calibration can be retrieved using <code>get_zed_calibration.py</code> and the devices serial number.</p> <pre><code>python3 get_zed_calibration.py -s [Serial Number]\n</code></pre>","tags":["Computer Vision and Perception","Video","Stereo Vision","Depth","Camera","Visualization","Image Processing"]},{"location":"applications/stereo_vision/#input-video","title":"Input video","text":"<p>For the input video stream, either use a v4l2 stereo camera such as those produced by stereolabs or included recorded video. The <code>stereo-plants.mp4</code> video is provided here and will be downloaded and converted to the necessary format when building the application.</p> <p>The source device in <code>stereo_vision.yaml</code> should be modified to match the device the v4l2 video is using. This can be found using <code>v4l2-ctl --list-devices</code>.</p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Depth","Camera","Visualization","Image Processing"]},{"location":"applications/stereo_vision/#models","title":"Models","text":"<p>This demo requires the ESS DNN Stereo Disparity available from the NGC catalog for disparity estimation. This model is downloaded when you build the application.</p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Depth","Camera","Visualization","Image Processing"]},{"location":"applications/stereo_vision/#ess-dnn","title":"ESS DNN","text":"<p>The ESS engine files generated in this demo application is specific to TRT8.6; make sure you build the devcontainer with a compatible <code>base_img</code> as shown in the Build and Run Instructions section.</p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Depth","Camera","Visualization","Image Processing"]},{"location":"applications/stereo_vision/#build-and-run-instructions","title":"Build and Run Instructions","text":"<p>Run the following command to build and run application using the recorded video: <pre><code>./holohub run stereo_vision --base_img nvcr.io/nvidia/clara-holoscan/holoscan:v2.4.0-dgpu\n</code></pre></p> <p>To run the application using a v4l2 compatible stereo camera, run: <pre><code>./holohub run stereo_vision --base_img nvcr.io/nvidia/clara-holoscan/holoscan:v2.4.0-dgpu --run-args=\"--source v4l2\"\n</code></pre></p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Depth","Camera","Visualization","Image Processing"]},{"location":"applications/surgical_scene_recon/","title":"Surgical Scene Reconstruction with Gaussian Splatting","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64 Language: Python Last modified: November 21, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 3.7.0 Tested Holoscan SDK versions: 3.7.0 Contribution metric: Level 2 - Trusted</p> <p>This application demonstrates real-time 3D surgical scene reconstruction by combining Holoscan SDK for high-performance streaming, 3D Gaussian Splatting for neural 3D representation, and temporal deformation networks for accurate modeling of dynamic tissue.</p> <p></p> <p>The application provides a complete end-to-end pipeline\u2014from raw surgical video to real-time 3D reconstruction. Researchers and developers can use it to train custom models on their own endoscopic data and visualize results with GPU-accelerated rendering.</p> <p>Features of this application include:</p> <ul> <li>Real-time Visualization: Stream surgical scene reconstruction at &gt;30 FPS using Holoscan</li> <li>Temporal Deformation: Accurate per-frame tissue modeling as it deforms over time</li> <li>Tool Removal: Tissue-only reconstruction mode (surgical instruments automatically excluded)</li> <li>End-to-End Training: On-the-fly model training from streamed endoscopic data</li> <li>Two Operation Modes: Inference-only (with pre-trained checkpoint) OR train-then-render</li> <li>Production Ready: Tested and optimized Holoscan pipeline with complete Docker containerization</li> </ul> <p>It takes input from EndoNeRF surgical datasets (RGB images + stereo depth + camera poses + tool masks). It processes the input using multi-frame Gaussian Splatting with a 4D spatiotemporal deformation network. And it outputs real-time 3D tissue reconstruction without surgical instruments.</p> <p>It is ideal for use cases, such as:</p> <ul> <li>Surgical scene understanding and visualization</li> <li>Tool-free tissue reconstruction for analysis</li> <li>Research in surgical vision and 3D reconstruction</li> <li>Development of real-time surgical guidance systems</li> </ul>","tags":["Healthcare AI","Endoscopy","3D Reconstruction","Gaussian Splatting","Neural Rendering","Temporal Deformation"]},{"location":"applications/surgical_scene_recon/#quick-start","title":"Quick Start","text":"","tags":["Healthcare AI","Endoscopy","3D Reconstruction","Gaussian Splatting","Neural Rendering","Temporal Deformation"]},{"location":"applications/surgical_scene_recon/#step-1-clone-the-holohub-repository","title":"Step 1: Clone the HoloHub Repository","text":"<pre><code>git clone https://github.com/nvidia-holoscan/holohub.git\ncd holohub\n</code></pre>","tags":["Healthcare AI","Endoscopy","3D Reconstruction","Gaussian Splatting","Neural Rendering","Temporal Deformation"]},{"location":"applications/surgical_scene_recon/#step-2-read-and-agree-to-the-terms-and-conditions-of-the-endonerf-sample-dataset","title":"Step 2: Read and Agree to the Terms and Conditions of the EndoNeRF Sample Dataset","text":"<ol> <li>Read and agree to the Terms and Conditions for the EndoNeRF dataset.</li> <li>EndoNeRF sample dataset is being downloaded automatically when building the application. </li> <li>Optionally, for manual download of the dataset, refer to the Data section below.</li> <li> <p>Optionally, if you do not agree to the terms and conditions, set the <code>HOLOHUB_DOWNLOAD_DATASETS</code> environment variable to <code>OFF</code> and manually download the dataset and place it in the correct location by following the instructions in the Data section below.</p> <pre><code>export HOLOHUB_DOWNLOAD_DATASETS=OFF\n</code></pre> </li> </ol>","tags":["Healthcare AI","Endoscopy","3D Reconstruction","Gaussian Splatting","Neural Rendering","Temporal Deformation"]},{"location":"applications/surgical_scene_recon/#step-3-run-training","title":"Step 3: Run Training","text":"<p>To run the model training:</p> <pre><code>./holohub run surgical_scene_recon train\n</code></pre>","tags":["Healthcare AI","Endoscopy","3D Reconstruction","Gaussian Splatting","Neural Rendering","Temporal Deformation"]},{"location":"applications/surgical_scene_recon/#step-4-dynamic-rendering-with-a-trained-model","title":"Step 4: Dynamic Rendering with a Trained Model","text":"<p>After training completes, to visualize your results in real-time, run the surgical render:</p> <pre><code>./holohub run surgical_scene_recon render\n</code></pre> <p></p>","tags":["Healthcare AI","Endoscopy","3D Reconstruction","Gaussian Splatting","Neural Rendering","Temporal Deformation"]},{"location":"applications/surgical_scene_recon/#obtaining-the-pulling-soft-tissues-dataset","title":"Obtaining the Pulling Soft Tissues Dataset","text":"<p>This application uses the EndoNeRF \"pulling_soft_tissues\" dataset, which contains:</p> <ul> <li>63 RGB endoscopy frames (640\u00d7512 pixels)</li> <li>Corresponding depth maps</li> <li>Tool segmentation masks for instrument removal</li> <li>Camera poses and bounds (poses_bounds.npy)</li> </ul>","tags":["Healthcare AI","Endoscopy","3D Reconstruction","Gaussian Splatting","Neural Rendering","Temporal Deformation"]},{"location":"applications/surgical_scene_recon/#download-the-dataset","title":"Download the Dataset","text":"<p>You can download the dataset from one of the following locations:</p> <ul> <li> <p>\ud83d\udce6 Direct Google Drive: https://drive.google.com/drive/folders/1zTcX80c1yrbntY9c6-EK2W2UVESVEug8?usp=sharing</p> </li> <li> <p>In the Google Drive folder, you'll see:</p> <ul> <li><code>cutting_tissues_twice</code></li> <li><code>pulling_soft_tissues</code></li> </ul> </li> <li> <p>Download <code>pulling_soft_tissues</code>.</p> </li> <li> <p>Visit the EndoNeRF repository.</p> </li> </ul>","tags":["Healthcare AI","Endoscopy","3D Reconstruction","Gaussian Splatting","Neural Rendering","Temporal Deformation"]},{"location":"applications/surgical_scene_recon/#dataset-setup","title":"Dataset Setup","text":"<p>The dataset will be automatically used by the application when placed in the correct location. Refer to the HoloHub glossary for definitions of HoloHub-specific directory terms used below.</p> <p>To place the dataset at <code>&lt;HOLOHUB_ROOT&gt;/data/EndoNeRF/pulling/</code>:</p> <ol> <li> <p>From the HoloHub root directory:     <pre><code>mkdir -p data/EndoNeRF\n</code></pre></p> </li> <li> <p>Extract and move (or copy) the downloaded dataset: </p> <pre><code>mv /path/to/pulling_soft_tissues data/EndoNeRF/pulling\n</code></pre> </li> </ol> <p>Important: The dataset MUST be physically at the path above, do NOT use symlinks. Docker containers cannot follow symlinks outside mounted volumes.</p>","tags":["Healthcare AI","Endoscopy","3D Reconstruction","Gaussian Splatting","Neural Rendering","Temporal Deformation"]},{"location":"applications/surgical_scene_recon/#verify-the-dataset-structure","title":"Verify the Dataset Structure","text":"<p>Verify that your dataset has this structure:</p> <pre><code>&lt;HOLOHUB_ROOT&gt;/\n\u2514\u2500\u2500 data/\n    \u2514\u2500\u2500 EndoNeRF/\n        \u2514\u2500\u2500 pulling/\n            \u251c\u2500\u2500 images/              # 63 RGB frames (.png)\n            \u251c\u2500\u2500 depth/               # 63 depth maps (.png)\n            \u251c\u2500\u2500 masks/               # 63 tool masks (.png)\n            \u2514\u2500\u2500 poses_bounds.npy     # Camera poses (8.5 KB)\n</code></pre>","tags":["Healthcare AI","Endoscopy","3D Reconstruction","Gaussian Splatting","Neural Rendering","Temporal Deformation"]},{"location":"applications/surgical_scene_recon/#models-used-by-the-surgical_scene_recon-application","title":"Models Used by the <code>surgical_scene_recon</code> Application","text":"<p>The <code>surgical_scene_recon</code> application uses a 4D Dynamic Gaussian Splatting model that combines:</p> <ul> <li>3D Gaussian Splatting model - A point-based neural scene representation</li> <li> <p>HexPlane Temporal Deformation Network - A spatiotemporal feature grid with MLPs for modeling tissue dynamics</p> </li> <li> <p>Gaussian Splatting Model</p> </li> </ul> <p>The Gaussian Splatting model can be described as:</p> <ul> <li>Architecture: 3D Gaussians with learned position, scale, rotation, opacity, and color</li> <li>Initialization: Multi-frame point cloud (~30,000-50,000 points from all frames)</li> <li>Renderer: <code>gsplat</code> library (CUDA-accelerated differentiable rasterization)</li> <li>Spherical Harmonics of degree 3 (16 coefficients per Gaussian for view-dependent color)</li> <li> <p>Resolution: 640\u00d7512 pixels (RGB, three channels)</p> </li> <li> <p>Temporal Deformation Network Model</p> </li> </ul> <p>The Temporal Deformation Network model deforms 3D Gaussians and can be described as:</p> <ul> <li>Architecture: HexPlane 4D spatiotemporal grid + MLP decoder</li> <li>Input: 3D position + normalized time value [0, 1]</li> <li>Output: Deformed position, scale, rotation, and opacity changes</li> <li>Training: Two-stage process (coarse: static, fine: with deformation)</li> <li>Inference: Direct PyTorch (no conversion, full precision)</li> </ul>","tags":["Healthcare AI","Endoscopy","3D Reconstruction","Gaussian Splatting","Neural Rendering","Temporal Deformation"]},{"location":"applications/surgical_scene_recon/#about-the-model-training-process","title":"About the Model Training Process","text":"<p>The application trains in two stages:</p> <ol> <li>The Coarse Stage where the application learns the base static Gaussian models without deformation.</li> <li>The Fine Stage where a temporal deformation network model is added for dynamic tissue modeling.</li> </ol> <p>The training uses:</p> <ul> <li>Multi-modal Data: RGB images, depth maps, tool segmentation masks</li> <li>Loss Functions: RGB loss, depth loss, TV loss, masking losses</li> <li>Optimization: Adam optimizer with batch-size scaled learning rates</li> <li>Tool Removal: Segmentation masks applied during training for tissue-only reconstruction</li> </ul> <p>The training pipeline (<code>gsplat_train.py</code>) runs in the following order:</p> <ol> <li>Data Loading using EndoNeRF parser loads RGB, depth, masks, and poses.</li> <li>Initialization uses Multi-frame point cloud (~30k points).</li> <li>Training happens in two stages:</li> <li>Coarse</li> <li>Fine</li> <li>Optimization is done by the Adam (Adaptive Moment Estimation) optimizer with batch-size scaled learning rates.</li> <li>Regularization, for depth loss, TV loss, and masking losses, is performed on the data.</li> </ol> <p>The default training command trains a model on all 63 frames with 2000 iterations, producing smooth temporal deformation and high-quality reconstruction.</p> <p>Training outputs are saved to <code>&lt;HOLOHUB_APP_BIN&gt;/output/trained_model/</code>, where <code>&lt;HOLOHUB_APP_BIN&gt;</code> is <code>&lt;HOLOHUB_ROOT&gt;/build/surgical_scene_recon/applications/surgical_scene_recon/</code> by default.</p> <ul> <li><code>ckpts/fine_best_psnr.pt</code> - Best checkpoint (use for rendering)</li> <li><code>ckpts/fine_step00XXX.pt</code> - Regular checkpoints</li> <li><code>logs/</code> - Training logs</li> <li><code>tb/</code> - TensorBoard logs</li> <li><code>renders/</code> - Saved render frames</li> </ul> <p>You can view training logs using TensorBoard:</p> <pre><code>tensorboard --logdir &lt;HOLOHUB_APP_BIN&gt;/output/trained_model/tb\n</code></pre>","tags":["Healthcare AI","Endoscopy","3D Reconstruction","Gaussian Splatting","Neural Rendering","Temporal Deformation"]},{"location":"applications/surgical_scene_recon/#holoscan-pipeline-architecture","title":"Holoscan Pipeline Architecture","text":"<p>The real-time rendering uses the following Holoscan operators:</p> <pre><code>EndoNeRFLoaderOp \u2192 GsplatLoaderOp \u2192 GsplatRenderOp \u2192 HolovizOp\n                                                    \u2193\n                                              ImageSaverOp\n</code></pre> <ul> <li>EndoNeRFLoaderOp: Streams camera poses and timestamps</li> <li>GsplatLoaderOp: Loads checkpoint and deformation network</li> <li>GsplatRenderOp: Applies temporal deformation and renders</li> <li>HolovizOp: Real-time GPU-accelerated visualization</li> <li>ImageSaverOp: Optional frame saving</li> </ul>","tags":["Healthcare AI","Endoscopy","3D Reconstruction","Gaussian Splatting","Neural Rendering","Temporal Deformation"]},{"location":"applications/surgical_scene_recon/#requirements-for-the-surgical_scene_recon-application","title":"Requirements for the <code>surgical_scene_recon</code> Application","text":"<ul> <li>Hardware:</li> <li>NVIDIA GPU (RTX 3000+ series recommended, tested on RTX 6000 Ada Generation)</li> <li>~2 GB free disk space (for the dataset)</li> <li>~30 GB free disk space (for Docker containers)</li> <li>Software:</li> <li>Docker with NVIDIA GPU support</li> <li>X11 display server (for visualization)</li> <li>Holoscan SDK 3.7.0 or later (automatically provided in containers)</li> </ul>","tags":["Healthcare AI","Endoscopy","3D Reconstruction","Gaussian Splatting","Neural Rendering","Temporal Deformation"]},{"location":"applications/surgical_scene_recon/#application-integration-testing","title":"Application Integration Testing","text":"<p>We provide integration tests.</p> <p>To test the application for training and inference, run:</p> <pre><code>./holohub test surgical_scene_recon --verbose\n</code></pre>","tags":["Healthcare AI","Endoscopy","3D Reconstruction","Gaussian Splatting","Neural Rendering","Temporal Deformation"]},{"location":"applications/surgical_scene_recon/#performance","title":"Performance","text":"<p>Tested Configuration:</p> <ul> <li>GPU: NVIDIA RTX 6000 Ada Generation</li> <li>Container: Holoscan SDK 3.7.0</li> <li>Training Time: ~5 minutes (63 frames, 2000 iterations)</li> <li>Rendering: Real-time &gt;30 FPS</li> </ul> <p>Quality Metrics (train mode):</p> <ul> <li>PSNR: ~36-38 dB</li> <li>SSIM: ~0.80</li> <li>Gaussians: ~50,000 splats</li> <li>Deformation: Smooth temporal consistency</li> </ul>","tags":["Healthcare AI","Endoscopy","3D Reconstruction","Gaussian Splatting","Neural Rendering","Temporal Deformation"]},{"location":"applications/surgical_scene_recon/#troubleshooting","title":"Troubleshooting","text":"","tags":["Healthcare AI","Endoscopy","3D Reconstruction","Gaussian Splatting","Neural Rendering","Temporal Deformation"]},{"location":"applications/surgical_scene_recon/#problem-filenotfounderror-poses_boundsnpy-not-found","title":"Problem: \"FileNotFoundError: poses_bounds.npy not found\"","text":"<ul> <li>Cause: Dataset not in correct location or symlink used</li> <li>Solution: Ensure dataset is physically at <code>&lt;HOLOHUB_ROOT&gt;/data/EndoNeRF/pulling/</code></li> <li>Verify: Run <code>file data/EndoNeRF</code> - should show \"directory\", not \"symbolic link\"</li> </ul>","tags":["Healthcare AI","Endoscopy","3D Reconstruction","Gaussian Splatting","Neural Rendering","Temporal Deformation"]},{"location":"applications/surgical_scene_recon/#problem-unable-to-find-image-holohub-surgical_scene_recon","title":"Problem: \"Unable to find image holohub-surgical_scene_recon\"","text":"<ul> <li>Cause: Container not built yet</li> <li>Solution: Remove <code>--no-docker-build</code> flag (let CLI build automatically)</li> <li>Or: Manually build: <code>./holohub build-container surgical_scene_recon</code></li> </ul>","tags":["Healthcare AI","Endoscopy","3D Reconstruction","Gaussian Splatting","Neural Rendering","Temporal Deformation"]},{"location":"applications/surgical_scene_recon/#problem-holoviz-window-doesnt-appear","title":"Problem: Holoviz window doesn't appear","text":"<ul> <li>Cause: X11 forwarding not enabled</li> <li>Solution: Run <code>xhost +local:docker</code> before training</li> <li>Verify: Check <code>echo $DISPLAY</code> shows a value</li> </ul>","tags":["Healthcare AI","Endoscopy","3D Reconstruction","Gaussian Splatting","Neural Rendering","Temporal Deformation"]},{"location":"applications/surgical_scene_recon/#problem-gpu-out-of-memory","title":"Problem: GPU out of memory","text":"<ul> <li>Cause: Another process using GPU</li> <li>Solution: Check <code>nvidia-smi</code> and stop other processes</li> <li>Or: Reduce batch size (advanced - edit training config)</li> </ul>","tags":["Healthcare AI","Endoscopy","3D Reconstruction","Gaussian Splatting","Neural Rendering","Temporal Deformation"]},{"location":"applications/surgical_scene_recon/#acknowledgements","title":"Acknowledgements","text":"","tags":["Healthcare AI","Endoscopy","3D Reconstruction","Gaussian Splatting","Neural Rendering","Temporal Deformation"]},{"location":"applications/surgical_scene_recon/#citation","title":"Citation","text":"<p>If you use this work, cite the following:</p> <ul> <li>EndoNeRF:</li> </ul> <pre><code>@inproceedings{wang2022endonerf,\n  title={EndoNeRF: Neural Rendering for Stereo 3D Reconstruction of Deformable Tissues in Robotic Surgery},\n  author={Wang, Yuehao and Yifan, Wang and Tao, Rui and others},\n  booktitle={MICCAI},\n  year={2022}\n}\n</code></pre> <ul> <li>3D Gaussian Splatting:</li> </ul> <pre><code>@article{kerbl20233d,\n  title={3d gaussian splatting for real-time radiance field rendering},\n  author={Kerbl, Bernhard and Kopanas, Georgios and Leimk{\\\"u}hler, Thomas and Drettakis, George},\n  journal={ACM Transactions on Graphics},\n  year={2023}\n}\n</code></pre> <ul> <li><code>gsplat</code> Library:</li> </ul> <pre><code>@software{ye2024gsplat,\n  title={gsplat},\n  author={Ye, Vickie and Turkulainen, Matias and others},\n  year={2024},\n  url={https://github.com/nerfstudio-project/gsplat}\n}\n</code></pre>","tags":["Healthcare AI","Endoscopy","3D Reconstruction","Gaussian Splatting","Neural Rendering","Temporal Deformation"]},{"location":"applications/surgical_scene_recon/#license","title":"License","text":"<p>This application is licensed under Apache 2.0. See individual files for specific licensing:</p> <ul> <li>Application code: Apache 2.0 (NVIDIA)</li> <li>Training utilities: MIT License (EndoGaussian Project)</li> <li>Spherical harmonics utils: BSD-2-Clause (PlenOctree)</li> </ul>","tags":["Healthcare AI","Endoscopy","3D Reconstruction","Gaussian Splatting","Neural Rendering","Temporal Deformation"]},{"location":"applications/synthetic_aperture_radar/","title":"Streaming Synthetic Aperture Radar","text":"<p>     \u25b6 Run Locally  Authors: Dan Campbell (NVIDIA), Amanda Butler (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 4 - Experimental</p>","tags":["Signal Processing","Imaging","Backprojection","Synthetic Aperture Beamforming","Holoviz"]},{"location":"applications/synthetic_aperture_radar/#description","title":"Description","text":"<p>This application is a demonstration of using Holoscan to construct Synthetic Aperture Radar (SAR) imagery from a data collection.  In current form, the data is assumed to be precollected and contained in a particular binary format.  It has been tested with 2 versions of the publicly available GOTCHA volumetric SAR data collection.  Python-based converters are included to manipulate the public datasets into the binary format expected by the application.  The application implements Backprojection for image formation.</p>","tags":["Signal Processing","Imaging","Backprojection","Synthetic Aperture Beamforming","Holoviz"]},{"location":"applications/synthetic_aperture_radar/#requirements","title":"Requirements","text":"<ul> <li>Holoscan (&gt;=0.5)</li> <li>Python implementation:<ul> <li>Python3</li> <li>CuPy or Numpy</li> <li>Pillow</li> </ul> </li> <li>Scripts in <code>deploy/</code> will build and execute a docker environment that meets the requirements for systems using nvidia-docker</li> </ul>","tags":["Signal Processing","Imaging","Backprojection","Synthetic Aperture Beamforming","Holoviz"]},{"location":"applications/synthetic_aperture_radar/#obtain-and-format-gotcha-dataset","title":"Obtain and Format GOTCHA Dataset","text":"<ul> <li>Navigate to https://www.sdms.afrl.af.mil/index.php?collection=gotcha </li> <li>Click the DOWNLOAD link below the images</li> <li>Log in.  You may need to create an account to do so</li> <li>Under \"GOTCHA Volumetric SAR Data Set Challenge Problem\" download \"Disc 1 of 2\".<ul> <li>The data in \"Disc 2 of 2\" is compatible with this demo but not used</li> </ul> </li> <li>Unpack the contents of \"Disc 1 of 2\" into the <code>data/</code> directory.  This should create a subdirectry named <code>GOTCHA-CP_Disc1/</code></li> <li><code>cd data</code></li> <li><code>python3 cp-large_convert.py</code></li> <li>This should create a data file named <code>gotcha-cp-td-os.dat</code> that has a file size 2766987672 bytes, and a md5sum of 554b509c2d5c2c3de8e5643983a9748d</li> </ul>","tags":["Signal Processing","Imaging","Backprojection","Synthetic Aperture Beamforming","Holoviz"]},{"location":"applications/synthetic_aperture_radar/#build-and-use-docker-container-optional","title":"Build and Use Docker Container (Optional)","text":"<ul> <li>This demonstration is distributed with tools to build a docker container that meets the demonstration's system requirements.  This approach will only work properly with nvidia-docker</li> <li>From the demonstration root directory:</li> <li><code>cd deploy</code></li> <li><code>bash build_application_container.sh</code> - this will build the container</li> <li><code>bash run_application_container.sh</code> - this will launch a container that meets the demonstration system requirements</li> </ul>","tags":["Signal Processing","Imaging","Backprojection","Synthetic Aperture Beamforming","Holoviz"]},{"location":"applications/synthetic_aperture_radar/#build-and-execute","title":"Build and Execute","text":"<ul> <li>Python: <ul> <li><code>python3 holosar.py</code></li> </ul> </li> </ul> <p>The application will create a window with the resolved SAR image, and update after each group of 100 pulses received.  The image represents the strength of reflectivity at points on the ground within the imaging window.  The text at the top of the window indicates the (X,Y) position of the collecting radar at the most recent pulse, along with the total count of pulses received.  The red line points in the direction of the collection vehicle's location at the most recent pulse.  </p> <p>A screen grab is included below for reference:</p> <p></p>","tags":["Signal Processing","Imaging","Backprojection","Synthetic Aperture Beamforming","Holoviz"]},{"location":"applications/tao_peoplenet/","title":"TAO PeopleNet Detection Model on V4L2 Video Stream","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0, 1.0.3, 2.0.0, 2.1.0 Contribution metric: Level 2 - Trusted</p> <p>Use the TAO PeopleNet available on NGC to detect faces and people in a V4L2 supported video stream. HoloViz is used to draw bounding boxes around the detections.</p>","tags":["Computer Vision and Perception","Video","Detection","Holoviz"]},{"location":"applications/tao_peoplenet/#model","title":"Model","text":"<p>This application uses the TAO PeopleNet model from NGC for face and person classification. The model is downloaded when building the application.</p>","tags":["Computer Vision and Perception","Video","Detection","Holoviz"]},{"location":"applications/tao_peoplenet/#data","title":"Data","text":"<p>This application downloads a pre-recorded video from Pexels when the application is built for use with this application.  Please review the license terms from Pexels.</p> <p>NOTE: The user is responsible for checking if the dataset license is fit for the intended purpose.</p>","tags":["Computer Vision and Perception","Video","Detection","Holoviz"]},{"location":"applications/tao_peoplenet/#input","title":"Input","text":"<p>This app supports two different input options.  If you have a v4l2 compatible device plugged into your machine such as a webcam, you can run this application with option 1.  Otherwise you can run this application using a pre-recorded video with option 2.</p> <ol> <li>v4l2 compatible input device (default, see V4L2 Support below)</li> <li>pre-recorded video (see Video Replayer Support below)</li> </ol> <p>To see the list of v4l2 devices connected to your machine, install <code>v4l-utils</code> if it's not already installed:</p> <pre><code>sudo apt-get install v4l-utils\n</code></pre> <p>Then run:</p> <pre><code>v4l2-ctl --list-devices\n</code></pre>","tags":["Computer Vision and Perception","Video","Detection","Holoviz"]},{"location":"applications/tao_peoplenet/#run-instructions","title":"Run Instructions","text":"","tags":["Computer Vision and Perception","Video","Detection","Holoviz"]},{"location":"applications/tao_peoplenet/#v4l2-support","title":"V4L2 Support","text":"<p>This application supports v4l2 compatible devices as input.  To run this application with your v4l2 compatible device, please plug in your input device and run: <pre><code>./holohub run tao_peoplenet\n</code></pre></p> <p>By default, this application expects the input device to be mounted at <code>/dev/video0</code>.  If this is not the case, please update <code>applications/tao_peoplenet/tao_peoplenet.yaml</code> and set it to use the corresponding input device before running the application.  You can also override the default input device on the command line by running: <pre><code>./holohub run tao_peoplenet --run-args=\"--video_device /dev/video0\"\n</code></pre></p>","tags":["Computer Vision and Perception","Video","Detection","Holoviz"]},{"location":"applications/tao_peoplenet/#video-replayer-support","title":"Video Replayer Support","text":"<p>If you don't have a v4l2 compatible device plugged in, you may also run this application on a pre-recorded video. To launch the application using the Video Stream Replayer as the input source, run:</p> <pre><code>./holohub run tao_peoplenet --run-args=\"--source replayer\"\n</code></pre>","tags":["Computer Vision and Perception","Video","Detection","Holoviz"]},{"location":"applications/tao_peoplenet/#known-issues","title":"Known Issues","text":"<p>There is a known issue running this application on IGX w/ iGPU and on Jetson AGX (see #500). The workaround is to update the device to avoid picking up the libnvv4l2.so library.</p> <pre><code>cd /usr/lib/aarch64-linux-gnu/\nls -l libv4l2.so.0.0.999999\nsudo rm libv4l2.so.0.0.999999\nsudo ln -s libv4l2.so.0.0.0.0  libv4l2.so.0.0.999999\n</code></pre>","tags":["Computer Vision and Perception","Video","Detection","Holoviz"]},{"location":"applications/ultrasound_postprocessing/","title":"Ultrasound Post-Processing Filter Design","text":"<p>     \u25b6 Run Locally  Authors: Walter Simson (NVIDIA) Supported platforms: x86_64, aarch64 Language: python Last modified: January 21, 2026 Latest version: 0.1.0 Minimum Holoscan SDK version: 3.9.0 Tested Holoscan SDK versions: 3.9.0, 3.10.0 Contribution metric: Level 1 - Highly Reliable</p> <p>A collection of tools for developing and deploying realtime ultrasound post-processing filters.</p> <p></p>","tags":["Medical Imaging","Ultrasound","Filter","Designer","Operator","Real-Time","Academia","Industry","UFF","Beamform","Dynamic","Tools"]},{"location":"applications/ultrasound_postprocessing/#mission","title":"Mission","text":"<p>Enable the ultrasound community (academia and industry) to have reproducible, real-time post-processing.</p>","tags":["Medical Imaging","Ultrasound","Filter","Designer","Operator","Real-Time","Academia","Industry","UFF","Beamform","Dynamic","Tools"]},{"location":"applications/ultrasound_postprocessing/#overview","title":"Overview","text":"<p>This project includes an Ultrasound Post-Processing Filter Designer and light-weight library that enables you to design and run your ultrasound post-processing filter collection in Holoscan, all from a small and simple YAML config file.</p> <p>The Filter Designer enables you to create the YAML configs working on Ultrasound File Format (UFF) files (beamformed but not log compressed). You can load and filter your own data locally.</p> <p>The included Holoscan Runner tool enables you to run those configurations in real-time using the Holoscan SDK.</p>","tags":["Medical Imaging","Ultrasound","Filter","Designer","Operator","Real-Time","Academia","Industry","UFF","Beamform","Dynamic","Tools"]},{"location":"applications/ultrasound_postprocessing/#prerequisites","title":"Prerequisites","text":"","tags":["Medical Imaging","Ultrasound","Filter","Designer","Operator","Real-Time","Academia","Industry","UFF","Beamform","Dynamic","Tools"]},{"location":"applications/ultrasound_postprocessing/#hardware","title":"Hardware","text":"<ul> <li>Linux device with NVIDIA GPU</li> <li>Tested with x86_64 workstation + RTX Ampere 6000 GPU</li> </ul>","tags":["Medical Imaging","Ultrasound","Filter","Designer","Operator","Real-Time","Academia","Industry","UFF","Beamform","Dynamic","Tools"]},{"location":"applications/ultrasound_postprocessing/#software","title":"Software","text":"<ul> <li>CUDA Driver R580 or later</li> <li>See additional HoloHub CLI prerequisites here</li> </ul>","tags":["Medical Imaging","Ultrasound","Filter","Designer","Operator","Real-Time","Academia","Industry","UFF","Beamform","Dynamic","Tools"]},{"location":"applications/ultrasound_postprocessing/#quick-start","title":"Quick Start","text":"<p>First, clone this repository: <pre><code>git clone https://github.com/nvidia-holoscan/holohub.git\n</code></pre></p> <p>Then, run the containerized application.</p> <p>To run the filter designer webpage for assembling filter pipelines and tuning filter parameters: <pre><code>./holohub run ultrasound_postprocessing designer\n</code></pre></p> <p>To run a filter pipeline preset in real time with Holoscan SDK: <pre><code>./holohub run ultrasound_postprocessing realtime\n</code></pre></p>","tags":["Medical Imaging","Ultrasound","Filter","Designer","Operator","Real-Time","Academia","Industry","UFF","Beamform","Dynamic","Tools"]},{"location":"applications/ultrasound_postprocessing/#install-and-run-with-python-packaging","title":"Install and Run with Python Packaging","text":"<p>As an alternative to HoloHub CLI, you can instead design and run filters with project Python entrypoints.</p> <p>Install this project with your Python package manager: <pre><code>python3 -m pip install git+ssh://github.com/nvidia-holoscan/holohub.git@main#subdirectory=applications/ultrasound_postprocessing\n</code></pre></p> <p>Then run your application of choice: <pre><code>python3 -m streamlit run applications/ultrasound_postprocessing/ultra_post/app/streamlit_app.py\n</code></pre></p> <pre><code>python3 -m ultra_post.app.holoscan_app [--uff path/to/myfile.uff] [--fps 10]\n</code></pre> <p>The sample dataset used in this project is available at http://www.ustb.no/datasets.</p>","tags":["Medical Imaging","Ultrasound","Filter","Designer","Operator","Real-Time","Academia","Industry","UFF","Beamform","Dynamic","Tools"]},{"location":"applications/ultrasound_postprocessing/#key-concepts","title":"Key Concepts","text":"","tags":["Medical Imaging","Ultrasound","Filter","Designer","Operator","Real-Time","Academia","Industry","UFF","Beamform","Dynamic","Tools"]},{"location":"applications/ultrasound_postprocessing/#glossary","title":"Glossary","text":"<ul> <li>Filters are stateful, CuPy-based Python functions that operate on ultrasound images.</li> <li>The Filter Registry is a collection of pre-defined filters for use in apps such as the Filter Designer and in Holoscan SDK pipelines.</li> <li>Pipelines are a set of one or more filter objects assembled into a serialized graph or \"chain\".</li> <li>Presets are instructions for configuring a pipeline out of specific filters.</li> <li>Operators are Holoscan SDK \"nodes\" wrapping filter definitions for real-time serial execution.</li> </ul>","tags":["Medical Imaging","Ultrasound","Filter","Designer","Operator","Real-Time","Academia","Industry","UFF","Beamform","Dynamic","Tools"]},{"location":"applications/ultrasound_postprocessing/#development","title":"Development","text":"","tags":["Medical Imaging","Ultrasound","Filter","Designer","Operator","Real-Time","Academia","Industry","UFF","Beamform","Dynamic","Tools"]},{"location":"applications/ultrasound_postprocessing/#project-structure","title":"Project Structure","text":"<pre><code>ultrasound_postprocessing\n\u251c\u2500\u2500 CMakeLists.txt\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 external\n\u251c\u2500\u2500 metadata.json\n\u251c\u2500\u2500 plans\n\u251c\u2500\u2500 presets\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 tests\n\u251c\u2500\u2500 tools\n\u2514\u2500\u2500 ultra_post (Python module)\n</code></pre>","tags":["Medical Imaging","Ultrasound","Filter","Designer","Operator","Real-Time","Academia","Industry","UFF","Beamform","Dynamic","Tools"]},{"location":"applications/ultrasound_postprocessing/#review-existing-filters","title":"Review Existing Filters","text":"<pre><code>./holohub run ultrasound_postprocessing list-filters\n</code></pre>","tags":["Medical Imaging","Ultrasound","Filter","Designer","Operator","Real-Time","Academia","Industry","UFF","Beamform","Dynamic","Tools"]},{"location":"applications/ultrasound_postprocessing/#validate-a-preset","title":"Validate a preset","text":"<pre><code>./holohub run ultrasound_postprocessing validate &lt;presets/my-preset.yml&gt;\n</code></pre>","tags":["Medical Imaging","Ultrasound","Filter","Designer","Operator","Real-Time","Academia","Industry","UFF","Beamform","Dynamic","Tools"]},{"location":"applications/ultrasound_postprocessing/#run-tests","title":"Run Tests","text":"<pre><code>./holohub test ultrasound_postprocessing\n</code></pre>","tags":["Medical Imaging","Ultrasound","Filter","Designer","Operator","Real-Time","Academia","Industry","UFF","Beamform","Dynamic","Tools"]},{"location":"applications/ultrasound_postprocessing/#contributing","title":"Contributing","text":"<p>We welcome contributions that align with our mission to enable the ultrasound community to have reproducible, real-time post-processing.</p>","tags":["Medical Imaging","Ultrasound","Filter","Designer","Operator","Real-Time","Academia","Industry","UFF","Beamform","Dynamic","Tools"]},{"location":"applications/ultrasound_postprocessing/#ways-to-contribute","title":"Ways to Contribute","text":"<ol> <li>Presets: Create a processing pipeline preset (YAML) and save it to <code>presets/</code>.</li> <li>Filters:<ul> <li>Add your filter implementation to <code>ultra_post/filters/</code>.</li> <li>Register it in <code>ultra_post/filters/registry.py</code> (add to <code>FILTERS</code> and <code>DEFAULT_PARAMS</code>).</li> </ul> </li> <li>Improvements: Bug fixes and performance optimizations are highly encouraged.<ul> <li>Our goal is to have code that is easy to understand and highly performant.</li> </ul> </li> </ol> <p>Please see the HoloHub Contributing Guide for developer guidance.</p>","tags":["Medical Imaging","Ultrasound","Filter","Designer","Operator","Real-Time","Academia","Industry","UFF","Beamform","Dynamic","Tools"]},{"location":"applications/ultrasound_postprocessing/#code-style","title":"Code Style","text":"<ul> <li>Concise &amp; Approachable: We prioritize readability so new users can quickly understand the post-processing logic.</li> <li>Performant: We aim for code that is both expressive and performant, enabling real-time processing.</li> </ul>","tags":["Medical Imaging","Ultrasound","Filter","Designer","Operator","Real-Time","Academia","Industry","UFF","Beamform","Dynamic","Tools"]},{"location":"applications/ultrasound_postprocessing/#license","title":"License","text":"<p>This project is licensed under the Apache-2.0 License. See the HoloHub LICENSE file for details.</p>","tags":["Medical Imaging","Ultrasound","Filter","Designer","Operator","Real-Time","Academia","Industry","UFF","Beamform","Dynamic","Tools"]},{"location":"applications/ultrasound_postprocessing/#authors","title":"Authors","text":"<ul> <li>Walter Simson - NVIDIA Holoscan Team</li> </ul>","tags":["Medical Imaging","Ultrasound","Filter","Designer","Operator","Real-Time","Academia","Industry","UFF","Beamform","Dynamic","Tools"]},{"location":"applications/ultrasound_postprocessing/#acknowledgments","title":"Acknowledgments","text":"<ul> <li>NVIDIA Holoscan Team</li> <li>Open source community contributors</li> </ul>","tags":["Medical Imaging","Ultrasound","Filter","Designer","Operator","Real-Time","Academia","Industry","UFF","Beamform","Dynamic","Tools"]},{"location":"applications/ultrasound_postprocessing/#reference-data","title":"Reference Data","text":"<p>This project uses UFF sample data hosted by the Ultrasound Toolbox.</p> <p>H. Liebgott, A. Rodriguez-Molares, F. Cervenansky, J. A. Jensen and O. Bernard, \"Plane-Wave Imaging Challenge in Medical Ultrasound,\" 2016 IEEE International Ultrasonics Symposium (IUS), Tours, France, 2016, pp. 1-4, doi: 10.1109/ULTSYM.2016.7728908.</p>","tags":["Medical Imaging","Ultrasound","Filter","Designer","Operator","Real-Time","Academia","Industry","UFF","Beamform","Dynamic","Tools"]},{"location":"applications/ultrasound_segmentation/","title":"Ultrasound Bone Scoliosis Segmentation","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: February 4, 2026 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0, 3.10.0, 3.11.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This section describes the details of the ultrasound segmentation sample application as well as how to load a custom inference model into the application for some limited customization. Out of the box, the ultrasound segmentation application comes as a \"video replayer\" and \"AJA source\", where the user can replay a pre-recorded ultrasound video file included in the holoscan container or stream data from an AJA capture device directly through the GPU respectively.</p> <p>This application performs an automatic segmentation of the spine from a trained AI model for the purpose of scoliosis visualization and measurement.</p> <p>This application is available in C++ and Python API variants.</p> <p> Fig. 1 Spine segmentation of ultrasound data (NGC Resource)</p>","tags":["Healthcare AI","Video","Ultrasound","Segmentation","AJA","Visualization"]},{"location":"applications/ultrasound_segmentation/#video-stream-replayer-input","title":"Video Stream Replayer Input","text":"<p> Fig. 2 Segmentation application workflow with replay from file</p> <p>The pipeline uses a recorded ultrasound video file (generated by <code>convert_video_to_gxf_entities</code> script) for input frames. Each input frame in the file is loaded by Video Stream Replayer and passed to the following two branches: - In the first branch, the input frames are directly passed to Holoviz for rendering in the background. - In the second branch, the frames go through the Format Converter to convert the data type of the image from <code>uint8</code> to <code>float32</code> and resize the image before it is fed into the segmentation model using TensorRT Inference. The result is then ingested by the Segmentation Postprocessor which extracts the masks from the inference output, before Holoviz renders them as overlays.</p>","tags":["Healthcare AI","Video","Ultrasound","Segmentation","AJA","Visualization"]},{"location":"applications/ultrasound_segmentation/#aja-card-input","title":"AJA Card input","text":"<p> Fig. 3 Segmentation application workflow with input from AJA video source</p> <p>The pipeline is similar to the one using the recorded video, with the exceptions below: - the input source is replaced with AJA Source (pixel format is <code>RGBA8888</code> with a resolution of 1920x1080) - a Format Converter is added in the inference pipeline to convert from <code>RGBA8888</code> (note: could have updated the configuration of the next Format Converter when using AJA instead of adding another operator in the pipeline)</p>","tags":["Healthcare AI","Video","Ultrasound","Segmentation","AJA","Visualization"]},{"location":"applications/ultrasound_segmentation/#holoscan-sdk-version","title":"Holoscan SDK version","text":"<p>Ultrasound segmentation application in HoloHub required version 0.6 of the Holoscan SDK. If the Holoscan SDK version is 0.5 or lower, following code changes must be made in the application:</p> <ul> <li>In cpp/main.cpp: <code>#include &lt;holoscan/operators/inference/inference.hpp&gt;</code> is replaced with <code>#include &lt;holoscan/operators/multiai_inference/multiai_inference.hpp&gt;</code></li> <li>In cpp/main.cpp: <code>ops::InferenceOp</code> is replaced with <code>ops::MultiAIInferenceOp</code></li> <li>In cpp/CMakeLists.txt: update the holoscan SDK version from <code>0.6</code> to <code>0.5</code></li> <li>In cpp/CMakeLists.txt: <code>holoscan::ops::inference</code> is replaced with <code>holoscan::ops::multiai_inference</code></li> <li>In python/CMakeLists.txt: update the holoscan SDK version from <code>0.6</code> to <code>0.5</code></li> <li>In python/multiai_ultrasound.py: <code>InferenceOp</code> is replaced with <code>MultiAIInferenceOp</code></li> </ul>","tags":["Healthcare AI","Video","Ultrasound","Segmentation","AJA","Visualization"]},{"location":"applications/ultrasound_segmentation/cpp/","title":"Ultrasound Bone Scoliosis Segmentation","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ (see Python version) Last modified: February 4, 2026 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0, 3.10.0, 3.11.0 Contribution metric: Level 1 - Highly Reliable</p> <p>Full workflow including a generic visualization of segmentation results from a spinal scoliosis segmentation model of ultrasound videos. The model used is stateless, so this workflow could be configured to adapt to any vanilla DNN model. </p>","tags":["Healthcare AI","Video","Ultrasound","Segmentation","AJA","Visualization"]},{"location":"applications/ultrasound_segmentation/cpp/#requirements","title":"Requirements","text":"<p>The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the ultrasound data (replayer). Follow the setup instructions from the user guide to use the AJA capture card.</p>","tags":["Healthcare AI","Video","Ultrasound","Segmentation","AJA","Visualization"]},{"location":"applications/ultrasound_segmentation/cpp/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Bone Scoliosis Segmentation</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Healthcare AI","Video","Ultrasound","Segmentation","AJA","Visualization"]},{"location":"applications/ultrasound_segmentation/cpp/#build-and-run-instructions","title":"Build and Run Instructions","text":"<p>Please refer to the top level Holohub README.md for more information about the HoloHub CLI.</p>","tags":["Healthcare AI","Video","Ultrasound","Segmentation","AJA","Visualization"]},{"location":"applications/ultrasound_segmentation/cpp/#pre-recorded-video-input","title":"Pre-recorded Video Input","text":"<pre><code>./holohub run ultrasound_segmentation --language=cpp [--local]\n</code></pre>","tags":["Healthcare AI","Video","Ultrasound","Segmentation","AJA","Visualization"]},{"location":"applications/ultrasound_segmentation/cpp/#aja-capture-card-input","title":"AJA Capture Card Input","text":"<pre><code>sed -i -e 's#^source:.*#source: aja#' applications/ultrasound_segmentation/cpp/ultrasound_segmentation.yaml\n./holohub run ultrasound_segmentation --language=cpp [--local] \\\n    --configure-args=\"-DOP_aja_source:BOOL=ON\"\n</code></pre>","tags":["Healthcare AI","Video","Ultrasound","Segmentation","AJA","Visualization"]},{"location":"applications/ultrasound_segmentation/python/","title":"Ultrasound Bone Scoliosis Segmentation","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ (see C++ version) Last modified: February 4, 2026 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0, 3.10.0, 3.11.0 Contribution metric: Level 1 - Highly Reliable</p> <p>Full workflow including a generic visualization of segmentation results from a spinal scoliosis segmentation model of ultrasound videos. The model used is stateless, so this workflow could be configured to adapt to any vanilla DNN model. </p>","tags":["Healthcare AI","Video","Ultrasound","Segmentation","AJA","Visualization"]},{"location":"applications/ultrasound_segmentation/python/#requirements","title":"Requirements","text":"<ul> <li>Python 3.9+</li> <li>The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the ultrasound data (replayer). Follow the setup instructions from the user guide to use the AJA capture card.</li> </ul>","tags":["Healthcare AI","Video","Ultrasound","Segmentation","AJA","Visualization"]},{"location":"applications/ultrasound_segmentation/python/#data","title":"Data","text":"<p>\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Bone Scoliosis Segmentation</p> <p>The data is automatically downloaded and converted to the correct format when building the application. If you want to manually convert the video data, please refer to the instructions for using the convert_video_to_gxf_entities script.</p>","tags":["Healthcare AI","Video","Ultrasound","Segmentation","AJA","Visualization"]},{"location":"applications/ultrasound_segmentation/python/#build-and-run-instructions","title":"Build and Run Instructions","text":"<p>Please refer to the top level Holohub README.md for more information about the HoloHub CLI.</p>","tags":["Healthcare AI","Video","Ultrasound","Segmentation","AJA","Visualization"]},{"location":"applications/ultrasound_segmentation/python/#pre-recorded-video-input","title":"Pre-recorded Video Input","text":"<pre><code>./holohub run ultrasound_segmentation --language=python [--local]\n</code></pre>","tags":["Healthcare AI","Video","Ultrasound","Segmentation","AJA","Visualization"]},{"location":"applications/ultrasound_segmentation/python/#aja-capture-card-input","title":"AJA Capture Card Input","text":"<pre><code>./holohub run ultrasound_segmentation --language=python [--local] \\\n    --configure-args=\"-DOP_aja_source:BOOL=ON\" \\\n    --run-args=\"--source=aja\"\n</code></pre>","tags":["Healthcare AI","Video","Ultrasound","Segmentation","AJA","Visualization"]},{"location":"applications/velodyne_lidar_app/","title":"Velodyne VLP-16 Lidar Viewer","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA), nvMap Team (NVIDIA), nvMap Embedded Team (NVIDIA), Tom Birdsong (NVIDIA), Julien Jomier (NVIDIA), Jiahao Yin (NVIDIA), Marlene Wan (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 4 - Experimental</p> <p></p>","tags":["Signal Processing","Lidar","Point Cloud","UDP","Visualization"]},{"location":"applications/velodyne_lidar_app/#overview","title":"Overview","text":"<p>In this application we demonstrate how to use Holoscan SDK for low-latency lidar processing. We receive lidar packets from a Velodyne VLP-16 lidar sensor, convert packet information to a rolling Cartesian point cloud on GPU, then visualize the results with HoloViz.</p>","tags":["Signal Processing","Lidar","Point Cloud","UDP","Visualization"]},{"location":"applications/velodyne_lidar_app/#background","title":"Background","text":"<p>\"Lidar\" (LIght Detection And Ranging) is a technique by which \"light\", typically of wavelengths in the infrared spectrum, is used to determine the position of reflective surfaces surrounding a sensor. A 3D lidar sensor often employs a stacked vertical array of infrared laser emitters and sources that it spins rapidly. Similar to radar, the strength and timing of reflected lasers can be used to generate a 360-degree 3D point cloud view of the surrounding environment, with each point corresponding to an estimated point of reflection.</p> <p>For demonstration purposes we selected the Velodyne VLP-16 lidar sensor as our input source. We adapted existing packet processing code from NVIDIA DeepMap SDK into a custom Holoscan operator, <code>VelodyneLidarOp</code>, and connected it with the existing <code>BasicNetworkOp</code> and <code>HoloVizOp</code> operators to provide a complete viewing pipeline. We performed initial benchmarking on an NVIDIA IGX devkit.</p>","tags":["Signal Processing","Lidar","Point Cloud","UDP","Visualization"]},{"location":"applications/velodyne_lidar_app/#requirements","title":"Requirements","text":"<p>This application is intended to run on a Holoscan SDK support platform, namely a Linux x64 system or an NVIDIA IGX developer kit.</p> <p>To run the application you need a live or replayer source to stream Velodyne VLP-16 packet data to the application. That may be either: - A Velodyne VLP-16 lidar sensor. Review the VLP-16 user manual - A VLP-16 <code>.pcap</code> recording file and a packet replayer software.   - Visit Kitware's VeloView Velodyne Lidar collection for sample VLP-16 <code>.pcap</code> files.   - Visit the third party Wireshark wiki for a curated list of software options for generating traffic from <code>.pcap</code> files.</p>","tags":["Signal Processing","Lidar","Point Cloud","UDP","Visualization"]},{"location":"applications/velodyne_lidar_app/#running-the-application","title":"Running the Application","text":"<p>First, start your lidar stream source. If you are using a VLP-16 lidar sensor, review the VLP-16 user manual for instructions on how to properly set up your network configuration.</p> <p>Then, build and start the Holoscan lidar viewing application:</p> <pre><code>./holohub run velodyne_lidar_app\n</code></pre>","tags":["Signal Processing","Lidar","Point Cloud","UDP","Visualization"]},{"location":"applications/velodyne_lidar_app/#benchmarks","title":"Benchmarks","text":"<p>We performed benchmarking on an NVIDIA IGX developer kit with an A4000 GPU. (Note that an A6000 GPU is standard for IGX.) We used the holoscan_flow_benchmarking project to collect and summarize performance. The performance for each component in the Holoscan SDK pipeline is shown in the image below.</p> <p>Key statistics:</p> Minimum Latency 1.03 milliseconds Average Latency 1.12 milliseconds Maximum Latency 1.34 milliseconds <p>By comparison, the VLP-16 lidar publishes packets at a rate of approximately 1.33 milliseconds per packet.</p> <p></p> <p></p>","tags":["Signal Processing","Lidar","Point Cloud","UDP","Visualization"]},{"location":"applications/velodyne_lidar_app/#frequently-asked-questions-faq","title":"Frequently Asked Questions (FAQ)","text":"","tags":["Signal Processing","Lidar","Point Cloud","UDP","Visualization"]},{"location":"applications/velodyne_lidar_app/#how-does-the-application-work","title":"How does the application work?","text":"<p>The application flow is as follows:</p> <ol> <li>A UDP packet is emitted from the Velodyne VLP-16 lidar sensor and received on port 2368 in the Holoscan <code>BasicNetworkOp</code> operator.</li> <li>The packet payload is forwarded to the Holoscan <code>VelodyneLidarOp</code> operator. The operator decodes the packet according to the Velodyne lidar specification, where the VLP-16 packet defines 384 spherical points from laser firings. The operator converts the spherical points to Cartesian points on the GPU device and adds the resulting cloud to a rolling, accumulated point cloud.</li> <li>The Velodyne operator forwards the rolling point cloud to HoloViz, which renders the GPU point cloud to the screen.</li> </ol>","tags":["Signal Processing","Lidar","Point Cloud","UDP","Visualization"]},{"location":"applications/velodyne_lidar_app/#what-are-some-limitations-of-the-application","title":"What are some limitations of the application?","text":"<p>This application is intended as a simple demonstration of how a lidar sensor can be integrated for input to Holoscan SDK for low latency processing. It does not propose any novel features. Some limitations compared with more complete lidar solutions are: - No cloud filtering -- all zero-ranged points are kept in the buffer and visualized. - No advanced inference techniques -- the cloud is simply translated and visualized. - No RDMA -- VLP-16 lidar packets are received via the host ethernet interface on the IGX or x86_64 machine and then copied to the GPU device. - Monochrome visual -- HoloViz operator cloud support is currently limited to one color.</p> <p>Each of these limitations is merely a result of our scope of work, and could be overcome with additional attention.</p>","tags":["Signal Processing","Lidar","Point Cloud","UDP","Visualization"]},{"location":"applications/velodyne_lidar_app/#why-is-holoviz-not-responding","title":"Why is HoloViz not responding?","text":"<p>In most cases this indicates that the Holoscan application is not receiving UDP packets. There are several reasons that this could be the case: - The VLP-16 lidar sensor is not turned on, or the ethernet cable is disconnected.   The sensor typically takes approximately 30 seconds between powering on and transmitting packets. - The VLP-16 lidar sensor network interface is not properly configured to receive packets. You can use a tool   such as Wireshark to review live packets on the network interface. Review the VLP-16   user manual for troubleshooting. - The HoloHub application is not properly configured. Review the <code>lidar.yaml</code> configuration   and confirm that the port and IP address match the VLP-16 configuration.</p>","tags":["Signal Processing","Lidar","Point Cloud","UDP","Visualization"]},{"location":"applications/velodyne_lidar_app/#acknowledgements","title":"Acknowledgements","text":"<p>This operator was developed in part with support from the NVIDIA nvMap team and adapts portions of the NVIDIA DeepMap SDK.</p>","tags":["Signal Processing","Lidar","Point Cloud","UDP","Visualization"]},{"location":"applications/video_deidentification/","title":"Real-Time Face and Text Deidentification","text":"<p>     \u25b6 Run Locally  Authors: Wendell Hom (NVIDIA), Jonathan McLeod (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 2 - Trusted</p> <p>This sample application demonstrates the use of face and text detection models to do real-time video deidentification. Regions identified to be face or text are blurred out from the final image.</p> <p>NOTE: This application is a demonstration of real-time face and text deidentification and is not meant to be used in critical applications that has zero error tolerance.  The models used in this sample application have limitations, e.g., in detecting faces and text that are partially occluded, in low lighting situations, when there is motion blur, etc.</p>","tags":["Computer Vision and Perception","Video","Deidentification","Detection","Image Processing"]},{"location":"applications/video_deidentification/#models","title":"Models","text":"<p>This application uses TAO PeopleNet model from NGC for detecting faces. The model is downloaded when building the application.</p> <p>For text detection, this application uses EasyOCR python library which uses Character Region Awareness for Text Detection (CRAFT).</p>","tags":["Computer Vision and Perception","Video","Deidentification","Detection","Image Processing"]},{"location":"applications/video_deidentification/#data","title":"Data","text":"<p>This application downloads a pre-recorded video from Pexels when the application is built for use with this application.  Please review the license terms from Pexels.</p> <p>NOTE: The user is responsible for checking if the dataset license is fit for the intended purpose.</p>","tags":["Computer Vision and Perception","Video","Deidentification","Detection","Image Processing"]},{"location":"applications/video_deidentification/#input","title":"Input","text":"<p>This app currently supports three different input options:</p> <ol> <li>v4l2 compatible input device (default, see V4L2 Support below)</li> <li>pre-recorded video (see Video Replayer Support below)</li> </ol>","tags":["Computer Vision and Perception","Video","Deidentification","Detection","Image Processing"]},{"location":"applications/video_deidentification/#run-instructions","title":"Run Instructions","text":"","tags":["Computer Vision and Perception","Video","Deidentification","Detection","Image Processing"]},{"location":"applications/video_deidentification/#v4l2-support","title":"V4L2 Support","text":"<p>This application supports v4l2 compatible devices as input.  To run this application with your v4l2 compatible device, please plug in your input device and run: <pre><code>./holohub run video_deidentification\n</code></pre></p> <p>By default, this application expects the input device to be mounted at <code>/dev/video0</code>.  If this is not the case, please update <code>applications/video_deidentification/video_deidentification.yaml</code> and set it to use the corresponding input device before running the application.  You can also override the default input device on the command line by running: <pre><code>./holohub run video_deidentification --run-args=\"--video_device /dev/video0\"\n</code></pre></p>","tags":["Computer Vision and Perception","Video","Deidentification","Detection","Image Processing"]},{"location":"applications/video_deidentification/#video-replayer-support","title":"Video Replayer Support","text":"<p>If you don't have a v4l2 compatible device plugged in, you may also run this application on a pre-recorded video. To launch the application using the Video Stream Replayer as the input source, run:</p> <pre><code>./holohub run video_deidentification --run-args=\"--source replayer\"\n</code></pre>","tags":["Computer Vision and Perception","Video","Deidentification","Detection","Image Processing"]},{"location":"applications/video_deidentification/#known-issues","title":"Known Issues","text":"<p>There is a known issue running this application on IGX w/ iGPU and on Jetson AGX (see #500). The workaround is to update the device to avoid picking up the libnvv4l2.so library.</p> <pre><code>cd /usr/lib/aarch64-linux-gnu/\nls -l libv4l2.so.0.0.999999\nsudo rm libv4l2.so.0.0.999999\nsudo ln -s libv4l2.so.0.0.0.0  libv4l2.so.0.0.999999\n</code></pre>","tags":["Computer Vision and Perception","Video","Deidentification","Detection","Image Processing"]},{"location":"applications/video_streaming/","title":"Video Streaming Demo","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: November 3, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 3.5.0 Tested Holoscan SDK versions: 3.5.0 Contribution metric: Level 2 - Trusted</p> <p>This unified application demonstrates how to use the Holoscan SDK to create both streaming client and server applications for bidirectional video communication. This demo application demonstrates bidirectional video communication between client and server with real-time visualization.</p> <p> Fig. 1: Example of surgical video streaming with bidirectional communication showing the client receiving and displaying frames from the server.</p>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#overview","title":"Overview","text":"<p>The video streaming demo provides:</p> <ul> <li>Streaming Client: Captures video from V4L2 cameras or video files and streams to a server</li> <li>Streaming Server: Comprehensive server architecture with three main components:</li> <li>StreamingServerResource: Manages server connections and client lifecycle</li> <li>StreamingServerUpstreamOp: Receives video streams from clients</li> <li>StreamingServerDownstreamOp: Sends video frames back to clients (passthrough/echo mode)</li> <li>Bidirectional Communication: Both sending and receiving video frames</li> <li>Multiple Source Support: V4L2 cameras, video replay files</li> </ul>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#requirements","title":"Requirements","text":"<ul> <li>NVIDIA GPU</li> <li>CUDA 12.x (currently not working with CUDA 13.x)</li> <li>Holoscan SDK 3.5.0 or higher</li> <li>V4L2 camera (optional, for live streaming)</li> </ul>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#client-dependencies","title":"Client Dependencies","text":"<p>Download the client streaming binaries from NGC:</p> <pre><code># Navigate to the client operator directory from the holohub root directory\ncd operators/video_streaming/video_streaming_client\n\n# Download using NGC CLI\nngc registry resource download-version \"nvidia/holoscan_client_cloud_streaming:0.2\"\nunzip -o holoscan_client_cloud_streaming_v0.2/holoscan_client_cloud_streaming.zip -d holoscan_client_cloud_streaming\n\n# Clean up\nrm -rf holoscan_client_cloud_streaming_v0.2\ncd - # Return to the original directory\n</code></pre>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#server-dependencies","title":"Server Dependencies","text":"<p>Download the server streaming binaries from NGC:</p> <pre><code># Navigate to the server operator directory from the holohub root directory\ncd operators/video_streaming/video_streaming_server\n\n# Download using NGC CLI\nngc registry resource download-version \"nvidia/holoscan_server_cloud_streaming:0.2\"\nunzip -o holoscan_server_cloud_streaming_v0.2/holoscan_server_cloud_streaming.zip -d holoscan_server_cloud_streaming\n\n# Clean up\nrm -rf holoscan_server_cloud_streaming_v0.2\ncd - # Return to the original directory\n</code></pre>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#running-the-applications","title":"Running the Applications","text":"<p>The unified application provides both client and server applications.</p> <p>\u26a0\ufe0f Important: These applications are currently only compatible with CUDA 12.x. If your system uses CUDA 13.x, ensure you add the <code>--cuda 12</code> flag to all command-line invocations shown below.</p> <p>\u2139\ufe0f The client requires OpenSSL 3.4.0, which is installed inside the custom Dockerfile.</p>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#1-start-the-streaming-server","title":"1. Start the Streaming Server","text":"<p>You can start the server in either C++ or Python mode.</p> <p>C++ Server:</p> <pre><code>./holohub run video_streaming_server --language cpp\n</code></pre> <p>Python Server:</p> <pre><code>./holohub run video_streaming_server --language python\n</code></pre>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#2-start-the-streaming-client-in-another-terminal","title":"2. Start the Streaming Client (in another terminal)","text":"<p>You can start the client in either C++ or Python mode regardless of the server language.</p>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#option-a-v4l2-camera-webcam","title":"Option A: V4L2 Camera (Webcam)","text":"<p>It uses <code>video_streaming_client_demo.yaml</code> and captures video from webcam with 640x480 resolution.</p> <ul> <li> <p>C++ V4L2 Camera Client:</p> <pre><code>./holohub run video_streaming_client v4l2 --language cpp\n</code></pre> </li> <li> <p>Python V4L2 Camera Client:</p> <pre><code>./holohub run video_streaming_client v4l2 --language python\n</code></pre> </li> </ul>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#option-b-video-replayer","title":"Option B: Video Replayer","text":"<p>It uses <code>video_streaming_client_demo_replayer.yaml</code> and replays a pre-recorded video file with 854x480 resolution.</p> <ul> <li>C++ Video Replayer Client:</li> </ul> <pre><code>./holohub run video_streaming_client replayer --language cpp\n</code></pre> <ul> <li>Python Video Replayer Client:</li> </ul> <pre><code>./holohub run video_streaming_client replayer --language python\n</code></pre> <p>Note: - The video streaming server and client are cross-language compatible. You can start the server in one language (C++ or Python) and then connect with a client running in either language. - You can switch between the client modes (replayer or V4L2 camera) at any time without restarting the server\u2014just stop one client and start the other. The server automatically manages client connections.</p>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#python-bindings","title":"Python Bindings","text":"<p>The Python applications use these Holoscan operator bindings:</p> <p>Server Components:</p> <ul> <li><code>holohub.video_streaming_server.StreamingServerResource</code> - Manages server connections</li> <li><code>holohub.video_streaming_server.StreamingServerUpstreamOp</code> - Receives frames from clients</li> <li><code>holohub.video_streaming_server.StreamingServerDownstreamOp</code> - Sends frames to clients</li> </ul> <p>Client Components:</p> <ul> <li><code>holohub.video_streaming_client.VideoStreamingClientOp</code> - Bidirectional client streaming</li> </ul> <p>Holoscan Core Operators:</p> <ul> <li><code>holoscan.operators.VideoStreamReplayerOp</code> - Video file playback</li> <li><code>holoscan.operators.V4L2VideoCaptureOp</code> - Webcam capture</li> <li><code>holoscan.operators.FormatConverterOp</code> - Format conversion (RGBA\u2192RGB\u2192BGR)</li> <li><code>holoscan.operators.HolovizOp</code> - Visualization</li> </ul>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#python-implementation-overview","title":"Python Implementation Overview","text":"<p>Server Architecture:</p> <ul> <li><code>StreamingServerResource</code> manages streaming connections and client lifecycle</li> <li><code>StreamingServerUpstreamOp</code> receives frames from clients (output port: <code>output_frames</code>)</li> <li><code>StreamingServerDownstreamOp</code> sends frames to clients (input port: <code>input_frames</code>)</li> <li>Simple pipeline: <code>upstream_op \u2192 downstream_op</code> (passthrough/echo mode)</li> </ul> <p>Client Architecture:</p> <ul> <li>Video source: <code>VideoStreamReplayerOp</code> or <code>V4L2VideoCaptureOp</code></li> <li><code>FormatConverterOp</code> handles format conversion (RGBA\u2192RGB\u2192BGR)</li> <li><code>VideoStreamingClientOp</code> manages bidirectional streaming (send and receive)</li> <li><code>HolovizOp</code> visualizes received frames</li> </ul> <p>For complete Python implementation examples and code, see:</p> <ul> <li>Server README - Full Python server implementation</li> <li>Client README - Full Python client implementation (replayer and V4L2 modes)</li> </ul>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#command-line-options-python","title":"Command Line Options (Python)","text":"<p>Server Options:</p> <ul> <li><code>--port PORT</code>: Server port (default: 48010)</li> <li><code>--width WIDTH</code>: Frame width (default: 854)</li> <li><code>--height HEIGHT</code>: Frame height (default: 480)</li> <li><code>--fps FPS</code>: Frames per second (default: 30)</li> <li><code>--config PATH</code> or <code>-c PATH</code>: Path to YAML configuration file</li> <li><code>--create-config PATH</code>: Create default configuration file at specified path</li> <li><code>--help</code>: Show help message</li> </ul> <p>Client Options:</p> <ul> <li><code>--source {replayer,v4l2}</code>: Video source type (default: replayer)</li> <li><code>--server-ip IP</code>: Server IP address (default: 127.0.0.1)</li> <li><code>--port PORT</code>: Server port (default: 48010)</li> <li><code>--width WIDTH</code>: Frame width (default: 854 for replayer, 640 for v4l2)</li> <li><code>--height HEIGHT</code>: Frame height (default: 480)</li> <li><code>--fps FPS</code>: Frames per second (default: 30)</li> <li><code>--config PATH</code> or <code>-c PATH</code>: Path to YAML configuration file</li> <li><code>--help</code>: Show help message</li> </ul>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#compatibility","title":"Compatibility","text":"<ul> <li>\u2705 Python server \u2194 C++ client - Fully compatible and tested</li> <li>\u2705 Python client \u2194 C++ server - Fully compatible and tested</li> <li>\u2705 Python server \u2194 Python client - Fully compatible and tested</li> <li>\u2705 C++ server \u2194 C++ client - Fully compatible and tested</li> <li>\u2705 All combinations are fully supported - Mix and match as needed</li> </ul>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#cross-language-compatibility-testing","title":"Cross-Language Compatibility Testing","text":"<p>Python clients are fully compatible with C++ servers and vice versa:</p> <p>Terminal 1 - C++ Server:</p> <pre><code>./holohub run video_streaming\n</code></pre> <p>Terminal 2 - Python Client:</p> <pre><code>./holohub run video_streaming client_python\n</code></pre>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#python-troubleshooting","title":"Python Troubleshooting","text":"<p>Import Error:</p> <ul> <li>Ensure Holoscan SDK Python bindings are installed</li> <li>Verify build with: <code>./holohub build video_streaming --configure-args='-DHOLOHUB_BUILD_PYTHON=ON'</code></li> </ul> <p>Camera Not Found:</p> <ul> <li>Check V4L2 device path: <code>ls -l /dev/video*</code></li> <li>Test camera: <code>v4l2-ctl --device=/dev/video0 --info</code></li> </ul> <p>Connection Failed:</p> <ul> <li>Verify server is running and ports are correct</li> <li>Check: <code>netstat -tlnp | grep 48010</code></li> </ul> <p>Video Files Not Found:</p> <ul> <li>Check data directory path: <code>/workspace/holohub/data/endoscopy/</code></li> <li>Ensure video files exist in the data directory</li> </ul> <p>Resolution Mismatch:</p> <ul> <li>Replayer default: 854x480</li> <li>V4L2 default: 640x480</li> <li>Server default: 854x480</li> <li>Ensure client and server resolutions match</li> </ul>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#configuration-files","title":"Configuration Files","text":"<p>Python Server: <code>video_streaming_server/python/video_streaming_server_demo.yaml</code> Python Client (Replayer): <code>video_streaming_client/python/video_streaming_client_demo_replayer.yaml</code> Python Client (V4L2): <code>video_streaming_client/python/video_streaming_client_demo.yaml</code></p>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#detailed-documentation","title":"Detailed Documentation","text":"<p>For complete implementation details, see the component-specific READMEs:</p> <ul> <li>Server README - Complete server documentation (C++ and Python)</li> <li>Client README - Complete client documentation (C++ and Python)</li> </ul>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#command-line-options","title":"Command Line Options","text":"","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#server-options","title":"Server Options","text":"<ul> <li><code>-h, --help</code>: Show help message</li> <li><code>-c, --config &lt;file&gt;</code>: Configuration file path (default: video_streaming_server_demo.yaml)</li> <li><code>-d, --data &lt;directory&gt;</code>: Data directory for video files</li> </ul>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#client-options","title":"Client Options","text":"<ul> <li><code>-h, --help</code>: Show help message</li> <li><code>-c, --config &lt;file&gt;</code>: Configuration file path (default: video_streaming_client_demo.yaml)</li> <li><code>-d, --data &lt;directory&gt;</code>: Data directory for video files</li> </ul> <p>Note: Video source type (V4L2 vs replayer) is configured in the YAML file, not via command line arguments.</p>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#camera-setup-and-testing","title":"Camera Setup and Testing","text":"<p>\ud83d\udcd6 For detailed camera configuration and troubleshooting, see the Client Operator README which includes advanced v4l2-ctl commands, YAML configuration examples, and camera-specific settings.</p>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#testing-your-v4l2-camera","title":"Testing Your V4L2 Camera","text":"<p>Before starting the streaming client with camera input:</p> <pre><code># Check available video devices\nls -la /dev/video*\n\n# Get camera information\nv4l2-ctl --device=/dev/video0 --info\n\n# Test camera with recommended resolution\nv4l2-ctl --device=/dev/video0 --set-fmt-video=width=1280,height=720,pixelformat=MJPG --stream-mmap --stream-count=10\n\n# List supported formats\nv4l2-ctl --device=/dev/video0 --list-formats-ext\n</code></pre>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#recommended-resolution-settings","title":"Recommended Resolution Settings","text":"<p>For V4L2 cameras (like Logitech C920):</p> <ul> <li>1280x720 @ 30fps - Best balance of quality and performance</li> <li>1920x1080 @ 30fps - High quality streaming (if supported)</li> <li>854x480 @ 30fps - Default, good for testing and lower bandwidth</li> </ul> <p>Important: Ensure both client and server use matching resolution settings for optimal performance.</p>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#troubleshooting","title":"Troubleshooting","text":"","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#camera-issues","title":"Camera Issues","text":"<ul> <li>Camera not detected:</li> </ul> <pre><code>sudo usermod -a -G video $USER\n# Log out and back in, then test again\n</code></pre> <ul> <li>Permission denied:</li> </ul> <pre><code>sudo chmod 666 /dev/video0\n</code></pre>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#performance-issues","title":"Performance Issues","text":"<ul> <li>Poor streaming quality:</li> <li>Try lower resolution (854x480 or 640x480)</li> <li>Reduce frame rate to 15 or 24 FPS</li> <li>Ensure client and server resolutions match</li> </ul>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#connection-issues","title":"Connection Issues","text":"<ul> <li>Server not starting:</li> </ul> <pre><code># Check if port is already in use\nnetstat -tlnp | grep 48010\n\n# Kill existing process if needed\nsudo lsof -ti:48010 | xargs sudo kill -9\n</code></pre> <ul> <li>Client connection timeout:</li> <li>Verify server is running first</li> <li>Check firewall settings for port 48010</li> <li>Ensure server_ip and port match in both configurations</li> </ul>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#video-replayer-issues","title":"Video Replayer Issues","text":"<ul> <li>Config file not found:</li> </ul> <pre><code># Ensure the replayer config exists in build directory\ncp applications/video_streaming/video_streaming_client/cpp/video_streaming_client_demo_replayer.yaml build/video_streaming_client_demo/\n</code></pre> <ul> <li>Format converter errors:</li> <li><code>Invalid channel count for RGBA8888 3 != 4</code>: Video replayer outputs RGB888 (3 channels), not RGBA8888 (4 channels)</li> <li> <p>Solution: Use <code>video_streaming_client_demo_replayer.yaml</code> which has correct format converter settings</p> </li> <li> <p>Resolution mismatch:</p> </li> <li>Video file is 854x480, ensure all components use matching resolution</li> <li>Check <code>video_streaming_client</code>, <code>holoviz</code>, and <code>format_converter</code> settings</li> </ul>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#expected-behavior-and-logs","title":"Expected Behavior and Logs","text":"<p>Client Application: The streaming client may show <code>GXF_EXCEEDING_PREALLOCATED_SIZE</code> errors during BGR\u2192BGRA conversion. This is expected behavior as the operators handle dynamic buffer allocation internally.</p> <p>Server Application: The server should display connection status and frame processing information. Look for messages about client connections and frame throughput.</p> <p>Successful Video Replayer Logs:</p> <pre><code>[info] Source set to: replayer\n[info] Using video replayer as source\n[info] Connection established successfully\n[info] Tensor validation passed: 480x854x3, 1229760 bytes\n[info] Frame sent successfully\n</code></pre>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#integration-testing","title":"Integration Testing","text":"<p>The video streaming demo includes comprehensive integration testing for both C++ and Python implementations.</p>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#quick-start","title":"Quick Start","text":"<pre><code>./holohub test video_streaming\n</code></pre> <p>You can use <code>--verbose</code> flag to get more detailed output.</p>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#run-specific-tests","title":"Run specific tests","text":"<pre><code># Run C++ integration test\n./applications/video_streaming/integration_test_cpp.sh\n\n# Run Python integration test\n./applications/video_streaming/integration_test_python.sh\n</code></pre> <p>Or use direct HoloHub CLI commands:</p> <pre><code># Run C++ integration test using HoloHub CLI\n./holohub test video_streaming \\\n  --ctest-options=\"-R video_streaming_integration_test_cpp\"\n\n# Run Python integration test using HoloHub CLI\n./holohub test video_streaming \\\n  --ctest-options=\"-R video_streaming_integration_test_python\"\n</code></pre> <p>Test Scripts:</p> <ul> <li><code>integration_test_cpp.sh</code> - C++ server and client test (SDK 3.5.0)</li> <li><code>integration_test_python.sh</code> - Python server and client test (SDK 3.6.0)</li> </ul> <p>\u26a0\ufe0f Important: Both scripts run in Docker and build from committed source code. Commit your changes before running tests.</p> <p>For complete testing documentation, including expected outputs, verification criteria, and troubleshooting, see TESTING.md.</p>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#operator-documentation","title":"Operator Documentation","text":"<p>For detailed information about the underlying video streaming operators used in this application, see:</p> <p>\ud83d\udccb Video Streaming Operators - Complete operator documentation</p> <p>The operator documentation includes:</p> <ul> <li>Client Components: VideoStreamingClientOp, FrameSaverOp</li> <li>Server Components: StreamingServerResource, StreamingServerUpstreamOp, StreamingServerDownstreamOp</li> <li>Parameters and Configuration: Detailed parameter descriptions and usage examples</li> <li>Testing Documentation: Comprehensive test suite with 40+ tests passing</li> <li>API Reference: Complete API documentation for all components</li> </ul>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/#performance-notes","title":"Performance Notes","text":"<ul> <li>GPU Memory: Configure appropriate allocator block sizes for your resolution</li> <li>Network Bandwidth: Monitor bandwidth usage for remote streaming scenarios  </li> <li>Frame Rate: Higher frame rates require more GPU/CPU resources</li> <li>Resolution: Balance between quality and performance based on your hardware</li> </ul>","tags":["Video","Streaming","Client","Server","Demo","V4L2","Bidirectional"]},{"location":"applications/video_streaming/video_streaming_client/","title":"Video Streaming Client Demo","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: November 3, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 3.5.0 Tested Holoscan SDK versions: 3.5.0 Contribution metric: Level 2 - Trusted</p> <p>This application demonstrates how to create a bidirectional video streaming client that sends video frames to a server and receives frames back. Both C++ and Python implementations are available with support for V4L2 cameras and video file replay.</p> <p>\ud83d\udcda Related Documentation:</p> <ul> <li>Main README - Application overview, quick start, and common configuration</li> <li>Server README - Server setup and configuration</li> <li>Testing Documentation - Integration testing and verification</li> </ul>","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_client/#features","title":"Features","text":"<ul> <li>Multiple Video Sources:</li> <li>V4L2 Camera (webcam) support with configurable resolution</li> <li>Video file replay for testing and demos</li> <li>Real-time Visualization: Holoviz integration for displaying received frames</li> <li>Configurable: YAML configuration file support and command-line options</li> <li>C++ and Python: Full implementations in both languages with compatible APIs</li> <li>Format Conversion: Automatic format conversion for different video sources</li> </ul>","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_client/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK 3.5.0 or higher</li> <li>Custom Dockerfile with OpenSSL 3.4.0 (for running via holohub CLI)</li> <li>For Python: Python 3.8+ and bindings built with <code>-DHOLOHUB_BUILD_PYTHON=ON</code></li> <li>CUDA 12.x (currently not working with CUDA 13.x)</li> <li>OpenCV</li> <li><code>video_streaming_client</code> operator</li> <li>V4L2 camera (optional, for live streaming)</li> </ul>","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_client/#download-client-cloud-streaming","title":"Download Client Cloud Streaming","text":"<p>Download the Holoscan Client Cloud Streaming binaries from NGC:</p> <pre><code># Navigate to the client operator directory from the holohub root directory\ncd operators/video_streaming/video_streaming_client\n\n# Download using NGC CLI\nngc registry resource download-version \"nvidia/holoscan_client_cloud_streaming:0.2\"\nunzip -o holoscan_client_cloud_streaming_v0.2/holoscan_client_cloud_streaming.zip -d holoscan_client_cloud_streaming\n\n# Clean up\nrm -rf holoscan_client_cloud_streaming_v0.2\ncd - # Return to the original directory\n</code></pre>","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_client/#usage","title":"Usage","text":"<p>\u26a0\ufe0f Important: This application is currently only compatible with CUDA 12.x. If your system uses CUDA 13.x, ensure you add the <code>--cuda 12</code> flag to all command-line invocations shown below.</p> <p>\u2139\ufe0f The client requires OpenSSL 3.4.0, which is installed inside the custom Dockerfile.</p>","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_client/#c-client","title":"C++ Client","text":"<p>Video Replayer Mode (Default - 854x480):</p> <pre><code># From holohub root directory - runs with video file playback\n./holohub run video_streaming_client replayer --language cpp\n</code></pre> <p>V4L2 Camera Mode (640x480):</p> <pre><code># From holohub root directory - runs with V4L2 camera (webcam)\n./holohub run video_streaming_client v4l2 --language cpp\n</code></pre>","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_client/#python-client","title":"Python Client","text":"<p>Video Replayer Mode (Default - 854x480):</p> <pre><code># From holohub root directory - runs with video file playback\n./holohub run video_streaming_client replayer --language python\n</code></pre> <p>V4L2 Camera Mode (640x480):</p> <pre><code># From holohub root directory - runs with V4L2 camera (webcam)\n./holohub run video_streaming_client v4l2 --language python\n</code></pre> <p>Default Client Configurations:</p> <p>Video Replayer Mode:</p> <ul> <li>Source: Video file (surgical_video)</li> <li>Resolution: 854x480</li> <li>Frame Rate: 30 fps</li> <li>Server: 127.0.0.1:48010</li> </ul> <p>V4L2 Camera Mode:</p> <ul> <li>Source: /dev/video0 (webcam)</li> <li>Resolution: 640x480</li> <li>Frame Rate: 30 fps</li> <li>Server: 127.0.0.1:48010</li> </ul> <p>Important: Ensure the server is configured to match the client's resolution for optimal performance.</p>","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_client/#command-line-options","title":"Command Line Options","text":"<p>Python Client:</p> <ul> <li><code>--source {replayer,v4l2}</code>: Video source type (default: replayer)</li> <li><code>--server-ip IP</code>: Server IP address (default: 127.0.0.1)</li> <li><code>--port PORT</code>: Server port (default: 48010)</li> <li><code>--width WIDTH</code>: Frame width (default: 854 for replayer, 640 for v4l2)</li> <li><code>--height HEIGHT</code>: Frame height (default: 480)</li> <li><code>--fps FPS</code>: Frames per second (default: 30)</li> <li><code>--no-viz</code>: Disable visualization (Holoviz)</li> <li><code>--config PATH</code> or <code>-c PATH</code>: Path to YAML configuration file</li> <li><code>--create-config PATH</code>: Create default configuration file at specified path</li> <li><code>--help</code>: Show help message</li> </ul> <p>C++ Client:</p> <ul> <li><code>-c PATH</code> or <code>--config PATH</code>: Path to YAML configuration file</li> <li><code>-d PATH</code> or <code>--data PATH</code>: Data directory path (for video files)</li> <li><code>-h</code> or <code>--help</code>: Show help message</li> </ul>","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_client/#configuration","title":"Configuration","text":"","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_client/#c-configuration","title":"C++ Configuration","text":"<p>The C++ application is configured via YAML file. Configuration varies based on video source:</p> <p>Video Replayer Configuration (<code>cpp/video_streaming_client_demo_replayer.yaml</code>):</p> <pre><code>%YAML 1.2\n---\napplication:\n  title: Streaming Client Test App\n  version: 1.0\n  log_level: INFO\n\n# Source configuration\nsource: \"replayer\"\n\n# Video replayer configuration\nreplayer:\n  directory: \"/workspace/holohub/data/endoscopy\"\n  basename: \"surgical_video\"\n  frame_rate: 30\n  repeat: true\n  realtime: true\n  count: 0\n\n# Format converter - replayer outputs RGB888 (3 channels)\nformat_converter:\n  in_dtype: \"rgb888\"\n  out_dtype: \"rgb888\"\n  out_tensor_name: tensor\n  scale_min: 0.0\n  scale_max: 255.0\n  out_channel_order: [2, 1, 0]  # Convert RGB to BGR\n\n# Streaming client settings\nvideo_streaming_client:\n  width: 854\n  height: 480\n  fps: 30\n  server_ip: \"127.0.0.1\"\n  signaling_port: 48010\n  send_frames: true\n  receive_frames: true\n  min_non_zero_bytes: 10\n\n# Visualization\nvisualize_frames: true\n\nholoviz:\n  width: 854\n  height: 480\n  tensors:\n    - name: \"bgra_tensor\"\n      type: color\n      image_format: \"b8g8r8a8_unorm\"\n      opacity: 1.0\n      priority: 0\n\n# Buffer pool configuration\nallocator:\n  block_size: 4194304\n  num_blocks: 12\n\nscheduler: \"greedy\"\n</code></pre> <p>V4L2 Camera Configuration (<code>cpp/video_streaming_client_demo.yaml</code>):</p> <pre><code>source: \"v4l2\"\n\n# V4L2 camera configuration\nv4l2_source:\n  device: \"/dev/video0\"\n  width: 640\n  height: 480\n  frame_rate: 30\n  pixel_format: \"YUYV\"\n\n# Format converter - V4L2 outputs RGBA8888 (4 channels)\nformat_converter:\n  in_dtype: \"rgba8888\"\n  out_dtype: \"rgb888\"\n  out_tensor_name: tensor\n  scale_min: 0.0\n  scale_max: 255.0\n  out_channel_order: [2, 1, 0]\n\n# Streaming client settings\nvideo_streaming_client:\n  width: 640\n  height: 480\n  fps: 30\n  server_ip: \"127.0.0.1\"\n  signaling_port: 48010\n  send_frames: true\n  receive_frames: true\n  min_non_zero_bytes: 10\n</code></pre> <p>Key Configuration Notes:</p> <ul> <li>Format Converter: V4L2 outputs <code>rgba8888</code> (4 channels), video replayer outputs <code>rgb888</code> (3 channels)</li> <li>Channel Order: <code>out_channel_order: [2, 1, 0]</code> converts RGB to BGR</li> <li>Resolution: V4L2 default is 640x480, video replayer is 854x480</li> <li>Allocator: Buffer pool sized for BGRA frames (4MB blocks)</li> </ul>","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_client/#python-configuration","title":"Python Configuration","text":"<p>The Python application is primarily configured via command-line arguments:</p> <p>Command-Line Parameters (recommended):</p> <pre><code># Video replayer mode (854x480)\npython3 video_streaming_client_demo.py --source replayer --width 854 --height 480\n\n# V4L2 camera mode (640x480)\npython3 video_streaming_client_demo.py --source v4l2 --width 640 --height 480\n\n# Custom server\npython3 video_streaming_client_demo.py --server-ip 192.168.1.100 --port 48010\n</code></pre> <p>Python YAML Structure (optional, auto-selected based on source):</p> <pre><code>application:\n  title: \"Streaming Client Python Demo\"\n  version: \"1.0\"\n  log_level: \"INFO\"\n\n# Source configuration\nsource: \"replayer\"  # or \"v4l2\"\n\n# Streaming client settings\nvideo_streaming_client:\n  width: 854\n  height: 480\n  fps: 30\n  server_ip: \"127.0.0.1\"\n  signaling_port: 48010\n  send_frames: true\n  receive_frames: true\n  min_non_zero_bytes: 10\n\n# Video replayer settings\nreplayer:\n  directory: \"/workspace/holohub/data/endoscopy\"\n  basename: \"surgical_video\"\n  frame_rate: 30\n  repeat: true\n  realtime: true\n\n# V4L2 camera settings\nv4l2:\n  device: \"/dev/video0\"\n  width: 640\n  height: 480\n  frame_rate: 30\n  pixel_format: \"YUYV\"\n\n# Visualization settings\nvisualization:\n  enabled: true\n  width: 854\n  height: 480\n</code></pre> <p>Configuration Files:</p> <ul> <li>C++ V4L2: <code>cpp/video_streaming_client_demo.yaml</code></li> <li>C++ Replayer: <code>cpp/video_streaming_client_demo_replayer.yaml</code></li> <li>Python V4L2: <code>python/video_streaming_client_demo.yaml</code></li> <li>Python Replayer: <code>python/video_streaming_client_demo_replayer.yaml</code></li> </ul> <p>Note: Python parameters set via command-line take precedence over YAML configuration. The Python app auto-selects the appropriate config file based on the <code>--source</code> argument.</p>","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_client/#pipeline-architecture","title":"Pipeline Architecture","text":"<p>The client implements a bidirectional streaming pipeline with format conversion:</p> <p>Video Replayer Pipeline:</p> <pre><code>VideoStreamReplayerOp \u2192 FormatConverterOp \u2192 VideoStreamingClientOp \u2192 HoloVizOp\n                                                    \u2193\n                                            (sends to server)\n                                                    \u2193\n                                            (receives from server)\n                                                    \u2193\n                                            output_frames \u2192 HoloVizOp\n</code></pre> <p>V4L2 Camera Pipeline:</p> <pre><code>V4L2VideoCaptureOp \u2192 FormatConverterOp \u2192 VideoStreamingClientOp \u2192 HoloVizOp\n                                                 \u2193\n                                         (sends to server)\n                                                 \u2193\n                                         (receives from server)\n                                                 \u2193\n                                         output_frames \u2192 HoloVizOp\n</code></pre>","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_client/#how-it-works","title":"How It Works","text":"<ol> <li>Video Source: Either <code>VideoStreamReplayerOp</code> (file) or <code>V4L2VideoCaptureOp</code> (camera) provides video frames</li> <li>Format Converter: Converts video format to RGB888 for streaming</li> <li>VideoStreamingClientOp: Sends frames to server and receives processed frames back</li> <li>HoloVizOp: Displays the received frames from the server</li> </ol>","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_client/#c-implementation","title":"C++ Implementation","text":"<p>The C++ implementation (<code>cpp/video_streaming_client_demo.cpp</code>) demonstrates usage of the streaming client operator:</p> <p>Video Replayer Mode:</p> <pre><code>#include \"video_streaming_client.hpp\"\n#include &lt;holoscan/operators/format_converter/format_converter.hpp&gt;\n#include &lt;holoscan/operators/holoviz/holoviz.hpp&gt;\n#include &lt;holoscan/operators/video_stream_replayer/video_stream_replayer.hpp&gt;\n\n// Create video source (replayer)\nauto replayer = make_operator&lt;ops::VideoStreamReplayerOp&gt;(\n    \"replayer\",\n    Arg(\"directory\", std::string(\"/workspace/holohub/data/endoscopy\")),\n    Arg(\"basename\", std::string(\"surgical_video\")),\n    Arg(\"frame_rate\", 30.0f)\n);\n\n// Create format converter (RGB to BGR)\nauto format_converter = make_operator&lt;ops::FormatConverterOp&gt;(\n    \"format_converter\",\n    Arg(\"in_dtype\", std::string(\"rgb888\")),\n    Arg(\"out_dtype\", std::string(\"rgb888\")),\n    Arg(\"out_tensor_name\", std::string(\"tensor\")),\n    Arg(\"out_channel_order\", std::vector&lt;int&gt;{2, 1, 0})  // RGB to BGR\n);\n\n// Create streaming client\nauto video_streaming_client = make_operator&lt;ops::VideoStreamingClientOp&gt;(\n    \"video_streaming_client\",\n    Arg(\"server_ip\", std::string(\"127.0.0.1\")),\n    Arg(\"signaling_port\", uint16_t{48010}),\n    Arg(\"width\", 854U),\n    Arg(\"height\", 480U),\n    Arg(\"fps\", uint16_t{30}),\n    Arg(\"send_frames\", true),\n    Arg(\"receive_frames\", true),\n    Arg(\"min_non_zero_bytes\", static_cast&lt;uint32_t&gt;(10))\n);\n\n// Create visualization\nauto holoviz = make_operator&lt;ops::HolovizOp&gt;(\n    \"holoviz\",\n    Arg(\"width\", 854U),\n    Arg(\"height\", 480U)\n);\n\n// Connect the pipeline\nadd_flow(replayer, format_converter, ('output', 'source_video'));\nadd_flow(format_converter, video_streaming_client);\nadd_flow(video_streaming_client, holoviz, ('output_frames', 'receivers'));\n</code></pre> <p>V4L2 Camera Mode:</p> <pre><code>#include \"video_streaming_client.hpp\"\n#include &lt;holoscan/operators/format_converter/format_converter.hpp&gt;\n#include &lt;holoscan/operators/holoviz/holoviz.hpp&gt;\n#include &lt;holoscan/operators/v4l2_video_capture/v4l2_video_capture.hpp&gt;\n\n// Create video source (V4L2 camera)\nauto v4l2_source = make_operator&lt;ops::V4L2VideoCaptureOp&gt;(\n    \"v4l2_camera\",\n    Arg(\"device\", std::string(\"/dev/video0\")),\n    Arg(\"width\", 640U),\n    Arg(\"height\", 480U),\n    Arg(\"num_buffers\", 4U),\n    Arg(\"pixel_format\", std::string(\"YUYV\"))\n);\n\n// Create format converter (RGBA to RGB/BGR)\nauto format_converter = make_operator&lt;ops::FormatConverterOp&gt;(\n    \"format_converter\",\n    Arg(\"in_dtype\", std::string(\"rgba8888\")),  // V4L2 outputs RGBA\n    Arg(\"out_dtype\", std::string(\"rgb888\")),   // Convert to RGB\n    Arg(\"out_tensor_name\", std::string(\"tensor\")),\n    Arg(\"out_channel_order\", std::vector&lt;int&gt;{2, 1, 0})  // RGB to BGR\n);\n\n// Create streaming client\nauto video_streaming_client = make_operator&lt;ops::VideoStreamingClientOp&gt;(\n    \"video_streaming_client\",\n    Arg(\"server_ip\", std::string(\"127.0.0.1\")),\n    Arg(\"signaling_port\", uint16_t{48010}),\n    Arg(\"width\", 640U),\n    Arg(\"height\", 480U),\n    Arg(\"fps\", uint16_t{30}),\n    Arg(\"send_frames\", true),\n    Arg(\"receive_frames\", true),\n    Arg(\"min_non_zero_bytes\", static_cast&lt;uint32_t&gt;(10))\n);\n\n// Create visualization\nauto holoviz = make_operator&lt;ops::HolovizOp&gt;(\n    \"holoviz\",\n    Arg(\"width\", 640U),\n    Arg(\"height\", 480U)\n);\n\n// Connect the pipeline\nadd_flow(v4l2_source, format_converter, ('signal', 'source_video'));\nadd_flow(format_converter, video_streaming_client);\nadd_flow(video_streaming_client, holoviz, ('output_frames', 'receivers'));\n</code></pre>","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_client/#python-implementation","title":"Python Implementation","text":"<p>The Python implementation (<code>python/video_streaming_client_demo.py</code>) demonstrates usage of the Python bindings:</p> <p>Video Replayer Mode:</p> <pre><code>from holohub.video_streaming_client import VideoStreamingClientOp\nfrom holoscan.operators import (\n    FormatConverterOp,\n    HolovizOp,\n    VideoStreamReplayerOp,\n)\nfrom holoscan.resources import UnboundedAllocator\n\nclass StreamingClientApp(Application):\n    def compose(self):\n        # Create allocator\n        allocator = UnboundedAllocator(self, name=\"allocator\")\n\n        # Create video source (replayer)\n        replayer = VideoStreamReplayerOp(\n            self,\n            name=\"replayer\",\n            directory=\"/workspace/holohub/data/endoscopy\",\n            basename=\"surgical_video\",\n            frame_rate=30,\n        )\n\n        # Create format converter (RGB to BGR)\n        format_converter = FormatConverterOp(\n            self,\n            name=\"format_converter\",\n            in_dtype=\"rgb888\",\n            out_dtype=\"rgb888\",\n            out_tensor_name=\"tensor\",\n            scale_min=0.0,\n            scale_max=255.0,\n            out_channel_order=[2, 1, 0],  # Convert RGB to BGR\n        )\n\n        # Create streaming client\n        video_streaming_client = VideoStreamingClientOp(\n            self,\n            allocator,  # Allocator for output buffer\n            name=\"video_streaming_client\",\n            server_ip=\"127.0.0.1\",\n            signaling_port=48010,\n            width=854,\n            height=480,\n            fps=30,\n            send_frames=True,\n            receive_frames=True,\n            min_non_zero_bytes=10,\n        )\n\n        # Create visualization (optional)\n        holoviz = HolovizOp(\n            self,\n            name=\"holoviz\",\n            width=854,\n            height=480,\n        )\n\n        # Connect the pipeline\n        self.add_flow(replayer, format_converter, {(\"output\", \"source_video\")})\n        self.add_flow(format_converter, video_streaming_client)\n        self.add_flow(video_streaming_client, holoviz, {(\"output_frames\", \"receivers\")})\n</code></pre> <p>V4L2 Camera Mode:</p> <pre><code>from holohub.video_streaming_client import VideoStreamingClientOp\nfrom holoscan.operators import (\n    FormatConverterOp,\n    HolovizOp,\n    V4L2VideoCaptureOp,\n)\nfrom holoscan.resources import UnboundedAllocator\n\nclass StreamingClientApp(Application):\n    def compose(self):\n        # Create allocator\n        allocator = UnboundedAllocator(self, name=\"allocator\")\n\n        # Create video source (V4L2 camera)\n        v4l2_source = V4L2VideoCaptureOp(\n            self,\n            name=\"v4l2_camera\",\n            device=\"/dev/video0\",\n            width=640,\n            height=480,\n            frame_rate=30,\n            pixel_format=\"YUYV\",\n            allocator=allocator,\n        )\n\n        # Create format converter (RGBA to RGB/BGR)\n        format_converter = FormatConverterOp(\n            self,\n            name=\"format_converter\",\n            in_dtype=\"rgba8888\",  # V4L2 outputs RGBA\n            out_dtype=\"rgb888\",   # Convert to RGB\n            out_tensor_name=\"tensor\",\n            scale_min=0.0,\n            scale_max=255.0,\n            out_channel_order=[2, 1, 0],  # Convert RGB to BGR\n            pool=allocator,\n        )\n\n        # Create streaming client\n        video_streaming_client = VideoStreamingClientOp(\n            self,\n            allocator,  # Allocator for output buffer\n            name=\"video_streaming_client\",\n            server_ip=\"127.0.0.1\",\n            signaling_port=48010,\n            width=640,\n            height=480,\n            fps=30,\n            send_frames=True,\n            receive_frames=True,\n            min_non_zero_bytes=10,\n        )\n\n        # Create visualization (optional)\n        holoviz = HolovizOp(\n            self,\n            name=\"holoviz\",\n            width=640,\n            height=480,\n        )\n\n        # Connect the pipeline\n        self.add_flow(v4l2_source, format_converter, {(\"signal\", \"source_video\")})\n        self.add_flow(format_converter, video_streaming_client)\n        self.add_flow(video_streaming_client, holoviz, {(\"output_frames\", \"receivers\")})\n</code></pre> <p>Key Points:</p> <ul> <li>The <code>VideoStreamingClientOp</code> requires an allocator (passed as a positional argument) for output buffer allocation</li> <li>The <code>VideoStreamingClientOp</code> handles bidirectional streaming (sends and receives frames)</li> <li>Parameters are set via constructor arguments (from command-line or defaults), not from YAML</li> <li>The constructor parameters (<code>source</code>, <code>server_ip</code>, <code>port</code>, <code>width</code>, <code>height</code>, <code>fps</code>) configure the application</li> <li>Format conversion is necessary to convert source formats to RGB for streaming</li> <li>V4L2 always outputs RGBA8888 (4 channels) regardless of input format and uses \"signal\" output port</li> <li>Video replayer outputs RGB888 (3 channels) and uses \"output\" output port</li> <li>The <code>min_non_zero_bytes</code> parameter prevents sending empty frames during startup</li> <li>The <code>output_frames</code> port receives processed frames from the server</li> <li>Holoviz displays the received frames using the <code>receivers</code> input port</li> <li>The Python app auto-selects the appropriate config file based on <code>--source</code> argument if no config is specified</li> </ul>","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_client/#troubleshooting","title":"Troubleshooting","text":"","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_client/#common-issues","title":"Common Issues","text":"<ol> <li>Connection Failed: Verify server is running and ports are correct</li> </ol> <pre><code># Check if server is listening\nnetstat -tlnp | grep 48010\n</code></pre> <ol> <li>Camera Not Found: Check V4L2 device path</li> </ol> <pre><code># List available cameras\nls -l /dev/video*\n\n# Get camera info\nv4l2-ctl --device=/dev/video0 --info\n\n# Test camera\nv4l2-ctl --device=/dev/video0 --set-fmt-video=width=640,height=480,pixelformat=YUYV --stream-mmap --stream-count=10\n</code></pre> <ol> <li>Permission Denied (Camera): Add user to video group</li> </ol> <pre><code>sudo usermod -a -G video $USER\n# Log out and back in\n\n# Or change permissions\nsudo chmod 666 /dev/video0\n</code></pre> <ol> <li>Import Error (Python): Ensure Holoscan SDK Python bindings are installed</li> </ol> <pre><code># Build with Python bindings\n./holohub build video_streaming --configure-args='-DHOLOHUB_BUILD_PYTHON=ON'\n</code></pre> <ol> <li>Video Files Not Found: Check data directory path</li> </ol> <pre><code># Ensure video files exist\nls -l /workspace/holohub/data/endoscopy/surgical_video*\n</code></pre> <ol> <li>Resolution Mismatch: Ensure client and server resolutions match</li> <li>Replayer default: 854x480</li> <li>V4L2 default: 640x480</li> <li> <p>Server default: 854x480</p> </li> <li> <p>Format Converter Errors:</p> </li> <li><code>Invalid channel count for RGBA8888 3 != 4</code>: Video replayer outputs RGB888 (3 channels), not RGBA8888</li> <li>Solution: Use correct configuration file (<code>video_streaming_client_demo_replayer.yaml</code>)</li> </ol>","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_client/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging by setting log level in configuration:</p> <pre><code>application:\n  log_level: \"DEBUG\"\n</code></pre>","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_client/#examples","title":"Examples","text":"<p>See the included configuration files for complete examples:</p> <ul> <li>C++ V4L2: <code>cpp/video_streaming_client_demo.yaml</code></li> <li>C++ Replayer: <code>cpp/video_streaming_client_demo_replayer.yaml</code></li> <li>Python V4L2: <code>python/video_streaming_client_demo.yaml</code></li> <li>Python Replayer: <code>python/video_streaming_client_demo_replayer.yaml</code></li> </ul>","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_client/#integration-with-server","title":"Integration with Server","text":"","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_client/#testing-with-python-server","title":"Testing with Python Server","text":"<p>Option 1: Using Holohub CLI (Recommended)</p> <p>Terminal 1 - Start Python Server:</p> <pre><code># From holohub root directory\n./holohub run video_streaming server_python \\\n  --docker-file applications/video_streaming/Dockerfile \\\n  --docker-opts='-e EnableHybridMode=1' \\\n  --configure-args='-DHOLOHUB_BUILD_PYTHON=ON'\n</code></pre> <p>Terminal 2 - Start Python Client with Video Replayer (854x480):</p> <pre><code># From holohub root directory\n./holohub run video_streaming client_python \\\n  --docker-file applications/video_streaming/Dockerfile \\\n  --docker-opts='-e EnableHybridMode=1' \\\n  --configure-args='-DHOLOHUB_BUILD_PYTHON=ON'\n</code></pre> <p>Terminal 2 - Or Start Python Client with V4L2 Camera (640x480):</p> <pre><code># From holohub root directory\n./holohub run video_streaming client_python_v4l2 \\\n  --docker-file applications/video_streaming/Dockerfile \\\n  --docker-opts='-e EnableHybridMode=1' \\\n  --configure-args='-DHOLOHUB_BUILD_PYTHON=ON'\n</code></pre>","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_client/#testing-with-c-server","title":"Testing with C++ Server","text":"<p>Terminal 1 - Start Server (C++ or Python):</p> <pre><code># C++ Server\n./holohub run video_streaming\n\n# OR Python Server\n./holohub run video_streaming server_python \\\n  --docker-file applications/video_streaming/Dockerfile \\\n  --docker-opts='-e EnableHybridMode=1' \\\n  --configure-args='-DHOLOHUB_BUILD_PYTHON=ON'\n</code></pre> <p>Terminal 2 - Start C++ Client:</p> <pre><code># Video Replayer Mode\n./holohub run video_streaming client_replayer\n\n# OR V4L2 Camera Mode\n./holohub run video_streaming client_v4l2\n</code></pre> <p>Important:</p> <ul> <li>C++ and Python implementations are fully compatible - you can mix and match (C++ client with Python server, etc.)</li> <li>Ensure client and server resolutions match for optimal performance</li> <li>The server must be started before the client</li> <li>For V4L2 mode, ensure camera permissions are set correctly</li> </ul>","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_client/#video-source-modes","title":"Video Source Modes","text":"","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_client/#v4l2-camera-vs-video-replayer","title":"V4L2 Camera vs Video Replayer","text":"Feature V4L2 Camera Video Replayer Config File <code>video_streaming_client_demo.yaml</code> <code>video_streaming_client_demo_replayer.yaml</code> Source Type <code>source: \"v4l2\"</code> <code>source: \"replayer\"</code> Input Format <code>rgba8888</code> (4 channels) <code>rgb888</code> (3 channels) Resolution 640x480 (configurable) 854x480 Data Source Live webcam Pre-recorded surgical video Use Case Real-time streaming Testing, demos, development","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_client/#switching-between-modes","title":"Switching Between Modes","text":"<p>To switch between V4L2 camera and video replayer:</p> <ol> <li>Stop the current client (Ctrl+C)</li> <li>Use the appropriate command:</li> <li>For camera: <code>./holohub run video_streaming client_v4l2</code> (or <code>client_python_v4l2</code>)</li> <li>For video replay: <code>./holohub run video_streaming client_replayer</code> (or <code>client_python</code>)</li> </ol> <p>Important: The server doesn't need to be restarted when switching client modes.</p>","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_client/#dependencies","title":"Dependencies","text":"<ul> <li>Holoscan SDK 3.5.0+</li> <li>video_streaming operator</li> <li>OpenCV</li> <li>CUDA 12.x</li> <li>OpenSSL 3.4.0 (installed inside the custom Dockerfile)</li> <li>V4L2 compatible camera (for camera mode)</li> </ul>","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_client/#see-also","title":"See Also","text":"<ul> <li>Main Video Streaming README - Complete application documentation with Python bindings and integration testing</li> <li>Video Streaming Server - Server application documentation</li> <li>Video Streaming Operators - Complete operator documentation</li> </ul>","tags":["Streaming","Video","Client","Real-time","Network","V4L2"]},{"location":"applications/video_streaming/video_streaming_server/","title":"Video Streaming Server Demo","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: November 3, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 3.5.0 Tested Holoscan SDK versions: 3.5.0 Contribution metric: Level 2 - Trusted</p> <p>This application demonstrates how to create a bidirectional video streaming server that receives frames from clients and sends them back. Both C++ and Python implementations are available.</p> <p>\ud83d\udcda Related Documentation:</p> <ul> <li>Main README - Application overview, quick start, and common configuration</li> <li>Client README - Client setup and configuration</li> <li>Testing Documentation - Integration testing and verification</li> </ul>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"applications/video_streaming/video_streaming_server/#features","title":"Features","text":"<ul> <li>Bidirectional Streaming: Receives frames from clients (upstream) and sends frames back (downstream)</li> <li>Multi-client Support: Handles multiple client connections via shared StreamingServerResource</li> <li>Simple Pipeline: StreamingServerUpstreamOp \u2192 StreamingServerDownstreamOp (passthrough/echo mode)</li> <li>Configurable: YAML configuration file support and command-line options</li> <li>C++ and Python: Full implementations in both languages with compatible APIs</li> <li>Real-time Processing: Low-latency video streaming</li> </ul>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"applications/video_streaming/video_streaming_server/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK 3.5.0 or higher</li> <li>Custom Dockerfile with OpenSSL 3.4.0 (for running via holohub CLI)</li> <li>For Python: Python 3.8+ and bindings built with <code>-DHOLOHUB_BUILD_PYTHON=ON</code></li> <li>CUDA 12.x (currently not working with CUDA 13.x)</li> <li>video_streaming operator</li> </ul>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"applications/video_streaming/video_streaming_server/#download-server-cloud-streaming","title":"Download Server Cloud Streaming","text":"<p>Download the Holoscan Server Cloud Streaming binaries from NGC:</p> <pre><code># Navigate to the server operator directory from the holohub root directory\ncd operators/video_streaming/video_streaming_server\n\n# Download using NGC CLI\nngc registry resource download-version \"nvidia/holoscan_server_cloud_streaming:0.2\"\nunzip -o holoscan_server_cloud_streaming_v0.2/holoscan_server_cloud_streaming.zip -d holoscan_server_cloud_streaming\n\n# Clean up\nrm -rf holoscan_server_cloud_streaming_v0.2\ncd - # Return to the original directory\n</code></pre>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"applications/video_streaming/video_streaming_server/#usage","title":"Usage","text":"<p>\u26a0\ufe0f Important: This application is currently only compatible with CUDA 12.x. If your system uses CUDA 13.x, ensure you add the <code>--cuda 12</code> flag to all command-line invocations shown below.</p>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"applications/video_streaming/video_streaming_server/#c-server","title":"C++ Server","text":"<pre><code># From holohub root directory - runs with default settings (854x480 @ 30fps)\n./holohub run video_streaming_server --language cpp\n</code></pre>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"applications/video_streaming/video_streaming_server/#python-server","title":"Python Server","text":"<pre><code># From holohub root directory - runs with default settings (854x480 @ 30fps)\n./holohub run video_streaming_server --language python\n\n# With custom parameters via command-line arguments\n./holohub run video_streaming_server --language python \\\n  --extra-args '--port 48010 --width 854 --height 480 --fps 30'\n</code></pre> <p>Default Configuration:</p> <ul> <li>Port: 48010</li> <li>Resolution: 854x480</li> <li>Frame Rate: 30 fps</li> <li>Pipeline: StreamingServerUpstreamOp \u2192 StreamingServerDownstreamOp (passthrough/echo mode)</li> </ul> <p>Note: The server defaults to 854x480 resolution. For V4L2 clients that use 640x480, ensure the client is configured to match the server's resolution.</p>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"applications/video_streaming/video_streaming_server/#command-line-options","title":"Command Line Options","text":"<p>Python Server:</p> <ul> <li><code>--port PORT</code>: Server port (default: 48010)</li> <li><code>--width WIDTH</code>: Frame width (default: 854)</li> <li><code>--height HEIGHT</code>: Frame height (default: 480)</li> <li><code>--fps FPS</code>: Frames per second (default: 30)</li> <li><code>--config PATH</code> or <code>-c PATH</code>: Path to YAML configuration file</li> <li><code>--create-config PATH</code>: Create default configuration file at specified path</li> <li><code>--help</code>: Show help message</li> </ul> <p>C++ Server:</p> <ul> <li><code>-c PATH</code> or <code>--config PATH</code>: Path to YAML configuration file</li> <li><code>-?</code> or <code>--help</code>: Show help message</li> </ul>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"applications/video_streaming/video_streaming_server/#configuration","title":"Configuration","text":"","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"applications/video_streaming/video_streaming_server/#c-configuration","title":"C++ Configuration","text":"<p>The C++ application is configured via YAML file. Example configuration file structure:</p> <pre><code>%YAML 1.2\n---\n# Application metadata\napplication:\n  title: Streaming Server Test App\n  version: 1.0\n  log_level: INFO\n\n# Streaming server settings\nvideo_streaming_server:\n  # Video/stream parameters\n  width: 854\n  height: 480\n  fps: 30\n\n  # Server connection settings\n  port: 48010\n  multi_instance: false\n  server_name: \"StreamingServerTest\"\n\n  # Operation mode - Bidirectional streaming\n  receive_frames: true\n  send_frames: true\n  allocator: !ref \"allocator\"\n\n# Upstream operator configuration (receives frames from clients)\nupstream_op: {}\n\n# Downstream operator configuration (sends frames to clients)\ndownstream_op: {}\n\n# Memory allocator configuration\nallocator:\n  type: \"holoscan::UnboundedAllocator\"\n\n# Scheduler configuration\nscheduler: \"multi_thread\"\n\nmulti_thread_scheduler:\n  worker_thread_number: 2\n  stop_on_deadlock: true\n  stop_on_deadlock_timeout: 5000\n\n# Enable data flow tracking for debugging/profiling\ntracking: false\n</code></pre> <p>C++ Configuration File: <code>cpp/video_streaming_server_demo.yaml</code></p>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"applications/video_streaming/video_streaming_server/#python-configuration","title":"Python Configuration","text":"<p>The Python application is primarily configured via command-line arguments, with optional YAML support for advanced settings:</p> <p>Command-Line Parameters (recommended):</p> <ul> <li><code>--port PORT</code>: Server port (default: 48010)</li> <li><code>--width WIDTH</code>: Frame width (default: 854)</li> <li><code>--height HEIGHT</code>: Frame height (default: 480)</li> <li><code>--fps FPS</code>: Frames per second (default: 30)</li> <li><code>--config PATH</code> or <code>-c PATH</code>: Path to YAML configuration file</li> <li><code>--create-config PATH</code>: Create default configuration file</li> </ul> <p>Python YAML Structure (optional, different from C++):</p> <pre><code>application:\n  title: \"Streaming Server Python Demo\"\n  version: \"1.0\"\n  log_level: \"INFO\"\n\n# Server configuration (if using YAML)\nserver:\n  signaling_port: 48010\n  streaming_port: 48020\n  standalone_mode: false\n\n# Stream settings\nstream:\n  width: 854\n  height: 480\n  fps: 30\n\n# Scheduler configuration\nscheduler: \"multi_thread\"\n\nmulti_thread_scheduler:\n  worker_thread_number: 2\n  stop_on_deadlock: true\n  stop_on_deadlock_timeout: 5000\n</code></pre> <p>Python Configuration File: <code>python/video_streaming_server_demo.yaml</code></p> <p>Note: Python parameters set via command-line take precedence over YAML configuration. For most use cases, command-line arguments are sufficient.</p>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"applications/video_streaming/video_streaming_server/#pipeline-architecture","title":"Pipeline Architecture","text":"<p>The server implements a simple bidirectional streaming pipeline:</p> <pre><code>Client Streams \u2192 StreamingServerUpstreamOp \u2192 StreamingServerDownstreamOp \u2192 Client Streams\n</code></pre>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"applications/video_streaming/video_streaming_server/#how-it-works","title":"How It Works","text":"<ol> <li>StreamingServerUpstreamOp: Receives video frames from connected streaming clients</li> <li>StreamingServerDownstreamOp: Sends frames back to all connected streaming clients</li> <li>Both operators share a StreamingServerResource that manages the streaming connections</li> </ol>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"applications/video_streaming/video_streaming_server/#c-implementation","title":"C++ Implementation","text":"<p>The C++ implementation (<code>cpp/streaming_server_demo.cpp</code>) demonstrates usage of the streaming server operators:</p> <pre><code>#include \"video_streaming_server_downstream_op.hpp\"\n#include \"video_streaming_server_resource.hpp\"\n#include \"video_streaming_server_upstream_op.hpp\"\n\n// Create shared streaming server resource from config\n// Configuration loaded from YAML 'video_streaming_server' section\nholoscan::ArgList video_streaming_server_args;\ntry {\n    video_streaming_server_args = from_config(\"video_streaming_server\");\n} catch (const std::exception&amp; e) {\n    HOLOSCAN_LOG_WARN(\"Missing video_streaming_server config section, using defaults\");\n}\nauto video_streaming_server_resource =\n    make_resource&lt;ops::StreamingServerResource&gt;(\"video_streaming_server_resource\",\n                                                 video_streaming_server_args);\n\n// Upstream operator (receives from clients)\n// Configuration loaded from YAML 'upstream_op' section\nholoscan::ArgList upstream_args;\ntry {\n    upstream_args = from_config(\"upstream_op\");\n} catch (const std::exception&amp; e) {\n    HOLOSCAN_LOG_WARN(\"Missing upstream_op config section, using defaults\");\n}\nauto upstream_op = make_operator&lt;ops::StreamingServerUpstreamOp&gt;(\"upstream_op\", upstream_args);\nupstream_op-&gt;add_arg(Arg(\"video_streaming_server_resource\", video_streaming_server_resource));\n\n// Downstream operator (sends to clients)\n// Configuration loaded from YAML 'downstream_op' section\nholoscan::ArgList downstream_args;\ntry {\n    downstream_args = from_config(\"downstream_op\");\n} catch (const std::exception&amp; e) {\n    HOLOSCAN_LOG_WARN(\"Missing downstream_op config section, using defaults\");\n}\nauto downstream_op =\n    make_operator&lt;ops::StreamingServerDownstreamOp&gt;(\"downstream_op\", downstream_args);\ndownstream_op-&gt;add_arg(Arg(\"video_streaming_server_resource\", video_streaming_server_resource));\n\n// Connect: upstream -&gt; downstream (passthrough/echo mode)\nadd_flow(upstream_op, downstream_op, ('output_frames', 'input_frames'));\n</code></pre> <p>Key Points:</p> <ul> <li>All operators use the <code>ops::</code> namespace prefix</li> <li>Configuration is loaded from YAML using <code>from_config()</code> for flexibility</li> <li>The <code>StreamingServerResource</code> is created first and passed to operators using <code>add_arg()</code></li> <li>The resource is configured from the <code>video_streaming_server</code> YAML section</li> <li>Upstream and downstream operators are configured from their respective YAML sections (<code>upstream_op</code>, <code>downstream_op</code>)</li> <li>Both operators must reference the same shared <code>video_streaming_server_resource</code></li> <li>This pattern allows for dynamic configuration without recompiling</li> </ul>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"applications/video_streaming/video_streaming_server/#python-implementation","title":"Python Implementation","text":"<p>The Python implementation (<code>python/streaming_server_demo.py</code>) demonstrates usage of the Python bindings:</p> <pre><code>from holohub.video_streaming_server import (\n    StreamingServerDownstreamOp,\n    StreamingServerResource,\n    StreamingServerUpstreamOp,\n)\n\nclass StreamingServerApp(Application):\n    def __init__(self, port=48010, width=854, height=480, fps=30):\n        \"\"\"Initialize the streaming server application.\n\n        Args:\n            port: Server port (set via --port command-line argument)\n            width: Frame width (set via --width command-line argument)\n            height: Frame height (set via --height command-line argument)\n            fps: Frames per second (set via --fps command-line argument)\n        \"\"\"\n        super().__init__()\n        self.port = port\n        self.width = width\n        self.height = height\n        self.fps = fps\n\n    def compose(self):\n        \"\"\"Compose the application pipeline.\n\n        Simple bidirectional streaming:\n        upstream_op (receives from clients) -&gt; downstream_op (sends back to clients)\n        \"\"\"\n\n        # Create shared streaming server resource with parameters from constructor\n        video_streaming_resource = StreamingServerResource(\n            self,\n            name=\"video_streaming_server_resource\",\n            port=self.port,\n            width=self.width,\n            height=self.height,\n            fps=self.fps,\n            enable_upstream=True,\n            enable_downstream=True,\n        )\n\n        # Upstream operator (receives from clients)\n        upstream_op = StreamingServerUpstreamOp(\n            self, name=\"upstream_op\", video_streaming_server_resource=video_streaming_resource\n        )\n\n        # Downstream operator (sends to clients)\n        downstream_op = StreamingServerDownstreamOp(\n            self, name=\"downstream_op\", video_streaming_server_resource=video_streaming_resource\n        )\n\n        # Connect: upstream -&gt; downstream (passthrough/echo mode)\n        self.add_flow(upstream_op, downstream_op, {(\"output_frames\", \"input_frames\")})\n</code></pre> <p>Key Points:</p> <ul> <li>Both operators share the same <code>StreamingServerResource</code> to manage streaming connections</li> <li>The resource is configured with <code>port</code>, <code>width</code>, <code>height</code>, <code>fps</code>, and enables both upstream and downstream</li> <li>The upstream operator receives frames from clients on its <code>output_frames</code> port</li> <li>The downstream operator receives those frames on its <code>input_frames</code> port and sends them back to clients</li> <li>This creates a simple passthrough/echo streaming pipeline</li> <li>Parameters are set via constructor arguments (from command-line or defaults), not from YAML</li> <li>The constructor parameters (<code>port</code>, <code>width</code>, <code>height</code>, <code>fps</code>) are passed directly to <code>StreamingServerResource</code></li> </ul>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"applications/video_streaming/video_streaming_server/#troubleshooting","title":"Troubleshooting","text":"","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"applications/video_streaming/video_streaming_server/#common-issues","title":"Common Issues","text":"<ol> <li>Port Already in Use: Check if ports are available or use different ports</li> </ol> <pre><code># Check if port is in use\nnetstat -tlnp | grep 48010\nsudo lsof -ti:48010 | xargs sudo kill -9\n</code></pre> <ol> <li> <p>No Clients Connected: Verify client configuration matches server ports and resolution</p> </li> <li> <p>Import Error (Python): Ensure Holoscan SDK Python bindings are installed</p> </li> <li> <p>Video Files Not Found: Check data directory path (standalone mode)</p> </li> <li> <p>Build Failures: Clean build and retry</p> </li> </ol> <pre><code>rm -rf build/\n./holohub build video_streaming --language cpp\n# or for Python\n./holohub build video_streaming --configure-args='-DHOLOHUB_BUILD_PYTHON=ON'\n</code></pre>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"applications/video_streaming/video_streaming_server/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging by setting log level in configuration:</p> <pre><code>application:\n  log_level: \"DEBUG\"\n</code></pre>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"applications/video_streaming/video_streaming_server/#configuration-examples","title":"Configuration Examples","text":"<p>See the included configuration files for complete examples:</p> <ul> <li><code>cpp/video_streaming_server_demo.yaml</code> - C++ server configuration (used by C++ application)</li> <li><code>python/video_streaming_server_demo.yaml</code> - Python server configuration (optional, Python primarily uses command-line args)</li> </ul> <p>Note: C++ and Python use different YAML structures. The C++ version uses <code>video_streaming_server</code> section with direct parameters, while Python uses <code>server</code> and <code>stream</code> sections. For Python, command-line arguments are the recommended configuration method.</p>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"applications/video_streaming/video_streaming_server/#integration-with-client","title":"Integration with Client","text":"","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"applications/video_streaming/video_streaming_server/#testing-with-python-client","title":"Testing with Python Client","text":"","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"applications/video_streaming/video_streaming_server/#option-1-using-holohub-cli-recommended","title":"Option 1: Using Holohub CLI (Recommended)","text":"<p>Terminal 1 - Start Python Server:</p> <pre><code># From holohub root directory\n./holohub run video_streaming server_python \\\n  --configure-args='-DHOLOHUB_BUILD_PYTHON=ON'\n</code></pre> <p>Terminal 2 - Start Python Client with Video Replayer (854x480):</p> <pre><code># From holohub root directory\n./holohub run video_streaming client_python \\\n  --configure-args='-DHOLOHUB_BUILD_PYTHON=ON'\n</code></pre>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"applications/video_streaming/video_streaming_server/#testing-with-c-client","title":"Testing with C++ Client","text":"<p>Terminal 1 - Start Server (C++ or Python):</p> <pre><code># C++ Server\n./holohub run video_streaming\n\n# OR Python Server\n./holohub run video_streaming server_python \\\n  --configure-args='-DHOLOHUB_BUILD_PYTHON=ON'\n</code></pre> <p>Terminal 2 - Start C++ Client:</p> <pre><code># Video Replayer Mode\n./holohub run video_streaming client_replayer\n\n# OR V4L2 Camera Mode\n./holohub run video_streaming client_v4l2\n</code></pre> <p>Important:</p> <ul> <li>C++ and Python implementations are fully compatible - you can mix and match (C++ server with Python client, etc.)</li> <li>Ensure client and server resolutions match for optimal performance</li> <li>The server must be started before the client</li> </ul>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"applications/video_streaming/video_streaming_server/#dependencies","title":"Dependencies","text":"<ul> <li>Holoscan SDK 3.5.0</li> <li>video_streaming operator</li> <li>OpenCV</li> <li>CUDA 12.x</li> <li>OpenSSL 3.4.0</li> </ul>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"applications/video_streaming/video_streaming_server/#see-also","title":"See Also","text":"<ul> <li>Main Video Streaming README - Complete application documentation with integration testing</li> <li>Video Streaming Client - Client application documentation</li> </ul>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"applications/vila_live/","title":"VILA Live","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates how to run VILA 1.5 models on live video feed with the possibility of changing the prompt in real time.</p> <p>VILA 1.5 is a family of Vision Language Models (VLM) created by NVIDIA &amp; MIT. It uses SigLIP to encode images into tokens which are fed into an LLM with an accompanying prompt. This application collects video frames from the V4L2 operator and feeds them to an AWQ-quantized VILA 1.5 for inference using the TinyChat library. This allows users to interact with a Generative AI model that is \"watching\" a chosen video stream in real-time.</p> <p> Note: This demo currently uses Llama-3-VILA1.5-8b-AWQ, but any of the following AWQ-quantized models from the VILA 1.5 family should work as long as the file names are changed in the Dockerfile and run_vila_live.sh: - VILA1.5-3b-AWQ - VILA1.5-3b-s2-AWQ - Llama-3-VILA1.5-8b-AWQ - VILA1.5-13b-AWQ - VILA1.5-40b-AWQ</p>","tags":["Computer Vision and Perception","LLM","Large Vision Model","Multimodal Model","Video"]},{"location":"applications/vila_live/#setup-instructions","title":"\u2699\ufe0f Setup Instructions","text":"<p>The app defaults to using the video device at <code>/dev/video0</code></p> <p>Note: You can use a USB webcam as the video source, or an MP4 video by following the instructions for the V4L2_Camera example app.</p> <p>To debug if this is the correct device download <code>v4l2-ctl</code>: <pre><code>sudo apt-get install v4l-utils\n</code></pre> To check for your devices run: <pre><code>v4l2-ctl --list-devices\n</code></pre> This command will output something similar to this: <pre><code>NVIDIA Tegra Video Input Device (platform:tegra-camrtc-ca):\n        /dev/media0\n\nvi-output, lt6911uxc 2-0056 (platform:tegra-capture-vi:0):\n        /dev/video0\n\nDummy video device (0x0000) (platform:v4l2loopback-000):\n        /dev/video3\n</code></pre> Determine your desired video device and edit the source device in vila_live.yaml</p>","tags":["Computer Vision and Perception","LLM","Large Vision Model","Multimodal Model","Video"]},{"location":"applications/vila_live/#build-and-run-instructions","title":"\ud83d\ude80 Build and Run Instructions","text":"<p>From the Holohub main directory run the following command: <pre><code>./holohub run vila_live\n</code></pre> or running with a replayer: <pre><code>./holohub run vila_live --run-args=\"--source replayer\"\n</code></pre> Note: The first build will take ~1.5 hours if you're on ARM64. This is largely due to building Flash Attention 2 since pre-built wheels are not distributed for ARM64 platforms.</p> <p>Once the main LMM-based app is running, you will see a link for the app at <code>http://127.0.0.1:8050</code>. To receive the video stream, please also ensure port 49000 is open.</p>","tags":["Computer Vision and Perception","LLM","Large Vision Model","Multimodal Model","Video"]},{"location":"applications/vila_live/#supported-hardware","title":"\ud83d\udcbb Supported Hardware","text":"<ul> <li>IGX w/ dGPU</li> <li>x86 w/ dGPU</li> <li>IGX w/ iGPU and Jetson AGX supported with workaround   There is a known issue running this application on IGX w/ iGPU and on Jetson AGX (see #500).   The workaround is to update the device to avoid picking up the libnvv4l2.so library.</li> </ul> <pre><code>cd /usr/lib/aarch64-linux-gnu/\nls -l libv4l2.so.0.0.999999\nsudo rm libv4l2.so.0.0.999999\nsudo ln -s libv4l2.so.0.0.0.0  libv4l2.so.0.0.999999\n</code></pre>","tags":["Computer Vision and Perception","LLM","Large Vision Model","Multimodal Model","Video"]},{"location":"applications/vila_live/#video-options","title":"\ud83d\udcf7\u2699\ufe0f Video Options","text":"<p>There are three options to ingest video data.</p> <ol> <li>use a physical device or capture card, such as a v4l2 device as described in the Setup Instructions. Make sure the vila_live.yaml contains the v4l2_source group and specifies the device correctly (<code>pixel_format</code> may be tuned accordingly, e.g. <code>pixel_format: \"auto\"</code>).</li> <li> <p>convert a video file to a gxf-compatible format using the convert_video_to_gxf_entities.py script. See the yolo_model_deployment application for a detailed example. When using the replayer, configure the replayer_source in the yaml file and launch the application with:     <pre><code>./run_vila_live.sh --source \"replayer\"\n</code></pre> This application downloads a pre-recorded video from Pexels when the application is built.  Please review the license terms from Pexels.</p> </li> <li> <p>create a virtual video device, that mounts a video file and replays it, as detailed in the v4l2_camera examples in holoscan-sdk. This approach may require signing the v4l2loopback kernel module, when using a system with secure-boot enabled. Make sure the vila_live.yaml contains the v4l2_source group and specifies the virtual device correctly. replay the video, using for example:     <pre><code>ffmpeg -stream_loop -1 -re -i &lt;your_video_path&gt; -pix_fmt yuyv422 -f v4l2 /dev/video3\n</code></pre></p> </li> </ol>","tags":["Computer Vision and Perception","LLM","Large Vision Model","Multimodal Model","Video"]},{"location":"applications/vila_live/#acknowledgements","title":"\ud83d\ude4c Acknowledgements","text":"<ul> <li>Jetson AI Lab, Live LLaVA: for the inspiration to create this app</li> <li>Jetson-Containers repo: For the Flask web-app with WebSockets</li> <li>LLM-AWQ repo: For the example code to create AWQ-powered LLM servers</li> <li>flash-attention-prebuild for CUDA13 x86 prebuilt wheels</li> </ul>","tags":["Computer Vision and Perception","LLM","Large Vision Model","Multimodal Model","Video"]},{"location":"applications/volume_rendering/","title":"Volume rendering using ClaraViz","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python, C++ Last modified: February 4, 2026 Latest version: 1.0.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3, 2.0.0, 2.1.0, 3.1.0, 3.10.0, 3.11.0 Contribution metric: Level 1 - Highly Reliable</p> <p></p> <p>This application loads a medical CT scan and renders it in real time at interactive frame rates using ClaraViz (https://github.com/NVIDIA/clara-viz).</p> <p>The application uses the <code>VolumeLoaderOp</code> operator to load the medical volume data, the <code>VolumeRendererOp</code> operator to render the volume and the <code>HolovizOp</code> operator to display the result and handle the camera movement.</p>","tags":["Healthcare AI","Visualization","Hounsfield Scale Transfer Functions","Volume","Segmentation","TotalSegmentator"]},{"location":"applications/volume_rendering/#data","title":"Data","text":"<p>You can find CT scan datasets for use with this application from embodi3d.</p> <p>Datasets are bundled with a default ClaraViz JSON configuration file for volume rendering. See <code>VolumeRendererOp</code> documentation for details on configuration schema.</p> <p>See <code>VolumeLoaderOp</code> documentation for supported volume formats.</p>","tags":["Healthcare AI","Visualization","Hounsfield Scale Transfer Functions","Volume","Segmentation","TotalSegmentator"]},{"location":"applications/volume_rendering/#build-and-run-instructions","title":"Build and Run Instructions","text":"<p>To build and run this application, use the <code>holohub</code> script:</p> <pre><code># C++\n ./holohub run volume_rendering --language cpp\n\n # Python\n  ./holohub run volume_rendering --language python\n</code></pre> <p>The path of the volume configuration file, volume density file and volume mask file can be passed to the application.</p> <p>You can use the following command to get more information on command line parameters for this application:</p> <pre><code>./holohub run volume_rendering --language [cpp|python] --run-args=\"--usages\"\n</code></pre>","tags":["Healthcare AI","Visualization","Hounsfield Scale Transfer Functions","Volume","Segmentation","TotalSegmentator"]},{"location":"applications/volume_rendering/#dev-container","title":"Dev Container","text":"<p>To start the the Dev Container, run the following command from the root directory of Holohub:</p> <pre><code>./holohub vscode volume_rendering\n</code></pre>","tags":["Healthcare AI","Visualization","Hounsfield Scale Transfer Functions","Volume","Segmentation","TotalSegmentator"]},{"location":"applications/volume_rendering/#vs-code-launch-profiles","title":"VS Code Launch Profiles","text":"","tags":["Healthcare AI","Visualization","Hounsfield Scale Transfer Functions","Volume","Segmentation","TotalSegmentator"]},{"location":"applications/volume_rendering/#c","title":"C++","text":"<p>Use the (gdb) volume_rendering/cpp launch profile to run and debug the C++ application.</p>","tags":["Healthcare AI","Visualization","Hounsfield Scale Transfer Functions","Volume","Segmentation","TotalSegmentator"]},{"location":"applications/volume_rendering/#python","title":"Python","text":"<p>There are a couple of launch profiles configured for this application:</p> <ol> <li>(debugpy) python_rendering/python: Launch the Volume Rendering application with the ability to debug Python code.</li> <li>(pythoncpp) python_rendering/python: Launch the Volume Rendering application with the ability to debug both Python and C++ code.</li> </ol>","tags":["Healthcare AI","Visualization","Hounsfield Scale Transfer Functions","Volume","Segmentation","TotalSegmentator"]},{"location":"applications/volume_rendering/#command-line-arguments","title":"Command Line Arguments","text":"<pre><code>Holoscan ClaraViz volume renderer.\nUsage: ./applications/volume_rendering/volume_rendering [options]\nOptions:\n  -h,-u, --help, --usages               Display this information\n  -c &lt;FILENAME&gt;, --config &lt;FILENAME&gt;    Name of the renderer JSON configuration file to load (default '../../../data/volume_rendering/config.json')\n  -p &lt;FILENAME&gt;, --preset &lt;FILENAME&gt;    Name of the renderer JSON preset file to load. This will be merged into the settings loaded from the configuration file. Multiple presets can be specified.\n  -w &lt;FILENAME&gt;, --write_config &lt;FILENAME&gt; Name of the renderer JSON configuration file to write to (default '')\n  -d &lt;FILENAME&gt;, --density &lt;FILENAME&gt;   Name of density volume file to load (default '../../../data/volume_rendering/highResCT.mhd')\n  -i &lt;MIN&gt;, --density_min &lt;MIN&gt;         Set the minimum of the density element values. If not set this is calculated from the volume data. In practice CT volumes have a minimum value of -1024 which corresponds to the lower value of the Hounsfield scale range usually used.\n  -a &lt;MAX&gt;, --density_max &lt;MAX&gt;         Set the maximum of the density element values. If not set this is calculated from the volume data. In practice CT volumes have a maximum value of 3071 which corresponds to the upper value of the Hounsfield scale range usually used.\n  -m &lt;FILENAME&gt;, --mask &lt;FILENAME&gt;      Name of mask volume file to load (default '../../../data/volume_rendering/smoothmasks.seg.mhd')\n  -n &lt;COUNT&gt;, --count &lt;COUNT&gt;           Duration to run application (default '-1' for unlimited duration)\n  ```\n\n### Importing CT datasets\n\nThis section describes the steps to user CT datasets additionally to the dataset provided by the volume rendering application.\n\nFirst get the data in a supported format. Supported formats are:\n* [MHD](https://itk.org/Wiki/ITK/MetaIO/Documentation)\n* [NIFTI](https://nifti.nimh.nih.gov/)\n* [NRRD](https://teem.sourceforge.net/nrrd/format.html)\n\nCT Data for the example dataset is downloaded to the `data/volume_rendering` folder when the application builds.\n\nAdditionally information on lighting, transfer functions and other settings is needed for the renderer to create an image. These settings are loaded from JSON files. The JSON files for the included example dataset is here `data/volume_rendering/config.json`.\n\nThere are two options to create a config file for a new dataset. First, use the example config as a reference to create a new config and modify parameters. Or let the renderer create a config file with settings deduced from the dataset.\n\nAssuming the volume file is is named `new_volume.nrrd`. Specify the new volume file (`-d new_volume.nrrd`), set the config file option to an empty string (`-c \"\"`) to force the renderer to deduce settings and specify the name of the config file to write (`-w new_config.json`):\n\n```bash\n  ./applications/volume_rendering/cpp/volume_rendering -d new_volume.nrrd -c \"\" -w new_config.json\n</code></pre> <p>This will create a file <code>new_config.json</code>. If there is a segmentation volume present add it with <code>-m new_seg_volume.nrrd</code>.</p> <p>By default the configuration is set up for rendering still images. For interactive rendering change the <code>timeSlot</code> setting in <code>RenderSettings</code> to the desired frame time in milliseconds, e.g. <code>33.0</code> for 30 fps.</p> <p>Also by default all lights and the background are shown in the scene. To avoid this change all <code>\"show\": true,</code> values to <code>\"show\": false,</code>.</p> <p>Modify the configuration file to your needs. To display the volume with the new configuration file add the configuration with the <code>-c new_config.json</code> argument:</p> <pre><code>  ./applications/volume_rendering/cpp/volume_rendering -d new_volume.nrrd -c new_config.json\n</code></pre> <p>It's possible to load preset JSON configuration files by using the <code>--preset preset.json</code> command line option. Presets are merged into the settings loaded from the configuration file. Multiple presets can be specified.</p> <p>A preset for bones is included. To load that preset use this command:</p> <pre><code>  ./applications/volume_rendering/cpp/volume_rendering -d new_volume.nrrd -c new_config.json -p presets/bones.json\n</code></pre>","tags":["Healthcare AI","Visualization","Hounsfield Scale Transfer Functions","Volume","Segmentation","TotalSegmentator"]},{"location":"applications/volume_rendering/#transfer-functions","title":"Transfer functions","text":"<p>Usually CT datasets are stored in Hounsfield scale. The renderer maps these values in Hounsfield scale to opacity in order to display the volume. These mappings are called transfer functions. Multiple transfer functions for different input value regions can be defined. Transfer functions also include material properties like diffuse, specular and emissive color. The range of input values the transfer function is applied to is in normalized input range <code>[0, 1]</code>.</p>","tags":["Healthcare AI","Visualization","Hounsfield Scale Transfer Functions","Volume","Segmentation","TotalSegmentator"]},{"location":"applications/volume_rendering/#segmentation-volume","title":"Segmentation volume","text":"<p>Different organs often have very similar Hounsfield values, therefore additionally an segmentation volume is supported. The segmentation volume contains an integer index for each element of the volume. Transfer functions can be restricted on specific segmentation indices. The segmentation volume can, for example, be generated using TotalSegmentator.</p>","tags":["Healthcare AI","Visualization","Hounsfield Scale Transfer Functions","Volume","Segmentation","TotalSegmentator"]},{"location":"applications/volume_rendering_xr/","title":"Medical Image Viewer in XR","text":"<p>     \u25b6 Run Locally  Authors: Andreas Heumann (NVIDIA), Connor Smith (NVIDIA), Cristiana Dinea (NVIDIA), Tom Birdsong (NVIDIA), Antonio Ospite (Magic Leap), Jiwen Cai (Magic Leap), Jochen Stier (Magic Leap), Korcan Hussein (Magic Leap), Robbie Bridgewater (Magic Leap) Supported platforms: x86_64, aarch64 Language: C++ Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p></p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#description","title":"Description","text":"<p>We collaborated with Magic Leap on a proof of concept mixed reality viewer for medical imagery built on the Holoscan platform.</p> <p>Medical imagery is one of the fastest-growing sources of data in any industry. When we think about typical diagnostic imaging, X-ray, CT scans, and MRIs come to mind. X-rays are 2D images, so viewing them on a lightbox or, if they\u2019re digital, a computer, is fine. But CT scans and MRIs are 3D. They\u2019re incredibly important technologies, but our way of interacting with them is flawed. This technology helps physicians in so many ways, from training and education to making more accurate diagnoses and ultimately to planning and even delivering more effective treatments.</p> <p>You can use this viewer to visualize a segmented medical volume with a mixed reality device.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#prerequisites","title":"Prerequisites","text":"","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#host-machine","title":"Host Machine","text":"<p>Review the HoloHub README document for supported platforms and software requirements.</p> <p>The application supports x86_64 or IGX dGPU platforms. IGX iGPU, AGX, and RHEL platforms are not fully tested at this time.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#magic-leap-2-device","title":"Magic Leap 2 Device","text":"<p>The following packages and applications are required to run remote rendering with a Magic Leap 2 device:</p> Requirement Platform Version Source Magic Leap Hub Windows or macOS PC latest Magic Leap Website Headset Firmware Magic Leap 2 v1.6.0 Magic Leap Hub Headset Remote Rendering Viewer (.apk) Magic Leap 2 1.11.64 Magic Leap Download Link Windrunner OpenXR Backend HoloHub Container 1.11.74 Included in Container Magic Leap 2 Pro License Magic Leap <p>Refer to the Magic Leap 2 documentation for more information: - Updating your device with Magic Leap Hub; - Installing <code>.apk</code> packages with Magic Leap Hub</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#quick-start","title":"Quick Start","text":"","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#running-with-apple-vision-pro","title":"Running with Apple Vision Pro","text":"<p>To stream this XR application to devices like Apple Vision Pro, refer to the CloudXR Runtime tutorial for setup instructions. Change <code>&lt;app_name&gt;</code> in the instructions to <code>volume_rendering_xr</code>.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#running-with-magic-leap-or-simulator","title":"Running with Magic Leap or Simulator","text":"<p>The default configuration runs without Magic Leap setup. To explicitly enable Magic Leap devices, use: <pre><code>./holohub run volume_rendering_xr --run-args=\"--magic-leap\"\n</code></pre></p> <p>A QR code will be visible in the console log. Refer to Magic Leap 2 Remote Rendering Setup documentation to pair the host and device in preparation for remote viewing. Refer to the Remote Viewer section to regenerate the QR code as needed, or to use the local debugger GUI in place of a physical device.</p> <p>The application supports the following hand or controller interactions by default: - Translate: Reach and grab inside the volume with your hand or with the controller trigger to move the volume. - Scale: Grab any face of the bounding box and move your hand or controller to scale the volume. - Rotate: Grab any edge of the bounding box and move your hand or controller to rotate the volume. - Crop: Grab any vertex of the bounding box and move your hand or controller to translate the cropping planes.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#testing-utility","title":"Testing Utility","text":"<p>We provide a simple test application in <code>utils/xr_hello_holoscan</code> for validating basic XR functionality. This utility uses the same XR operators and configuration as the main application but with minimal rendering setup. See utils/xr_hello_holoscan/README.md for details on running the test utility.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#advanced-setup","title":"Advanced Setup","text":"<p>You can use the <code>--dryrun</code> option to see the individual commands run by the quick start option above: <pre><code>./holohub run volume_rendering_xr --dryrun\n</code></pre></p> <p>Alternatively, follow the steps below to set up the interactive container session.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#build-the-container","title":"Build the Container","text":"<p>Run the following commands to build and enter the interactive container environment: <pre><code>./holohub run-container volume_rendering_xr # Build and launch the container\n</code></pre></p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#build-the-application","title":"Build the Application","text":"<p>Inside the container environment, build the application: <pre><code>./holohub build volume_rendering_xr # Build the application\n</code></pre></p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#run-the-application","title":"Run the Application","text":"<p>Inside the container environment, start the application: <pre><code>export ML_START_OPTIONS=&lt;\"\"/\"debug\"&gt; # Defaults to \"debug\" to run XR device simulator GUI\n./holohub run volume_rendering_xr --run-args=\"--magic-leap\"\n</code></pre></p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#deploying-as-a-standalone-application","title":"Deploying as a Standalone Application","text":"<p><code>volume_rendering_xr</code> can be packaged in a self-contained release container with datasets and binaries.</p> <p>To build the release container: <pre><code># Generate HoloHub `volume_rendering_xr` installation in the \"holohub/install\" folder\n./holohub build volume_rendering_xr --configure-args=\"-DCMAKE_INSTALL_PREFIX:PATH=/workspace/holohub/install\"\n./holohub run-container volume_rendering_xr --docker-opts=\"--entrypoint=bash\" -- -c cmake --build ./build --target install\n\n# Copy files into a release container\n./holohub build-container --img holohub:volume_rendering_xr_rel --docker-file ./applications/volume_rendering_xr/scripts/Dockerfile.rel --base-img nvcr.io/nvidia/cuda:12.4.1-runtime-ubuntu22.04\n</code></pre></p> <p>To run the release container, first create the container startup script: <pre><code>docker run --rm holohub:volume_rendering_xr_rel &gt; ./render-volume-xr\nchmod +x ./render-volume-xr\n</code></pre></p> <p>Then execute the script to start the Windrunner service and the app: <pre><code>./render-volume-xr\n</code></pre></p> <p>For more options, e.g. list available datasets or to select a different dataset, type <pre><code>./render-volume-xr --help\n</code></pre></p> <p>Options not recognized by the render-volume-xr script are forwarded to the application.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#additional-notes","title":"Additional Notes","text":"","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#supported-formats","title":"Supported Formats","text":"<p>This application loads static volume files from the local disk. See HoloHub <code>VolumeLoaderOp</code> documentation for supported volume formats and file conversion tools.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#launch-options","title":"Launch Options","text":"<p>Use the <code>--extra-args</code> to see all options, including how to specify a different dataset or configuration file to use. <pre><code>./holohub run volume_rendering_xr --run-args=\"--help\"\n...\nHoloscan OpenXR volume renderer.Usage: /workspace/holohub/build/applications/volume_rendering_xr/volume_rendering_xr [options]\nOptions:\n  -h, --help                            Display this information\n  -c &lt;FILENAME&gt;, --config &lt;FILENAME&gt;    Name of the renderer JSON configuration file to load (default '/workspace/holoscan-openxr/data/volume_rendering/config.json')\n  -d &lt;FILENAME&gt;, --density &lt;FILENAME&gt;   Name of density volume file to load (default '/workspace/holoscan-openxr/data/volume_rendering/highResCT.mhd')\n  -m &lt;FILENAME&gt;, --mask &lt;FILENAME&gt;      Name of mask volume file to load (default '/workspace/holoscan-openxr/data/volume_rendering/smoothmasks.seg.mhd')\n</code></pre></p> <p>To use a new dataset with the application, mount its volume location from the host machine when launching the container and pass all required arguments explicitly to the executable: <pre><code>./holohub run-container --docker-opts=\"-u root\" --img holohub:openxr-dev --add-volume /host/path/to/data-dir\n./build/applications/volume_rendering_xr/volume_rendering_xr \\\n      -c /workspace/holohub/data/volume_rendering/config.json \\\n      -d /workspace/volumes/path/to/data-dir/dataset.nii.gz \\\n      -m /workspace/volumes/path/to/data-dir/dataset.seg.nii.gz\n</code></pre></p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#starting-the-magic-leap-openxr-runtime","title":"Starting the Magic Leap OpenXR runtime","text":"<p>OpenXR runtimes are implementations of the OpenXR API that allow the Holoscan XR operators to create XR sessions and render content. The Magic Leap OpenXR runtime including a CLI are by default installed in the dev container. From a terminal inside the dev container you can execute the following scripts:</p> <p><pre><code>ml_start.sh\n</code></pre> starts the OpenXR runtime service. After executing this command, the remote viewer on the Magic Leap device should connect to this runtime service. If not, then you still have to pair the device with the host computer running the Holoscan application.</p> <p>For rapid iteration without a Magic Leap device, pass the argument <code>debug</code> to <code>ml_start.sh</code> i.e. <pre><code>ml_start.sh debug\n</code></pre> This will enable a debug view on your computer showing what the headset would see. You may click into this window and navigate with the keyboard and mouse to manipulate the virtual head position.</p> <p>If you connect an ML2 while the debug view is active, you can continue to view the content on the debug view but can no longer adjust the virtual position, as the real position is used instead.</p> <p><pre><code>ml_pair.sh\n</code></pre> displays a QR code used to pair the device with the host. Start the QR code reader App on the device and scan the QR code displayed in the terminal. Note that the OpenXR runtime has to have been started using the ml_start command in order for the paring script to execute correctly.</p> <p><pre><code>ml_stop.sh\n</code></pre> stops the OpenXR runtime service.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#starting-the-magic-leap-2-remote-viewer","title":"Starting the Magic Leap 2 Remote Viewer","text":"<p>When using a Magic Leap 2 device for the first time or after a software upgrade, the device must be provided with the IP address of the host running the OpenXR runtime. From a terminal inside the dev container run the</p> <pre><code>ml_pair.sh\n</code></pre> <p>command, which will bring up a QR code that has to be scanned using the QR Code App on the Magic Leap 2 device. Once paired with the host, the device  will automatically start the remote viewer which will then prompt you to start an OpenXR application on the host. Any time thereafter, start the remote viewer via the App menu.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#developing-with-a-different-openxr-backend","title":"Developing with a Different OpenXR Backend","text":"<p><code>volume_renderer_xr</code> is an OpenXR compatible application. The Magic Leap Remote Rendering runtime is installed in the application container by default, but a compatible runtime can be used if appropriate to your use case. See https://www.khronos.org/openxr/ for more information on conformant OpenXR runtimes.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#volume-rendering","title":"Volume Rendering","text":"<p>The application carries out volume rendering via the HoloHub <code>volume_renderer</code> operator, which in turn wraps the NVIDIA ClaraViz rendering project. ClaraViz JSON configurations provided in the config folder are available for specifying default scene parameters.</p> <p>See <code>volume_renderer</code> Configuration section for details on manipulating configuration values, along with how to create a new configuration file to fit custom data.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#troubleshooting","title":"Troubleshooting","text":"<p>Please verify that you are building from the latest HoloHub <code>main</code> branch before reviewing troubleshooting steps.</p> <pre><code>git checkout main\n</code></pre>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#libraries-are-missing-when-building-the-application-vulkan-openxr-etc","title":"Libraries are missing when building the application (Vulkan, OpenXR, etc)","text":"<p>This error may indicate that you are building inside the default HoloHub container instead of the expected <code>volume_rendering_xr</code> container. Review the build steps and ensure that you have launched the container with the appropriate <code>holohub run-container --img</code> option.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#unexpected-cmake-errors","title":"Unexpected CMake errors","text":"<p>You may need to clear your CMake build cache. See the HoloHub Cleaning section for instructions.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#seccomp-errors","title":"\"Seccomp\" Errors","text":"<p>The Magic Leap Windrunner OpenXR backend and remote rendering host application use seccomp to limit syscalls on Linux platforms. You can exempt individual syscalls for local development by adding them to the application syscall whitelist.</p>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/#debug-gui-does-not-appear","title":"Debug GUI does not appear","text":"<p>The <code>./holohub run volume_rendering_xr</code> command initializes the Magic Leap Windrunner debug GUI by default. If you do not see the debug GUI appear in your application, or if the application appears to stall with no further output after the pairing QR code appears, try any of the following:</p> <ol> <li> <p>Manually set the <code>ML_START_OPTIONS</code> environment variable so that <code>holohub run</code> initializes with the debug view: <pre><code>export ML_START_OPTIONS=\"debug\"\n</code></pre></p> </li> <li> <p>Follow Advanced Setup Instructions and add the <code>-u root</code> option to launch the container with root permissions. <pre><code>./holohub run --img holohub:volume_rendering_xr --docker-opts\"-u root\"\n</code></pre></p> </li> <li> <p>Clear the build cache and any home cache folders in the HoloHub workspace. <pre><code>./holohub clear-cache\nrm -rf .cache/ .cmake/ .config/ .local/\n</code></pre></p> </li> </ol>","tags":["Extended Reality","Visualization","Depth Conversion","Detection","Holoviz","Volume"]},{"location":"applications/volume_rendering_xr/operators/XrFrameOp/","title":"XrFrame Operator","text":"<p>     \u25b6 Run Locally  Authors: Magic Leap Team (Magic Leap) Supported platforms: x86_64, aarch64 Language: C++ Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>The <code>XrFrameOp</code> directory contains the <code>XrBeginFrameOp</code> and the <code>XrEndFrameOp</code>. <code>XrBeginFrameOp</code> operator initiates the rendering of a single OpenXR frame. It converts render buffers and events generated by the OpenXR runtime into Holoscan specific events to be consumed by downstream operators. Application specific operators are then expected to render left and right camera views into the given color and depth buffers. Once complete, the buffers must be passed to <code>XrEndFrameOp</code> in order to deliver the frame back to the OpenXR runtime. Note that a single connection xr_frame from <code>XrBeginFrameOp</code> to <code>XrEndFrameOp</code> is required to synchronize the OpenXR calls issued by the two operators.</p>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrFrameOp/begin_frame/","title":"XRBeginFrame Operator","text":"<p>     \u25b6 Run Locally  Authors: Magic Leap Team (Magic Leap) Supported platforms: x86_64, aarch64 Language: C++ Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>The <code>XrBeginFrameOp</code> operator initiates the rendering of a single OpenXR frame. It converts render buffers and events generated by the OpenXR runtime into Holoscan specific events to be consumed by downstream operators. Application specific operators are then expected to render left and right camera views into the given color and depth buffers. Once complete, the buffers must be passed to <code>XrEndFrameOp</code> in order to deliver the frame back to the OpenXR runtime. Note that a single arc xr_frame from <code>XrBeginFrameOp</code> to <code>XrEndFrameOp</code> is required to synchronize the OpenXR calls issued by the two operators.</p>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrFrameOp/begin_frame/#holoscanopenxrxrbeginframeop","title":"<code>holoscan::openxr::XrBeginFrameOp</code>","text":"","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrFrameOp/begin_frame/#outputs","title":"Outputs","text":"<p>Output for camera state  - <code>left_camera_pose</code>: camera pose for the left eye   - type: <code>nvidia::gxf::Pose3D</code> - <code>right_camera_pose</code>: camera pose for the right eye   - type: <code>nvidia::gxf::Pose3D</code> - <code>left_camera_model</code>: camera model for the left eye   - type: <code>nvidia::gxf::CameraModel</code> - <code>right_camera_model</code>: camera model for the right eye   - type: <code>nvidia::gxf::CameraModel</code> - <code>depth_range</code>: depth range   - type: <code>nvidia::gxf::Vector2f</code></p> <p>Output for input state  - <code>trigger_click</code>: trigger click , values true/false   - type: <code>bool</code> - <code>shoulder_click</code>: shoulder click , values true/false   - type: <code>bool</code> - <code>trackpad_touch</code>: trackpad touch , values true/false   - type: <code>bool</code> - <code>trackpad</code>: trackpad values [x.y]   - type: <code>std::array&lt;float, 2&gt;</code> - <code>aim_pose</code>: aim pose for the controller specific for the right hand   - type: <code>nvidia::gxf::Pose3D</code> - <code>head_pose</code>: head pose    - type: <code>nvidia::gxf::Pose3D</code> - <code>color_buffer</code>: color buffer   - type: <code>holoscan::gxf::Entity</code> - <code>depth_buffer</code>: depth buffer   - type: <code>holoscan::gxf::Entity</code></p>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrFrameOp/begin_frame/#parameters","title":"Parameters","text":"<ul> <li><code>XrSession</code>: A class that encapsulates a single OpenXR session</li> <li>type: <code>holoscan::openxr::XrSession</code></li> </ul> <p>Note:</p> <ul> <li><code>XrCudaInteropSwapchain</code>: A class that encapsulates the Vulkan buffers of the OpenXR runtime and compatible CUDA buffer to provide interoperability between ClaraViz and OpenXR</li> </ul>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrFrameOp/convert_depth/","title":"Convert Depth To Screen Space Operator","text":"<p>     \u25b6 Run Locally  Authors: Magic Leap team (Magic Leap) Supported platforms: x86_64, aarch64 Language: C++ Last modified: October 9, 2025 Latest version: 0.0 Minimum Holoscan SDK version: 0.6 Tested Holoscan SDK versions: 0.6, 2.0 Contribution metric: Level 2 - Trusted</p> <p>The <code>ConvertDepthToScreenSpaceOp</code> operator remaps the depth buffer from Clara Viz to an OpenXR specific range. The depth buffer is converted in place.</p>","tags":["Rendering","Convert","Depth"]},{"location":"applications/volume_rendering_xr/operators/XrFrameOp/convert_depth/#holoscanopenxrconvertdepthtoscreenspaceop","title":"<code>holoscan::openxr::ConvertDepthToScreenSpaceOp</code>","text":"<p>Converts a depth buffer from linear world units to screen space ([0,1])</p>","tags":["Rendering","Convert","Depth"]},{"location":"applications/volume_rendering_xr/operators/XrFrameOp/convert_depth/#inputs","title":"Inputs","text":"<ul> <li><code>depth_buffer_in</code>: input depth buffer to be remapped</li> <li>type: <code>holoscan::gxf::VideoBuffer</code></li> <li><code>depth_range</code>: Allocator used to allocate the volume data</li> <li>type: <code>nvidia::gxf::Vector2f</code></li> </ul>","tags":["Rendering","Convert","Depth"]},{"location":"applications/volume_rendering_xr/operators/XrFrameOp/convert_depth/#outputs","title":"Outputs","text":"<ul> <li><code>depth_buffer_out</code>: output depth buffer </li> <li>type: <code>holoscan::gxf::Entity</code></li> </ul>","tags":["Rendering","Convert","Depth"]},{"location":"applications/volume_rendering_xr/operators/XrFrameOp/end_frame/","title":"XrEndFrame Operator","text":"<p>     \u25b6 Run Locally  Authors: Magic Leap Team (Magic Leap) Supported platforms: x86_64, aarch64 Language: C++ Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>The <code>XrEndFrameOp</code> operator completes the rendering of a single OpenXR frame by passing populated color and depth buffer for the left and right eye to the OpenXR device. Note that a single connection <code>xr_frame</code> from <code>XrBeginFrameOp</code> to <code>XrEndFrameOp</code> is required to synchronize the OpenXR calls issued by the two operators.</p>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrFrameOp/end_frame/#holoscanopenxrxrendframeop","title":"<code>holoscan::openxr::XrEndFrameOp</code>","text":"","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrFrameOp/end_frame/#parameters","title":"Parameters","text":"<ul> <li><code>XrSession</code>: A class that encapsulates a single OpenXR session</li> <li>type: <code>holoscan::openxr::XrSession</code></li> </ul>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrFrameOp/end_frame/#inputs","title":"Inputs","text":"<p>Render buffers populated by application - <code>color_buffer</code>: color buffer   - type: <code>holoscan::gxf::VideoBuffer</code> - <code>depth_buffer</code>: depth buffer   - type: <code>holoscan::gxf::VideoBuffer</code></p> <p>OpenXR synchronization - <code>XrFrame</code>: connection to synchronize <code>XrBeginFrameOp</code> and <code>XrEndFrameOp</code>   - type: <code>XrFrame</code></p> <p>Note:</p> <ul> <li><code>XrCudaInteropSwapchain</code>: A class that encapsulates the Vulkan buffers of the OpenXR runtime and compatible CUDA buffer to provide interoperability between ClaraViz and OpenXR</li> </ul>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrTransformOp/","title":"User interface Control Operator","text":"<p>     \u25b6 Run Locally  Authors: Magic Leap Team (Magic Leap) Supported platforms: x86_64, aarch64 Language: C++ Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>The <code>XrTransformControlOp</code> maintains the state of the mixed reality user interface for the volumetric rendering application. It consumes controller events and produces user interface state structures as well as parameters for the volume rendering operator. The operator is application specific and will grow over time to include additional user interface widgets.</p>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrTransformOp/#holoscanopenxrxrtransformcontrolop","title":"<code>holoscan::openxr::XrTransformControlOp</code>","text":"","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrTransformOp/#inputs","title":"Inputs","text":"<p>Controller state - <code>trigger_click</code>: trigger button state   - type: <code>bool</code> - <code>shoulder_click</code>: shoulder button state   - type: <code>bool</code> - <code>trackpad_touch</code>: trackpad state   - type: <code>bool</code> - <code>trackpad</code>: trackpad values [x,y]   - type: <code>std::array&lt;float, 2&gt;</code> - <code>aim_pose</code>: world space pose of the controller tip   - type: <code>nvidia::gxf::Pose3D</code></p> <p>Device state - <code>head_pose</code>: world space head pose of the device   - type: <code>nvidia::gxf::Pose3D</code></p> <p>Volume state - <code>extent</code>: size of bounding box containing volume   - type: <code>std::array&lt;float, 3&gt;</code></p>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrTransformOp/#outputs","title":"Outputs","text":"<p>User interface widget state structures - <code>ux_box</code>: bounding box state structure   - type: <code>UxBoundingBox</code> - <code>ux_cursor</code>: cursor state structure   - type: <code>UxCursor</code></p> <p>Volume rendering parameters - <code>volume_pose</code>: world pose of dataset    - type: <code>nvidia::gxf::Pose3D</code> - <code>crop_box</code>: axis aligned cropping planes in local coordinates   - type: <code>std::array&lt;nvidia::gxf::Vector2f, 3&gt;</code></p>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrTransformOp/XrTransformControlOp/","title":"User interface Control Operator","text":"<p>     \u25b6 Run Locally  Authors: Magic Leap Team (Magic Leap) Supported platforms: x86_64, aarch64 Language: C++ Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>The <code>XrTransformControlOp</code> maintains the state of the mixed reality user interface for the volumetric rendering application. It consumes controller events and produces user interface state structures as well as parameters for the volume rendering operator. The operator is application specific and will grow over time to include additional user interface widgets.</p>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrTransformOp/XrTransformControlOp/#holoscanopenxrxrtransformcontrolop","title":"<code>holoscan::openxr::XrTransformControlOp</code>","text":"","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrTransformOp/XrTransformControlOp/#inputs","title":"Inputs","text":"<p>Controller state - <code>trigger_click</code>: trigger button state   - type: <code>bool</code> - <code>shoulder_click</code>: shoulder button state   - type: <code>bool</code> - <code>trackpad_touch</code>: trackpad state   - type: <code>bool</code> - <code>trackpad</code>: trackpad values [x,y]   - type: <code>std::array&lt;float, 2&gt;</code> - <code>aim_pose</code>: world space pose of the controller tip   - type: <code>nvidia::gxf::Pose3D</code></p> <p>Device state - <code>head_pose</code>: world space head pose of the device   - type: <code>nvidia::gxf::Pose3D</code></p> <p>Volume state - <code>extent</code>: size of bounding box containing volume   - type: <code>std::array&lt;float, 3&gt;</code></p>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrTransformOp/XrTransformControlOp/#outputs","title":"Outputs","text":"<p>User interface widget state structures - <code>ux_box</code>: bounding box state structure   - type: <code>UxBoundingBox</code> - <code>ux_cursor</code>: cursor state structure   - type: <code>UxCursor</code></p> <p>Volume rendering parameters - <code>volume_pose</code>: world pose of dataset    - type: <code>nvidia::gxf::Pose3D</code> - <code>crop_box</code>: axis aligned cropping planes in local coordinates   - type: <code>std::array&lt;nvidia::gxf::Vector2f, 3&gt;</code></p>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrTransformOp/XrTransformRenderOp/","title":"User interface Render Operator","text":"<p>     \u25b6 Run Locally  Authors: Magic Leap Team (Magic Leap) Supported platforms: x86_64, aarch64 Language: C++ Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>The <code>XrTransformRenderOp</code> renders the mixed reality user interface of the volumetric rendering application. It consumes interface widget state structures as well as render buffers into which to overlay the interface widgets. The operator is application specific and will grow over time to include additional user interface widgets.</p>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrTransformOp/XrTransformRenderOp/#holoscanopenxrxrtransformrenderop","title":"<code>holoscan::openxr::XrTransformRenderOp</code>","text":"","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrTransformOp/XrTransformRenderOp/#parameters","title":"Parameters","text":"<ul> <li><code>display_width</code>: pixel height of display</li> <li>type: <code>int</code></li> <li><code>display_height</code>: pixel width of display</li> <li>type: <code>int</code></li> </ul>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrTransformOp/XrTransformRenderOp/#inputs","title":"Inputs","text":"<p>Camera state for stereo view - <code>left_camera_pose</code>: world space pose of the left eye   - type: <code>nvidia::gxf::Pose3D</code> - <code>right_camera_pose</code>: world space pose of the right eye   - type: <code>nvidia::gxf::Pose3D</code> - <code>left_camera_model</code>: camera model for the left eye   - type: <code>nvidia::gxf::CameraModel</code> - <code>right_camera_model</code>: camera model for the right eye   - type: <code>nvidia::gxf::CameraModel</code> - <code>depth_range</code>: depth range</p> <p>User interface widget state structures - <code>ux_box</code>: bounding box state structure   - type: <code>UxBoundingBox</code> - <code>ux_cursor</code>: cursor state structure   - type: <code>UxCursor</code></p> <p>Render buffers to be populated - <code>Collor buffer_in</code>: color buffer   - type: <code>holoscan::gxf::VideoBuffer</code> - <code>Depth buffer_in</code>: depth buffer   - type: <code>holoscan::gxf::VideoBuffer</code></p>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/operators/XrTransformOp/XrTransformRenderOp/#outputs","title":"Outputs","text":"<p>Render buffers including interface widgets - <code>color_buffer_out</code>: color buffer   - type: <code>holoscan::gxf::VideoBuffer</code> - <code>depth_buffer_out</code>: depth buffer   - type: <code>holoscan::gxf::VideoBuffer</code></p>","tags":["Extended Reality"]},{"location":"applications/volume_rendering_xr/utils/xr_hello_holoscan/","title":"XR Demo","text":"<p>     \u25b6 Run Locally  Authors: Andreas Heumann (NVIDIA), Connor Smith (NVIDIA), Cristiana Dinea (NVIDIA), Tom Birdsong (NVIDIA), Antonio Ospite (Magic Leap), Jiwen Cai (Magic Leap), Jochen Stier (Magic Leap), Korcan Hussein (Magic Leap), Robbie Bridgewater (Magic Leap) Supported platforms: x86_64, aarch64 Language: C++ Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>This application provides a simple scene demonstrating mixed reality viewing with Holoscan SDK. It can be served as a testing tool for validating basic XR features.</p> <p></p>","tags":["Extended Reality","Visualization","Holoviz","Stereo Vision"]},{"location":"applications/volume_rendering_xr/utils/xr_hello_holoscan/#background","title":"Background","text":"<p>We created this test application as part of a collaboration between the Magic Leap and NVIDIA Holoscan teams. See the <code>volume_rendering_xr</code> application for a demonstration of medical viewing in XR with Holoscan SDK.</p>","tags":["Extended Reality","Visualization","Holoviz","Stereo Vision"]},{"location":"applications/volume_rendering_xr/utils/xr_hello_holoscan/#description","title":"Description","text":"<p>The application provides a blueprint for how to set up a mixed reality scene for viewing with Holoscan SDK and HoloHub components.</p> <p>The mixed reality demonstration scene includes: - Static components such as scene axes and cube primitives; - A primitive overlay on the tracked controller input; - A static UI showcasing sensor inputs and tracking.</p>","tags":["Extended Reality","Visualization","Holoviz","Stereo Vision"]},{"location":"applications/volume_rendering_xr/utils/xr_hello_holoscan/#getting-started","title":"Getting Started","text":"<p>Refer to the <code>volume_rendering_xr</code> README for details on hardware, firmware, and software prerequisites.</p> <p>This utility is part of the <code>volume_rendering_xr</code> application suite. </p>","tags":["Extended Reality","Visualization","Holoviz","Stereo Vision"]},{"location":"applications/volume_rendering_xr/utils/xr_hello_holoscan/#running-the-test-utility","title":"Running the Test Utility","text":"<p>Run the following command in the top-level HoloHub folder to build and run the host application:</p> <pre><code>./holohub run volume_rendering_xr --run-args=\"xr_hello_holoscan\"\n</code></pre> <p>Note that without specifying the extra arguments, it will launch the main volume rendering application by default.</p> <p>To pair your Magic Leap 2 device with the host, open the QR Reader application in the ML2 headset and scan the QR code printed in console output on the host machine.</p>","tags":["Extended Reality","Visualization","Holoviz","Stereo Vision"]},{"location":"applications/volume_rendering_xr/utils/xr_hello_holoscan/#frequently-asked-questions","title":"Frequently Asked Questions","text":"","tags":["Extended Reality","Visualization","Holoviz","Stereo Vision"]},{"location":"applications/volume_rendering_xr/utils/xr_hello_holoscan/#can-i-test-the-application-without-a-magic-leap-2-device","title":"Can I test the application without a Magic Leap 2 device?","text":"<p>Yes, a debug GUI not requiring a headset is installed inside the application container by default. Follow the steps below to launch the debug GUI and run the application:</p> <pre><code># Build and launch the container\n./holohub run-container xr_hello_holoscan\n\n# Enable the debug GUI\nexport ML_START_OPTIONS=\"debug\"\n\n# Build and run the application\n./holohub run xr_hello_holoscan\n</code></pre> <p>The ImGui debug application will launch. Click and slide the position entries to adjust your view of the scene.</p> <p></p>","tags":["Extended Reality","Visualization","Holoviz","Stereo Vision"]},{"location":"applications/volume_rendering_xr/utils/xr_hello_holoscan/xr_basic_render/","title":"XR Basic Rendering Operator","text":"<p>     \u25b6 Run Locally  Authors: Magic Leap Team (Magic Leap), NVIDIA Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>The <code>XrBasicRenderOp</code> defines and renders a basic scene to demonstrate OpenXR compatibility. It provides visuals for static primitives, controller tracking, and an ImGui window.</p> <p>See the <code>xr_hello_holoscan</code> application for a complete example demonstrating <code>XrBasicRenderOp</code> in a Holoscan SDK pipeline.</p>","tags":["Extended Reality"]},{"location":"applications/vpi_stereo/","title":"VPI Stereo Vision","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: October 29, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.4.0 Tested Holoscan SDK versions: 2.4.0, 2.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p> </p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Camera","Depth","Visualization"]},{"location":"applications/vpi_stereo/#overview","title":"Overview","text":"<p>Demo pipeline showing stereo disparity estimation using the Vision Programming Interface VPI.</p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Camera","Depth","Visualization"]},{"location":"applications/vpi_stereo/#description","title":"Description","text":"<p>This pipeline takes video from a stereo camera and uses VPI's stereo disparity estimation algorithm. The input video and estimate disparity map are displayed using Holoviz.</p> <p>The application will select accelerator backends if available (OFA, PVA and VIC). This demonstrates how VPI can be used to offload stereo disparity processing from the GPU on supported devices such as NVIDIA IGX, AGX, or NX platforms.</p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Camera","Depth","Visualization"]},{"location":"applications/vpi_stereo/#input-video","title":"Input Video","text":"<p>Requires a V4L2 stereo camera, or recorded stereo video, and matching calibration data. By default, the application will share the same source data as stereo_vision.</p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Camera","Depth","Visualization"]},{"location":"applications/vpi_stereo/#requirements","title":"Requirements","text":"<p>This demo requires VPI version 3.2 or greater. The included Dockerfile will install VPI and its dependencies for either an amd64 target (with discrete NVIDIA GPU), or arm64 target (NVIDIA IGX, AGX, or NX).</p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Camera","Depth","Visualization"]},{"location":"applications/vpi_stereo/#build-and-run-instructions","title":"Build and Run Instructions","text":"<p>Using default video source (same as stereo_vision application): <pre><code>./holohub run vpi_stereo\n</code></pre> Using a v4l2 video source (live camera or loopback device): <pre><code>./holohub run vpi_stereo --run-args=\"--source v4l2\"\n</code></pre></p>","tags":["Computer Vision and Perception","Video","Stereo Vision","Camera","Depth","Visualization"]},{"location":"applications/webrtc_holoviz_server/","title":"WebRTC Holoviz Server","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p></p> <p>This app generates video frames with user specified content using Holoviz and sends it to a browser using WebRTC. The goal is to show how to remote control operators and view the output of a Holoscan pipeline.</p> <p>The app starts a web server, the pipeline starts when a browser is connected to the web server and the <code>Start</code> button is pressed. The pipeline stops when the <code>Stop</code> button is pressed.</p> <p>The web page has user inputs for specifying text and for the speed the text moves across the screen.</p> <pre><code>flowchart LR\n    subgraph Server\n        GeometryGenerationOp --&gt; HolovizOp\n        HolovizOp --&gt; FormatConverterOp\n        FormatConverterOp --&gt; WebRTCServerOp\n        WebServer\n    end\n    subgraph Client\n        WebServer &lt;--&gt; Browser\n        WebRTCServerOp &lt;--&gt; Browser\n    end</code></pre> <p>NOTE: When using VPN there might be a delay of several seconds between pressing the <code>Start</code> button and the first video frames are display. The reason for this is that the STUN server <code>stun.l.google.com:19302</code> used by default might not be available when VPN is active and the missing support for Trickle ICE in the used aiortc library. Trickle ICE is an optimization to speed up connection establishment. Normally, possible connections paths are tested one after another. If connections time out this is blocking the whole process. Trickle ICE checks each possible connection path in parallel so the connection timing out won't block the process.</p>","tags":["Computer Vision and Perception","Networking and Distributed Computing","WebRTC","Visualization","Holoviz"]},{"location":"applications/webrtc_holoviz_server/#prerequisites","title":"Prerequisites","text":"<p>The app is using AIOHTTP for the web server and AIORTC for WebRTC. Install both using pip.</p> <pre><code>pip install aiohttp aiortc\n</code></pre>","tags":["Computer Vision and Perception","Networking and Distributed Computing","WebRTC","Visualization","Holoviz"]},{"location":"applications/webrtc_holoviz_server/#run-instructions","title":"Run Instructions","text":"<p>Run the command:</p> <pre><code>./holohub run webrtc_holoviz_server --local --no-local-build\n</code></pre> <p>On the same machine open a browser and connect to <code>127.0.0.1:8080</code>. You can also connect from a different machine by connecting to the IP address the app is running on.</p> <p>Press the <code>Start</code> button. Video frames are displayed. To stop, press the <code>Stop</code> button. Pressing <code>Start</code> again will continue the video.</p> <p>Change the text input and the speed slider to control the generated video frame content.</p>","tags":["Computer Vision and Perception","Networking and Distributed Computing","WebRTC","Visualization","Holoviz"]},{"location":"applications/webrtc_holoviz_server/#command-line-arguments","title":"Command Line Arguments","text":"<pre><code>usage: webrtc_server.py [-h] [--cert-file CERT_FILE] [--key-file KEY_FILE] [--host HOST] [--port PORT] [--verbose VERBOSE] [--ice-server ICE_SERVER]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --cert-file CERT_FILE\n                        SSL certificate file (for HTTPS)\n  --key-file KEY_FILE   SSL key file (for HTTPS)\n  --host HOST           Host for HTTP server (default: 0.0.0.0)\n  --port PORT           Port for HTTP server (default: 8080)\n  --verbose, -v\n  --ice-server ICE_SERVER\n                        ICE server config in the form of `turn:&lt;ip&gt;:&lt;port&gt;[&lt;username&gt;:&lt;password&gt;]` or `stun:&lt;ip&gt;:&lt;port&gt;`. This option can be specified multiple times to add multiple ICE servers.\n</code></pre>","tags":["Computer Vision and Perception","Networking and Distributed Computing","WebRTC","Visualization","Holoviz"]},{"location":"applications/webrtc_holoviz_server/#running-with-turn-server","title":"Running With TURN server","text":"<p>A TURN server may be needed if you're running in a containerized environment without host networking (e.g. Kubernetes or Docker). Here are some basic steps to run this example with a TURN server.</p> <p>Run the TURN server in the same machine that you're running the app on.</p> <p>Note: It is strongly recommended to run the TURN server with docker network=host for best performance</p> <pre><code># This is the external IP address of the machine running the TURN server\nexport TURN_SERVER_EXTERNAL_IP=\"&lt;ip&gt;\"\n\n# Command below use admin:admin as the username and password as an example\ndocker run -d --rm --network=host instrumentisto/coturn \\\n    -n --log-file=stdout \\\n    --external-ip=$TURN_SERVER_EXTERNAL_IP \\\n    --listening-ip=$TURN_SERVER_EXTERNAL_IP \\\n    --lt-cred-mech --fingerprint \\\n    --user=admin:admin \\\n    --no-multicast-peers \\\n    --verbose \\\n    --realm=default.realm.org\n</code></pre> <p>Then you can pass in the TURN server config into the app</p> <pre><code>python webrtc_server.py --ice-server \"turn:&lt;ip&gt;:3478[admin:admin]\"\n</code></pre> <p>This will enable you to access the webRTC browser application from different machines.</p>","tags":["Computer Vision and Perception","Networking and Distributed Computing","WebRTC","Visualization","Holoviz"]},{"location":"applications/webrtc_video_client/","title":"WebRTC Video Client","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p></p> <p>This app receives video frames from a web cam connected to a browser and display them on the screen.</p> <p>The app starts a web server, the pipeline starts when a browser is connected to the web server and the <code>Start</code> button is pressed. The pipeline stops when the <code>Stop</code> button is pressed.</p> <p>The video resolution and video codec can be selected in browser.</p> <pre><code>flowchart LR\n    subgraph Server\n        WebRTCClientOp --&gt; HolovizOp\n        WebServer\n    end\n    subgraph Client\n        Webcam --&gt; Browser\n        Browser &lt;--&gt; WebRTCClientOp\n        Browser &lt;--&gt; WebServer\n    end</code></pre> <p>NOTE: When using VPN there might be a delay of several seconds between pressing the <code>Start</code> button and the first video frames are display. The reason for this is that the STUN server <code>stun.l.google.com:19302</code> used by default might not be available when VPN is active and the missing support for Trickle ICE in the used aiortc library. Trickle ICE is an optimization to speed up connection establishment. Normally, possible connections paths are tested one after another. If connections time out this is blocking the whole process. Trickle ICE checks each possible connection path in parallel so the connection timing out won't block the process.</p>","tags":["Networking and Distributed Computing","Video","SDP Exchange","WebRTC","Camera","Visualization"]},{"location":"applications/webrtc_video_client/#prerequisites","title":"Prerequisites","text":"<p>The app is using AIOHTTP for the web server and AIORTC for WebRTC. Install both using pip.</p> <pre><code>pip install aiohttp aiortc\n</code></pre>","tags":["Networking and Distributed Computing","Video","SDP Exchange","WebRTC","Camera","Visualization"]},{"location":"applications/webrtc_video_client/#run-instructions","title":"Run Instructions","text":"<p>Run the command:</p> <pre><code>./holohub run webrtc_video_client --local --no-local-build\n</code></pre> <p>On the same machine open a browser and connect to <code>127.0.0.1:8080</code>.</p> <p>Select the video resolution and codec or keep the defaults.</p> <p>Press the <code>Start</code> button. Video frames are displayed. To stop, press the <code>Stop</code> button. Pressing <code>Start</code> again will continue the video.</p> <p>You can also connect from a different machine by connecting to the IP address the app is running on. Chrome disables features such as getUserMedia when it comes from an unsecured origin. <code>http://localhost</code> is considered as a secure origin by default, however if you use an origin that does not have an SSL/TLS certificate then Chrome will consider the origin as unsecured and disable getUserMedia.</p> <p>Solutions</p> <ul> <li>Create an self-signed SSL/TLS certificate with <code>openssl req -new -newkey rsa:4096 -x509 -sha256 -days 365 -nodes -out MyCertificate.crt -keyout MyKey.key</code>. Pass the generated files to the <code>webrtc_client</code> using the <code>--cert-file</code> and <code>--key-file</code> arguments. Connect the browser to <code>https://{YOUR HOST IP}:8080</code>.</li> <li>Go to chrome://flags, search for the flag <code>unsafely-treat-insecure-origin-as-secure</code>, enter the origin you want to treat as secure such as <code>http://{YOUR HOST IP}:8080</code>, enable the feature and relaunch the browser.</li> </ul>","tags":["Networking and Distributed Computing","Video","SDP Exchange","WebRTC","Camera","Visualization"]},{"location":"applications/webrtc_video_client/#command-line-arguments","title":"Command Line Arguments","text":"<pre><code>usage: webrtc_client.py [-h] [--cert-file CERT_FILE] [--key-file KEY_FILE] [--host HOST] [--port PORT] [--verbose VERBOSE]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --cert-file CERT_FILE\n                        SSL certificate file (for HTTPS)\n  --key-file KEY_FILE   SSL key file (for HTTPS)\n  --host HOST           Host for HTTP server (default: 0.0.0.0)\n  --port PORT           Port for HTTP server (default: 8080)\n  --verbose, -v\n</code></pre>","tags":["Networking and Distributed Computing","Video","SDP Exchange","WebRTC","Camera","Visualization"]},{"location":"applications/webrtc_video_server/","title":"WebRTC Video Server","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p></p> <p>This app reads video frames from a file and sends it to a browser using WebRTC.</p> <p>The app starts a web server, the pipeline starts when a browser is connected to the web server and the <code>Start</code> button is pressed. The pipeline stops when the <code>Stop</code> button is pressed.</p> <pre><code>flowchart LR\n    subgraph Server\n        A[(VideoFile)] --&gt; VideoStreamReplayerOp\n        VideoStreamReplayerOp --&gt; FormatConverterOp\n        FormatConverterOp --&gt; WebRTCServerOp\n        WebServer\n    end\n    subgraph Client\n        WebServer &lt;--&gt; Browser\n        WebRTCServerOp &lt;--&gt; Browser\n    end</code></pre> <p>NOTE: When using VPN there might be a delay of several seconds between pressing the <code>Start</code> button and the first video frames are display. The reason for this is that the STUN server <code>stun.l.google.com:19302</code> used by default might not be available when VPN is active and the missing support for Trickle ICE in the used aiortc library. Trickle ICE is an optimization to speed up connection establishment. Normally, possible connections paths are tested one after another. If connections time out this is blocking the whole process. Trickle ICE checks each possible connection path in parallel so the connection timing out won't block the process.</p>","tags":["Networking and Distributed Computing","Video","WebRTC","Communications"]},{"location":"applications/webrtc_video_server/#prerequisites","title":"Prerequisites","text":"<p>The app is using AIOHTTP for the web server and AIORTC for WebRTC. Install both using pip.</p> <pre><code>pip install aiohttp aiortc\n</code></pre>","tags":["Networking and Distributed Computing","Video","WebRTC","Communications"]},{"location":"applications/webrtc_video_server/#run-instructions","title":"Run Instructions","text":"<p>Run the command:</p> <pre><code>./holohub run webrtc_video_server --local --no-local-build\n</code></pre> <p>On the same machine open a browser and connect to <code>127.0.0.1:8080</code>. You can also connect from a different machine by connecting to the IP address the app is running on.</p> <p>Press the <code>Start</code> button. Video frames are displayed. To stop, press the <code>Stop</code> button. Pressing <code>Start</code> again will continue the video.</p>","tags":["Networking and Distributed Computing","Video","WebRTC","Communications"]},{"location":"applications/webrtc_video_server/#command-line-arguments","title":"Command Line Arguments","text":"<pre><code>usage: webrtc_server.py [-h] [--cert-file CERT_FILE] [--key-file KEY_FILE] [--host HOST] [--port PORT] [--verbose] [--ice-server ICE_SERVER]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --cert-file CERT_FILE\n                        SSL certificate file (for HTTPS)\n  --key-file KEY_FILE   SSL key file (for HTTPS)\n  --host HOST           Host for HTTP server (default: 0.0.0.0)\n  --port PORT           Port for HTTP server (default: 8080)\n  --verbose, -v\n  --ice-server ICE_SERVER\n                        ICE server config in the form of `turn:&lt;ip&gt;:&lt;port&gt;[&lt;username&gt;:&lt;password&gt;]` or `stun:&lt;ip&gt;:&lt;port&gt;`. This option can be specified multiple times to add multiple ICE servers.\n</code></pre>","tags":["Networking and Distributed Computing","Video","WebRTC","Communications"]},{"location":"applications/webrtc_video_server/#running-with-turn-server","title":"Running With TURN server","text":"<p>A TURN server may be needed if you're running in a containerized environment without host networking (e.g. Kubernetes or Docker). Here are some basic steps to run this example with a TURN server.</p> <p>Run the TURN server in the same machine that you're running the app on.</p> <p>Note: It is strongly recommended to run the TURN server with docker network=host for best performance</p> <pre><code># This is the external IP address of the machine running the TURN server\nexport TURN_SERVER_EXTERNAL_IP=\"&lt;ip&gt;\"\n\n# Command below use admin:admin as the username and password as an example\ndocker run -d --rm --network=host instrumentisto/coturn \\\n    -n --log-file=stdout \\\n    --external-ip=$TURN_SERVER_EXTERNAL_IP \\\n    --listening-ip=$TURN_SERVER_EXTERNAL_IP \\\n    --lt-cred-mech --fingerprint \\\n    --user=admin:admin \\\n    --no-multicast-peers \\\n    --verbose \\\n    --realm=default.realm.org\n</code></pre> <p>Then you can pass in the TURN server config into the app</p> <pre><code>python webrtc_server.py --ice-server \"turn:&lt;ip&gt;:3478[admin:admin]\"\n</code></pre> <p>This will enable you to access the webRTC browser application from different machines.</p>","tags":["Networking and Distributed Computing","Video","WebRTC","Communications"]},{"location":"applications/xr_gsplat/","title":"XR + Gaussian Splatting","text":"<p>     \u25b6 Run Locally  Authors: Connor Smith (NVIDIA), Mimi Liao (NVIDIA) Supported platforms: x86_64 Language: Python Last modified: October 9, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 3.1.0 Tested Holoscan SDK versions: 3.1.0 Contribution metric: Level 4 - Experimental</p> <p>This application demonstrates rendering a 3D scene using Gaussian Splatting in XR.  </p> <p> Demo running on Apple Vision Pro</p>","tags":["Extended Reality","OpenXR","Gaussian Splatting","3D Reconstruction"]},{"location":"applications/xr_gsplat/#flow-diagram","title":"Flow Diagram","text":"<p> Diagram illustrating the architecture and data flow of this application</p>","tags":["Extended Reality","OpenXR","Gaussian Splatting","3D Reconstruction"]},{"location":"applications/xr_gsplat/#0-training-a-gaussian-splatting-model","title":"0. Training a Gaussian Splatting Model","text":"<p>The below instructions are based on the gsplat colmap example.</p>","tags":["Extended Reality","OpenXR","Gaussian Splatting","3D Reconstruction"]},{"location":"applications/xr_gsplat/#01-clone-the-gsplat-repo","title":"0.1. Clone the gsplat repo","text":"<pre><code>git clone https://github.com/nerfstudio-project/gsplat.git\n</code></pre>","tags":["Extended Reality","OpenXR","Gaussian Splatting","3D Reconstruction"]},{"location":"applications/xr_gsplat/#02-install-dependencies-and-download-the-data","title":"0.2. Install dependencies and download the data","text":"<pre><code>cd gsplat/examples\n# Install torch\npip install torch\n# Install gsplat\npip install git+https://github.com/nerfstudio-project/gsplat.git\n# Install dependencies\npip install -r requirements.txt\n# Download the data\npython datasets/download_dataset.py\n</code></pre>","tags":["Extended Reality","OpenXR","Gaussian Splatting","3D Reconstruction"]},{"location":"applications/xr_gsplat/#03-train-the-model","title":"0.3. Train the model","text":"<pre><code>CUDA_VISIBLE_DEVICES=0 python simple_trainer.py default \\\n    --data_dir data/360_v2/garden/ --data_factor 4 \\\n    --result_dir ./results/garden\n</code></pre> <p>note: Training time is observed to take about 30 minutes on Intel i9 CPU + NVIDIA RTX A5000 dGPU</p>","tags":["Extended Reality","OpenXR","Gaussian Splatting","3D Reconstruction"]},{"location":"applications/xr_gsplat/#04-set-up-the-checkpoint-paths-in-configyaml","title":"0.4. Set up the checkpoint paths in <code>config.yaml</code>","text":"","tags":["Extended Reality","OpenXR","Gaussian Splatting","3D Reconstruction"]},{"location":"applications/xr_gsplat/#quick-start","title":"Quick Start","text":"","tags":["Extended Reality","OpenXR","Gaussian Splatting","3D Reconstruction"]},{"location":"applications/xr_gsplat/#option-1-stream-to-apple-vision-pro-cloudxr","title":"Option 1: Stream to Apple Vision Pro (CloudXR)","text":"<p>To stream this XR application to devices like Apple Vision Pro, refer to the CloudXR Runtime tutorial for setup instructions.</p>","tags":["Extended Reality","OpenXR","Gaussian Splatting","3D Reconstruction"]},{"location":"applications/xr_gsplat/#option-2-using-monado-openxr-runtime-local-development","title":"Option 2: Using Monado OpenXR Runtime (Local Development)","text":"<p>Monado is an open-source OpenXR runtime that supports various XR devices and includes a simulator for local development without physical XR hardware.</p>","tags":["Extended Reality","OpenXR","Gaussian Splatting","3D Reconstruction"]},{"location":"applications/xr_gsplat/#terminal-1-launch-container-and-start-monado-service","title":"Terminal 1: Launch Container and Start Monado Service","text":"<p><pre><code># If you're already in the container, skip this step\n./holohub run-container xr_gsplat\n\n# Inside the container, start the Monado XR runtime service\nmonado-service\n</code></pre> Keep this terminal open and running.</p>","tags":["Extended Reality","OpenXR","Gaussian Splatting","3D Reconstruction"]},{"location":"applications/xr_gsplat/#terminal-2-build-and-run-the-application","title":"Terminal 2: Build and Run the Application","text":"<pre><code># Enter the same container (replace &lt;container_id&gt; with actual ID from 'docker ps')\ndocker exec -it &lt;container_id&gt; bash\n\n# Build and run the application\n./holohub run xr_gsplat\n</code></pre>","tags":["Extended Reality","OpenXR","Gaussian Splatting","3D Reconstruction"]},{"location":"applications/xr_holoviz/","title":"XR + Holoviz","text":"<p>     \u25b6 Run Locally  Authors: Connor Smith (NVIDIA), Rafael Wiltz (NVIDIA), Mimi Liao (NVIDIA) Supported platforms: x86_64 Language: C++ Last modified: August 5, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 3.3.0 Tested Holoscan SDK versions: 3.3.0, 3.4.0 Contribution metric: Level 4 - Experimental</p> <p>This application demonstrates the integration of Holoscan-XR with Holoviz for extended reality visualization.</p> <p> Demo running on Apple Vision Pro</p>","tags":["Extended Reality","Visualization","Rendering","Stereo Vision"]},{"location":"applications/xr_holoviz/#flow-diagram","title":"Flow Diagram","text":"<p> Diagram illustrating the architecture and data flow of the XR + Holoviz application</p>","tags":["Extended Reality","Visualization","Rendering","Stereo Vision"]},{"location":"applications/xr_holoviz/#quick-start","title":"Quick Start","text":"","tags":["Extended Reality","Visualization","Rendering","Stereo Vision"]},{"location":"applications/xr_holoviz/#option-1-stream-to-apple-vision-pro-through-cloudxr-runtime","title":"Option 1: Stream to Apple Vision Pro through CloudXR Runtime","text":"<p>To stream this XR application to devices like Apple Vision Pro, refer to the CloudXR Runtime tutorial for setup instructions.</p>","tags":["Extended Reality","Visualization","Rendering","Stereo Vision"]},{"location":"applications/xr_holoviz/#option-2-using-monado-openxr-runtime-local-development","title":"Option 2: Using Monado OpenXR Runtime (Local Development)","text":"<p>Monado is an open-source OpenXR runtime that supports various XR devices and includes a simulator for local development without physical XR hardware.</p>","tags":["Extended Reality","Visualization","Rendering","Stereo Vision"]},{"location":"applications/xr_holoviz/#terminal-1-launch-container-and-start-monado-service","title":"Terminal 1: Launch Container and Start Monado Service","text":"<p><pre><code># If you're already in the container, skip this step\n./holohub run-container xr_holoviz\n\n# Inside the container, start the Monado XR runtime service\nmonado-service\n</code></pre> Keep this terminal open and running.</p>","tags":["Extended Reality","Visualization","Rendering","Stereo Vision"]},{"location":"applications/xr_holoviz/#terminal-2-build-and-run-the-application","title":"Terminal 2: Build and Run the Application","text":"<pre><code># Enter the same container (replace &lt;container_id&gt; with actual ID from 'docker ps')\ndocker exec -it &lt;container_id&gt; bash\n\n# Build and run the application\n./holohub run xr_holoviz\n</code></pre>","tags":["Extended Reality","Visualization","Rendering","Stereo Vision"]},{"location":"applications/xr_holoviz/#set-up-width-and-height-correctly","title":"Set up width and height correctly","text":"<p>The width and height of the HolovizOp should be set to the width and height of the XR display, which can only be obtained during runtime. To set the width and height correctly, we need to:</p> <ol> <li>Run the application</li> <li>Find the log showing  <pre><code>XrCompositionLayerManager initialized width: XXX height: YYY\n</code></pre></li> <li>Copy the width and height</li> <li>Set the width and height of the HolovizOp in <code>config.yaml</code></li> <li>Re-run the application</li> </ol>","tags":["Extended Reality","Visualization","Rendering","Stereo Vision"]},{"location":"applications/xr_holoviz/#troubleshooting","title":"Troubleshooting","text":"","tags":["Extended Reality","Visualization","Rendering","Stereo Vision"]},{"location":"applications/xr_holoviz/#buffer-size-too-small","title":"Buffer Size Too Small","text":"<p>If you encounter the following errors: <pre><code>[error] [gxf_executor.cpp:2506] Graph execution error: GXF_FAILURE\nterminate called after throwing an instance of 'std::runtime_error'\n  what():  The size of the buffer is too small\n</code></pre></p> <p>This error is typically caused by incorrect width and height configuration for the XR display. To resolve this issue:</p> <ol> <li>Follow the \"Set up width and height correctly\" section above to obtain the correct display dimensions</li> <li>Update the HolovizOp width and height values in <code>config.yaml</code> with the runtime-detected values</li> <li>Restart the application </li> </ol>","tags":["Extended Reality","Visualization","Rendering","Stereo Vision"]},{"location":"applications/yolo_model_deployment/","title":"Yolo Object Detection","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 2 - Trusted</p> <p>This project is aiming to provide basic guidance to deploy Yolo-based model to Holoscan SDK as \"Bring Your Own Model\"</p>","tags":["Computer Vision and Perception","Video","Tensor Optimization","YOLO Detection","Visualization","Camera"]},{"location":"applications/yolo_model_deployment/#supported-platforms","title":"Supported Platforms","text":"<p>The demo container environment supports x86 or arm64 platforms with a discrete GPU.</p> <p>Users targeting a Jetson platform (integrated GPU) may need to install Jetson PyTorch packages in their demo environment.</p>","tags":["Computer Vision and Perception","Video","Tensor Optimization","YOLO Detection","Visualization","Camera"]},{"location":"applications/yolo_model_deployment/#model","title":"Model","text":"<ul> <li>Yolo v8 model: https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt</li> <li>Yolo v8 export repository: https://github.com/triple-Mu/YOLOv8-TensorRT</li> </ul> <p>In this application example, we use the YOLOv8s model, which is converted to ONNX format using the repository mentioned above. Note that if you provide your own ONNX model, ensure it includes the EfficientNMS_TRT layer. You can verify it using Netron. Additionally, we employ the <code>graph_surgeon.py</code> script to modify the input shape. For more details on this script, refer to graph_surgeonpy.</p> <p>The detailed process is documented in the <code>CMakeLists.txt</code> file.</p>","tags":["Computer Vision and Perception","Video","Tensor Optimization","YOLO Detection","Visualization","Camera"]},{"location":"applications/yolo_model_deployment/#input-source","title":"Input Source","text":"<p>This app currently supports two input options:</p> <ol> <li>v4l2 compatible input device</li> <li>Pre-recorded video</li> </ol>","tags":["Computer Vision and Perception","Video","Tensor Optimization","YOLO Detection","Visualization","Camera"]},{"location":"applications/yolo_model_deployment/#data","title":"Data","text":"<p>This application downloads a pre-recorded video from Pexels when the application is built.  Please review the license terms from Pexels.</p>","tags":["Computer Vision and Perception","Video","Tensor Optimization","YOLO Detection","Visualization","Camera"]},{"location":"applications/yolo_model_deployment/#run","title":"Run","text":"<p>Build and launch container. Note that this will use a v4l2 input source as default.</p> <pre><code>./holohub run yolo_model_deployment\n</code></pre>","tags":["Computer Vision and Perception","Video","Tensor Optimization","YOLO Detection","Visualization","Camera"]},{"location":"applications/yolo_model_deployment/#video-replayer-support","title":"Video Replayer Support","text":"<p>If you don't have a v4l2 compatible device plugged in, you can also run this application on a pre-recorded video. To launch the application using the Video Stream Replayer as the input source, run:</p> <pre><code>./holohub run yolo_model_deployment --run-args=\"--source replayer\"\n</code></pre>","tags":["Computer Vision and Perception","Video","Tensor Optimization","YOLO Detection","Visualization","Camera"]},{"location":"applications/yolo_model_deployment/#configuration","title":"Configuration","text":"<p>For application configuration, please refer to the <code>yolo_detection.yaml</code>.</p>","tags":["Computer Vision and Perception","Video","Tensor Optimization","YOLO Detection","Visualization","Camera"]},{"location":"benchmarks/exclusive_display/","title":"Exclusive Display Benchmark","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: aarch64 Last modified: October 9, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This document investigates the performance of the exclusive display feature in <code>holoviz</code> operator of Holoscan SDK.</p>","tags":["Visualization","Benchmarking"]},{"location":"benchmarks/exclusive_display/#introduction","title":"Introduction","text":"<p>By default, Holoscan SDK application uses the <code>holoviz</code> operator for display, and it uses a windowing system like X11. X11 is a compositor which combines the display requests from different applications and renders them on the screen. In <code>exclusive_display</code> mode, <code>holoviz</code> turns off the default display manager and directly renders the output in full-screen mode. This mode is obviously more performant as a single application exclusively uses the monitor to display its output.</p> <p>In this document, we provide the performance measurements of the <code>exclusive_display</code> mode and compare it with the default mode that uses <code>X11</code>. We execute the endoscopy tool tracking in these two display modes and measure its maximum and average end-to-end latency. In addition, we also run a number of \"headless\" applications simultaneously. These headless applications are executing both AI workloads and graphics processing but do not utilize the screen to display any output. They are representative of background workloads. Usually, these background workloads run alongside a primary display application which, in this case, is the endoscopy tool tracking application using display in either <code>exclusive</code> or <code>default</code> mode.</p>","tags":["Visualization","Benchmarking"]},{"location":"benchmarks/exclusive_display/#platform","title":"Platform","text":"<p>The experiments are conducted with Holoscan v2.1 container on IGX Orin with RTX A6000 GPU flashed with IGX SW 1.0.</p>","tags":["Visualization","Benchmarking"]},{"location":"benchmarks/exclusive_display/#results","title":"Results","text":"<p>For the experiment, we use the endoscopy tool tracking application which is using the display monitor for outputs in two modes, as said above. This application is executed with <code>realtime: false</code> for the video stream replayer source, so that the source feeds the frames as fast as possible, without an external frame-rate limitation.</p> <p>For the headless applications, we run the same endoscopy tool application in different process instances but in <code>headless: true</code> mode.</p> <p>In the graphs below, Y-axis shows the end-to-end latency of the endoscopy tool tracking with display. In the X-axis, we vary the number of headless applications from 0 to 11. <code>0</code> means only the endoscopy application with display is running. We do not show any numbers when the latency is more than 200ms.</p>","tags":["Visualization","Benchmarking"]},{"location":"benchmarks/exclusive_display/#maximum-end-to-end-latency","title":"Maximum End-to-end Latency","text":"<p>The maximum end-to-end latency results are given below:</p> <p></p> <p>In the above graph, the maximum end-to-end latency for the default mode increases from 15 ms to 23 ms when the number of background headless applications rises from 0 to 3. For more than 3 background headless applications, the maximum end-to-end latency in default mode is more than 200 ms.</p> <p>The exclusive display mode performs much better than the default mode because of no overhead of the compositor. The maximum end-to-end latency is 20 to 30% lower in presence of up to 3 headless applications. The benefits are more pronounced when the number of background headless applications is more than 3.</p> <p>Despite better performance with exclusive display, the maximum end-to-end latency increases to 51 ms when the number of background headless applications is 11. Therefore, exclusive display mode alone cannot guarantee an upper bound on the latency if the number of applications using the GPU increases.</p>","tags":["Visualization","Benchmarking"]},{"location":"benchmarks/exclusive_display/#average-end-to-end-latency","title":"Average End-to-end Latency","text":"<p>The average end-to-end latency results are given below:</p> <p></p> <p>In the above graph, the average end-to-end latency for the default mode increases from 8 ms to 136 ms when the number of headless applications increases from 0 to 9. For more than 9 headless applications, the average end-to-end latency in default mode is more than 200 ms.</p> <p>The exclusive display mode performs much better than the default mode in average latency as well. Average end-to-end latency is up to 80% lower in exclusive display mode compared to the default mode, for up to 9 simultaneous headless applications. The average latency increases from 8 ms to 29 ms in exclusive mode when the number of headless applications increases from 0 to 11.</p>","tags":["Visualization","Benchmarking"]},{"location":"benchmarks/exclusive_display/#conclusions","title":"Conclusions","text":"<ul> <li>The exclusive display mode provides better average latency and deterministic performance   (maximum latency) than the default mode.</li> <li>Headless applications using the GPU which are running in the background, impact the performance both in default and exclusive display   modes. However, exclusive display mode is more resilient than the default mode to the background   applications using the GPU.</li> <li>Even in the exclusive display mode, the maximum latency, capturing the performance predictability,   increases 4-5x while the background headless applications increase from 0 to 11. Therefore, the exclusive mode does not provide a guarantee on the upper bound of the latency in presence of other GPU workloads.</li> </ul>","tags":["Visualization","Benchmarking"]},{"location":"benchmarks/green_context_benchmarking/","title":"Green Context CUDA Kernel Launch-Start Time Benchmark","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: cpp Last modified: October 27, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 3.6.0 Tested Holoscan SDK versions: 3.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This benchmark measures CUDA kernel launch-start time improvements provided by NVIDIA CUDA Green Context technology in the Holoscan SDK framework using NVIDIA CUPTI (CUDA Profiling Tools Interface) for precise GPU timing measurements.</p>","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#important-disclaimers","title":"\u26a0\ufe0f Important Disclaimers","text":"","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#cupti-profiling-overhead","title":"CUPTI Profiling Overhead","text":"<p>This benchmark uses NVIDIA CUPTI for timing measurements, which introduces profiling overhead that affects absolute timing values. The measurements include both the actual kernel scheduling latency and CUPTI's profiling overhead. However, the relative performance comparison between baseline and Green Context configurations remains valid for evaluating Green Context benefits.</p>","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#not-official-sol-numbers","title":"Not Official SOL Numbers","text":"<p>These timing measurements are NOT official NVIDIA CUDA launch latency specifications. The absolute timing values reported should not be used as reference numbers for CUDA kernel launch performance, as they include CUPTI profiling overhead and are specific to this benchmark's testing methodology. To publish official SOL (Speed of Light) performance numbers, additional validation and vetting processes would be required beyond the scope of this benchmark.</p> <p>This confirms that while absolute timing values include profiling overhead, the relative performance comparisons accurately represent Green Context benefits.</p>","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#overview","title":"Overview","text":"<p>The benchmark compares CUDA kernel launch-start times with and without Green Context by measuring the actual time from kernel launch to GPU execution start using NVIDIA CUPTI under realistic GPU contention scenarios. The benchmark uses a controlled A/B testing approach to isolate the pure Green Context benefit from general stream isolation effects.</p>","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#what-it-measures","title":"What it Measures","text":"<ul> <li>CUDA Kernel Launch-Start Time: CUPTI-measured time from <code>cudaLaunchKernel()</code> call to actual GPU execution start</li> <li>Background Load Performance: DummyLoadOp execution timing statistics</li> <li>Performance Improvement: Launch-start time reduction with Green Context isolation</li> <li>Tail Latency: P95/P99 improvements for predictable real-time performance</li> <li>Distribution Analysis: How Green Context affects timing consistency</li> </ul>","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#architecture","title":"Architecture","text":"","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#components","title":"Components","text":"<ol> <li>TimingBenchmarkOp: Measures CUPTI-based CUDA kernel launch-start time using lightweight kernels</li> <li>DummyLoadOp: Creates realistic GPU contention using compute-intensive kernels</li> <li>CuptiSchedulingProfiler: NVIDIA CUPTI-based profiler for accurate launch-start time measurement</li> <li>Green Context Setup: Configures GPU partitioning for isolated execution</li> </ol>","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#ab-testing-design","title":"A/B Testing Design","text":"<p>The benchmark uses a controlled comparison to isolate Green Context benefits:</p> <p>Baseline: Both kernels run on separate non-default CUDA streams WITHOUT Green Context partitions Green Context: Both kernels run on separate non-default CUDA streams WITH Green Context partitions</p> <p>This design ensures that any performance difference is due to Green Context partitioning, not simply moving off the default stream.</p>","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#usage","title":"Usage","text":"","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#basic-usage","title":"Basic Usage","text":"<p>Default (older GPUs and Orin systems): <pre><code>./holohub run green_context_benchmarking --docker-opts=\"--user root\"\n</code></pre></p> <p>Modern GPUs and Jetson Thor: <pre><code>./holohub run green_context_benchmarking --docker-opts=\"--user root\" --base-img=nvcr.io/nvidia/clara-holoscan/holoscan:v3.6.1-cuda13-dgpu\n</code></pre></p>","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#gpu-compatibility","title":"GPU Compatibility","text":"<p>Use default command for: - Older GPUs (compute capability &lt; 7.0, e.g., GTX 1080, GTX 1060, etc.) - NVIDIA IGX/Jetson Orin systems   If your system has a CUDA version <code>&lt; 13</code>, then you can use the default command as well.</p> <p>Use v3.6.1-cuda13 image for: - Modern GPUs (compute capability \u2265 7.0, e.g., RTX 2080, RTX 3080, RTX 4090, RTX 6000 Ada Generation) - NVIDIA IGX/Jetson Thor systems</p> <p>Note: CUDA 13.x does not support older GPU architectures (compute capability &lt; 7.0). If you encounter <code>nvcc fatal : Unsupported gpu architecture 'compute_XX'</code> errors, use the default command instead.</p>","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#command-line-options","title":"Command Line Options","text":"<pre><code>./holohub run green_context_benchmarking --docker-opts=\"--user root\" --run-args=\"[OPTIONS]\"\n\nOptions:\n  --samples N           Number of timing samples to measure (default: 1000)\n  --load-intensity N    GPU load intensity multiplier (default: 20)\n  --workload-size N     GPU memory size in MB for DummyLoadOp (default: 8)\n  --threads-per-block N CUDA threads per block for GPU kernels (default: 512)\n  --mode MODE          Run mode: 'baseline', 'green-context', or 'all' (default: all)\n                        baseline: Run only without green context\n                        green-context: Run only with green context\n                        all: Run both and show comparison\n  --help               Show this help message\n\nExamples:\n  # Default (older GPUs and Orin)\n  ./holohub run green_context_benchmarking --docker-opts=\"--user root\" --run-args=\"--help\"\n  ./holohub run green_context_benchmarking --docker-opts=\"--user root\" --run-args=\"--samples 1000 --load-intensity 10 --mode all\"\n  ./holohub run green_context_benchmarking --docker-opts=\"--user root\" --run-args=\"--workload-size 16 --threads-per-block 256 --mode baseline\"\n\n  # Modern GPUs and Jetson Thor\n  ./holohub run green_context_benchmarking --docker-opts=\"--user root\" --base-img=nvcr.io/nvidia/clara-holoscan/holoscan:v3.6.1-cuda13-dgpu --run-args=\"--samples 1000 --load-intensity 10\"\n  ./holohub run green_context_benchmarking --docker-opts=\"--user root\" --base-img=nvcr.io/nvidia/clara-holoscan/holoscan:v3.6.1-cuda13-dgpu --run-args=\"--workload-size 16 --mode all\"\n</code></pre>","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#parameters-explained","title":"Parameters Explained","text":"<ul> <li>samples: Number of launch-start time measurements per scenario (1000+ recommended for stable results)</li> <li>load-intensity: Computational intensity of background GPU workload (10-1000 range)</li> <li>workload-size: Memory footprint in MB for GPU kernels</li> <li>threads-per-block: CUDA thread block size for optimal GPU utilization</li> <li>mode: Which benchmark scenarios to run (baseline, green-context, or both)</li> </ul>","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#sample-output","title":"Sample Output","text":"<pre><code>================================================================================\nGreen Context CUDA Kernel Start Time Benchmark\n================================================================================\nBenchmark Configurations:\n  Benchmark Mode: all\n  Measurement Samples: 1000\n  Background Load Intensity: 20\n  Background Load Size: 8 MB (2097152 elements)\n  CUDA Threads Per Block: 512\n\nInitializing CUPTI profiler...\n[CUPTI] Successfully initialized scheduling latency profiler\n\n================================================================================\nRunning benchmark for baseline\n(non-default CUDA streams, without green context)\n================================================================================\n[info] [green_context_benchmark.cpp:302] [TimingBenchmarkOp] Collecting 1/1000 samples\n[info] [green_context_benchmark.cpp:302] [TimingBenchmarkOp] Collecting 100/1000 samples\n[info] [green_context_benchmark.cpp:302] [TimingBenchmarkOp] Collecting 200/1000 samples\n\n...\nBaseline benchmark completed\n\n================================================================================\nRunning main benchmark\n(with green context, separate partitions for each kernel)\n================================================================================\n[info] [green_context_benchmark.cpp:302] [TimingBenchmarkOp] Collecting 1/1000 samples\n[info] [green_context_benchmark.cpp:302] [TimingBenchmarkOp] Collecting 100/1000 samples\n...\nMain benchmark completed\n\n================================================================================\nBenchmark Configurations\n================================================================================\n  Benchmark Mode: all\n  Measurement Samples: 1000\n  Background Load Intensity: 20\n  Background Load Size: 8 MB (2097152 elements)\n  CUDA Threads Per Block: 512\n\n================================================================================\nComprehensive Timing Results\n================================================================================\n=== Without Green Context (Baseline) ===\nCUDA Kernel Launch-Start Time:\n  Average: 249.09 \u03bcs\n  Std Dev: 157.42 \u03bcs\n  Min:     1.53 \u03bcs\n  P50:     271.73 \u03bcs\n  P95:     460.79 \u03bcs\n  P99:     478.52 \u03bcs\n  Max:     586.59 \u03bcs\n  Samples: 1000\n\nCUDA Kernel Execution Time:\n  Average: 12.48 \u03bcs\n  Std Dev: 3.98 \u03bcs\n  Min:     1.34 \u03bcs\n  P50:     13.44 \u03bcs\n  P95:     16.83 \u03bcs\n  P99:     17.54 \u03bcs\n  Max:     18.59 \u03bcs\n  Samples: 1000\n\n=== With Green Context ===\nCUDA Kernel Launch-Start Time:\n  Average: 4.64 \u03bcs\n  Std Dev: 2.37 \u03bcs\n  Min:     2.78 \u03bcs\n  P50:     3.88 \u03bcs\n  P95:     10.04 \u03bcs\n  P99:     14.01 \u03bcs\n  Max:     23.98 \u03bcs\n  Samples: 1000\n\nCUDA Kernel Execution Time:\n  Average: 1.18 \u03bcs\n  Std Dev: 0.08 \u03bcs\n  Min:     0.99 \u03bcs\n  P50:     1.18 \u03bcs\n  P95:     1.31 \u03bcs\n  P99:     1.38 \u03bcs\n  Max:     1.95 \u03bcs\n  Samples: 1000\n\n================================================================================\nBaseline and Green Context Benchmark Comparison\n================================================================================\nLaunch-Start Latency:\n  Average Latency:    249.09 \u03bcs \u2192     4.64 \u03bcs  (+98.14%)\n  95th Percentile:   +460.79 \u03bcs \u2192   +10.04 \u03bcs  (+97.82%)\n  99th Percentile:   +478.52 \u03bcs \u2192   +14.01 \u03bcs  (+97.07%)\n\nKernel Execution Time:\n  Average Duration:    12.48 \u03bcs \u2192     1.18 \u03bcs  (+90.57%)\n  95th Percentile:    +16.83 \u03bcs \u2192    +1.31 \u03bcs  (+92.21%)\n  99th Percentile:    +17.54 \u03bcs \u2192    +1.38 \u03bcs  (+92.16%)\n\n================================================================================\nDummy Load Execution Time Statistics\n================================================================================\n=== Without Green Context (Baseline) ===\n  Average: 513.20 \u03bcs\n  Std Dev: 17.45 \u03bcs\n  Samples: 262730\n\n=== With Green Context ===\n  Average: 532.74 \u03bcs\n  Std Dev: 13.97 \u03bcs\n  Samples: 289987\n</code></pre>","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#benchmark-results","title":"Benchmark Results","text":"","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#executive-summary","title":"Executive Summary","text":"<p>Green Context delivers consistent, substantial performance improvements across both edge and high-end hardware:</p> Platform Best Case Improvement Optimal Configuration Launch-Start Time with GC Orin (16 SMs) 95.5% latency reduction 4MB workload, 128-256 threads 23-35\u03bcs RTX 6000 Ada (142 SMs) 97.9% latency reduction 8-16MB workload, any thread count 4-6\u03bcs","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#detailed-results-by-platform","title":"Detailed Results by Platform","text":"<p>Performance Matrix Parameter Mapping: - Load Int \u2192 <code>--load-intensity</code> - Size (MB) \u2192 <code>--workload-size</code> - Threads/Block \u2192 <code>--threads-per-block</code></p>","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#orin-16-sms","title":"Orin (16 SMs)","text":"<p>Performance Matrix : <pre><code>Load  Size  Threads   Avg%    Baseline    GC\nInt   (MB)  /Block    Impr    Avg(\u03bcs)     Avg(\u03bcs)\n----------------------------------------------------\n5     1     64        75.5    106.54      26.13\n5     1     128       72.5    99.74       27.45\n5     1     256       73.1    108.09      29.08\n5     1     512       70.1    91.63       27.36\n5     2     64        87.6    251.08      31.07\n5     2     128       89.0    253.93      27.82\n5     2     256       87.5    244.75      30.73\n5     2     512       87.2    250.33      32.00\n5     4     64        93.4    543.96      35.73\n5     4     128       95.5    531.56      23.86\n5     4     256       94.5    539.75      29.92\n5     4     512       94.8    546.71      28.39\n10    1     256       94.2    462.73      26.78\n10    1     512       87.2    199.80      25.47\n10    2     64        94.3    485.92      27.91\n10    2     128       94.5    475.67      26.38\n10    2     256       94.1    475.66      28.12\n10    2     512       94.6    479.83      26.04\n</code></pre></p> <p>Orin Key Insights: - Sweet Spot: 4MB workload with 128-256 threads per block achieves &gt;94% improvement - Reliability: Low variance in Green Context performance (23-35\u03bcs range)</p>","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#jetson-thor","title":"Jetson Thor","text":"<p>Performance Matrix : <pre><code>Load  Size  Threads   Avg%    Baseline    GC\nInt   (MB)  /Block    Impr    Avg(\u03bcs)     Avg(\u03bcs)\n----------------------------------------------------\n5     1     64        73.5    44.41       11.75\n5     1     128       69.9    39.59       11.93\n5     1     256       61.8    31.78       12.14\n5     1     512       73.5    38.54       10.19\n5     4     64        95.3    211.91      9.97\n5     4     128       95.2    271.31      12.95\n5     4     256       94.1    226.10      13.32\n5     4     512       96.2    278.20      10.62\n5     16    64        99.1    1169.41     10.02\n5     16    128       98.9    1192.72     12.83\n5     16    256       99.3    1224.37     8.02\n5     16    512       98.6    1096.32     15.40\n20    4     64        99.3    1045.09     7.66\n20    4     128       99.4    1092.23     6.67\n20    4     256       98.7    1080.32     13.56\n20    4     512       99.2    1056.79     8.94\n20    16    64        99.7    4517.73     12.22\n20    16    128       99.7    4500.81     13.12\n20    16    256       99.7    4425.59     12.73\n20    16    512       99.7    4477.69     13.71\n80    8     64        99.8    7518.14     12.21\n80    8     128       99.8    7321.67     12.52\n80    8     256       99.8    7153.78     12.16\n80    8     512       99.8    7273.29     13.58\n</code></pre></p> <p>Jetson Thor Key Insights: - Sweet Spot: 16MB+ workload with load-intensity 20+ achieves &gt;99% improvement - Excellent scaling: Performance improves dramatically with workload size (1MB\u219216MB) - Consistent GC performance: 8-15\u03bcs range regardless of baseline variability - High-end performance: Up to 99.8% improvement with large workloads</p>","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#rtx-6000-ada-generation-142-sms","title":"RTX 6000 Ada Generation (142 SMs)","text":"<p>Performance Matrix : <pre><code>Load  Size  Threads   Avg%    Baseline    GC\nInt   (MB)  /Block    Impr    Avg(\u03bcs)     Avg(\u03bcs)\n----------------------------------------------------\n5     1     64        2.7     3.61        3.51\n5     1     128       -0.8    3.35        3.38\n5     1     256       10.8    4.08        3.64\n5     1     512       6.3     3.48        3.26\n5     2     64        55.8    9.15        4.05\n5     2     128       46.0    8.18        4.41\n5     2     256       58.4    10.21       4.25\n5     2     512       55.5    9.70        4.31\n5     4     64        81.4    24.22       4.51\n5     4     128       80.8    24.15       4.63\n5     4     256       80.6    22.84       4.44\n5     4     512       81.6    23.40       4.31\n5     8     64        93.8    70.23       4.35\n5     8     128       93.3    68.74       4.59\n5     8     256       93.0    64.91       4.53\n5     8     512       92.0    57.69       4.61\n5     16    64        96.6    154.00      5.26\n5     16    128       96.3    143.26      5.36\n5     16    256       96.6    146.40      4.93\n5     16    512       95.9    138.77      5.74\n10    8     64        95.7    115.27      4.96\n10    8     128       95.5    108.94      4.95\n10    8     256       95.8    110.68      4.67\n10    8     512       95.6    112.37      4.98\n10    16    64        97.9    286.67      5.93\n10    16    128       97.5    271.31      6.80\n10    16    256       97.7    278.56      6.49\n10    16    512       97.8    280.72      6.06\n20    8     64        97.8    266.16      5.74\n20    8     128       97.9    266.65      5.59\n20    8     256       97.8    261.38      5.89\n20    8     512       97.7    268.53      6.08\n40    1     64        97.8    201.41      4.34\n40    4     64        97.1    210.54      6.08\n40    4     128       97.1    201.47      5.85\n40    4     256       97.3    215.81      5.89\n40    4     512       96.9    189.12      5.87\n</code></pre></p> <p>RTX 6000 Ada Key Insights: - Sweet Spot: 8-16MB workload with load-intensity 10+ achieves &gt;95% improvement - Threshold Effect: Minimal benefits below 2MB workload, dramatic improvements above 4MB - Reliability: Low variance in Green Context performance (4-6\u03bcs range)</p>","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#understanding-results","title":"Understanding Results","text":"","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#key-metrics","title":"Key Metrics","text":"<ul> <li>CUPTI-based Launch-Start Time: Hardware-measured time from kernel launch to GPU execution start</li> <li>Average/P95/P99 Percentiles: Statistical distribution of launch-start times</li> <li>Background Load Statistics: DummyLoadOp execution timing statistics</li> </ul>","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#what-to-expect","title":"What to Expect","text":"<p>Typical Performance Patterns: - Without Green Context: Higher average launch-start times with significant variability and inconsistent performance - With Green Context: Much lower and more consistent launch-start times with reduced variability - Performance Gains: Substantial reduction in launch-start times across all percentiles (average, P95, P99) - Consistency Improvement: Green Context typically shows much better timing consistency and predictability</p> <p>\u26a0\ufe0f Important Environmental Factor: Display Connection Impact: Having monitors connected via DisplayPort to the GPU significantly degrades launch-start time performance. For optimal benchmark results, disconnect displays and access the system via SSH/remote connection.</p>","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#warning-messages","title":"Warning Messages","text":"<p>The benchmark may include CUPTI-related warnings:</p> <pre><code>[CUPTI] WARNING: Activity buffer may have overflowed\n[CUPTI] Data polling timed out after 500 attempts\n</code></pre> <p>These indicate potential measurement issues under high GPU contention.</p>","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#technical-details","title":"Technical Details","text":"","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#gpu-workload","title":"GPU Workload","text":"<p>Timing Kernel (simple_benchmark_kernel): - Lightweight computation (sin/cos operations) - Fixed elements (1024) - 256 threads per block - Designed for minimal execution time to isolate launch-start latency</p> <p>Background Load (background_load_kernel): - Heavy computational loops with transcendental functions - Memory access patterns to stress memory subsystem - Configurable intensity (<code>--load-intensity</code>) and workload size (<code>--workload-size</code>) - Configurable threads per block (<code>--threads-per-block</code>) - Runs on separate non-default stream to create realistic GPU contention - In Green Context mode: runs in dedicated partition to test isolation</p>","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#green-context-configuration","title":"Green Context Configuration","text":"<p>The benchmark dynamically calculates optimal partition sizes:</p> <pre><code>// Dynamic partition sizing (roughly half total SMs per partition)\nint total_sms = prop.multiProcessorCount;\nint sms_per_partition = std::max(4, (total_sms / 2) &amp; ~3);  // Multiple of 4 SMs\n\nstd::vector&lt;uint32_t&gt; partitions = {sms_per_partition, sms_per_partition};\n\n// Separate partitions for each workload\n// Partition 0: DummyLoadOp (background contention)\n// Partition 1: TimingBenchmarkOp (latency measurement)\n</code></pre> <p>This ensures proper isolation with each workload getting dedicated GPU resources.</p>","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#troubleshooting","title":"Troubleshooting","text":"","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#common-issues","title":"Common Issues","text":"<ol> <li>Green Context not available:</li> <li>Check GPU compute capability: <code>deviceQuery</code></li> <li> <p>Verify GPU has enough SMs: minimum 8 SMs required (4 per partition \u00d7 2 partitions)</p> </li> <li> <p>High variability:</p> </li> <li>Increase <code>--samples</code> to 1000+ for more stable results</li> <li> <p>Check system load and background processes</p> </li> <li> <p>Low contention or insufficient GPU stress:</p> </li> <li>Increase <code>--load-intensity</code> to create more background computation</li> <li> <p>Increase <code>--workload-size</code> to use more GPU memory for background load</p> </li> <li> <p>Build cache contamination when switching between container images:</p> </li> </ol> <p>CUDA linking errors:    <pre><code>/usr/bin/ld: cannot find /usr/local/cuda-12.8/targets/sbsa-linux/lib/libcudart.so: No such file or directory\n/usr/bin/ld: cannot find /usr/local/cuda-12.8/targets/sbsa-linux/lib/libcupti.so: No such file or directory\n</code></pre></p> <p>CMake configuration errors:    <pre><code>CMake Error: Imported target \"holoscan::core\" includes non-existent path\n\"/usr/local/cuda/targets/x86_64-linux/include/cccl\"\n</code></pre></p> <p>Solution: Clear holohub cache to resolve build contamination:    <pre><code>sudo ./holohub clear-cache\n</code></pre>    Then retry the benchmark command. This resolves CUDA version path mismatches from cached build artifacts.</p>","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/green_context_benchmarking/#performance-tuning","title":"Performance Tuning","text":"<ul> <li>Low contention scenarios: Increase <code>--load-intensity</code> to 20-100</li> <li>More stable results: Use <code>--samples 1000+</code> (default)</li> <li>Large GPUs: Increase <code>--workload-size</code> to 16-32MB</li> <li>Memory-limited systems: Reduce <code>--workload-size</code> to 4-8MB</li> <li>Different thread configurations: Adjust <code>--threads-per-block</code> (256, 512, 1024)</li> <li>Specific testing: Use <code>--mode baseline</code> or <code>--mode green-context</code> for focused testing</li> </ul>","tags":["Benchmarking","CUDA","Green Context","Scheduling","CUPTI","Performance"]},{"location":"benchmarks/holoscan_flow_benchmarking/","title":"Holoscan Flow Benchmarking for HoloHub","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Last modified: August 5, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 3 - Developmental</p> <p>Holoscan Flow Benchmarking is a comprehensive performance evaluation tool designed to measure and analyze the execution characteristics of HoloHub and Holoscan applications. It provides detailed insights into operator execution times, data transfer latencies, and overall application performance.</p> <p>For detailed information, refer to:</p> <ul> <li>Holoscan Flow Benchmarking Tutorial (up-to-date)</li> <li>Holoscan Flow Benchmarking Whitepaper</li> </ul> <p>Key Features:</p> <ul> <li>Support for all Holoscan applications (Python support since v1.0)</li> <li>Real-time performance monitoring</li> <li>Detailed latency analysis</li> <li>Visual performance graphs</li> </ul>","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Holoscan Flow Benchmarking for HoloHub</li> <li>Table of Contents</li> <li>Environment Setup<ul> <li>Holohub Docker Container (recommended)</li> <li>Bare-metal Installation</li> </ul> </li> <li>Step-by-step Guide to Holoscan Flow Benchmarking<ul> <li>1. Build Applications</li> <li>Automatic Build with Benchmarking</li> <li>Manual Patching [only if need to]</li> <li>Important notes</li> <li>2. Run Benchmarks</li> <li>3. Analyze Results</li> <li>3.1 Basic Analysis</li> <li>3.2 Generate CDF Plot</li> <li>3.3 Historical Analysis</li> </ul> </li> <li>Generate Application Graph with Latency Numbers<ul> <li>Monitor application performance in real-time</li> </ul> </li> </ul>","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#environment-setup","title":"Environment Setup","text":"","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#holohub-docker-container-recommended","title":"Holohub Docker Container (recommended)","text":"<p>All dependencies are automatically managed by the Holohub Docker container. You can simply run:</p> <pre><code>./holohub run-container --extra-script benchmarking [&lt;application_name&gt;]\n</code></pre> <p>[!NOTE] This command sets up the benchmarking environment on top of the default Holohub container image, unless you specify an application with a custom Dockerfile. In that case, the benchmarking environment attempts to extend the application's Dockerfile. However, this process hasn't been fully tested for every possible custom Dockerfile, and success depends on how the Dockerfile is authored. Some Dockerfiles may already include the required benchmarking support and may work even without the <code>--extra-script benchmarking</code> option; in other cases, there may be conflicts or additional manual steps needed. Always refer to the relevant application's README for any application-specific benchmarking instructions or compatibility considerations.</p>","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#bare-metal-installation","title":"Bare-metal Installation","text":"<p>If not using the Holohub Docker container, apart from the holoscan and application's specific dependencies, additional Python packages should be installed:</p> <pre><code>pip install -r benchmarks/holoscan_flow_benchmarking/requirements.txt\n</code></pre> <p>These python dependencies include:</p> <ul> <li>numpy: Data processing</li> <li>matplotlib: Graph generation</li> <li>nvitop: GPU monitoring</li> <li>pydot: Graph creation</li> <li>xdot: Graph visualization</li> </ul> <p>Note: <code>xdot</code> has additional system dependencies. See xdot requirements.</p>","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#step-by-step-guide-to-holoscan-flow-benchmarking","title":"Step-by-step Guide to Holoscan Flow Benchmarking","text":"","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#1-build-applications","title":"1. Build Applications","text":"","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#automatic-build-with-benchmarking","title":"Automatic Build with Benchmarking","text":"<pre><code>./holohub build &lt;application_name&gt; [options] --benchmark\n</code></pre> <p>This command:</p> <ul> <li>Patches the application source</li> <li>Builds with benchmarking enabled</li> <li>Automatically restores source files after build</li> </ul> <p>For example: <pre><code>./holohub build endoscopy_tool_tracking --benchmark --language cpp\n</code></pre></p>","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#manual-patching-only-if-need-to","title":"Manual Patching [only if need to]","text":"<pre><code># Apply patches\n./benchmarks/holoscan_flow_benchmarking/patch_application.sh &lt;app_directory&gt;\n\n# Example: Patch endoscopy tool tracking\n./benchmarks/holoscan_flow_benchmarking/patch_application.sh applications/endoscopy_tool_tracking\n\n# Restore original files when done\n./benchmarks/holoscan_flow_benchmarking/restore_application.sh &lt;app_directory&gt;\n</code></pre> <p>Note: Source files are backed up as <code>*.bak</code> during patching.</p>","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#important-notes","title":"Important notes","text":"<ul> <li>Verify the application runs correctly after building and before proceeding with performance evaluation.   For example, run the app <code>./holohub run endoscopy_tool_tracking --language python</code> (append <code>--local</code> if you are using the bare-metal installation)</li> <li>For applications using TensorRT, run once to generate engine files (e.g., for the endoscopy tool tracking application).</li> <li>See patch_application.sh and restore_application.sh for more details about the patching process.</li> </ul>","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#2-run-benchmarks","title":"2. Run Benchmarks","text":"<pre><code>python benchmarks/holoscan_flow_benchmarking/benchmark.py -a &lt;app_name&gt; [options]\n</code></pre> <p>Key Options:</p> <ul> <li><code>-a, --app</code>: Application name</li> <li><code>-r, --runs</code>: Number of runs</li> <li><code>-i, --instances</code>: Instances per run</li> <li><code>-m, --max-frames</code>: Number of frames to process</li> <li><code>--sched</code>: Scheduler type</li> <li><code>-d, --directory</code>: Output directory</li> <li><code>--run-command</code>: Custom run command (if needed)</li> <li><code>-u, --monitor_gpu</code>: Monitor GPU utilization (discrete GPU only)</li> </ul> <p>For a complete list of arguments, run:</p> <pre><code>python benchmarks/holoscan_flow_benchmarking/benchmark.py -h\n</code></pre> <p>Example:</p> <pre><code># Run endoscopy tool tracking benchmark\npython benchmarks/holoscan_flow_benchmarking/benchmark.py \\\n    -a endoscopy_tool_tracking \\\n    -r 3 -i 3 -m 200 \\\n    --sched greedy \\\n    -d myoutputs\n</code></pre> <p>Output Files:</p> <ul> <li>Data flow logs: <code>logger_&lt;scheduler&gt;_&lt;run&gt;_&lt;instance&gt;.log</code></li> <li>GPU utilization: <code>gpu_utilization_&lt;scheduler&gt;_&lt;run&gt;.csv</code></li> </ul>","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#3-analyze-results","title":"3. Analyze Results","text":"","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#31-basic-analysis","title":"3.1 Basic Analysis","text":"<pre><code>python benchmarks/holoscan_flow_benchmarking/analyze.py \\\n    -g myoutputs/logger_greedy_* MyCustomGroup \\\n    -m -a  # Show max and average latencies\n</code></pre>","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#32-generate-cdf-plot","title":"3.2 Generate CDF Plot","text":"<pre><code>python benchmarks/holoscan_flow_benchmarking/analyze.py \\\n    --draw-cdf https://github.com/nvidia-holoscan/holohub/blob/main/benchmarks/holoscan_flow_benchmarking/single_path_cdf.png?raw=true \\\n    -g myoutputs/logger_greedy_* MyCustomGroup \\\n    --no-display-graphs\n</code></pre>","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#33-historical-analysis","title":"3.3 Historical Analysis","text":"<pre><code>python bar_plot_avg_datewise.py \\\n    avg_values_2023-10-{19,20,21}.csv \\\n    stddev_values_2023-10-{19,20,21}.csv\n</code></pre>","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#generate-application-graph-with-latency-numbers","title":"Generate Application Graph with Latency Numbers","text":"<p>The app_perf_graph.py script can be used to generate a graph of a Holoscan application with latency data from benchmarking embedded into the graph. The graph looks like the figure below, where graph nodes are operators along with their average and maximum execution times, and edges represent connection between operators along with the average and maximum data transfer latencies.</p> <p></p>","tags":["Benchmarking"]},{"location":"benchmarks/holoscan_flow_benchmarking/#monitor-application-performance-in-real-time","title":"Monitor application performance in real-time","text":"<pre><code># Terminal 1: Run benchmark\npython3 benchmarks/holoscan_flow_benchmarking/benchmark.py \\\n    -a endoscopy_tool_tracking \\\n    -i 1 -r 3 -m 1000 \\\n    --sched=greedy \\\n    -d endoscopy_results\n\n# Terminal 2: Generate live graph\npython3 benchmarks/holoscan_flow_benchmarking/app_perf_graph.py \\\n    -o live_app_graph.dot \\\n    -l endoscopy_results\n\n# Terminal 3: View live graph\nxdot live_app_graph.dot\n</code></pre>","tags":["Benchmarking"]},{"location":"benchmarks/model_benchmarking/","title":"Benchmark Model","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: cpp Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.2 Tested Holoscan SDK versions: 1.0.2 Contribution metric: Level 1 - Highly Reliable</p> <p>This application demonstrates an easy, quick and straightforward way to benchmark the scalaibility of inferencing with a model against a single video data stream in a typical Holoscan application. The video stream is played via a V4L2 loopback device. Then, the stream is preprocessed and fed to the model for inferencing. Then, the results are visualized after postprocessing.</p>","tags":["Benchmarking"]},{"location":"benchmarks/model_benchmarking/#usage","title":"Usage","text":"<p>As this application is, by default, set to use the ultrasound segmentation example, you can build and run the ultrasound segmentation example first, and then try running this benchmarking application.</p> <p>Build and run the ultrasound segmentation application: <pre><code>./holohub run ultrasound_segmentation --language=cpp --local\n</code></pre></p> <p>Now, this benchmarking application can be built and run. However, before doing so, the v4l2loopback must be run first. Check out the notes and prerequisites here to play a video via a V4L2 loopback device. Assuming, everything is set up correctly, the ultrasound segmentation example video could be run with the following command:</p> <p>Note: we are playing the video to <code>/dev/video6</code> after running <code>sudo modprobe v4l2loopback video_nr=6 max_buffers=4</code> <pre><code>$ ffmpeg -stream_loop -1 -re -i ./data/ultrasound_segmentation/ultrasound_256x256.avi -pix_fmt yuyv422 -f v4l2 /dev/video6\nffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/\n  ...\n  libavutil      56. 31.100 / 56. 31.100\n  libavcodec     58. 54.100 / 58. 54.100\n  ...\n[mov,mp4,m4a,3gp,3g2,mj2 @ 0x560a570b0740] st: 0 edit list: 1 Missing key frame while searching for timestamp: 0\n...\nInput #0, mov,mp4,m4a,3gp,3g2,mj2, from './data/ultrasound_segmentation/ultrasound_256x256.avi':\n...\n</code></pre></p> <p>Now, the benchmarking application can be built and run: <pre><code>./holohub run model_benchmarking --language=&lt;cpp/python&gt; --local\n</code></pre></p> <p>To use a different video, the video can be played via the above <code>ffmpeg</code> command.</p> <p>To use a different model, you can specify the data path in the <code>./holohub run model_benchmarking --language=&lt;cpp/python&gt; --local --no-local-build</code> command with the <code>-d</code> option, and the model name, residing in the data path directory, with the <code>-m</code> option.</p> <pre><code>./holohub run model_benchmarking --language=&lt;cpp/python&gt; --local --no-local-build --run-args=\"-d &lt;data_path&gt; -m &lt;model_name&gt;\"\n</code></pre> <p>To check the full list of options, run: <pre><code>./holohub run model_benchmarking --language=&lt;cpp/python&gt; --local --no-local-build --run-args=\"-h\"\n</code></pre></p>","tags":["Benchmarking"]},{"location":"benchmarks/model_benchmarking/#capabilities","title":"Capabilities","text":"<p>This benchmarking application can be used to measure performance of parallel inferences for the same model on a single video stream. The <code>-l</code> option can be used to specify the number of parallel inferences to run. Then, the same model will be loaded to the GPU multiple times defined by the <code>-l</code> parameter.</p> <p>The schematic diagram of this benchmarking application is in Figure 1. The visualization and (visualization + postprocessing) steps are marked as grey, as they can optionally be turned off with, respectively, <code>-p</code> and <code>-i</code> options. The figure shows a single video data stream is used in the application. Multiple ML/AI models are ingested by the Holoscan Inference operator to perform inference on a single data stream. The same ML model is replicated to be loaded multiple times to the GPU in this application.</p> <p></p> <p>Figure 1. The schematic diagram of the benchmarking application</p>","tags":["Benchmarking"]},{"location":"benchmarks/realtime_threads_benchmarking/","title":"Real-time Thread Scheduling Benchmark","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: cpp Last modified: October 27, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 3.5.0 Tested Holoscan SDK versions: 3.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This benchmark application demonstrates and evaluates the effectiveness of real-time thread scheduling in Holoscan applications. It compares the performance of normal thread scheduling against real-time scheduling policies (SCHED_DEADLINE, SCHED_FIFO, SCHED_RR) in scenarios with competing workloads.</p>","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/realtime_threads_benchmarking/#overview","title":"Overview","text":"<p>The benchmark creates a controlled environment to test real-time scheduling by: - Running a target Holoscan operator at a specific FPS (30 or 60 FPS) - Creating competing CPU load through load operators - Measuring timing precision and consistency for the Holoscan operator - Comparing normal scheduling vs real-time scheduling policies for Holoscan operator performance</p>","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/realtime_threads_benchmarking/#implementation","title":"Implementation","text":"<p>This benchmark is implemented in C++ for optimal performance and provides: - Low overhead and high precision timing measurements - Integrated automatic plot generation - Comprehensive real-time scheduling analysis - Configurable workload parameters for flexible testing</p>","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/realtime_threads_benchmarking/#usage","title":"Usage","text":"","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/realtime_threads_benchmarking/#basic-usage","title":"Basic Usage","text":"<p>Run the benchmark with default settings (60 FPS, 30 seconds, SCHED_DEADLINE): <pre><code>sudo ./holohub run realtime_threads_benchmarking \\\n  --docker-opts=\"--privileged -v /tmp/benchmark_plots:/tmp/benchmark_plots\"\n</code></pre></p> <p>Important: The benchmark requires: - <code>sudo</code> privileges to run Docker with <code>--privileged</code> flag - <code>--privileged</code> flag to enable real-time scheduling policies (SCHED_DEADLINE, SCHED_FIFO, SCHED_RR) - Volume mounting to access generated plots on the host system</p>","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/realtime_threads_benchmarking/#advanced-options","title":"Advanced Options","text":"<p>The benchmark supports several configuration options:</p> <pre><code>sudo ./holohub run realtime_threads_benchmarking \\\n  --docker-opts=\"--privileged -v /tmp/benchmark_plots:/tmp/benchmark_plots\" \\\n  --run-args=\"--target-fps 30 --duration 20 --scheduling-policy SCHED_DEADLINE --bg-load-intensity 2000\"\n</code></pre> <p>Available options: - <code>--target-fps</code>: Target FPS for the benchmark (30 or 60, default: 60) - <code>--duration</code>: Benchmark duration in seconds (default: 30) - <code>--scheduling-policy</code>: Real-time scheduling policy to test (SCHED_DEADLINE, SCHED_FIFO, SCHED_RR, default: SCHED_DEADLINE) - <code>--bg-load-intensity</code>: Background load intensity (iterations, default: 1000) - <code>--bg-workload-size</code>: Background workload size (data array size, default: 100) - <code>--bm-load-intensity</code>: Benchmark target load intensity (iterations, default: 100) - <code>--bm-workload-size</code>: Benchmark target workload size (data array size, default: 100) - <code>--worker-thread-number</code>: Worker thread number (default: 2) - <code>--dummy-load-number</code>: Number of dummy load operators (default: 2) - <code>--output</code>: Output JSON file for raw data (default: /tmp/benchmark_plots/realtime_thread_benchmark_results.json)</p>","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/realtime_threads_benchmarking/#timing-analysis-plots","title":"Timing Analysis Plots","text":"<p>The benchmark automatically generates detailed timing analysis plots including: - Frame period distribution histograms (full range and zoomed views) - Execution time distribution histograms - Time series plots showing frame periods and execution times over time</p> <p>To specify a custom output location: <pre><code>sudo ./holohub run realtime_threads_benchmarking \\\n  --docker-opts=\"--privileged -v /path/to/host/output:/custom/output\" \\\n  --run-args=\"--output /custom/output/my_results.json\"\n</code></pre></p> <p>Note: Plots are automatically saved to the same directory as the JSON output file. Use volume mounting to access them on the host system.</p>","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/realtime_threads_benchmarking/#architecture","title":"Architecture","text":"<p>The benchmark application consists of:</p> <ol> <li>Benchmark Operator: Main operator that aims to run at the specified FPS and measures timing performance</li> <li>Intentionally does NOT emit frame data to avoid framework overhead</li> <li>Focuses purely on operator scheduling and execution timing</li> <li>Measures frame periods and execution times</li> <li>Load Operators: Create CPU contention by performing computational work</li> <li>Configurable number of operators (default: 2)</li> <li>Run independently to create background CPU load</li> <li>No data flow between operators (avoids framework overhead)</li> <li>Thread Pools:</li> <li>Real-time pool for the benchmark operator (with Linux RT scheduling)</li> <li>Load pool for competing workloads (normal scheduling)</li> </ol>","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/realtime_threads_benchmarking/#scheduling-mode-comparison","title":"Scheduling Mode Comparison","text":"<p>The benchmark demonstrates the difference between normal and real-time scheduling:</p>","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/realtime_threads_benchmarking/#normal-scheduling-mode","title":"Normal Scheduling Mode","text":"<p>In normal scheduling, all operators compete equally for CPU resources, leading to timing variability and potential frame drops.</p>","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/realtime_threads_benchmarking/#real-time-scheduling-mode","title":"Real-time Scheduling Mode","text":"<p>With real-time scheduling, the target operator gets priority access to CPU resources, resulting in more consistent timing and better frame rate stability.</p>","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/realtime_threads_benchmarking/#metrics","title":"Metrics","text":"<p>The benchmark measures and compares:</p>","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/realtime_threads_benchmarking/#performance-metrics","title":"Performance Metrics","text":"<ul> <li>Frame Period Statistics: Mean, standard deviation, min/max of frame periods</li> </ul>","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/realtime_threads_benchmarking/#timing-analysis","title":"Timing Analysis","text":"<ul> <li>Frame Period Consistency: How consistently the target FPS is maintained</li> <li>Standard Deviation Reduction: Improvement in timing variability with real-time scheduling</li> <li>Resource Contention Impact: How competing workloads affect timing</li> </ul>","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/realtime_threads_benchmarking/#requirements","title":"Requirements","text":"","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/realtime_threads_benchmarking/#system-requirements","title":"System Requirements","text":"<ul> <li>Linux system with real-time scheduling support</li> <li>Docker with privileged mode support</li> <li><code>sudo</code> access to run Docker with <code>--privileged</code> flag</li> <li>Multiple CPU cores recommended for meaningful contention testing</li> </ul>","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/realtime_threads_benchmarking/#docker-requirements","title":"Docker Requirements","text":"<p>The benchmark requires running Docker in privileged mode to enable real-time scheduling: - Required Docker flag: <code>--privileged</code> - Required capabilities: <code>CAP_SYS_NICE</code> and <code>CAP_SYS_ADMIN</code> (automatically provided by <code>--privileged</code>) - Volume mounting: Required to access generated plots on the host system</p> <p>Without <code>--privileged</code>, you'll encounter \"Operation not permitted\" errors when trying to set real-time scheduling policies.</p>","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/realtime_threads_benchmarking/#understanding-results","title":"Understanding Results","text":"","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/realtime_threads_benchmarking/#example-output","title":"Example Output","text":"<p>The benchmark generates comprehensive visualization plots to help analyze real-time scheduling performance:</p>","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/realtime_threads_benchmarking/#timing-distribution-analysis","title":"Timing Distribution Analysis","text":"<p>This plot shows the distribution of frame periods and execution times, comparing normal scheduling vs real-time scheduling. The histograms reveal: - Frame Period Consistency: How tightly clustered the frame periods are around the target (16.67ms for 60 FPS) - Execution Time Stability: The variability in operator execution times - Scheduling Impact: Clear differences between normal and real-time scheduling policies</p>","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/realtime_threads_benchmarking/#timing-over-time-analysis","title":"Timing Over Time Analysis","text":"<p>This time-series plot demonstrates timing behavior throughout the benchmark duration, showing: - Frame Period Trends: How frame periods vary over time - Execution Time Patterns: Temporal patterns in operator execution - Real-time Benefits: Reduced standard deviation and more consistent timing with RT scheduling</p>","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/realtime_threads_benchmarking/#good-real-time-performance-indicators","title":"Good Real-time Performance Indicators","text":"<ul> <li>Frame Period Standard Deviation Reduction : Lower standard deviation in frame periods indicates more consistent timing</li> <li>Better handling of CPU contention under load</li> </ul>","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/realtime_threads_benchmarking/#example-output_1","title":"Example Output","text":"<pre><code>================================================================================\nBenchmark Configurations\n================================================================================\n  Target FPS: 60 (16.667 ms period)\n  Duration: 30s\n  Realtime: false\n  Background Load Intensity: 1000\n  Background Workload Size: 100\n  Benchmark Load Intensity: 100\n  Benchmark Workload Size: 100\n  Worker Thread Number: 2\n  Dummy Load Number: 2\n\n================================================================================\nBenchmark Results\n================================================================================\n=== Non-real-time Thread (Baseline) ===\nFrame period std: 0.257 ms\nFrame period mean: 16.667 ms\nFrame period min/max: 16.120 ms / 17.218 ms\n\n=== Real-time Thread ===\nFrame period std: 0.013 ms\nFrame period mean: 16.667 ms\nFrame period min/max: 16.569 ms / 16.753 ms\n\n================================================================================\nNon-real-time and Real-time Thread Benchmark Comparison\n================================================================================\nPeriod std comparison:     0.26 ms \u2192     0.01 ms  (+94.77%)\n\nRaw measurement data written to: /tmp/benchmark_plots/realtime_thread_benchmark_results.json\n\nGenerating plots...\n\nTiming plots saved to: /tmp/benchmark_plots\nGenerated plots:\n  - https://github.com/nvidia-holoscan/holohub/blob/main/benchmarks/realtime_threads_benchmarking/timing_over_time.png?raw=true (raw data points over time)\n  - https://github.com/nvidia-holoscan/holohub/blob/main/benchmarks/realtime_threads_benchmarking/simple_histograms.png?raw=true (distribution without overlays)\n</code></pre>","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/realtime_threads_benchmarking/#troubleshooting","title":"Troubleshooting","text":"","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/realtime_threads_benchmarking/#real-time-scheduling-permission-errors","title":"Real-time Scheduling Permission Errors","text":"<p>If you encounter errors like: <pre><code>[error] [event_based_scheduler.cpp:984] Failed to set SCHED_DEADLINE policy with policy=6, runtime=1666666, deadline=15833332, period=16666666: Operation not permitted\n[error] [event_based_scheduler.cpp:381] Failed to configure worker thread [pool name: realtime_pool, thread uid: 10]: GXF_FAILURE\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Ensure Docker privileged mode: <pre><code>sudo ./holohub run realtime_threads_benchmarking \\\n  --docker-opts=\"--privileged -v /tmp/benchmark_plots:/tmp/benchmark_plots\"\n</code></pre></p> </li> <li> <p>Remove kernel real-time runtime limits (run on host system, not in container): <pre><code>sudo sysctl -w kernel.sched_rt_runtime_us=-1\n</code></pre> This removes the kernel limit on real-time task runtime, which is often required for SCHED_DEADLINE scheduling.</p> </li> </ol> <p>Note: The kernel parameter change persists until reboot.</p>","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/realtime_threads_benchmarking/#missing-plot-files","title":"Missing Plot Files","text":"<p>If benchmark plots are not accessible on the host system, ensure proper volume mounting: - Plots are saved in the same directory as the JSON output file - Default: <code>/tmp/benchmark_plots/</code> (both JSON and plots) - Volume mount: <code>-v /tmp/benchmark_plots:/tmp/benchmark_plots</code> - For custom locations: Mount the directory containing your <code>--output</code> file path</p>","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/realtime_threads_benchmarking/#notes","title":"Notes","text":"<ul> <li>Real-time scheduling requires Docker <code>--privileged</code> mode and <code>sudo</code> privileges</li> <li>The benchmark automatically handles thread pool configuration and CPU pinning</li> <li>Detailed timing plots are automatically generated and saved to the specified directory</li> <li>Results may vary based on system load and hardware configuration</li> <li>For best results, run on a system with minimal background processes</li> </ul>","tags":["Threading","Benchmarking","Real-time","Performance"]},{"location":"benchmarks/release_benchmarking/","title":"Release Benchmarking Guide","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Last modified: August 5, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 2.3.0 Tested Holoscan SDK versions: 2.3.0, 2.6.0, 3.0.0, 3.3.0 Contribution metric: Level 3 - Developmental</p> <p>This tutorial provides a reproducible workflow for developers to accurately measure the latency of curated HoloHub applications across various SDK releases and different deployment scenarios, from single-application to multi-model use cases.</p> <p>Developers can use the Holoscan Flow Benchmarking tools referenced within this guide to systematically analyze performance bottlenecks, optimize execution times, and fine-tune their own applications for real-time, low-latency processing.</p>","tags":["Benchmarking","Performance"]},{"location":"benchmarks/release_benchmarking/#contents","title":"Contents","text":"<ul> <li>Background</li> <li>Previous Holoscan Release Benchmark Reports</li> <li>Running the Tutorial</li> <li>Running Benchmarks</li> <li>Summarizing Data</li> <li>Presenting Data</li> <li>Troubleshooting</li> <li>Developer References</li> </ul>","tags":["Benchmarking","Performance"]},{"location":"benchmarks/release_benchmarking/#background","title":"Background","text":"<p>Holoscan SDK emphasizes low end-to-end latency in application pipelines. In addition to other benchmarks, we can use HoloHub applications to evaluate Holoscan SDK performance over releases.</p> <p>In this tutorial we provide a reproducible workflow to evaluate end-to-end latency performance on the Endoscopy Tool Tracking and Multi-AI Ultrasound HoloHub projects. These projects are generally maintained by the NVIDIA Holoscan team and demonstrate baseline Holoscan SDK inference pipelines with video replay and Holoviz rendering output.</p> <p>Benchmark scenarios include: - Running multiple Holoscan SDK pipelines concurrently on a single machine - Running video replay input at real-time speeds or as fast as possible - Running Holoviz output with either visual rendering or in headless mode</p> <p>We plan to release HoloHub benchmarks in the release subfolder following Holoscan SDK general releases. You can follow the tutorial below to similarly evaluate performance on your own machine.</p> <p>Refer to related documents for more information: - the results report template file provides additional information on definitions and background - versioned releases are available for review in the release subfolder</p>","tags":["Benchmarking","Performance"]},{"location":"benchmarks/release_benchmarking/#previous-holoscan-release-benchmark-reports","title":"Previous Holoscan Release Benchmark Reports","text":"<ul> <li>Holoscan SDK v2.3.0</li> <li>Holoscan SDK v2.6.0</li> <li>Holoscan SDK v3.0.0</li> <li>Holoscan SDK v3.3.0</li> </ul>","tags":["Benchmarking","Performance"]},{"location":"benchmarks/release_benchmarking/#running-benchmarks-getting-started","title":"Running Benchmarks: Getting Started","text":"<p>Data collection can be run in the HoloHub base container for both the Endoscopy Tool Tracking and the Multi-AI Ultrasound applications. We've provided a custom Dockerfile with tools to process collected data into a benchmark report.</p> <pre><code># Build and launch the container\n./holohub run-container \\\n    --img holohub:release_benchmarking \\\n    --docker-file benchmarks/release_benchmarking/Dockerfile \\\n    --base-img nvcr.io/nvidia/clara-holoscan/holoscan:&lt;holoscan-sdk-version-gpu&gt;\n\n# Inside the container, build the applications in benchmarking mode\n./holohub build endoscopy_tool_tracking --benchmark --language=cpp\n./holohub build multiai_ultrasound --benchmark --language=cpp\n\n./holohub build release_benchmarking\n</code></pre> <p>Run the benchmarking script with no arguments to collect performance logs in the <code>./output</code> directory. <pre><code>./holohub run release_benchmarking --no-local-build\n</code></pre></p>","tags":["Benchmarking","Performance"]},{"location":"benchmarks/release_benchmarking/#summarizing-data","title":"Summarizing Data","text":"<p>After running benchmarks, inside the dev environment, use <code>./holohub run</code> to process data statistics and create bar plot PNGs: <pre><code>./holohub run-container --img holohub:release_benchmarking --no-docker-build\n./holohub run release_benchmarking --no-local-build --run-args \"--process benchmarks/release_benchmarking\"\n</code></pre></p> <p>Alternatively, collect results across platforms. On each machine: 1. Run benchmarks: <pre><code>./holohub run release_benchmarking --no-local-build\n</code></pre> 2. Add platform configuration information: <pre><code>./holohub run release_benchmarking --no-local-build --run-args \"--print\" &gt; benchmarks/release_benchmarking/output/platform.txt\n</code></pre> 3. Transfer output contents from each platform to a single machine: <pre><code># Compress information for transfer\npushd benchmarks/release_benchmarking\ntar cvf benchmarks-&lt;platform-name&gt;.tar.gz output/*\n\n# Migrate the results archive with your transfer tool of choice, such as SCP\n\n# Extract results to a subfolder on the target machine\nmkdir -p output/&lt;release&gt;/&lt;platform-name&gt;/\npushd output/&lt;release&gt;/&lt;platform-name&gt;\ntar xvf benchmarks-&lt;platform-name&gt;\n</code></pre> 4. Use multiple <code>--process</code> flags to generate a batch of bar plots for multiple platform results: <pre><code>./holohub run release_benchmarking --no-local-build --run-args \"\\\n    --process benchmarks/release_benchmarking/2.4/x86_64 \\\n    --process benchmarks/release_benchmarking/2.4/IGX_iGPU \\\n    --process benchmarks/release/benchmarking/2.4/IGX_dGPU\"\n</code></pre></p>","tags":["Benchmarking","Performance"]},{"location":"benchmarks/release_benchmarking/#presenting-data","title":"Presenting Data","text":"<p>You can use the template markdown file in the <code>template</code> folder to generate a markdown or PDF report with benchmark data with <code>pandoc</code> and <code>Jinja2</code>.</p> <ol> <li>Copy and edit <code>template/release.json</code> with information about the benchmarking configuration, including the release version, platform configurations, and local paths to processed data. Run <code>./holohub run</code> to print JSON-formatted platform details to the console about the current system: <pre><code>./holohub run-container --img holohub:release_benchmarking --no-docker-build\n./holohub run release_benchmarking --no-local-build --run-args=\"--print\"\n</code></pre></li> <li>Render the document with the Jinja CLI tool: <pre><code>pushd benchmarks/release_benchmarking\njinja2 template/results.md.tmpl template/&lt;release-version&gt;.json --format=json &gt; output/&lt;release-version&gt;.md\n</code></pre></li> </ol>","tags":["Benchmarking","Performance"]},{"location":"benchmarks/release_benchmarking/#optional-generating-a-pdf-report-document","title":"(Optional) Generating a PDF report document","text":"<p>You can convert the report to PDF format as an easy way to share your report as a single file with embedded plots.</p> <ol> <li>In your copy of <code>template/release.json</code>, update the <code>\"format\"</code> string to <code>\"pdf\"</code>.</li> <li>Follow the instructions above to generate your markdown report with Jinja2.</li> <li>Use <code>pandoc</code> to convert the markdown file to PDF: <pre><code>pushd output\npandoc &lt;release-version&gt;.md -o &lt;release-version&gt;.pdf --toc\n</code></pre></li> </ol>","tags":["Benchmarking","Performance"]},{"location":"benchmarks/release_benchmarking/#optional-submitting-results-to-holohub","title":"(Optional) Submitting Results to HoloHub","text":"<p>The Holoscan SDK team may submit release benchmarking reports to HoloHub git history for general visibility. We use Markdown formatting to make plot diagrams accessible for direct download.</p> <ol> <li>Move <code>&lt;release-version&gt;.md</code> and accompanying plots to a new <code>release/&lt;version&gt;</code> folder.</li> <li>Update image paths in <code>&lt;release-version.md&gt;</code> and verify locally with a markdown renderer such as VS Code.</li> <li>Commit changes, push to GitHub, and open a Pull Request.</li> </ol>","tags":["Benchmarking","Performance"]},{"location":"benchmarks/release_benchmarking/#cleanup","title":"Cleanup","text":"<p>Benchmarking changes to application YAML files can be discarded after benchmarks complete. <pre><code>git checkout applications/*.yaml\n</code></pre></p>","tags":["Benchmarking","Performance"]},{"location":"benchmarks/release_benchmarking/#troubleshooting","title":"Troubleshooting","text":"<p>Why am I seeing high end-to-end latency spikes as outliers in my data?</p> <p>Latency spikes may occur in display-driven benchmarking if the display goes to sleep. Please configure your display settings to prevent the display from going to sleep before running benchmarks.</p> <p>We have also infrequently observed latency spikes in cases where display drivers and CUDA Toolkit versions are not matched, and due to suboptimal GPU task preemption policies. We are still investigating these issues.</p> <p>Benchmark applications are failing silently without writing log files.</p> <p>Silent failures may indicate an issue with the underlying applications undergoing benchmarking. Try running the applications directly and verify execution is as expected: - <code>./holohub run endoscopy_tool_tracking --language=cpp</code> - <code>./holohub run multiai_ultrasound --language=cpp</code></p> <p>In some cases you may need to clear your HoloHub build or data folders to address errors: - <code>./holohub clear-cache</code> - <code>rm -rf ./data</code></p>","tags":["Benchmarking","Performance"]},{"location":"benchmarks/release_benchmarking/#developer-references","title":"Developer References","text":"<p>While this tutorial is tailored to curated configurations of the Endoscopy Tool Tracking and Multi-AI Ultrasound HoloHub applications, developers utilize underlying Holoscan data frame flow tracking tools to similarly measure and analyze performance in custom Holoscan applications.</p> <ul> <li>Refer to the Holoscan Flow Benchmarking project for general Holoscan performance profiling tools for both C++ and Python applications.</li> <li>Refer to the Holoscan Flow Benchmarking whitepaper and tutorial for a comprehensive overview of pipeline profiling tools.</li> <li>Refer to <code>run_benchmarks.sh</code> for additional examples demonstrating performance data collection and reporting with Holoscan Flow Tracking scripts.</li> </ul>","tags":["Benchmarking","Performance"]},{"location":"operators/advanced_network/","title":"Advanced Network library","text":"<p> Authors: Cliff Burdick (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: August 5, 2025 Latest version: 1.5 Minimum Holoscan SDK version: 2.6.0 Tested Holoscan SDK versions: 2.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>[!NOTE] The Advanced Network library previously included standard operators for transmitting and receiving packets to/from the NIC, also referred to as the Advanced Network Operator (ANO). These operators were removed to lower overhead, as aggregation/disaggregation of the packets still needed to be done in separate operators. We plan to provide more full-fledged generic operators in the future. In the meantime, you can continue to use this library to develop Holoscan operators adapted to your use case, now including the direct packet transaction with the NIC. Referred to the Benchmarking sample application for an example.</p> <p>[!WARNING] The library is undergoing large improvements as we aim to better support it as an NVIDIA product. API breakages might be more frequent until we reach version 1.0.</p> <p>[!TIP] Review the High Performance Networking tutorial for guided instructions to configure your system and test the Advanced Network library.</p> <p>The Advanced Network library provides a way for users to achieve the highest throughput and lowest latency for transmitting and receiving Ethernet frames out of and into Holoscan operators. Direct access to the NIC hardware is available in userspace, thus bypassing the kernel's networking stack entirely.</p>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#requirements","title":"Requirements","text":"<ul> <li>Linux</li> <li>An NVIDIA NIC with a ConnectX6-Dx or later chip</li> <li>System tuning as described here</li> <li>For DPDK, Rivermax: MLNX5/IB drivers with peermem support - either through:</li> <li>Inbox (i.e. standard) drivers on Ubuntu kernel versions &gt;= 5.4 and &lt; 6.8, or</li> <li>NVIDIA optimized kernels (IGX OS, DGX BaseOS), or</li> <li>OFED drivers from DOCA-Host 2.8 or later (install the <code>mlnx-ofed-kernel-dkms</code> package or the <code>doca-ofed</code> meta-package for extra tooling), or</li> <li>(deprecated) OFED drivers from MOFED 23.10 or later (<code>sudo ./mlnxofedinstall --kernel-only</code>).</li> <li>For GPUNetIO:</li> <li>The GDRCopy <code>gdrdrv</code> kernel module must be loaded on the bare-metal system.</li> <li>DOCA-OFED drivers from DOCA-Host 3.2.1 or later (install the <code>doca-ofed</code> package).</li> </ul> <p>User-space libraries are included in the Dockerfile for each networking backend. Inspect this file if you wish to know what is needed to build and run on baremetal instead.</p>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#features","title":"Features","text":"<ul> <li>High Throughput: Hundreds of gigabits per second is possible with the proper hardware</li> <li>Low Latency: With direct access to the NIC's ring buffers, most latency incurred is only PCIe latency</li> <li>Since the kernel's networking stack is bypassed, the user is responsible for defining the protocols used         over the network. In most cases Ethernet, IP, and UDP are ideal for this type of processing because of their         simplicity, but any type of protocol can be implemented or used. The advanced network library         gives the option to use several primitives to remove the need for filling out these headers for basic packet types,         but raw headers can also be constructed.</li> <li>GPUDirect: Optionally send data directly from the NIC to GPU, or directly from the GPU to NIC. GPUDirect has two modes:</li> <li>Header-data split: Split the header portion of the packet to the CPU and the rest (payload) to the GPU. The split point is     configurable by the user. This option should be the preferred method in most cases since it's easy to use and still     gives near peak performance.</li> <li>Batched GPU: Receive batches of whole packets directly into the GPU memory. This option requires the GPU kernel to inspect     and determine how to handle packets. While performance may increase slightly over header-data split, this method     requires more effort and should only be used for advanced users.</li> <li>GPUComms: Optionally control the send or receive communications from the GPU through the GPUDirect Async Kernel-Initiated network technology (enabled with the DOCA GPUNetIO backend only).</li> <li>Flow Configuration: Configure the NIC's hardware flow engine for configurable patterns. Currently only UDP source     and destination are supported.</li> </ul>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#limitations","title":"Limitations","text":"<p>The limitations below will be removed in a future release.</p> <ul> <li>Only UDP fill mode is supported</li> </ul>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#managers","title":"Managers","text":"<p>Internally the advanced network library is implemented by different backends, each offering different features.</p> <p>It is specified with the <code>manager</code> parameter, passed to the <code>advanced_network::adv_net_init</code> function before starting an application, along with all of the NIC parameters. This step allocates all packet buffers, initializes the queues on the NIC, and starts the appropriate number of internal threads to take packets off or put packets onto the NIC as fast as possible, using the backend-specific implementation.</p> <p>Developers can then use the rest of the Advanced Network library API to send and receive packets, and do any additional processing needed (e.g. aggregate, reorder, etc.), as described in the API Structures section.</p> <p>[!NOTE] To achieve zero copy throughout the whole pipeline only pointers are passed between each entity above. When the user receives the packets from the network library it's using the same buffers that the NIC wrote to either CPU or GPU memory. This architecture also implies that the user must explicitly decide when to free any buffers it's owning. Failure to free buffers will result in errors in the advanced network library not being able to allocate buffers.</p>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#dpdk","title":"DPDK","text":"<p>DPDK is an open-source userspace packet processing library supported across platforms and vendors.</p> <p>It is the default manager, and can be set with the values <code>dpdk</code> or <code>default</code>.</p> <p>Follow the instructions from the adv_networking_bench README to build the operator and sample application with DPDK (default).</p>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#doca-gpunetio","title":"DOCA GPUNetIO","text":"<p>NVIDIA DOCA brings together a wide range of powerful APIs, libraries, and frameworks for programming and accelerating modern data center infrastructures\u200b. DOCA GPUNetIO is one of the libraries included in the DOCA SDK. It enables the GPU to control, from a CUDA kernel, network communications directly interacting with the network card and completely removing the CPU from the critical data path.</p> <p>If the application wants to enable GPU communications, it must chose <code>gpunetio</code> as backend. The behavior of the GPUNetIO backend is similar to the DPDK one except that the receive and send are executed by CUDA kernels. Specifically:</p> <ul> <li>Receive: a persistent CUDA kernel is running on a dedicated stream and keeps receiving packets, providing packets' info to the application level. Due to the nature of the operator, the CUDA receiver kernel now is responsible only to receive packets but in a real-world application, it can be extended to receive and process in real-time network packets (DPI, filtering, decrypting, byte modification, etc..) before forwarding packets to the application.</li> <li>Send: every time the application wants to send packets it launches one or more CUDA kernels to prepare data and create Ethernet packets and then (without the need of synchronizing) forward the send request to the operator. The operator then launches another CUDA kernel that in turn sends the packets (still no need to synchronize with the CPU). The whole pipeline is executed on the GPU. Due to the nature of the operator, the packets' creation and packets' send must be split in two CUDA kernels but in a real-word application, they can be merged into a single CUDA kernel responsible for both packet processing and packet sending.</li> </ul> <p>Please refer to the DOCA GPUNetIO programming guide to correctly configure your system before using this transport layer.</p> <p>The GPUNetIO manager does not support the <code>split-boundary</code> option.</p> <p>Follow the instructions from the adv_networking_bench README to build the operator and sample application with DOCA GPUNetIO support.</p>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#rivermax","title":"RIVERMAX","text":"<p>NVIDIA Rivermax SDK Optimized networking SDK for media and data streaming applications. NVIDIA\u00ae Rivermax\u00ae offers a unique IP-based solution for any media and data streaming use case. Rivermax together with NVIDIA GPU accelerated computing technologies unlocks innovation for a wide range of applications in Media and Entertainment (M&amp;E), Broadcast, Healthcare, Smart Cities and more. Rivermax leverages NVIDIA ConnectX\u00ae and BlueField\u00ae DPU hardware-streaming acceleration technology that enables direct data transfers to and from the GPU, delivering best-in-class throughput and latency with minimal CPU utilization for streaming workloads. Rivermax is the only fully-virtualized streaming solution that complies with the stringent timing and traffic flow requirements of the SMPTE ST 2110-21 specification. Rivermax enables the future of cloud-based software-defined broadcasting. Product release highlights, documentation, platform support, installation and usage guides can be found in the Rivermax SDK Page. Frequently asked questions, customers product highlights, Video link and more are available on the Rivermax Product Page.</p> <p>To build and run the Dockerfile with <code>Rivermax</code> support, follow these steps:</p> <ul> <li>Visit the Rivermax SDK Page to download the Rivermax Release SDK.</li> <li>Obtain a Rivermax developer license from the same page. This is necessary for using the SDK.</li> <li>Copy the downloaded SDK tar file (e.g., <code>rivermax_ubuntu2204_1.70.31.tar.gz</code>) into your current working directory.</li> <li>You can adjust the path using the <code>RIVERMAX_SDK_ZIP_PATH</code> build argument if needed.</li> <li>Modify the version using the <code>RIVERMAX_VERSION</code> build argument if you're using a different SDK version.</li> <li>Place the obtained Rivermax developer license file (<code>rivermax.lic</code>) into the <code>/opt/mellanox/rivermax/</code> directory.</li> <li>Build the operator and sample application with Rivermax support, see adv_networking_bench README for instructions.</li> </ul>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#configuration-parameters","title":"Configuration Parameters","text":"","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#common-configuration","title":"Common Configuration","text":"<p>These common configurations are used by both TX and RX:</p> <ul> <li><code>version</code>: Version of the config. Only 1 is valid currently.</li> <li>type: <code>integer</code></li> <li><code>master_core</code>: Master core used to fork and join network threads. This core is not used for packet processing and can be bound to a non-isolated core. Should differ from isolated cores in queues below.</li> <li>type: <code>integer</code></li> <li><code>manager</code>: Backend networking library. default: <code>dpdk</code>. Other: <code>doca</code> (GPUNet IO), <code>rivermax</code></li> <li>type: <code>string</code></li> <li><code>log_level</code>: Backend log level. default: <code>warn</code>. Other: <code>trace</code> , <code>debug</code>, <code>info</code>, <code>error</code>, <code>critical</code>, <code>off</code></li> <li>type: <code>string</code></li> <li><code>tx_meta_buffers</code>: Metadata buffers for transmit. One buffer is used for each burst of packets (default: 4096)</li> <li>type: <code>integer</code></li> <li><code>rx_meta_buffers</code>: Metadata buffers for receive. One buffer is used for each burst of packets (default: 4096)</li> <li>type: <code>integer</code></li> </ul>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#memory-regions","title":"Memory regions","text":"<p><code>memory_regions:</code> List of regions where buffers are stored.</p> <ul> <li><code>name</code>: Memory Region name</li> <li>type: <code>string</code></li> <li><code>kind</code>: Location. Best options are <code>device</code> (GPU), or <code>huge</code> (pages - CPU). Not recommended: <code>host</code> (CPU), <code>host_pinned</code> (CPU).</li> <li>type: <code>string</code></li> <li><code>affinity</code>: GPU ID for GPU memory, NUMA Node ID for CPU memory</li> <li>type: <code>integer</code></li> <li><code>access</code>: Permissions to the rdma memory region ( <code>local</code> or <code>rmda_read</code> or <code>rdma_write</code>)</li> <li>type: <code>string</code></li> <li><code>num_bufs</code>: Higher value means more time to process, but less space on GPU BAR1. Too low means risk of dropped packets from NIC having nowhere to write (Rx) or higher latency from buffering (Tx). Good rule of \ud83d\udc4d : 3x batch_size</li> <li>type: <code>integer</code></li> <li><code>buf_size</code>: Size of buffer, equal to packet size or less if breaking down packets (ex: header data split)</li> <li>type: <code>integer</code></li> </ul>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#interfaces","title":"Interfaces","text":"<ul> <li><code>interfaces</code>:  List and configure ethernet interfaces     full path: <code>cfg\\interfaces\\</code><ul> <li><code>name</code>: Name of the interfaca</li> <li>type: <code>string</code></li> <li><code>address</code>: PCIe BDF address (lspci) or linux interface name for DPDK/GPUNetIO/RiverMax or IP address for RDMA</li> <li>type: <code>string</code></li> <li><code>rx|tx</code> category of queues below full path: <code>cfg\\interfaces\\[rx|tx]</code></li> </ul> </li> </ul>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#receive-configuration-rx","title":"Receive Configuration (rx)","text":"<ul> <li><code>queues</code>: List of queues on NIC     type: <code>list</code>     full path: <code>cfg\\interfaces\\rx\\queues</code><ul> <li><code>name</code>: Name of queue<ul> <li>type: <code>string</code></li> </ul> </li> <li><code>id</code>: Integer ID used for flow connection or lookup in operator compute method<ul> <li>type: <code>integer</code></li> </ul> </li> <li><code>cpu_core</code>: CPU core ID. Should be isolated when CPU polls the NIC for best performance.. Not in use for Doca GPUNetIO     Rivermax manager can accept coma separated list of CPU IDs<ul> <li>type: <code>string</code></li> </ul> </li> <li><code>batch_size</code>: Number of packets in a batch passed from the NIC to the downstream operator. A larger number increases throughput but reduces end-to-end latency, as it takes longer to populate a single buffer. A smaller number reduces end-to-end latency but can also reduce throughput.<ul> <li>type: <code>integer</code></li> </ul> </li> <li><code>memory_regions</code>: List of memory regions where buffers are stored. memory regions names are configured in the Memory Regions section     type: <code>list</code></li> <li><code>timeout_us</code>: Timeout value that a batch will be sent on even if not enough packets to fill a batch were received<ul> <li>type: <code>integer</code></li> </ul> </li> </ul> </li> <li> <p><code>flex_items</code>: Flexible parser flow items     type: <code>list</code>     full path: <code>cfg\\interfaces\\rx\\flex_items</code></p> <ul> <li><code>name</code>: Name of flow item<ul> <li>type: <code>string</code></li> </ul> </li> <li><code>id</code>: ID of the flow item<ul> <li>type: <code>integer</code></li> </ul> </li> <li><code>offset</code>: Offset in bytes of where to match after the UDP header. Must be a multiple of 4 and &lt; 28<ul> <li>type: <code>integer</code></li> </ul> </li> <li><code>udp_dst_port</code>: UDP destination port for flex item match         - type: <code>integer</code></li> </ul> </li> <li> <p><code>flows</code>: List of flows - rules to apply to packets, mostly to divert to the right queue. (Not in use for Rivermax manager)   type: <code>list</code>   full path: <code>cfg\\interfaces\\[rx|tx]\\flows</code></p> <ul> <li><code>name</code>: Name of the flow</li> <li>type: <code>string</code></li> <li><code>id</code>: ID of the flow</li> <li>type: <code>integer</code></li> <li><code>action</code>: Action section of flow (what happens. Currently only supports steering to a given queue)</li> <li>type: <code>sequence</code><ul> <li><code>type</code>: Type of action. Only <code>queue</code> is supported currently.</li> <li>type: <code>string</code></li> <li><code>id</code>: ID of queue to steer to<ul> <li>type: <code>integer</code></li> </ul> </li> </ul> </li> <li><code>match</code>: Match section of flow</li> <li>type: <code>sequence</code><ul> <li><code>udp_src</code>: UDP source port or a range of ports (eg 1000-1010)</li> <li>type: <code>integer</code></li> <li><code>udp_dst</code>: UDP destination port or a range of ports (eg 1000-1010)</li> <li>type: <code>integer</code></li> <li><code>ipv4_len</code>: IPv4 payload length</li> <li>type: <code>integer</code></li> <li><code>flex_item_id</code>: Flex item ID from RX section. Flex items cannot be applied if UDP or IP matching above are used</li> <li>type: <code>integer</code></li> <li><code>val</code>: 32b value to match on</li> <li>type: <code>integer</code></li> <li><code>mask</code>: 32b mask to apply before the match</li> <li>type: <code>integer</code></li> </ul> </li> </ul> </li> </ul>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#extended-receive-configuration-for-rivermax-manager","title":"Extended Receive Configuration for Rivermax manager","text":"<ul> <li> <p><code>rivermax_rx_settings</code>: Extended RX settings for Rivermax Manager. Rivermax Manager supports receiving the same stream from multiple redundant paths (IPO - Inline Packet Ordering).     Each path is a combination of a source IP address, a destination IP address, a destination port, and a local IP address of the receiver device.   type: <code>list</code>   full path: <code>cfg\\interfaces\\rx\\queues\\rivermax_rx_settings</code></p> <ul> <li><code>memory_registration</code>: Flag, when enabled, reduces the number of memory keys in use by registering all the memory in a single pass on the application side.     Can be used only together with HDS enabled<ul> <li>type: <code>boolean</code></li> <li>default:<code>false</code></li> </ul> </li> <li><code>max_path_diff_us</code>: Sets the maximum number of microseconds that receiver waits for the same packet to arrive from a different stream (if IPO is enabled)<ul> <li>type: <code>integer</code></li> <li>default:<code>0</code></li> </ul> </li> <li><code>ext_seq_num</code>: The RTP sequence number is used by the hardware to determine the location of arriving packets in the receive buffer.     The application supports two sequence number parsing modes: 16-bit RTP sequence number (default) and 32-bit extended sequence number,     consisting of 16 low order RTP sequence number bits and 16 high order bits from the start of RTP payload. When set to <code>true</code> 32-bit ext. sequence number will be used<ul> <li>type: <code>boolean</code></li> <li>default:<code>true</code></li> </ul> </li> <li><code>sleep_between_operations_us</code>: Specifies the duration, in microseconds, that the receiver will pause or sleep between two consecutive receive (RX) operations.<ul> <li>type: <code>integer</code></li> <li>default:<code>0</code></li> </ul> </li> <li><code>local_ip_addresses</code>: List of Local NIC IP Addresses (one address per receiving path)<ul> <li>type: <code>sequence</code></li> </ul> </li> <li><code>source_ip_addresses</code>: List of Sender IP Addresses (one address per receiving path)<ul> <li>type: <code>sequence</code></li> </ul> </li> <li><code>destination_ip_addresses</code>: List of Destination IP Addresses (one address per receiving path), can be multicast<ul> <li>type: <code>sequence</code></li> </ul> </li> <li><code>destination_ports</code>: List of Destination IP ports (one port per receiving path)<ul> <li>type: <code>sequence</code></li> </ul> </li> <li><code>stats_report_interval_ms</code>: Specifies the duration, in milliseconds, that the receiver will display statistics in the log. Set <code>0</code> to disable statistics logging feature<ul> <li>type: <code>integer</code></li> <li>default:<code>0</code></li> </ul> </li> <li><code>send_packet_ext_info</code>: Enables the transmission of extended metadata for each received packet<ul> <li>type: <code>boolean</code></li> <li>default:<code>true</code></li> </ul> </li> </ul> </li> <li> <p>Example of the Rivermax queue configuration for redundant stream using HDS and GPU   This example demonstrates receiving a redundant stream sent from a sender with source addresses 192.168.100.4 and 192.168.100.3.   The stream is received via NIC which have local IP (same) 192.168.100.5 (listed twice, once per stream).   The multicast addresses and UDP ports on which the stream is being received are 224.1.1.1:5001 and 224.1.1.2:5001  The incoming packets are of size 1152 bytes. The initial 20 bytes are stripped from the payload as an  application header and placed in buffers allocated in RAM.  The remaining 1132 bytes are placed in dedicated payload buffers.  In this case, the payload buffers are allocated in GPU 0 memory. <pre><code>    memory_regions:\n    - name: \"Data_RX_CPU\"\n      kind: \"huge\"\n      affinity: 0\n      access:\n        - local\n      num_bufs: 43200\n      buf_size: 20\n    - name: \"Data_RX_GPU\"\n      kind: \"device\"\n      affinity: 0\n      access:\n        - local\n      num_bufs: 43200\n      buf_size: 1132\n    interfaces:\n    - address: 0005:03:00.0\n      name: data1\n      rx:\n        queues:\n        - name: Data1\n          id: 0\n          cpu_core: '11'\n          batch_size: 4320\n          rivermax_rx_settings:\n            settings_type: \"ipo_receiver\"\n            memory_registration: true\n            max_path_diff_us: 10000\n            ext_seq_num: true\n            sleep_between_operations_us: 0\n            memory_regions:\n            - \"Data_RX_CPU\"\n            - \"Data_RX_GPU\"\n            local_ip_addresses:\n            - 192.168.100.5\n            - 192.168.100.5\n            source_ip_addresses:\n            - 192.168.100.4\n            - 192.168.100.4\n            destination_ip_addresses:\n            - 224.1.1.1\n            - 224.1.1.2\n            destination_ports:\n            - 50001\n            - 50001\n            stats_report_interval_ms: 3000\n            send_packet_ext_info: true\n</code></pre></p> </li> </ul>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#transmit-configuration-tx","title":"Transmit Configuration (tx)","text":"<ul> <li><code>queues</code>: List of queues on NIC     type: <code>list</code>     full path: <code>cfg\\interfaces\\tx\\queues</code><ul> <li><code>name</code>: Name of queue<ul> <li>type: <code>string</code></li> </ul> </li> <li><code>id</code>: Integer ID used for flow connection or lookup in operator compute method<ul> <li>type: <code>integer</code></li> </ul> </li> <li><code>cpu_core</code>: CPU core ID. Should be isolated when CPU polls the NIC for best performance.. Not in use for Doca GPUNetIO     Rivermax manager can accept coma separated list of CPU IDs<ul> <li>type: <code>string</code></li> </ul> </li> <li><code>batch_size</code>: Number of packets in a batch that the NIC needs to receive from the upstream operator before sending them over the network. A larger number increases throughput but reduces end-to-end latency. A smaller number reduces end-to-end latency but can also reduce throughput.<ul> <li>type: <code>integer</code></li> </ul> </li> <li><code>memory_regions</code>: List of memory regions where buffers are stored. memory regions names are configured in the Memory Regions section     type: <code>list</code></li> <li><code>accurate_send</code>: Accurate TX sending enabled for sending packets at a specific PTP timestamp<ul> <li>type: <code>boolean</code></li> </ul> </li> </ul> </li> </ul>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#transmit-configuration-tx_1","title":"Transmit Configuration (tx)","text":"<ul> <li> <p><code>queues</code>: List of queues on NIC   Type: <code>list</code> Full Path: <code>cfg\\interfaces\\tx\\queues</code></p> </li> <li> <p><code>name</code>: Name of the queue</p> <ul> <li>Type: <code>string</code></li> </ul> </li> <li> <p><code>id</code>: Integer ID used for flow connection or lookup in operator compute method</p> <ul> <li>Type: <code>integer</code></li> </ul> </li> <li> <p><code>cpu_core</code>: CPU core ID. Should be isolated when CPU polls the NIC for best performance. Not in use for DOCA GPUNetIO. Rivermax Manager can accept comma-separated list of CPU IDs.</p> <ul> <li>Type: <code>string</code></li> </ul> </li> <li> <p><code>batch_size</code>: Number of packets in batch passed between operators. Larger values increase throughput at cost of latency.</p> <ul> <li>Type: <code>integer</code></li> </ul> </li> <li> <p><code>memory_regions</code>: List of memory regions where buffers are stored (configured in Memory Regions section)</p> <ul> <li>Type: <code>list</code></li> </ul> </li> </ul>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#extended-transmit-configuration-for-rivermax-manager","title":"Extended Transmit Configuration for Rivermax manager","text":"<p>The Rivermax TX configuration enables hardware-assisted SMPTE 2110-20 compliant video streaming with: - Precision timestamping via PCIe PTP clock synchronization - Jitter-free packetization of rasterized video frames - Automatic UDP checksum offload - Traffic shaping for constant bitrate delivery</p> <ul> <li> <p><code>rivermax_tx_settings</code>: Extended TX settings for SMPTE 2110-20 media streaming     Type: <code>sequence</code> Full Path: <code>cfg\\interfaces\\tx\\queues\\rivermax_tx_settings</code></p> <ul> <li><code>settings_type</code>: Transmission mode. Must be <code>media_sender</code> for video</li> <li>Type: <code>string</code></li> <li>Required: Yes</li> <li> <p>Valid Values: <code>media_sender</code></p> </li> <li> <p><code>memory_registration</code>: Enables bulk registration of GPU/CPU memory regions with NIC for zero-copy transfers. Recommended for high-throughput scenarios.</p> </li> <li>Type: <code>boolean</code></li> <li> <p>Default: <code>true</code></p> </li> <li> <p><code>memory_allocation</code>:  I\\O Memory allocated by application</p> </li> <li>Type: <code>boolean</code></li> <li> <p>Default: <code>true</code></p> </li> <li> <p><code>memory_pool_location</code>: Buffer memory type (<code>device/huge_pages/host_pinned/host</code>)</p> </li> <li>Type: <code>string</code></li> <li> <p>Required: Yes</p> </li> <li> <p><code>local_ip_address</code>: Source IP address bound to transmitting network interface.</p> </li> <li>Type: <code>string</code></li> <li> <p>Required: Yes</p> </li> <li> <p><code>destination_ip_address</code>: Unicast/Multicast group address for media stream distribution.</p> </li> <li>Type: <code>string</code></li> <li> <p>Required: Yes</p> </li> <li> <p><code>destination_port</code>: UDP port number for media stream transmission</p> </li> <li>Type: <code>integer</code></li> <li>Range: 1024-65535</li> <li> <p>Required: Yes</p> </li> <li> <p><code>video_format</code>: Defines pixel sampling structure per SMPTE ST 2110-20</p> </li> <li>Type: <code>string</code></li> <li>Required: Yes</li> <li> <p>Valid Values: <code>YCbCr-4:2:2</code>, <code>YCbCr-4:4:4</code>,<code>YCbCr-4:2:0</code>, <code>RGB</code></p> </li> <li> <p><code>bit_depth</code>: Color component quantization precision</p> </li> <li>Type: <code>integer</code></li> <li>Required: Yes</li> <li> <p>Valid Values: <code>8</code>, <code>10</code>, <code>12</code></p> </li> <li> <p><code>frame_width</code>: Horizontal resolution in pixels</p> </li> <li>Type: <code>integer</code></li> <li>Required: Yes</li> <li> <p>Valid Values: <code>1920</code> for HD, <code>3840</code> for 4K UHD</p> </li> <li> <p><code>frame_height</code>: The vertical resolution of the video in pixels</p> </li> <li>Type: <code>integer</code></li> <li>Required: Yes</li> <li> <p>Valid Values: <code>1080</code> for HD, <code>2160</code> for 4K UHD</p> </li> <li> <p><code>frame_rate</code>: Frame rate in fps</p> </li> <li>Type: <code>integer</code></li> <li>Required: Yes</li> <li> <p>Valid Values:  <code>24</code>, <code>25</code>, <code>30</code>, <code>50</code>, and <code>60</code></p> </li> <li> <p><code>dummy_sender</code>: Test mode without NIC transmission</p> </li> <li>Type: <code>boolean</code></li> <li> <p>Default: <code>false</code></p> </li> <li> <p><code>stats_report_interval_ms</code>: Transmission stats logging interval (0=disable)</p> </li> <li>Type: <code>integer</code></li> <li> <p>Default: <code>0</code></p> </li> <li> <p><code>verbose</code>: Enable detailed transmission logging</p> </li> <li>Type: <code>boolean</code></li> <li> <p>Default: <code>false</code></p> </li> <li> <p><code>sleep_between_operations</code>: Add inter-burst delays for timing sync</p> </li> <li>Type: <code>boolean</code></li> <li>Default: <code>false</code></li> </ul> </li> </ul>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#example-configuration","title":"Example Configuration","text":"<pre><code>    memory_regions:\n    - name: \"Data_TX_CPU\"\n      kind: \"huge\"\n      affinity: 0\n      num_bufs: 43200\n      buf_size: 20\n    - name: \"Data_TX_GPU\"\n      kind: \"device\"\n      affinity: 0\n      num_bufs: 43200\n      buf_size: 1200\n\n    interfaces:\n    - name: \"tx_port\"\n      address: cc:00.1\n      tx:\n        queues:\n        - name: \"tx_q_1\"\n          id: 0\n          cpu_core:  \"13\"\n          batch_size: 4320\n          output_port: \"bench_tx_out_1\"\n          memory_regions:\n          - \"Data_TX_CPU\"\n          - \"Data_TX_GPU\"\n          rivermax_tx_settings:\n            settings_type: \"media_sender\"\n            memory_registration: true\n            memory_allocation: true\n            memory_pool_location: \"host_pinned\"\n            #allocator_type: \"huge_page_2mb\"\n            verbose: true\n            sleep_between_operations: false\n            local_ip_address: 2.1.0.12\n            destination_ip_address: 224.1.1.2\n            destination_port: 50001\n            stats_report_interval_ms: 1000\n            send_packet_ext_info: true\n            video_format: YCbCr-4:2:2\n            bit_depth: 10\n            frame_width: 1920\n            frame_height: 1080\n            frame_rate: 60\n            dummy_sender: false\n</code></pre>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#api-structures","title":"API Structures","text":"<p>The Advanced Network library uses a common structure named <code>BurstParams</code> to pass data to/from other operators. <code>BurstParams</code> provides pointers to packet memory locations (e.g., CPU or GPU) and contains metadata needed by any operator to track allocations. Interacting with <code>BurstParams</code> should only be done with the helper functions described below.</p>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#example-api-usage","title":"Example API Usage","text":"<p>For an entire list of API functions, please see the <code>advanced_network/common.h</code> header file.</p>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#receive","title":"Receive","text":"<p>The section below describes a workflow using GPUDirect to receive packets using header-data split. The job of the user's operator(s) is to process and free the buffers as quickly as possible. This might be copying to interim buffers or freeing before the entire pipeline is done processing. This allows the networking piece to use relatively few buffers while still achieving very high rates.</p> <p>The first step in receiving from the NIC is to receive a <code>BurstParams</code> structure when a batch is complete:</p> <pre><code>BurstParams *burst;\nint port_id_ = 0;\nint queue_id_ = 0;\nauto status = get_rx_burst(&amp;burst, port_id_, queue_id_);\n</code></pre> <p>The packets arrive in scattered packet buffers. Depending on the application, you may need to iterate through the packets to aggregate them into a single buffer. Alternatively the operator handling the packet data can operate on a list of packet pointers rather than a contiguous buffer. Below is an example of aggregating separate GPU packet buffers into a single GPU buffer:</p> <pre><code>  for (int p = 0; p &lt; get_num_packets(burst); p++) {\n    h_dev_ptrs_[aggr_pkts_recv_ + p]   = get_cpu_packet_ptr(burst, p);\n    ttl_bytes_in_cur_batch_           += get_gpu_packet_length(burst, p) + sizeof(UDPPkt);\n  }\n\n  simple_packet_reorder(buffer, h_dev_ptrs, packet_len, burst-&gt;hdr.num_pkts);\n</code></pre> <p>For this example we are tossing the header portion (CPU), so we don't need to examine the packets. Since we launched a reorder kernel to aggregate the packets in GPU memory, we are also done with the GPU pointers. All buffers may be freed for the NIC to reuse at this point:</p> <pre><code>free_all_burst_packets_and_burst(burst_bufs_[b]);\n</code></pre>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/advanced_network/#transmit","title":"Transmit","text":"<p>Transmitting packets works similar to the receive side, except the user is tasked with filling out the packets as much as it needs to. As mentioned above, helper functions are available to fill in most boilerplate header information if that doesn't change often.</p> <p>Before sending packets, the user's transmit operator must request a buffer from the NIC:</p> <pre><code>auto burst = create_tx_burst_params();\nset_header(burst, port_id, queue_id, batch_size, num_segments);\nif ((ret = get_tx_packet_burst(burst)) != Status::SUCCESS) {\n  HOLOSCAN_LOG_ERROR(\"Error returned from get_tx_packet_burst: {}\", static_cast&lt;int&gt;(ret));\n  return;\n}\n</code></pre> <p>The code above creates a shared <code>BurstParams</code>, and uses <code>get_tx_packet_burst</code> to populate the burst buffers with valid packet buffers. On success, the buffers inside the burst structure will be allocated and are ready to be filled in. Each packet must be filled in by the user. In this example we loop through each packet and populate a buffer:</p> <pre><code>for (int num_pkt = 0; num_pkt &lt; get_num_packets(burst); num_pkt++) {\n  void *payload_src = data_buf + num_pkt * payload_size;\n  if (set_udp_payload(burst, num_pkt, payload_src, payload_size) != Status::SUCCESS) {\n    HOLOSCAN_LOG_ERROR(\"Failed to create packet {}\", num_pkt);\n  }\n}\n</code></pre> <p>The code iterates over the number of packets in the burst (defined above by the user) and passes a pointer to the payload and the packet size to <code>set_udp_payload</code>. In this example our configuration is using <code>fill_mode</code> \"udp\" on the transmitter, so <code>set_udp_payload</code> will populate the Ethernet, IP, and UDP headers. The payload pointer passed by the user is also copied into the buffer. Alternatively a user could use the packet buffers directly as output from a previous stage to avoid this extra copy.</p> <p>With the <code>BurstParams</code> populated, the burst can be sent off to the NIC:</p> <pre><code>send_tx_burst(burst);\n</code></pre>","tags":["Networking and Distributed Computing","DPDK","UDP","IP","GPUDirect","RDMA"]},{"location":"operators/aja_source/","title":"AJA Source Operator","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The AJA Source operator provides functionality to capture high-quality video streams from AJA capture cards and devices. It offers comprehensive support for both SDI (Serial Digital Interface) and HDMI (High-Definition Multimedia Interface) input sources, allowing for professional video capture in various formats and resolutions. The operator is designed to work seamlessly with AJA's hardware capabilities, including features like frame synchronization and format detection. Additionally, it provides an optional overlay channel capability that enables real-time mixing and compositing of multiple video streams, making it suitable for applications requiring picture-in-picture, graphics overlay, or other video mixing scenarios.</p>","tags":["Camera","AJA"]},{"location":"operators/aja_source/#requirements","title":"Requirements","text":"<ul> <li>AJA capture card (e.g., KONA HDMI)</li> <li>CUDA-capable GPU</li> <li>Holoscan SDK 1.0.3 or later</li> </ul>","tags":["Camera","AJA"]},{"location":"operators/aja_source/#parameters","title":"Parameters","text":"<p>The following parameters can be configured for this operator:</p> Parameter Type Description Default <code>device</code> string Device specifier (e.g., \"0\" for device 0) \"0\" <code>channel</code> NTV2Channel Camera channel to use for input NTV2_CHANNEL1 <code>width</code> uint32_t Width of the video stream 1920 <code>height</code> uint32_t Height of the video stream 1080 <code>framerate</code> uint32_t Frame rate of the video stream 60 <code>interlaced</code> bool Whether the video is interlaced false <code>rdma</code> bool Enable RDMA for video input false <code>enable_overlay</code> bool Enable overlay channel false <code>overlay_channel</code> NTV2Channel Camera channel to use for overlay NTV2_CHANNEL2 <code>overlay_rdma</code> bool Enable RDMA for overlay false","tags":["Camera","AJA"]},{"location":"operators/aja_source/#supported-video-formats","title":"Supported Video Formats","text":"<p>The operator supports various video formats based on resolution, frame rate, and scan type:</p> <ul> <li>720p (1280x720) at 50/59.94/60 fps</li> <li>1080i (1920x1080) at 50/59.94/60 fps</li> <li>1080p (1920x1080) at 23.98/24/25/29.97/30/50/59.94/60 fps</li> <li>UHD (3840x2160) at 23.98/24/25/29.97/30/50/59.94/60 fps</li> <li>4K (4096x2160) at 23.98/24/25/29.97/30/50/59.94/60 fps</li> </ul>","tags":["Camera","AJA"]},{"location":"operators/aja_source/#input-ports","title":"Input Ports","text":"<ul> <li>overlay_buffer_input (optional): Video buffer for overlay mixing when <code>enable_overlay</code> is true</li> </ul>","tags":["Camera","AJA"]},{"location":"operators/aja_source/#output-ports","title":"Output Ports","text":"<ul> <li>video_buffer_output: Video buffer containing the captured frame</li> <li>overlay_buffer_output (optional): Empty video buffer for overlay when <code>enable_overlay</code> is true</li> </ul>","tags":["Camera","AJA"]},{"location":"operators/apriltag_detector/","title":"AprilTag Detection Operator","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.2.0 Tested Holoscan SDK versions: 2.2.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>apriltag_detection</code> extension provides real-time detection of April tags from different tag families. The detection and processing is performed efficiently using CUDA acceleration for optimal performance in real-time applications.</p>","tags":["Image Processing","Camera"]},{"location":"operators/apriltag_detector/#overview","title":"Overview","text":"<p>April tags are 2D barcode-like patterns that can be used for camera calibration, pose estimation, and object tracking. This operator integrates the AprilTag detection library with NVIDIA Holoscan, providing GPU-accelerated tag detection capabilities.</p>","tags":["Image Processing","Camera"]},{"location":"operators/apriltag_detector/#features","title":"Features","text":"<ul> <li>CUDA-accelerated processing: Leverages GPU computing for high-performance tag detection</li> <li>Multiple tag family support: Compatible with various AprilTag families</li> <li>Real-time performance: Optimized for low-latency applications</li> </ul>","tags":["Image Processing","Camera"]},{"location":"operators/apriltag_detector/#usage","title":"Usage","text":"<pre><code>from holoscan.operators import ApriltagDetectorOp\n\n# Create the operator\napriltag_op = ApriltagDetectorOp(\n    width=1920,\n    height=1080,\n    number_of_tags=10\n)\n</code></pre>","tags":["Image Processing","Camera"]},{"location":"operators/apriltag_detector/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>width</code> <code>int</code> <code>None</code> Width of the input video stream in pixels. Must match the actual input resolution. <code>height</code> <code>int</code> <code>None</code> Height of the input video stream in pixels. Must match the actual input resolution. <code>number_of_tags</code> <code>int</code> <code>None</code> Maximum number of April tags to detect and output. Higher values may impact performance.","tags":["Image Processing","Camera"]},{"location":"operators/apriltag_detector/#inputoutput-ports","title":"Input/Output Ports","text":"<ul> <li><code>input</code>: Video stream input</li> <li><code>output</code>: Detected AprilTag ID and corner coordinates</li> </ul>","tags":["Image Processing","Camera"]},{"location":"operators/basic_network/","title":"Basic networking operator","text":"<p> Authors: Cliff Burdick (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>basic_network_operator</code> operator provides a way to send and receive data over Linux sockets. The destination can be on the same machine or over a network. The basic network operator contains separate operators for transmit and receive. Users may choose one or the other, or use both in applications requiring bidirectional traffic.</p> <p>For TCP sockets the basic network operator only supports a single stream currently. Future versions may expand this to launch multiple threads to listen on different streams.</p> <p>The basic networking operators use class names: <code>BasicNetworkOpTx</code> and <code>BasicNetworkOpRx</code></p>","tags":["Networking and Distributed Computing","UDP","IP","TCP"]},{"location":"operators/basic_network/#nvidiaholoscanbasic_network_operator","title":"<code>nvidia::holoscan::basic_network_operator</code>","text":"<p>Basic networking operator</p>","tags":["Networking and Distributed Computing","UDP","IP","TCP"]},{"location":"operators/basic_network/#receiver-configuration-parameters","title":"Receiver Configuration Parameters","text":"<ul> <li><code>batch_size</code>: Bytes in batch</li> <li>type: <code>integer</code></li> <li><code>max_payload_size</code>: Maximum payload size for a single packet</li> <li>type: <code>integer</code></li> <li><code>udp_dst_port</code>: UDP destination port for packets</li> <li>type: <code>integer</code></li> <li><code>l4_proto</code>: Layer 4 protocol</li> <li>type: <code>string</code> (<code>udp</code>/<code>tcp</code>)</li> <li><code>ip_addr</code>: Destination IP address</li> <li>type: <code>string</code></li> <li><code>max_burst_interval</code> (Optional): Maximum time interval between bursts (ms)</li> <li>type: <code>integer</code></li> </ul>","tags":["Networking and Distributed Computing","UDP","IP","TCP"]},{"location":"operators/basic_network/#transmitter-configuration-parameters","title":"Transmitter Configuration Parameters","text":"<ul> <li><code>max_payload_size</code>: Maximum payload size for a single packet</li> <li>type: <code>integer</code></li> <li><code>udp_dst_port</code>: UDP destination port for packets</li> <li>type: <code>integer</code></li> <li><code>l4_proto</code>: Layer 4 protocol</li> <li>type: <code>string</code> (<code>udp</code>/<code>tcp</code>)</li> <li><code>ip_addr</code>: Destination IP address</li> <li>type: <code>string</code></li> <li><code>min_ipg_ns</code>: Minimum inter-packet gap in nanoseconds</li> <li>type: <code>integer</code></li> <li><code>delete_payload</code> (Optional): Delete payload memory after sending (only applicable for C++ implementation)</li> <li>type: <code>boolean</code></li> </ul>","tags":["Networking and Distributed Computing","UDP","IP","TCP"]},{"location":"operators/basic_network/#transmitter-and-receiver-operator-parameters","title":"Transmitter and Receiver Operator Parameters","text":"<p>The transmitter and receiver operator both use the <code>NetworkOpBurstParams</code> structure as input and output to their ports, respectively. <code>NetworkOpBurstParams</code> contains the following fields:</p> <ul> <li><code>data</code>: Pointer to batch of packet data</li> <li>type: <code>uint8_t *</code></li> <li><code>len</code>: Length of total buffer in bytes</li> <li>type: <code>integer</code></li> <li><code>num_pkts</code>: Number of packets in batch</li> <li>type: <code>integer</code></li> </ul> <p>To receive messages from the Receive operator use the output port <code>burst_out</code>. To send messages to the Transmit operator use the input port <code>burst_in</code>.</p>","tags":["Networking and Distributed Computing","UDP","IP","TCP"]},{"location":"operators/cvcuda_holoscan_interop/","title":"CVCUDA Holoscan Interoperability Operators","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This directory contains two operators to enable interoperability between the CVCUDA and Holoscan tensors: <code>holoscan::ops::CvCudaToHoloscan</code> and <code>holoscan::ops::HoloscanToCvCuda</code>.</p>","tags":["Image Processing","CV CUDA","Computer Vision and Perception"]},{"location":"operators/cvcuda_holoscan_interop/#holoscanopscvcudatoholoscan","title":"<code>holoscan::ops::CvCudaToHoloscan</code>","text":"<p>Operator class to convert a <code>nvcv::Tensor</code> to a <code>holoscan::Tensor</code>.</p>","tags":["Image Processing","CV CUDA","Computer Vision and Perception"]},{"location":"operators/cvcuda_holoscan_interop/#inputs","title":"Inputs","text":"<ul> <li><code>input</code>: a CV-CUDA tensor</li> <li>type: <code>nvcv::Tensor</code></li> </ul>","tags":["Image Processing","CV CUDA","Computer Vision and Perception"]},{"location":"operators/cvcuda_holoscan_interop/#outputs","title":"Outputs","text":"<ul> <li><code>output</code>: a Holoscan tensor as <code>holoscan::Tensor</code> in <code>holoscan::TensorMap</code></li> <li>type: <code>holoscan::TensorMap</code></li> </ul>","tags":["Image Processing","CV CUDA","Computer Vision and Perception"]},{"location":"operators/cvcuda_holoscan_interop/#holoscanopsholoscantocvcuda","title":"<code>holoscan::ops::HoloscanToCvCuda</code>","text":"","tags":["Image Processing","CV CUDA","Computer Vision and Perception"]},{"location":"operators/cvcuda_holoscan_interop/#inputs_1","title":"Inputs","text":"<ul> <li><code>input</code>: a <code>gxf::Entity</code> containing a Holoscan tensor as <code>holoscan::Tensor</code></li> <li>type: <code>gxf::Entity</code></li> </ul>","tags":["Image Processing","CV CUDA","Computer Vision and Perception"]},{"location":"operators/cvcuda_holoscan_interop/#outputs_1","title":"Outputs","text":"<ul> <li><code>output</code>: a CV-CUDA tensor</li> <li>type: <code>nvcv::Tensor</code></li> </ul>","tags":["Image Processing","CV CUDA","Computer Vision and Perception"]},{"location":"operators/dds/","title":"Data-Distribution Service (DDS) Operators","text":"<p> Authors: Ian Stewart (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>This folder contains operators that allow applications to publish or subscribe to data topics in a DDS domain using RTI Connext. These operators demonstrate the ability for Holoscan applications to integrate and interoperate with applications outside of Holoscan, taking advantage of the data-centric and distributed nature of DDS to quickly enable communication with a wide array of external applications and platforms.</p>","tags":["Networking and Distributed Computing","DDS","RTI Connext"]},{"location":"operators/dds/#requirements","title":"Requirements","text":"<p>RTI Connext must be installed on the system and a valid RTI Connext license must be installed to run any application using one of these operators. To build on an IGX devkit (using the <code>armv8</code> architecture), follow the instructions to build Connext DDS applications for embedded Arm targets up to step 5 (Installing Java and setting JREHOME).</p> <p>To build the operators, the <code>RTI_CONNEXT_DDS_DIR</code> CMake variable must point to the installation path for RTI Connext. This can be done automatically by setting the <code>NDDSHOME</code> environment variable to the RTI Connext installation directory (such as when using the RTI <code>setenv</code> scripts), or manually at build time, e.g.:</p> <pre><code>$ ./holohub build dds_video --configure-args=\"-DRTI_CONNEXT_DDS_DIR=~/rti/rti_connext_dds-7.3.0\"\n</code></pre>","tags":["Networking and Distributed Computing","DDS","RTI Connext"]},{"location":"operators/dds/#using-a-development-container","title":"Using a Development Container","text":"<p>Due to the license requirements of RTI Connext it is not currently supported to install RTI Connext into a development container. Instead, if a development container is to be used, Connext should be installed onto the host as above and then the container can be launched with the RTI Connext folder mounted at runtime. To do so, ensure that the <code>NDDSHOME</code> and <code>CONNEXTDDS_ARCH</code> environment variables are set (which can be done using the RTI <code>setenv</code> script) and use the following:</p> <pre><code>./holohub run --docker-opts \"-v $NDDSHOME:/opt/dds -e NDDSHOME=/opt/dds -e CONNEXTDDS_ARCH=$CONNEXTDDS_ARCH\"\n</code></pre>","tags":["Networking and Distributed Computing","DDS","RTI Connext"]},{"location":"operators/dds/base/","title":"DDS Base Operator","text":"<p> Authors: Ian Stewart (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>The DDS Base Operator provides a base class which can be inherited by any operator class which requires access to a DDS domain.</p> <p>This operator requires an installation of RTI Connext to provide access to the DDS domain, as specified by the OMG Data-Distribution Service.</p> <p>You can obtain a license/activation key for RTI Connext directly from RTI by downloading it here. For additional information on RTI Connext and how it integrates with NVIDIA products, please refer to the RTI-NVIDIA integration page.</p> <p>If you have questions, please email evaluations@rti.com.</p>","tags":["Networking and Distributed Computing","DDS","RTI Connext"]},{"location":"operators/dds/base/#holoscanopsddsoperatorbase","title":"<code>holoscan::ops::DDSOperatorBase</code>","text":"<p>Base class which provides the parameters and members required to access a DDS domain.</p> <p>For more documentation about how these parameters (and other similar inheriting-class parameters) are used, see the RTI Connext Documentation.</p>","tags":["Networking and Distributed Computing","DDS","RTI Connext"]},{"location":"operators/dds/base/#parameters","title":"Parameters","text":"<ul> <li><code>qos_provider</code>: URI for the DDS QoS Provider</li> <li>type: <code>std::string</code></li> <li><code>participant_qos</code>: Name of the QoS profile to use for the DDS DomainParticipant</li> <li>type: <code>std::string</code></li> <li><code>domain_id</code>: The ID of the DDS domain to use</li> <li>type: <code>uint32_t</code></li> </ul>","tags":["Networking and Distributed Computing","DDS","RTI Connext"]},{"location":"operators/dds/dds_shapes_subscriber/","title":"DDS Shape Subscriber Operator","text":"<p> Authors: Ian Stewart (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>The DDS Shape Subscriber Operator subscribes to and reads from the <code>Square</code>, <code>Circle</code>, and <code>Triangle</code> shape topics as used by the RTI Shapes Demo. It will then translate the received shape data to an internal <code>Shape</code> datatype for output to downstream operators.</p> <p>This operator requires an installation of RTI Connext to provide access to the DDS domain, as specified by the OMG Data-Distribution Service.</p> <p>You can obtain a license/activation key for RTI Connext directly from RTI by downloading it here. For additional information on RTI Connext and how it integrates with NVIDIA products, please refer to the RTI-NVIDIA integration page.</p> <p>If you have questions, please email evaluations@rti.com.</p>","tags":["Networking and Distributed Computing","DDS","RTI Connext"]},{"location":"operators/dds/dds_shapes_subscriber/#holoscanopsddsshapessubscriberop","title":"<code>holoscan::ops::DDSShapesSubscriberOp</code>","text":"<p>Operator class for the DDS Shapes Subscriber.</p> <p>This operator also inherits the parameters from DDSOperatorBase.</p>","tags":["Networking and Distributed Computing","DDS","RTI Connext"]},{"location":"operators/dds/dds_shapes_subscriber/#parameters","title":"Parameters","text":"<ul> <li><code>reader_qos</code>: The name of the QoS profile to use for the DDS DataReader</li> <li>type: <code>std::string</code></li> </ul>","tags":["Networking and Distributed Computing","DDS","RTI Connext"]},{"location":"operators/dds/dds_shapes_subscriber/#outputs","title":"Outputs","text":"<ul> <li><code>output</code>: Output shapes, translated from those read from DDS</li> <li>type: <code>holoscan::ops::DDSShapesSubscriberOp::Shape</code></li> </ul>","tags":["Networking and Distributed Computing","DDS","RTI Connext"]},{"location":"operators/dds/video/","title":"DDS Video Operators","text":"<p> Authors: Ian Stewart (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 2 - Trusted</p> <p>The DDS Video Operators allow applications to read or write video buffers to a DDS databus, enabling communication with other applications via the VideoFrame DDS topic.</p> <p>This operator requires an installation of RTI Connext to provide access to the DDS domain, as specified by the OMG Data-Distribution Service.</p> <p>You can obtain a license/activation key for RTI Connext directly from RTI by downloading it here. For additional information on RTI Connext and how it integrates with NVIDIA products, please refer to the RTI-NVIDIA integration page.</p> <p>If you have questions, please email evaluations@rti.com.</p>","tags":["Networking and Distributed Computing","DDS","RTI Connext","Video"]},{"location":"operators/dds/video/#holoscanopsddsvideopublisherop","title":"<code>holoscan::ops::DDSVideoPublisherOp</code>","text":"<p>Operator class for the DDS video publisher. This operator accepts <code>VideoBuffer</code> objects as input and publishes each buffer to DDS as a VideoFrame.</p> <p>This operator also inherits the parameters from DDSOperatorBase.</p>","tags":["Networking and Distributed Computing","DDS","RTI Connext","Video"]},{"location":"operators/dds/video/#parameters","title":"Parameters","text":"<ul> <li><code>writer_qos</code>: The name of the QoS profile to use for the DDS DataWriter</li> <li>type: <code>std::string</code></li> <li><code>stream_id</code>: The ID to use for the video stream</li> <li>type: <code>uint32_t</code></li> </ul>","tags":["Networking and Distributed Computing","DDS","RTI Connext","Video"]},{"location":"operators/dds/video/#inputs","title":"Inputs","text":"<ul> <li><code>input</code>: Input video buffer</li> <li>type: <code>nvidia::gxf::VideoBuffer</code></li> </ul>","tags":["Networking and Distributed Computing","DDS","RTI Connext","Video"]},{"location":"operators/dds/video/#holoscanopsddsvideosubscriberop","title":"<code>holoscan::ops::DDSVideoSubscriberOp</code>","text":"<p>Operator class for the DDS video subscriber. This operator reads from the VideoFrame DDS topic and outputs each received frame as <code>VideoBuffer</code> objects.</p> <p>This operator also inherits the parameters from DDSOperatorBase.</p>","tags":["Networking and Distributed Computing","DDS","RTI Connext","Video"]},{"location":"operators/dds/video/#parameters_1","title":"Parameters","text":"<ul> <li><code>reader_qos</code>: The name of the QoS profile to use for the DDS DataReader</li> <li>type: <code>std::string</code></li> <li><code>stream_id</code>: The ID of the video stream to filter for</li> <li>type: <code>uint32_t</code></li> <li><code>allocator</code>: Allocator used to allocate the output data</li> <li>type: <code>std::shared_ptr&lt;Allocator&gt;</code></li> </ul>","tags":["Networking and Distributed Computing","DDS","RTI Connext","Video"]},{"location":"operators/dds/video/#outputs","title":"Outputs","text":"<ul> <li><code>output</code>: Output video buffer</li> <li>type: <code>nvidia::gxf::VideoBuffer</code></li> </ul>","tags":["Networking and Distributed Computing","DDS","RTI Connext","Video"]},{"location":"operators/deidentification/pixelator/","title":"Pixelator Operator","text":"<p> Authors: NVIDIA Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 3.0.0 Tested Holoscan SDK versions: 3.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p>A Pixelation-based Deidentification Operator</p>","tags":["Image Processing","Deidentification","Anonymization"]},{"location":"operators/deidentification/pixelator/#overview","title":"Overview","text":"<p>In medical and sensitive imaging workflows, pixelation is a common method for deidentification. The <code>PixelatorOp</code> is a Holoscan operator that performs pixelation-based deidentification on input images, suitable for applications such as surgical video anonymization where the camera may get out of the body and capture sensitive or protected information.</p>","tags":["Image Processing","Deidentification","Anonymization"]},{"location":"operators/deidentification/pixelator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK</li> <li>cupy</li> </ul>","tags":["Image Processing","Deidentification","Anonymization"]},{"location":"operators/deidentification/pixelator/#example-usage","title":"Example Usage","text":"<pre><code>from holohub.operators.deidentification.pixelator import PixelatorOp\nop = PixelatorOp(block_size_h=16, block_size_w=16)\n</code></pre>","tags":["Image Processing","Deidentification","Anonymization"]},{"location":"operators/deidentification/pixelator/#parameters","title":"Parameters","text":"<ul> <li><code>tensor_name</code>: The name of the tensor to be pixelated.</li> <li><code>block_size_h</code>: Height of the pixelation block.</li> <li><code>block_size_w</code>: Width of the pixelation block.</li> </ul>","tags":["Image Processing","Deidentification","Anonymization"]},{"location":"operators/deltacast_videomaster/","title":"DELTACAST VideoMaster Operators","text":"<p> Authors: Laurent Radoux (Deltacast), Thomas Dethier (Deltacast) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: January 12, 2026 Latest version: 1.1 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 3.6.0 Contribution metric: Level 2 - Trusted</p> <p>The DELTACAST VideoMaster operator provides functionality to capture and stream high-quality video streams from DELTACAST cards. It supports both SDI and HDMI input and output sources, enabling professional video capture in various formats and resolutions. DELTACAST VideoMaster operators are designed to work seamlessly with DELTACAST's hardware capabilities.</p> <p>This library contains two operators: - videomaster_source: Captures a signal from the DELTACAST capture card. - videomaster_transmitter: Streams a signal through the DELTACAST capture card.</p>","tags":["Camera","Deltacast"]},{"location":"operators/deltacast_videomaster/#requirements","title":"Requirements","text":"<ul> <li>VideoMaster SDK: Operators require the VideoMaster SDK from Deltacast.</li> <li>DELTACAST Hardware: Compatible DELTACAST capture cards.</li> <li>VideoMaster driver: To detect and use DELTACAST capture cards.</li> </ul>","tags":["Camera","Deltacast"]},{"location":"operators/deltacast_videomaster/#parameters","title":"Parameters","text":"","tags":["Camera","Deltacast"]},{"location":"operators/deltacast_videomaster/#videomaster_source","title":"videomaster_source","text":"<p>The following parameters can be configured for this operator:</p> Parameter Type Description Default <code>board</code> uint32_t Index of the DELTACAST.TV board to use as source 0 <code>rdma</code> bool Enable RDMA for video input (DELTACAST driver must be compiled with RDMA enabled to use this option) false <code>input</code> uint32_t Index of the RX channel to use on the selected board 0","tags":["Camera","Deltacast"]},{"location":"operators/deltacast_videomaster/#videomaster_transmitter","title":"videomaster_transmitter","text":"<p>The following parameters can be configured for this operator:</p> Parameter Type Description Default <code>board</code> uint32_t Index of the DELTACAST.TV board to use as source 0 <code>rdma</code> bool Enable RDMA for video input (DELTACAST driver must be compiled with RDMA enabled to use this option) false <code>output</code> uint32_t Index of the TX channel to use on the selected board 0 <code>width</code> uint32_t The width of the output stream 1920 <code>height</code> uint32_t The height of the output stream 1080 <code>progressive</code> bool interleaved or progressive true <code>framerate</code> uint32_t The framerate of the output stream 60 <code>enable_overlay</code> bool Is overlay provided by the card or not false","tags":["Camera","Deltacast"]},{"location":"operators/deltacast_videomaster/#building-the-operator","title":"Building the operator","text":"<p>As part of Holohub, running CMake on Holohub and point to Holoscan SDK install tree.</p> <p>The path to the VideoMaster SDK is also mandatory and can be given through the VideoMaster_SDK_DIR parameter.</p>","tags":["Camera","Deltacast"]},{"location":"operators/deltacast_videomaster/#tests","title":"Tests","text":"<p>All tests performed with the DELTACAST VideoMaster SDK <code>6.32</code>.</p> Application Device Configuration Holoscan SDK 3.6 deltacast_transmitter DELTA-12G-elp-key 11 TX0 (SDI) / ~~RDMA~~ PASSED deltacast_transmitter DELTA-12G-elp-key 11 TX0 (SDI) / RDMA PASSED deltacast_receiver DELTA-12G-elp-key 11 RX0 (SDI) / ~~RDMA~~ PASSED deltacast_receiver DELTA-12G-elp-key 11 RX0 (SDI) / RDMA PASSED endoscopy_tool_tracking DELTA-12G-elp-key 11 RX0 (SDI) / ~~overlay~~ / ~~RDMA~~ PASSED endoscopy_tool_tracking DELTA-12G-elp-key 11 RX0 (SDI) / ~~overlay~~ / RDMA PASSED endoscopy_tool_tracking DELTA-12G-elp-key 11 RX0/TX0 (SDI) / overlay / ~~RDMA~~ PASSED endoscopy_tool_tracking DELTA-12G-elp-key 11 RX0/TX0 (SDI) / overlay / RDMA PASSED","tags":["Camera","Deltacast"]},{"location":"operators/ehr_query_llm/","title":"EHR Query LLM Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: October 9, 2025 Latest version: 1.1 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.5.0, 3.3.0 Contribution metric: Level 2 - Trusted</p>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/#overview","title":"Overview","text":"<p>The EHR Query LLM Operators are a Holoscan operator that provides a robust interface for querying and processing Electronic Health Records (EHR) using the FHIR (Fast Healthcare Interoperability Resources) standard. It enables seamless integration with FHIR services, supports OAuth2 authentication, and provides standardized medical record processing capabilities.</p>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/#features","title":"Features","text":"<ul> <li>FHIR service querying with support for patient search</li> <li>OAuth2 authentication support</li> <li>Configurable FHIR endpoint</li> <li>Support for various FHIR resource types including:</li> <li>Patient</li> <li>Observation</li> <li>Condition</li> <li>DiagnosticReport</li> <li>ImagingStudy</li> <li>DocumentReference</li> <li>And more</li> <li>ZeroMQ-based message handling for distributed systems</li> <li>Standardized medical record sanitization and processing</li> <li>Comprehensive error handling and logging</li> </ul>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/#components","title":"Components","text":"","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/#fhir-client-operator","title":"FHIR Client Operator","text":"<ul> <li>Handles FHIR service queries</li> <li>Supports patient search and resource retrieval</li> <li>Configurable authentication via OAuth2</li> <li>Processes FHIR responses into standardized format</li> </ul>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/#fhir-resource-sanitizer-operator","title":"FHIR Resource Sanitizer Operator","text":"<ul> <li>Sanitizes and standardizes FHIR resources</li> <li>Transforms raw FHIR data into AI-friendly format</li> <li>Maintains essential medical information</li> <li>Supports multiple resource types</li> </ul>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/#zeromq-message-handling","title":"ZeroMQ Message Handling","text":"<ul> <li>Publisher/Subscriber pattern for distributed communication</li> <li>Configurable topics and endpoints</li> <li>Support for both blocking and non-blocking operations</li> </ul>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/#dependencies","title":"Dependencies","text":"<ul> <li>holoscan &gt;= 2.5.0</li> <li>fhir.resources &gt;= 7.0.0</li> <li>pyzmq &gt;= 25.1.0</li> <li>requests &gt;= 2.31.0</li> <li>pydantic &gt;= 2.0.0</li> </ul>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/fhir_client_op/","title":"FHIR Client Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: October 9, 2025 Latest version: 1.1 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.5.0, 3.3.0 Contribution metric: Level 2 - Trusted</p> <p>A Holoscan operator that enables seamless interaction with FHIR (Fast Healthcare Interoperability Resources) services for querying and retrieving patient medical records.</p>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/fhir_client_op/#overview","title":"Overview","text":"<p>The FHIR Client Operator provides a standardized interface for healthcare data exchange through FHIR services. It enables applications to securely query patient medical records while handling authentication, resource management, and error handling. The operator is designed to work with any FHIR-compliant server and supports various FHIR resource types.</p> <p>Key features:</p> <ul> <li>FHIR service querying with patient search capabilities</li> <li>OAuth2 authentication support</li> <li>Configurable FHIR endpoint</li> <li>Support for various FHIR resource types</li> <li>Comprehensive error handling and logging</li> </ul>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/fhir_client_op/#requirements","title":"Requirements","text":"<ul> <li>holoscan</li> <li>requests</li> <li>fhir.resources</li> </ul>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/fhir_client_op/#example-usage","title":"Example Usage","text":"<p>Please check fhir_client.py in Generative AI Application on Holoscan integrating with FHIR Services.</p>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/fhir_client_op/#name-inputoutput","title":"Name Input/Output","text":"<ul> <li>Input: <code>request</code> - JSON representation of the FHIRQuery object containing search parameters</li> <li>Output: <code>out</code> - FHIRQueryResponse object containing the original request ID and matching patient records</li> </ul>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/fhir_client_op/#parameters","title":"Parameters","text":"<ul> <li><code>fhir_endpoint</code> (str): FHIR service endpoint URL (default: \"http://localhost:8080/\")</li> <li><code>token_provider</code> (TokenProvider): Optional OAuth2 token provider for authentication</li> <li><code>verify_cert</code> (bool): Whether to verify server certificates (default: True)</li> </ul>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/fhir_resource_sanitizer_op/","title":"FHIR Resource Sanitizer Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: October 9, 2025 Latest version: 1.1 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.5.0, 3.3.0 Contribution metric: Level 2 - Trusted</p> <p>A Holoscan operator that processes and sanitizes FHIR medical records into a standardized, AI-friendly format while maintaining essential medical information.</p>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/fhir_resource_sanitizer_op/#overview","title":"Overview","text":"<p>The FHIR Resource Sanitizer Operator is designed to transform raw FHIR (Fast Healthcare Interoperability Resources) medical records into a more standardized format suitable for AI processing. It handles various FHIR resource types including Patient, Observation, Condition, DiagnosticReport, ImagingStudy, and more, while implementing robust error handling and logging mechanisms.</p>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/fhir_resource_sanitizer_op/#requirements","title":"Requirements","text":"<ul> <li>holoscan</li> <li>fhir.resources</li> <li>pydantic</li> </ul>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/fhir_resource_sanitizer_op/#example-usage","title":"Example Usage","text":"<p>Please check fhir_client.py in Generative AI Application on Holoscan integrating with FHIR Services.</p>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/fhir_resource_sanitizer_op/#name-inputoutput","title":"Name Input/Output","text":"<ul> <li> <p>Input:  <code>records</code>: A FHIRQueryResponse object containing patient medical records</p> </li> <li> <p>Output: <code>out</code>: A sanitized FHIRQueryResponse object with standardized medical records</p> </li> </ul>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/fhir_resource_sanitizer_op/#parameters","title":"Parameters","text":"<ul> <li><code>fhir_endpoint</code> (str): FHIR service endpoint URL (default: \"http://localhost:8080/\")</li> </ul>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/zero_mq_publisher_op/","title":"ZeroMQ Publisher Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: October 9, 2025 Latest version: 1.1 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.5.0, 3.3.0 Contribution metric: Level 2 - Trusted</p> <p>A Holoscan operator that publishes messages to a ZeroMQ message queue using the PUB/SUB pattern.</p>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/zero_mq_publisher_op/#overview","title":"Overview","text":"<p>The ZeroMQ Publisher Operator provides a standardized interface for publishing messages to a ZeroMQ message queue. It enables applications to send messages to subscribers while handling connection management and error handling.</p>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/zero_mq_publisher_op/#requirements","title":"Requirements","text":"<ul> <li>holoscan</li> <li>pyzmq</li> </ul>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/zero_mq_publisher_op/#example-usage","title":"Example Usage","text":"<p>Please check fhir_client.py in Generative AI Application on Holoscan integrating with FHIR Services.</p>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/zero_mq_publisher_op/#name-inputoutput","title":"Name Input/Output","text":"<ul> <li>Input: <code>message</code>: Message to be published to ZeroMQ</li> <li>Output: None</li> </ul>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/zero_mq_publisher_op/#parameters","title":"Parameters","text":"<ul> <li><code>topic</code> (str): Topic name for message filtering</li> <li><code>queue_endpoint</code> (str): ZeroMQ endpoint URL (e.g., \"tcp://*:5556\") </li> </ul>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/zero_mq_subscriber_op/","title":"ZeroMQ Subscriber Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: October 9, 2025 Latest version: 1.1 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.5.0, 3.3.0 Contribution metric: Level 2 - Trusted</p> <p>A Holoscan operator that subscribes to messages from a ZeroMQ message queue using the PUB/SUB pattern.</p>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/zero_mq_subscriber_op/#overview","title":"Overview","text":"<p>The ZeroMQ Subscriber Operator provides a standardized interface for receiving messages from a ZeroMQ message queue. It enables applications to receive messages from publishers while handling connection management and error handling.</p>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/zero_mq_subscriber_op/#requirements","title":"Requirements","text":"<ul> <li>holoscan</li> <li>pyzmq</li> </ul>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/zero_mq_subscriber_op/#example-usage","title":"Example Usage","text":"<p>Please check fhir_client.py in Generative AI Application on Holoscan integrating with FHIR Services.</p>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/zero_mq_subscriber_op/#name-inputoutput","title":"Name Input/Output","text":"<ul> <li>Input: None</li> <li>Output: <code>request</code>: Message received from ZeroMQ</li> </ul>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/ehr_query_llm/zero_mq_subscriber_op/#parameters","title":"Parameters","text":"<ul> <li><code>topic</code> (str): Topic name for message filtering</li> <li><code>queue_endpoint</code> (str): ZeroMQ endpoint URL (e.g., \"tcp://localhost:5556\")</li> <li><code>blocking</code> (bool): Whether to use blocking receive (default: False) </li> </ul>","tags":["Networking and Distributed Computing","LLM","Healthcare Interop"]},{"location":"operators/emergent_source/","title":"Emergent Source Operator","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This operator provides support for Emergent Vision Technologies cameras as video sources. This operator enables high-performance video streaming through Mellanox ConnectX SmartNIC using the Rivermax SDK.</p>","tags":["Camera"]},{"location":"operators/emergent_source/#overview","title":"Overview","text":"<p>The <code>EmergentSourceOp</code> is designed to capture video streams from Emergent Vision Technologies cameras with high frame rates and resolution support. It leverages RDMA (Remote Direct Memory Access) capabilities for efficient data transfer and supports various camera parameters for optimal performance. Please refer to Holoscan EVT Setup for more information.</p>","tags":["Camera"]},{"location":"operators/emergent_source/#features","title":"Features","text":"<ul> <li>High Resolution Support: Default resolution of 4200x2160 pixels</li> <li>High Frame Rate: Default frame rate of 240 FPS</li> <li>RDMA Support: Optional RDMA for enhanced performance</li> <li>Configurable Parameters: Adjustable width, height, framerate, exposure, and gain</li> <li>Cross-Platform: Supports x86_64 and aarch64 architectures</li> <li>Python &amp; C++ APIs: Available in both programming languages</li> </ul>","tags":["Camera"]},{"location":"operators/emergent_source/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK: Minimum version 0.5.0 (tested with 0.5.0)</li> <li>GXF Extensions: Requires <code>emergent_source</code> extension version 1.0</li> <li>Hardware: Mellanox ConnectX SmartNIC for optimal performance</li> <li>Camera: Emergent Vision Technologies camera</li> </ul>","tags":["Camera"]},{"location":"operators/emergent_source/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>signal</code> <code>gxf::Handle&lt;gxf::Transmitter&gt;</code> - Output signal channel <code>width</code> <code>uint32_t</code> 4200 Width of the video stream in pixels <code>height</code> <code>uint32_t</code> 2160 Height of the video stream in pixels <code>framerate</code> <code>uint32_t</code> 240 Frame rate of the video stream in FPS <code>rdma</code> <code>bool</code> false Enable RDMA for enhanced performance <code>exposure</code> <code>uint32_t</code> 3072 Exposure time setting <code>gain</code> <code>uint32_t</code> 4095 Analog gain setting","tags":["Camera"]},{"location":"operators/emergent_source/#usage-examples","title":"Usage Examples","text":"","tags":["Camera"]},{"location":"operators/emergent_source/#c-example","title":"C++ Example","text":"<pre><code>#include \"holoscan/operators/emergent_source/emergent_source.hpp\"\n\n// Create the operator\nauto emergent_source = fragment.make_operator&lt;holoscan::ops::EmergentSourceOp&gt;(\n    \"emergent_source\",\n    holoscan::Arg{\"width\", 1920},\n    holoscan::Arg{\"height\", 1080},\n    holoscan::Arg{\"framerate\", 60},\n    holoscan::Arg{\"rdma\", true},\n    holoscan::Arg{\"exposure\", 2048},\n    holoscan::Arg{\"gain\", 2048}\n);\n</code></pre>","tags":["Camera"]},{"location":"operators/emergent_source/#python-example","title":"Python Example","text":"<pre><code>from holoscan.operators import EmergentSourceOp\n\n# Create the operator\nemergent_source = EmergentSourceOp(\n    fragment,\n    width=1920,\n    height=1080,\n    framerate=60,\n    rdma=True,\n    exposure=2048,\n    gain=2048,\n    name=\"emergent_source\"\n)\n</code></pre> <p>Please refer to High Speed Endoscopy and Laser Detection for requirements and usage examples.</p>","tags":["Camera"]},{"location":"operators/fft/","title":"FFT (latest)","text":"","tags":["Signal Processing"]},{"location":"operators/fft/#fast-fourier-transform-fft-operator","title":"Fast Fourier Transform (FFT) Operator","text":"<p> Authors: John Moon john.moon@vts-i.com (Valley Tech Systems, Inc.), Eric Ferrara eric.ferrara@vts-i.com (Valley Tech Systems, Inc.), Matthew Luckenbihl matthew.luckenbihl@vts-i.com (Valley Tech Systems, Inc.) Supported platforms: x86_64 Language: C++ Last modified: August 5, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.5.0 Tested Holoscan SDK versions: 2.5.0, 2.6.0, 2.7.0, 2.8.0, 2.9.0, 3.0.0, 3.1.0 Contribution metric: Level 3 - Developmental</p>","tags":["Signal Processing"]},{"location":"operators/fft/#overview","title":"Overview","text":"<p>A thin wrapper over the MatX <code>fft()</code> executor.</p>","tags":["Signal Processing"]},{"location":"operators/fft/#description","title":"Description","text":"<p>The FFT operator takes in a tensor of complex float data, performs an FFT, and emits the resultant tensor.</p>","tags":["Signal Processing"]},{"location":"operators/fft/#requirements","title":"Requirements","text":"<ul> <li>MatX (dependency - assumed to be installed on system)</li> </ul>","tags":["Signal Processing"]},{"location":"operators/fft/#example-usage","title":"Example Usage","text":"<p>For an example of how to use this operator, see the <code>psd_pipeline</code> application.</p>","tags":["Signal Processing"]},{"location":"operators/fft/#multiple-channels","title":"Multiple Channels","text":"<p>The zero-indexed <code>channel_number</code> key will be looked up in <code>metadata()</code> on each <code>compute()</code> run. If no value is found, the default channel number is <code>0</code>.</p>","tags":["Signal Processing"]},{"location":"operators/fft/#configuration","title":"Configuration","text":"<p>The FFT operator takes in a few parameters:</p> <pre><code>fft:\n  burst_size: 1280\n  num_bursts: 625\n  num_channels: 1\n  spectrum_type: 1\n  averaging_type: 1\n  window_time: 0\n  window_type: 0\n  transform_points: 1280\n  window_points: 1280\n  resolution: 6250\n  span: 8000000\n  weighting_factor: 0\n  f1_index: -640\n  f2_index: 639\n  window_time_delta: 0\n</code></pre> <p>The only parameters that actually impacts FFT computation at this point are the <code>burst_size</code> and <code>num_bursts</code> params. The rest of the parameters are simply passed along in the metadata.</p> <ul> <li><code>burst_size</code>: Number of samples to process in each burst</li> <li><code>num_bursts</code>: Number of bursts to process at once</li> <li><code>num_channels</code>: Number of channels for which to allocate memory</li> <li><code>spectrum_type</code>: VITA 49.2 spectrum type to pass along in metadata</li> <li><code>spectrum_type</code>: VITA 49.2 spectrum type to pass along in metadata</li> <li><code>averaging_type</code>: VITA 49.2 averaging type to pass along in metadata</li> <li><code>window_time</code>: VITA 49.2 window time to pass along in metadata</li> <li><code>window_type</code>: VITA 49.2 window type to pass along in metadata</li> <li><code>transform_points</code>: Number of FFT points to take and VITA 49.2 transform points to pass along in metadata</li> <li><code>window_points</code>: VITA 49.2 window points to pass along in metadata</li> <li><code>resolution</code>: VITA 49.2 resolution to pass along in metadata</li> <li><code>span</code>: VITA 49.2 span to pass along in metadata</li> <li><code>weighting_factor</code>: VITA 49.2 weighting factor to pass along in metadata</li> <li><code>f1_index</code>: VITA 49.2 F1 index to pass along in metadata</li> <li><code>f2_index</code>: VITA 49.2 F2 index to pass along in metadata</li> <li><code>window_time_delta</code>: VITA 49.2 window time delta to pass along in metadata</li> </ul>","tags":["Signal Processing"]},{"location":"operators/gamma_correction/","title":"Gamma Correction Operator","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: October 9, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 3.5.0 Tested Holoscan SDK versions: 3.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>GammaCorrectionOp</code> is a Holoscan operator that applies gamma correction to images using GPU-accelerated compute shaders. It provides efficient gamma correction processing for both single-channel and multi-channel images, with support for various data types and automatic normalization.</p>","tags":["Image Processing","gamma","correction"]},{"location":"operators/gamma_correction/#overview","title":"Overview","text":"<p>The GammaCorrectionOp extends the SlangShaderOp to provide specialized gamma correction functionality. It automatically handles data type conversion, normalization for integer types, and multi-component processing while maintaining high performance through GPU acceleration.</p>","tags":["Image Processing","gamma","correction"]},{"location":"operators/gamma_correction/#features","title":"Features","text":"<ul> <li>Automatic Data Type Handling: Supports various data types (uint8, uint16, float32, etc.) with automatic normalization</li> <li>Multi-Component Support: Processes images with 1-4 components (grayscale, RGB, RGBA)</li> <li>Configurable Gamma Value: Adjustable gamma correction factor (default: 2.2)</li> <li>GPU Acceleration: Leverages CUDA compute shaders for high-performance processing</li> <li>Python and C++ APIs: Available in both Python and C++ interfaces</li> </ul>","tags":["Image Processing","gamma","correction"]},{"location":"operators/gamma_correction/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK 3.4.0 or later</li> <li>CUDA-compatible GPU</li> <li>Supported platforms: x86_64, aarch64</li> </ul>","tags":["Image Processing","gamma","correction"]},{"location":"operators/gamma_correction/#installation","title":"Installation","text":"<p>The GammaCorrectionOp is included as part of the HoloHub operators. It will be automatically built when you build the HoloHub project.</p>","tags":["Image Processing","gamma","correction"]},{"location":"operators/gamma_correction/#usage","title":"Usage","text":"","tags":["Image Processing","gamma","correction"]},{"location":"operators/gamma_correction/#basic-usage","title":"Basic Usage","text":"<p>The operator can be configured with data type and component count parameters:</p> <pre><code>from holoscan.operators import GammaCorrectionOp\n\n# Basic gamma correction for uint8 grayscale image\nop = GammaCorrectionOp(\n    fragment=app,\n    data_type=\"uint8_t\",\n    component_count=1,\n    gamma=2.2,\n    name=\"gamma_correction\"\n)\n</code></pre>","tags":["Image Processing","gamma","correction"]},{"location":"operators/gamma_correction/#c-usage","title":"C++ Usage","text":"<pre><code>#include &lt;gamma_correction/gamma_correction.hpp&gt;\n\n// Create the operator with uint8_t data type\nauto gamma_op = make_operator&lt;holoscan::ops::GammaCorrectionOp&gt;(\"gamma_correction\",\n    Arg(\"data_type\", \"uint8_t\"),\n    Arg(\"component_count\", 3),  // RGB\n    Arg(\"gamma\", 2.2f));\n</code></pre>","tags":["Image Processing","gamma","correction"]},{"location":"operators/gamma_correction/#parameters","title":"Parameters","text":"<ul> <li><code>data_type</code> (required): The data type of the input buffer</li> <li>Supported types: <code>int8_t</code>, <code>uint8_t</code>, <code>int16_t</code>, <code>uint16_t</code>, <code>int32_t</code>, <code>uint32_t</code>, <code>float</code>, <code>double</code></li> <li><code>component_count</code> (optional): Number of components in the input buffer</li> <li>Default: 1 (grayscale)</li> <li>Supported: 1-4 components</li> <li><code>gamma</code> (optional): Gamma correction factor</li> <li>Default: 2.2</li> <li>Range: Any positive float value</li> </ul>","tags":["Image Processing","gamma","correction"]},{"location":"operators/gamma_correction/#supported-data-types","title":"Supported Data Types","text":"","tags":["Image Processing","gamma","correction"]},{"location":"operators/gamma_correction/#integer-types","title":"Integer Types","text":"<ul> <li><code>int8_t</code>, <code>uint8_t</code>, <code>int16_t</code>, <code>uint16_t</code>, <code>int32_t</code>, <code>uint32_t</code>: Automatically normalized to [0,1] range before processing</li> <li>Gamma correction applied in normalized space</li> <li>Results scaled back to original range</li> </ul>","tags":["Image Processing","gamma","correction"]},{"location":"operators/gamma_correction/#floating-point-types","title":"Floating Point Types","text":"<ul> <li><code>float</code>, <code>double</code>: Processed directly without normalization</li> <li>Assumes input values are already in [0,1] range</li> </ul>","tags":["Image Processing","gamma","correction"]},{"location":"operators/gamma_correction/#example-rgb-image-processing","title":"Example: RGB Image Processing","text":"<pre><code># Process RGB image with custom gamma\nop = GammaCorrectionOp(\n    fragment=app,\n    data_type=\"uint8_t\",\n    component_count=3,\n    gamma=1.8,\n    name=\"rgb_gamma_correction\"\n)\n</code></pre>","tags":["Image Processing","gamma","correction"]},{"location":"operators/gamma_correction/#example-float-image-processing","title":"Example: Float Image Processing","text":"<pre><code># Process float image (assumes values in [0,1] range)\nop = GammaCorrectionOp(\n    fragment=app,\n    data_type=\"float\",\n    component_count=1,\n    gamma=2.2,\n    name=\"float_gamma_correction\"\n)\n</code></pre>","tags":["Image Processing","gamma","correction"]},{"location":"operators/gamma_correction/#testing","title":"Testing","text":"<p>The GammaCorrectionOp includes comprehensive testing to ensure reliability and correctness:</p>","tags":["Image Processing","gamma","correction"]},{"location":"operators/gamma_correction/#running-tests","title":"Running Tests","text":"<pre><code>./holohub test gamma_correction\n</code></pre>","tags":["Image Processing","gamma","correction"]},{"location":"operators/gamma_correction/#contributing","title":"Contributing","text":"<p>The GammaCorrectionOp is part of the HoloHub project. Contributions are welcome through the standard HoloHub contribution process.</p>","tags":["Image Processing","gamma","correction"]},{"location":"operators/gamma_correction/#license","title":"License","text":"<p>This operator is licensed under the Apache License 2.0, same as the HoloHub project.</p>","tags":["Image Processing","gamma","correction"]},{"location":"operators/grpc_operators/","title":"Holohub gRPC Plugins for Holoscan SDK","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.6.0 Tested Holoscan SDK versions: 2.6.0 Contribution metric: Level 1 - Highly Reliable</p>","tags":["Networking and Distributed Computing","gRPC","Visualization"]},{"location":"operators/grpc_operators/#overview","title":"Overview","text":"<p>This directory contains the Holohub gRPC plugins for Holoscan SDK, including:</p> <ul> <li><code>client</code>: gRPC client and Holoscan Operators for sending, receiving, and streaming data to a gRPC server.</li> <li><code>server</code>: gRPC server and Holoscan Operators for handling requests from a gRPC client and transmitting data back to the client.</li> <li><code>protos</code>: Protocol buffers definitions of Holoscan SDK.</li> <li><code>common</code>: Tensor &lt;-&gt; protobuf converters and Holoscan Resources to handle incoming and outgoing data.</li> </ul> <p>Please refer to the gRPC h.264 Endoscopy Tool Tracking application for additional details.</p>","tags":["Networking and Distributed Computing","gRPC","Visualization"]},{"location":"operators/gstreamer/","title":"GStreamer Bridge Components","text":"<p> Authors: NVIDIA Corporation (NVIDIA Corporation) Supported platforms: x86_64, aarch64 Language: C++ Last modified: December 10, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 3.8.0 Tested Holoscan SDK versions: 3.8.0 Contribution metric: Level 2 - Trusted</p> <p>This directory contains components that provide a bridge between Holoscan and GStreamer, enabling integration with the vast ecosystem of GStreamer plugins for video encoding, streaming, and multimedia processing.</p>","tags":["gstreamer","video","recording","multimedia","streaming","cuda"]},{"location":"operators/gstreamer/#overview","title":"Overview","text":"<p>The GStreamer bridge provides:</p> <ul> <li>Operators for integrating GStreamer functionality into Holoscan pipelines</li> <li>Resources for managing GStreamer elements and memory allocation within the Holoscan resource framework</li> <li>Low-level bridge objects that can be used in both Holoscan and non-Holoscan applications</li> <li>C++ RAII wrappers for safe and convenient GStreamer API usage</li> </ul> <p>These components allow you to:</p> <ul> <li>Integrate GStreamer's extensive plugin ecosystem into Holoscan applications</li> <li>Record and stream video using GStreamer encoders and protocols</li> <li>Support both host (CPU) and device (GPU) memory for efficient processing</li> <li>Build complex media processing pipelines combining Holoscan and GStreamer</li> <li>Use GStreamer functionality in custom applications outside the Holoscan framework</li> </ul>","tags":["gstreamer","video","recording","multimedia","streaming","cuda"]},{"location":"operators/gstreamer/#components","title":"Components","text":"","tags":["gstreamer","video","recording","multimedia","streaming","cuda"]},{"location":"operators/gstreamer/#operators","title":"Operators","text":"","tags":["gstreamer","video","recording","multimedia","streaming","cuda"]},{"location":"operators/gstreamer/#gstvideorecorderop","title":"GstVideoRecorderOp","text":"<p>Records incoming Holoscan tensors to video files using GStreamer encoding pipelines.</p> <p>Key Features:</p> <ul> <li>Multiple codec support: H.264 (nvh264, x264), H.265 (nvh265, x265)</li> <li>Configurable encoder properties (bitrate, preset, quality, etc.)</li> <li>Support for both host and device (CUDA) memory</li> <li>MP4 and MKV container output</li> </ul> <p>Input:</p> <ul> <li>Port <code>\"input\"</code>: Video frames as Holoscan tensors (RGB888/RGBA8888 format)</li> </ul> <p>Parameters:</p> <ul> <li><code>filename</code> (string): Output video file path (default: \"output.mp4\")</li> <li><code>encoder</code> (string): Encoder to use - \"nvh264\", \"nvh265\", \"x264\", \"x265\" (default: \"nvh264\")</li> <li><code>format</code> (string): Pixel format - \"RGBA\", \"RGB\", \"BGRA\", \"BGR\", \"GRAY8\" (default: \"RGBA\")</li> <li><code>framerate</code> (string): Target framerate as fraction \"num/den\" or decimal (default: \"30/1\")</li> <li><code>properties</code> (map): Encoder-specific properties (e.g., bitrate, preset, gop-size) <li><code>max-buffers</code> (size_t): Maximum number of buffers to queue, 0 = unlimited (default: 10)</li> <li><code>block</code> (bool): Block when queue is full vs. drop/timeout (default: true)</li>","tags":["gstreamer","video","recording","multimedia","streaming","cuda"]},{"location":"operators/gstreamer/#resources","title":"Resources","text":"<p>Resources provide Holoscan-managed wrappers for GStreamer elements and allocators, integrating them into the Holoscan resource lifecycle and memory management system.</p> <p>Note: Resource implementations will be added in future updates.</p>","tags":["gstreamer","video","recording","multimedia","streaming","cuda"]},{"location":"operators/gstreamer/#low-level-bridge-objects","title":"Low-Level Bridge Objects","text":"<p>Low-level bridge objects implement core GStreamer integration functionality in a framework-agnostic way. These can be used:</p> <ul> <li>Within Holoscan applications (via operators and resources)</li> <li>In standalone C++ applications without Holoscan dependencies</li> <li>In custom integration scenarios</li> </ul> <p>These objects handle the detailed work of data transfer, format conversion, and GStreamer API interaction.</p>","tags":["gstreamer","video","recording","multimedia","streaming","cuda"]},{"location":"operators/gstreamer/#gstsrcbridge","title":"GstSrcBridge","text":"<p><code>GstSrcBridge</code> is a framework-agnostic class that bridges tensor/video data into GStreamer pipelines via the <code>appsrc</code> element. It can be used independently of Holoscan in any C++ application that needs to feed data into GStreamer.</p> <p>Key capabilities:</p> <ul> <li>Push video frames from host or device (CUDA) memory into GStreamer pipelines</li> <li>Configurable caps (capabilities) for proper format specification</li> <li>Buffer queuing with configurable size limits and blocking behavior</li> <li>Support for various pixel formats (RGBA, RGB, NV12, I420, etc.)</li> <li>Automatic timestamp generation based on framerate</li> <li>End-of-stream (EOS) signaling</li> <li>Zero-copy operation when wrapping tensor memory</li> </ul>","tags":["gstreamer","video","recording","multimedia","streaming","cuda"]},{"location":"operators/gstreamer/#c-wrapper-classes","title":"C++ Wrapper Classes","text":"<p>The bridge includes a set of C++ RAII wrapper classes for GStreamer objects (located in the <code>gst/</code> subdirectory).</p> <p>Key wrapper classes include (subset):</p> <ul> <li><code>gst::Element</code>: Wrapper for GstElement (pipeline elements)</li> <li><code>gst::Pipeline</code>: Wrapper for GstPipeline (top-level pipelines)</li> <li><code>gst::Bus</code>: Wrapper for GstBus (message handling)</li> <li><code>gst::Caps</code>: Wrapper for GstCaps (media format capabilities)</li> <li><code>gst::Buffer</code>: Wrapper for GstBuffer (data buffers)</li> <li><code>gst::Message</code>: Wrapper for GstMessage (bus messages)</li> <li><code>gst::Allocator</code>: Wrapper for GstAllocator (memory allocation)</li> </ul> <p>These wrappers provide:</p> <ul> <li>Automatic reference counting and cleanup</li> <li>Type-safe property setting with compile-time string conversion</li> <li>Convenient API for common GStreamer operations</li> <li>Exception-based error handling</li> </ul> <p>Note: These wrapper classes can be used independently in non-Holoscan applications for safer GStreamer programming.</p>","tags":["gstreamer","video","recording","multimedia","streaming","cuda"]},{"location":"operators/gstreamer/#usage-example","title":"Usage Example","text":"<p>For a complete working example demonstrating how to use <code>GstVideoRecorderOp</code> in a Holoscan application, see the <code>gst_video_recorder</code> application. It shows:</p> <ul> <li>Integration with <code>V4L2VideoCaptureOp</code> for camera input</li> <li>Integration with pattern generators for synthetic video</li> <li>Proper use of <code>FormatConverterOp</code> for format handling</li> <li>Configuration of encoder properties</li> <li>YAML-based configuration</li> </ul>","tags":["gstreamer","video","recording","multimedia","streaming","cuda"]},{"location":"operators/gstreamer/#requirements","title":"Requirements","text":"<p>System Packages:</p> <p>A Dockerfile with all dependencies pre-installed is provided at <code>applications/gstreamer/gst_video_recorder/Dockerfile</code> for containerized builds.</p> <p>For local development, install all required dependencies using the provided script:</p> <pre><code>./applications/gstreamer/gst_video_recorder/install_deps.sh\n</code></pre> <p>Optional (for CUDA support):</p> <ul> <li><code>gstreamer1.0-cuda</code> (requires GStreamer 1.24+)</li> </ul> <p>Holoscan SDK:</p> <ul> <li>Minimum version: 3.8.0</li> </ul>","tags":["gstreamer","video","recording","multimedia","streaming","cuda"]},{"location":"operators/gstreamer/#advanced-configuration","title":"Advanced Configuration","text":"","tags":["gstreamer","video","recording","multimedia","streaming","cuda"]},{"location":"operators/gstreamer/#encoder-properties","title":"Encoder Properties","text":"<p>Encoder properties are passed directly to the underlying GStreamer encoder element. Common properties include:</p> <p>NVIDIA H.264/H.265 (nvh264enc/nvh265enc):</p> <ul> <li><code>bitrate</code>: Target bitrate in kbps (e.g., \"5000\")</li> <li><code>preset</code>: Encoding preset 0-3 (0=slowest/best quality, 3=fastest/lower quality)</li> <li><code>gop-size</code>: GOP (Group of Pictures) size in frames</li> </ul> <p>x264/x265 (software encoders):</p> <ul> <li><code>bitrate</code>: Target bitrate in kbps</li> <li><code>speed-preset</code>: Encoding speed preset (e.g., \"ultrafast\", \"medium\", \"slow\")</li> <li><code>tune</code>: Tuning preset (e.g., \"zerolatency\", \"film\")</li> </ul> <p>For a complete list of available properties, consult the GStreamer plugin documentation or use <code>gst-inspect-1.0 &lt;encoder&gt;</code> (e.g., <code>gst-inspect-1.0 nvh264enc</code>).</p>","tags":["gstreamer","video","recording","multimedia","streaming","cuda"]},{"location":"operators/gstreamer/#cuda-memory-support","title":"CUDA Memory Support","text":"<p>When <code>HOLOSCAN_GSTREAMER_CUDA_SUPPORT</code> is enabled (requires GStreamer 1.24+), the operator automatically detects and uses CUDA memory for zero-copy data transfer from GPU to encoder.</p>","tags":["gstreamer","video","recording","multimedia","streaming","cuda"]},{"location":"operators/gstreamer/#references","title":"References","text":"<ul> <li>GStreamer Documentation</li> <li>Holoscan SDK Documentation</li> <li>Application Example: gst_video_recorder</li> </ul>","tags":["gstreamer","video","recording","multimedia","streaming","cuda"]},{"location":"operators/high_rate_psd/","title":"HighRatePSD (latest)","text":"","tags":["Signal Processing"]},{"location":"operators/high_rate_psd/#high-rate-psd-operator","title":"High Rate PSD Operator","text":"<p> Authors: John Moon john.moon@vts-i.com (Valley Tech Systems, Inc.), Eric Ferrara eric.ferrara@vts-i.com (Valley Tech Systems, Inc.), Matthew Luckenbihl matthew.luckenbihl@vts-i.com (Valley Tech Systems, Inc.) Supported platforms: x86_64 Language: C++ Last modified: August 5, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.5.0 Tested Holoscan SDK versions: 2.5.0, 2.6.0, 2.7.0, 2.8.0, 2.9.0, 3.0.0, 3.1.0 Contribution metric: Level 3 - Developmental</p>","tags":["Signal Processing"]},{"location":"operators/high_rate_psd/#overview","title":"Overview","text":"<p>A thin wrapper over the MatX <code>abs2()</code> executor.</p>","tags":["Signal Processing"]},{"location":"operators/high_rate_psd/#description","title":"Description","text":"<p>The high rate PSD operator... - takes in a tensor of complex float data, - performs a squared absolute value operation on the tensor: real(t)^2 + imag(t)^2, - divides by the number of input elements - emits the resultant tensor</p>","tags":["Signal Processing"]},{"location":"operators/high_rate_psd/#requirements","title":"Requirements","text":"<ul> <li>MatX (dependency - assumed to be installed on system)</li> </ul>","tags":["Signal Processing"]},{"location":"operators/high_rate_psd/#example-usage","title":"Example Usage","text":"<p>For an example of how to use this operator, see the <code>psd_pipeline</code> application.</p>","tags":["Signal Processing"]},{"location":"operators/high_rate_psd/#multiple-channels","title":"Multiple Channels","text":"<p>The zero-indexed <code>channel_number</code> key will be looked up in <code>metadata()</code> on each <code>compute()</code> run. If no value is found, the default channel number is <code>0</code>.</p>","tags":["Signal Processing"]},{"location":"operators/high_rate_psd/#configuration","title":"Configuration","text":"<p>The operator only takes the following parameters:</p> <pre><code>high_rate_psd:\n  burst_size: 1280\n  num_bursts: 625\n  num_channels: 1\n</code></pre> <ul> <li><code>burst_size</code>: Number of samples to process in each burst</li> <li><code>num_bursts</code>: Number of bursts to process at once</li> <li><code>num_channels</code>: Number of channels for which to allocate memory</li> </ul>","tags":["Signal Processing"]},{"location":"operators/holoscan_ros2/","title":"Holoscan ROS2 Bridge Extension","text":"<p> Authors: NVIDIA (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 3.0.0 Tested Holoscan SDK versions: 3.0.0 Contribution metric: Level 1 - Highly Reliable</p>","tags":["Robotics","ROS2","Bridge","Interface"]},{"location":"operators/holoscan_ros2/#overview","title":"Overview","text":"<p>The Holoscan ROS2 Bridge extension provides interoperability between NVIDIA Holoscan and ROS2 (Robot Operating System 2) applications. It consists of: - A C++ header-only library for seamless integration with ROS2 <code>rclcpp</code>-based applications - A Python package for integration with ROS2 <code>rclpy</code>-based applications</p> <p>Both implementations enable seamless data and message exchange between Holoscan SDK operators and ROS2 nodes. You can use either implementation depending on your preferred programming language. Example applications can be found under the <code>applications/holoscan_ros2</code> directory.</p>","tags":["Robotics","ROS2","Bridge","Interface"]},{"location":"operators/holoscan_ros2/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Prerequisites</li> <li>Architecture</li> <li>Additional Resources</li> </ul>","tags":["Robotics","ROS2","Bridge","Interface"]},{"location":"operators/holoscan_ros2/#prerequisites","title":"Prerequisites","text":"<ul> <li>NVIDIA Holoscan SDK v3.0 or later</li> <li>ROS2 Jazzy (tested with Jazzy; other distributions may work but are not tested)</li> </ul> <p>For development: - C++ compiler with C++17 support (for C++ bridge) - Python 3.8+ (for Python bridge) - CMake (for building C++ applications)</p> <p>For examples and testing: - Docker (with NVIDIA Container Toolkit) - NVIDIA GPU drivers (suitable for your hardware and Holoscan SDK)</p> <p>Note: Example applications are available in the <code>applications/holoscan_ros2/</code> directory. Refer to their respective README files for specific requirements and instructions.</p>","tags":["Robotics","ROS2","Bridge","Interface"]},{"location":"operators/holoscan_ros2/#architecture","title":"Architecture","text":"<p>The Holoscan ROS2 Bridge provides two main components:</p>","tags":["Robotics","ROS2","Bridge","Interface"]},{"location":"operators/holoscan_ros2/#c-bridge-holoscanros2","title":"C++ Bridge (<code>holoscan::ros2</code>)","text":"<ul> <li>Header-only library for easy integration</li> <li>Bridge class manages ROS2 node lifecycle and communication</li> <li>Operator base class simplifies creating ROS2-aware Holoscan operators</li> <li>Type conversion between Holoscan and ROS2 message types</li> </ul>","tags":["Robotics","ROS2","Bridge","Interface"]},{"location":"operators/holoscan_ros2/#python-bridge-holoscan_ros2","title":"Python Bridge (<code>holoscan_ros2</code>)","text":"<ul> <li>Python package for <code>rclpy</code>-based applications</li> <li>Bridge resource manages ROS2 node within Holoscan Python operators</li> <li>Automatic message conversion between Holoscan tensors and ROS2 messages</li> <li>Threading support for non-blocking ROS2 operations</li> </ul> <p>Both implementations provide: - Automatic ROS2 node management - Message type conversion - Publisher/Subscriber abstractions - Integration with Holoscan's data flow architecture</p>","tags":["Robotics","ROS2","Bridge","Interface"]},{"location":"operators/holoscan_ros2/#additional-resources","title":"Additional Resources","text":"","tags":["Robotics","ROS2","Bridge","Interface"]},{"location":"operators/holoscan_ros2/#documentation","title":"Documentation","text":"<ul> <li>Holoscan SDK Documentation</li> <li>ROS2 Humble Documentation</li> <li>ROS2 Tutorials</li> </ul>","tags":["Robotics","ROS2","Bridge","Interface"]},{"location":"operators/holoscan_ros2/#related-examples","title":"Related Examples","text":"<ul> <li>Simple Examples: <code>applications/holoscan_ros2/pubsub/</code> - Basic publisher/subscriber communication</li> <li>Camera Examples: <code>applications/holoscan_ros2/vb1940/</code> - Advanced camera integration with VB1940 hardware</li> <li>Operator Source: <code>operators/holoscan_ros2/</code> - Bridge implementation and headers</li> </ul>","tags":["Robotics","ROS2","Bridge","Interface"]},{"location":"operators/holoscan_ros2/#community-and-support","title":"Community and Support","text":"<ul> <li>Holoscan SDK GitHub</li> <li>ROS2 Community</li> <li>NVIDIA Developer Forums</li> </ul>","tags":["Robotics","ROS2","Bridge","Interface"]},{"location":"operators/holoscan_ros2/#development","title":"Development","text":"<ul> <li>C++ API Reference: See <code>operators/holoscan_ros2/cpp/holoscan/ros2/</code> for headers</li> <li>Python API Reference: See <code>operators/holoscan_ros2/python/holoscan_ros2/</code> for implementation</li> <li>Contributing: Follow standard Holoscan SDK contribution guidelines</li> </ul>","tags":["Robotics","ROS2","Bridge","Interface"]},{"location":"operators/iio_controller/","title":"IIO Controller Operator","text":"<p> Authors: Andrei-Fabian Pop (Analog Devices Inc.) Supported platforms: x86_64 Language: C++, Python Last modified: October 9, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 3.2.0 Tested Holoscan SDK versions: 3.2.0 Contribution metric: Level 4 - Experimental</p>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#overview","title":"Overview","text":"<p>The IIO Controller provides a comprehensive set of operators for interfacing with Industrial I/O (IIO) devices in Holoscan applications. These operators enable real-time streaming and control of software-defined radio (SDR) devices, data acquisition systems, and various sensors through the Linux IIO subsystem.</p>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#description","title":"Description","text":"<p>The IIO Controller operators abstract the complexities of the Linux IIO framework, providing high-performance, low-latency access to: - Software Defined Radios (SDRs) like ADALM-Pluto for RF signal processing - Data Acquisition Systems for high-speed analog/digital conversion - Sensors including accelerometers, gyroscopes, magnetometers, and environmental sensors - Signal Generators and other test equipment</p>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#what-is-iio","title":"What is IIO?","text":"<p>The Industrial I/O (IIO) subsystem is a Linux kernel framework that provides: - Unified API for diverse hardware devices - High-performance data streaming - Real-time configuration of device parameters - Support for triggered sampling and buffered operations</p>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#requirements","title":"Requirements","text":"","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#software","title":"Software","text":"<ul> <li>libiio (version 0.X)</li> <li>Holoscan SDK</li> </ul>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#operator-types","title":"Operator Types","text":"<p>This package provides 5 specialized operators:</p>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#1-iioattributeread-device-parameter-reading","title":"1. <code>IIOAttributeRead</code> - Device Parameter Reading","text":"<p>Reads configuration parameters and real-time status from IIO devices. Use for: - Monitoring device temperature, gain, frequency settings - Reading calibration status - Checking signal strength indicators</p>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#2-iioattributewrite-device-parameter-control","title":"2. <code>IIOAttributeWrite</code> - Device Parameter Control","text":"<p>Writes configuration parameters to IIO devices. Use for: - Setting RF frequency, gain, bandwidth - Configuring sampling rates - Enabling/disabling device features</p>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#3-iiobufferread-high-speed-data-acquisition","title":"3. <code>IIOBufferRead</code> - High-Speed Data Acquisition","text":"<p>Streams data from IIO device buffers with DMA support. Use for: - Capturing buffer samples from SDRs - Reading multi-channel ADC data - Acquiring sensor data streams</p>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#4-iiobufferwrite-high-speed-data-transmission","title":"4. <code>IIOBufferWrite</code> - High-Speed Data Transmission","text":"<p>Streams data to IIO device buffers for output. Use for: - Transmitting buffer samples through SDRs - Generating analog waveforms via DACs - Outputting test patterns</p>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#5-iioconfigurator-automated-device-setup","title":"5. <code>IIOConfigurator</code> - Automated Device Setup","text":"<p>Applies complex configurations from YAML files. Use for: - Initializing devices with multiple parameters - Switching between operational modes - Applying calibration profiles</p>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#iioattributeread-operator","title":"IIOAttributeRead Operator","text":"","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#configuration-parameters","title":"Configuration Parameters","text":"<ul> <li><code>ctx</code>: (Mandatory) The IIO context URI:</li> <li><code>\"ip:192.168.2.1\"</code> - Network connection (e.g., ADALM-Pluto default)</li> <li><code>\"usb:3.2.5\"</code> - Direct USB connection</li> <li><code>\"local:\"</code> - Local IIO devices</li> <li><code>\"serial:/dev/ttyUSB0,115200\"</code> - Serial connection</li> <li><code>dev</code>: (Optional) Device name (e.g., <code>\"ad9361-phy\"</code> for Pluto's transceiver)</li> <li><code>chan</code>: (Optional) Channel name (e.g., <code>\"voltage0\"</code> for RF input)</li> <li><code>channel_is_output</code>: (Optional) True for TX channels, false for RX channels</li> <li><code>attr_name</code>: (Mandatory) Attribute to read (e.g., <code>\"frequency\"</code>, <code>\"sampling_frequency\"</code>, <code>\"gain\"</code>).</li> </ul>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#ports","title":"Ports","text":"<ul> <li>To receive the data read from the <code>IIOAttributeRead</code> operator, use the   output port named <code>value</code> of type <code>std::string</code>.</li> </ul>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#operator-example-reading-rf-frequency-from-adalm-pluto","title":"Operator Example - Reading RF Frequency from ADALM-Pluto","text":"<pre><code>// Read the current RF frequency from ADALM-Pluto receiver\nauto freq_reader = make_operator&lt;ops::IIOAttributeRead&gt;(\n    \"PlutoFreqReader\",\n    Arg(\"ctx\") = std::string(\"ip:192.168.2.1\"),  // Pluto's default IP\n    Arg(\"dev\") = std::string(\"ad9361-phy\"),      // RF transceiver device\n    Arg(\"chan\") = std::string(\"altvoltage0\"),    // RX LO channel\n    Arg(\"channel_is_output\") = false,             // Input channel\n    Arg(\"attr_name\") = std::string(\"frequency\")   // Read frequency attribute\n);\n\n// Connect to a display operator\nadd_flow(freq_reader, display_op, ('value', 'frequency'));\n</code></pre>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#iioattributewrite-operator","title":"IIOAttributeWrite Operator","text":"","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#configuration-parameters_1","title":"Configuration Parameters","text":"<ul> <li><code>ctx</code>: (Mandatory) The URI of the IIO context to connect to the device.</li> <li><code>dev</code>: (Optional) The name of the IIO device to write to. If not     specified, it will write to the context attributes.</li> <li><code>chan</code>: (Optional) The name of the IIO channel to write to. If not     specified, it will write to the device attributes (the dev parameter must     be specified then).</li> <li><code>channel_is_output</code>: (Optional) If true, the channel is treated as an output     channel. Defaults to false. If the <code>chan</code> parameter is set, this     parameter must also be set.</li> <li><code>attr_name</code>: (Mandatory) The name of the attribute to write to.</li> </ul>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#ports_1","title":"Ports","text":"<ul> <li>To send the data to be written to the <code>IIOAttributeWrite</code> operator, use the   input port named <code>value</code> of type <code>std::string</code>.</li> </ul>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#operator-example","title":"Operator Example","text":"<pre><code>auto iio_write_op = make_operator&lt;ops::IIOAttributeWrite&gt;(\n    \"IIOAttributeWrite\",\n    Arg(\"ctx\") = std::string(\"ip:192.168.2.1\"),\n    Arg(\"dev\") = std::string(\"ad9361-phy\"),\n    Arg(\"chan\") = std::string(\"voltage0\"),\n    Arg(\"channel_is_output\") = false,\n    Arg(\"attr_name\") = std::string(\"raw\")\n);\n\nadd_flow(basic_emitter_op, iio_write_op, ('value', 'value'));\n</code></pre>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#iiobufferread-operator","title":"IIOBufferRead Operator","text":"","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#configuration-parameters_2","title":"Configuration Parameters","text":"<ul> <li><code>ctx</code>: (Mandatory) IIO context URI (e.g., <code>\"ip:192.168.2.1\"</code> for ADALM-Pluto)</li> <li><code>dev</code>: (Mandatory) Device name:</li> <li><code>\"cf-ad9361-lpc\"</code> - Pluto's RX data streaming device</li> <li>Device name for your specific hardware</li> <li><code>is_cyclic</code>: (Mandatory) Buffer mode:</li> <li><code>true</code> - Continuous streaming (typical for SDR applications)</li> <li><code>false</code> - One-shot capture</li> <li><code>samples_count</code>: (Mandatory) Samples per channel per buffer (e.g., 8192)</li> <li><code>enabled_channel_names</code>: (Mandatory) Channel list:</li> <li><code>[\"voltage0\", \"voltage1\"]</code></li> <li><code>[\"voltage0\"]</code></li> <li><code>enabled_channel_input</code>: (Mandatory) Channel direction list:</li> <li><code>[true, true]</code> - Input channels for RX</li> <li>Must match the order of <code>enabled_channel_names</code></li> </ul>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#ports_2","title":"Ports","text":"<ul> <li>To receive the data read from the <code>IIOBufferRead</code> operator, use the output   port named <code>buffer</code> of type <code>iio_buffer_info_t</code> as a shared pointer. This   structure contains a the sample count and a void pointer to the data.   This is the data read from the buffer, in order to interpret it, please   refer to the available example application or the libiio documentation (preferred).</li> </ul>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#operator-example-capturing-iq-data-from-adalm-pluto","title":"Operator Example - Capturing IQ Data from ADALM-Pluto","text":"<pre><code>// Configure channels for IQ data reception\nstd::vector&lt;std::string&gt; rx_channels = {\"voltage0\", \"voltage1\"};\nstd::vector&lt;bool&gt; rx_input_flags = {true, true};  // Both are input channels\n\n// Create SDR receiver operator\nauto sdr_receiver = make_operator&lt;ops::IIOBufferRead&gt;(\n    \"PlutoReceiver\",\n    Arg(\"ctx\") = std::string(\"ip:192.168.2.1\"),\n    Arg(\"dev\") = std::string(\"cf-ad9361-lpc\"),     // RX streaming device\n    Arg(\"is_cyclic\") = true,                        // Continuous streaming\n    Arg(\"samples_count\") = static_cast&lt;size_t&gt;(8192), // 8K samples per buffer\n    Arg(\"enabled_channel_names\") = rx_channels,\n    Arg(\"enabled_channel_input\") = rx_input_flags\n);\n\n// Connect to signal processing pipeline\nadd_flow(sdr_receiver, fft_processor, ('buffer', 'iq_data'));\n</code></pre>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#iiobufferwrite-operator","title":"IIOBufferWrite Operator","text":"","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#configuration-parameters_3","title":"Configuration Parameters","text":"<ul> <li><code>ctx</code>: (Mandatory) IIO context URI</li> <li><code>dev</code>: (Mandatory) Device name:</li> <li><code>\"cf-ad9361-dds-core-lpc\"</code> - Pluto's TX data streaming device</li> <li><code>is_cyclic</code>: (Mandatory) Buffer mode:</li> <li><code>true</code> - Continuous transmission (typical for signal generation)</li> <li><code>false</code> - Single buffer transmission</li> <li><code>enabled_channel_names</code>: (Mandatory) Output channels:</li> <li><code>[\"voltage0\", \"voltage1\"]</code></li> <li><code>enabled_channel_output</code>: (Mandatory) Channel direction:</li> <li><code>[true, true]</code> - Output channels for TX</li> </ul>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#ports_3","title":"Ports","text":"<ul> <li>To send the data to be written to the <code>IIOBufferWrite</code> operator, use the   input port named <code>buffer</code> of type <code>iio_buffer_info_t</code> as a shared pointer.   This structure contains a the sample count and a void pointer to the data.   This is the data to be written to the buffer, in order to form it,   please refer to the available example application or the libiio documentation.</li> </ul>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#operator-example-transmitting-data-with-adalm-pluto","title":"Operator Example - Transmitting Data with ADALM-Pluto","text":"<pre><code>// Configure TX channels for data transmission\nstd::vector&lt;std::string&gt; tx_channels = {\"voltage0\", \"voltage1\"}; // Two channels\nstd::vector&lt;bool&gt; tx_output_flags = {true, true};  // Both are output channels\n\n// Create SDR transmitter operator\nauto sdr_transmitter = make_operator&lt;ops::IIOBufferWrite&gt;(\n    \"PlutoTransmitter\",\n    Arg(\"ctx\") = std::string(\"ip:192.168.2.1\"),\n    Arg(\"dev\") = std::string(\"cf-ad9361-dds-core-lpc\"), // TX streaming device\n    Arg(\"is_cyclic\") = true,                             // Continuous transmission\n    Arg(\"enabled_channel_names\") = tx_channels,\n    Arg(\"enabled_channel_output\") = tx_output_flags\n);\n\n// Connect data source to transmitter - buffer should contain raw interleaved samples\nadd_flow(data_generator, sdr_transmitter, ('output_buffer', 'buffer'));\n</code></pre>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#iioconfigurator-operator","title":"IIOConfigurator Operator","text":"","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#configuration-parameters_4","title":"Configuration Parameters","text":"<ul> <li><code>cfg</code>: (Mandatory) Path to YAML configuration file</li> </ul>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#yaml-configuration-example-for-adalm-pluto","title":"YAML Configuration Example for ADALM-Pluto","text":"<pre><code>cfg:\n  uri: \"ip:192.168.2.1\"\n  setup:\n    devices:\n      - ad9361-phy:\n          attrs:\n            - calib_mode: \"manual\"\n            - ensm_mode: \"fdd\"\n          debug-attrs:\n            - loopback: 1\n      - cf-ad9361-dds-core-lpc:\n          channels:\n            output:\n              - voltage1:\n                  attrs:\n                    - sampling_frequency: 30719999\n      - cf-ad9361-lpc:\n          channels:\n            input:\n              - voltage0:\n                  attrs:\n                    - sampling_frequency: 30719999\n          attrs:\n            - sync_start_enable: \"arm\"\n          buffer-attrs:\n            - length_align_bytes: 8\n</code></pre>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#operator-example_1","title":"Operator Example","text":"<pre><code>// Initialize ADALM-Pluto with complex configuration\nauto pluto_config = make_operator&lt;ops::IIOConfigurator&gt;(\n    \"PlutoConfig\",\n    Arg(\"cfg\") = std::string(\"pluto_setup.yaml\")\n);\n\n// Apply configuration at startup\nadd_flow(start_op(), pluto_config);\n</code></pre>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#data-format-and-buffer-structure","title":"Data Format and Buffer Structure","text":"","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#important-notes-on-data-handling","title":"Important Notes on Data Handling","text":"<p>The IIO operators provide direct access to raw device buffers without any automatic data conversion or interpretation:</p> <ol> <li>No Automatic Data Conversion: The operators do not automatically interpret channels data (needs conversion in application)</li> <li>Raw Buffer Access: Data is passed as-is from/to the IIO device buffers</li> <li>Application Responsibility: Your application must handle any necessary data interpretation or conversion</li> <li>Channel Independence: Each channel (voltage0, voltage1, etc.) is an independent data stream</li> </ol>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#buffer-memory-layout","title":"Buffer Memory Layout","text":"<p>When multiple channels are enabled, samples are interleaved in the buffer: <pre><code>Single channel: [Ch0_S0, Ch0_S1, Ch0_S2, ...]\nDual channel:   [Ch0_S0, Ch1_S0, Ch0_S1, Ch1_S1, ...]\n</code></pre></p> <p>The exact data format (sample size, endianness, etc.) depends on the specific IIO device configuration.</p>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/iio_controller/#additional-resources","title":"Additional Resources","text":"<ul> <li>Scopy Application: GUI for IIO devices</li> <li>PyADI-IIO: Python bindings for IIO devices</li> <li>Holoscan IIO Examples: Complete application examples</li> </ul>","tags":["Signal Processing","iio","libiio"]},{"location":"operators/low_rate_psd/","title":"Low Rate PSD Operator","text":"<p> Authors: John Moon john.moon@vts-i.com (Valley Tech Systems, Inc.), Eric Ferrara eric.ferrara@vts-i.com (Valley Tech Systems, Inc.), Matthew Luckenbihl matthew.luckenbihl@vts-i.com (Valley Tech Systems, Inc.) Supported platforms: x86_64 Language: C++ Last modified: August 5, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.5.0 Tested Holoscan SDK versions: 2.5.0, 2.6.0, 2.7.0, 2.8.0, 2.9.0, 3.0.0, 3.1.0 Contribution metric: Level 3 - Developmental</p> <p>A Power Spectral Density (PSD) accumulator and averager operator.</p>","tags":["Signal Processing"]},{"location":"operators/low_rate_psd/#overview","title":"Overview","text":"<p>The Low Rate PSD Operator is a utility for computing and averaging the Power Spectral Density (PSD) of input signals. PSD is a fundamental tool in signal processing for analyzing the power distribution of a signal across frequency components. This operator is designed to efficiently accumulate and average PSDs over multiple bursts of input data, making it suitable for applications such as spectrum monitoring, signal diagnostics, and real-time analysis in embedded or high-throughput environments.</p> <p>The Low Rate PSD Operator performs the following steps:</p> <ol> <li>Input Accumulation: Receives <code>num_averages</code> tensors containing float data representing signal samples or pre-computed PSDs.</li> <li>Averaging: Computes the average of all accumulated tensors to smooth out noise and fluctuations.</li> <li>Logarithmic Scaling: Applies a <code>10 * log10()</code> operation to convert the averaged power values to decibel (dB) scale, which is standard for PSD representation.</li> <li>Clamping: Restricts the data to the range of 8-bit signed integers to ensure compatibility and efficient storage.</li> <li>Casting: Converts the clamped values to signed 8-bit integers.</li> <li>Emission: Outputs the final tensor for downstream processing or analysis.</li> </ol>","tags":["Signal Processing"]},{"location":"operators/low_rate_psd/#requirements","title":"Requirements","text":"<ul> <li>MatX: Required for tensor operations (assumed to be installed on your system).</li> </ul>","tags":["Signal Processing"]},{"location":"operators/low_rate_psd/#example-usage","title":"Example Usage","text":"<p>For a practical example, see the <code>psd_pipeline</code> application.</p>","tags":["Signal Processing"]},{"location":"operators/low_rate_psd/#basic-workflow","title":"Basic Workflow","text":"<ol> <li>Configure the operator parameters (see below).</li> <li>Feed input tensors (float32 arrays) to the operator.</li> <li>Collect the output signed 8-bit integer tensor representing the averaged PSD in dB.</li> </ol>","tags":["Signal Processing"]},{"location":"operators/low_rate_psd/#multiple-channels","title":"Multiple Channels","text":"<p>The operator supports processing multiple channels in parallel. The zero-indexed <code>channel_number</code> key is retrieved from <code>metadata()</code> on each <code>compute()</code> invocation. If <code>channel_number</code> is not provided, the default is <code>0</code> (single channel).</p>","tags":["Signal Processing"]},{"location":"operators/low_rate_psd/#configuration","title":"Configuration","text":"<p>Configure the operator in your application (e.g., YAML config):</p> <pre><code>low_rate_psd:\n  burst_size: 1280         # Number of samples processed per compute() call\n  num_averages: 625        # Number of PSDs to accumulate before averaging\n  num_channels: 1          # Number of signal channels to process\n</code></pre>","tags":["Signal Processing"]},{"location":"operators/low_rate_psd/#inputoutput","title":"Input/Output","text":"<ul> <li>Input: Tensors of type <code>float32</code>, shape determined by <code>burst_size</code> and <code>num_channels</code>.</li> <li>Output: Tensor of type <code>int8</code>, representing the averaged PSD in dB scale, clamped to [-128, 127].</li> </ul>","tags":["Signal Processing"]},{"location":"operators/low_rate_psd/#notes","title":"Notes","text":"<ul> <li>Ensure the input data is properly normalized and formatted as expected by the operator.</li> <li>The operator is optimized for performance and memory efficiency, leveraging MatX for tensor operations.</li> <li>For advanced use cases, refer to the Holoscan SDK documentation and the example pipeline linked above.</li> </ul>","tags":["Signal Processing"]},{"location":"operators/lstm_tensor_rt_inference/","title":"Custom LSTM Inference","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>lstm_tensor_rt_inference</code> extension provides LSTM (Long-Short Term Memory) stateful inference module using TensorRT.</p>","tags":["Inference","LSTM","TensorRT"]},{"location":"operators/lstm_tensor_rt_inference/#nvidiaholoscanlstm_tensor_rt_inferencetensorrtinference","title":"<code>nvidia::holoscan::lstm_tensor_rt_inference::TensorRtInference</code>","text":"<p>Codelet, taking input tensors and feeding them into TensorRT for LSTM inference.</p> <p>This implementation is based on <code>nvidia::gxf::TensorRtInference</code>. <code>input_state_tensor_names</code> and <code>output_state_tensor_names</code> parameters are added to specify tensor names for states in LSTM model.</p>","tags":["Inference","LSTM","TensorRT"]},{"location":"operators/lstm_tensor_rt_inference/#parameters","title":"Parameters","text":"<ul> <li><code>model_file_path</code>: Path to ONNX model to be loaded</li> <li>type: <code>std::string</code></li> <li><code>engine_cache_dir</code>: Path to a directory containing cached generated engines to be serialized and loaded from</li> <li>type: <code>std::string</code></li> <li><code>plugins_lib_namespace</code>: Namespace used to register all the plugins in this library (default: <code>\"\"</code>)</li> <li>type: <code>std::string</code></li> <li><code>force_engine_update</code>: Always update engine regard less of existing engine file. Such conversion may take minutes (default: <code>false</code>)</li> <li>type: <code>bool</code></li> <li><code>input_tensor_names</code>: Names of input tensors in the order to be fed into the model</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>input_state_tensor_names</code>: Names of input state tensors that are used internally by TensorRT</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>input_binding_names</code>: Names of input bindings as in the model in the same order of what is provided in input_tensor_names</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>output_tensor_names</code>: Names of output tensors in the order to be retrieved from the model</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>input_state_tensor_names</code>: Names of output state tensors that are used internally by TensorRT</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>output_binding_names</code>: Names of output bindings in the model in the same order of of what is provided in output_tensor_names</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>pool</code>: Allocator instance for output tensors</li> <li>type: <code>gxf::Handle&lt;gxf::Allocator&gt;</code></li> <li><code>cuda_stream_pool</code>: Instance of gxf::CudaStreamPool to allocate CUDA stream</li> <li>type: <code>gxf::Handle&lt;gxf::CudaStreamPool&gt;</code></li> <li><code>max_workspace_size</code>: Size of working space in bytes (default: <code>67108864l</code> (64MB))</li> <li>type: <code>int64_t</code></li> <li><code>dla_core</code>: DLA Core to use. Fallback to GPU is always enabled. Default to use GPU only (<code>optional</code>)</li> <li>type: <code>int32_t</code></li> <li><code>max_batch_size</code>: Maximum possible batch size in case the first dimension is dynamic and used as batch size (default: <code>1</code>)</li> <li>type: <code>int32_t</code></li> <li><code>enable_fp16_</code>: Enable inference with FP16 and FP32 fallback (default: <code>false</code>)</li> <li>type: <code>bool</code></li> <li><code>verbose</code>: Enable verbose logging on console (default: <code>false</code>)</li> <li>type: <code>bool</code></li> <li><code>relaxed_dimension_check</code>: Ignore dimensions of 1 for input tensor dimension check (default: <code>true</code>)</li> <li>type: <code>bool</code></li> <li><code>rx</code>: List of receivers to take input tensors</li> <li>type: <code>std::vector&lt;gxf::Handle&lt;gxf::Receiver&gt;&gt;</code></li> <li><code>tx</code>: Transmitter to publish output tensors</li> <li>type: <code>gxf::Handle&lt;gxf::Transmitter&gt;</code></li> </ul>","tags":["Inference","LSTM","TensorRT"]},{"location":"operators/medical_imaging/","title":"Medical Imaging Operators","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>Medical image processing and inference operators.</p>","tags":["Medical Imaging","Healthcare Interop","AI","MONAI","STL"]},{"location":"operators/medical_imaging/#overview","title":"Overview","text":"<p>This set of operators accelerate the development of medical imaging AI inference application with DICOM imaging network integration by providing the following,</p> <ul> <li>application classes to automate the inference with MONAI Bundle as well as normal TorchScript models</li> <li>classes to load supported AI model from files to detected devices, GPU or CPU</li> <li>classes to parse runtime options and well-known environment variables</li> <li>DICOM study parsing and selection classes, as well as DICOM instance to volume image conversion</li> <li>DICOM instance writers to encapsulate AI inference results in these DICOM OID,</li> <li>DICOM Segmentation</li> <li>DICOM Basic Text Structured Report</li> <li>DICOM Encapsulated PDF</li> <li>Surface mesh generation and storage in STL format</li> <li>Visualization with Clara-Viz integration, as needed</li> </ul>","tags":["Medical Imaging","Healthcare Interop","AI","MONAI","STL"]},{"location":"operators/medical_imaging/#requirements","title":"Requirements","text":"<p>This set of operators depends on Holoscan SDK Python package, as well as directly on the following,</p> <ul> <li>highdicom</li> <li>monai</li> <li>nibabel</li> <li>numpy</li> <li>numpy-stl</li> <li>Pillow</li> <li>pydicom</li> <li>PyPDF2</li> <li>scikit-image</li> <li>SimpleITK</li> <li>torch</li> <li>trimesh</li> <li>typeguard</li> </ul>","tags":["Medical Imaging","Healthcare Interop","AI","MONAI","STL"]},{"location":"operators/medical_imaging/#notices","title":"Notices","text":"<p>Many of this set of operators are <code>Derivative Works</code> of MONAI Deploy App SDK under its Apache-2.0 license, and Nvidia employees have been the main contributors to MONAI Deploy App SDK.</p> <p>The dependency packages' licences can be viewed at their respective links as shown in the above section.</p>","tags":["Medical Imaging","Healthcare Interop","AI","MONAI","STL"]},{"location":"operators/medical_imaging/clara_viz_operator/","title":"Clara Viz Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This operator integrates Clara Viz visualization into medical imaging pipelines.</p>","tags":["Medical Imaging","Visualization","Clara Viz"]},{"location":"operators/medical_imaging/clara_viz_operator/#overview","title":"Overview","text":"<p>The <code>ClaraVizOperator</code> enables advanced visualization of medical imaging data using Clara Viz, supporting GPU-accelerated rendering and interaction.</p>","tags":["Medical Imaging","Visualization","Clara Viz"]},{"location":"operators/medical_imaging/clara_viz_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package</li> <li>clara-viz</li> <li>IPython</li> <li>ipywidgets</li> </ul>","tags":["Medical Imaging","Visualization","Clara Viz"]},{"location":"operators/medical_imaging/clara_viz_operator/#example-usage","title":"Example Usage","text":"<pre><code>from holoscan.core import Fragment\nfrom operators.medical_imaging.clara_viz_operator import ClaraVizOperator\n\nfragment = Fragment()\nviz_op = ClaraVizOperator(\n    fragment,\n    name=\"clara_viz\",  # Optional operator name\n    input_name_image=\"image\",  # Name of the input port for the image\n    input_name_seg_image=\"seg_image\"  # Name of the input port for the segmentation image\n)\n</code></pre>","tags":["Medical Imaging","Visualization","Clara Viz"]},{"location":"operators/medical_imaging/dicom_data_loader_operator/","title":"DICOM Data Loader Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This operator loads DICOM studies into memory from a folder of DICOM instance files.</p>","tags":["Medical Imaging","DICOM","Loader"]},{"location":"operators/medical_imaging/dicom_data_loader_operator/#overview","title":"Overview","text":"<p>The <code>DICOMDataLoaderOperator</code> loads DICOM studies from a specified folder, making them available as a list of <code>DICOMStudy</code> objects for downstream processing in Holoscan medical imaging pipelines.</p>","tags":["Medical Imaging","DICOM","Loader"]},{"location":"operators/medical_imaging/dicom_data_loader_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package</li> <li>pydicom</li> </ul>","tags":["Medical Imaging","DICOM","Loader"]},{"location":"operators/medical_imaging/dicom_data_loader_operator/#example-usage","title":"Example Usage","text":"<pre><code>from pathlib import Path\nfrom holoscan.core import Fragment\nfrom operators.medical_imaging.dicom_data_loader_operator import DICOMDataLoaderOperator\n\nfragment = Fragment()\ndicom_loader = DICOMDataLoaderOperator(\n    fragment,\n    name=\"dicom_loader\",  # Optional operator name\n    input_folder=Path(\"input\"),  # Path to folder containing DICOM files\n    output_name=\"dicom_study_list\",  # Name of the output port\n    must_load=True  # Whether to raise an error if no DICOM files are found\n)\n</code></pre>","tags":["Medical Imaging","DICOM","Loader"]},{"location":"operators/medical_imaging/dicom_encapsulated_pdf_writer_operator/","title":"DICOM Encapsulated PDF Writer Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This operator writes encapsulated PDF documents into DICOM format for medical imaging workflows.</p>","tags":["Medical Imaging","DICOM","PDF","Writer"]},{"location":"operators/medical_imaging/dicom_encapsulated_pdf_writer_operator/#overview","title":"Overview","text":"<p>The <code>DICOMEncapsulatedPDFWriterOperator</code> converts PDF files into DICOM-compliant encapsulated PDF objects for storage and interoperability in medical imaging pipelines.</p>","tags":["Medical Imaging","DICOM","PDF","Writer"]},{"location":"operators/medical_imaging/dicom_encapsulated_pdf_writer_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package</li> <li>pydicom</li> <li>PyPDF2</li> </ul>","tags":["Medical Imaging","DICOM","PDF","Writer"]},{"location":"operators/medical_imaging/dicom_encapsulated_pdf_writer_operator/#example-usage","title":"Example Usage","text":"<pre><code>from pathlib import Path\nfrom holoscan.core import Fragment\nfrom operators.medical_imaging.dicom_encapsulated_pdf_writer_operator import DICOMEncapsulatedPDFWriterOperator\nfrom operators.medical_imaging.utils.dicom_utils import ModelInfo, EquipmentInfo\n\nfragment = Fragment()\npdf_writer_op = DICOMEncapsulatedPDFWriterOperator(\n    fragment,\n    name=\"pdf_writer\",  # Optional operator name\n    output_folder=Path(\"output\"),  # Path to save the generated DICOM file(s)\n    model_info=ModelInfo(\n        creator=\"ExampleCreator\",\n        name=\"ExampleModel\",\n        version=\"1.0.0\",\n        uid=\"1.2.3.4.5.6.7.8.9\"\n    ),\n    equipment_info=EquipmentInfo(\n        manufacturer=\"ExampleManufacturer\",\n        manufacturer_model=\"ExampleModel\",\n        series_number=\"0000\",\n        software_version_number=\"1.0.0\"\n    ),\n    copy_tags=True,  # Set to True to copy tags from a DICOMSeries\n    custom_tags={\"PatientName\": \"DOE^JOHN\"}  # Optional: custom DICOM tags as a dict\n)\n</code></pre>","tags":["Medical Imaging","DICOM","PDF","Writer"]},{"location":"operators/medical_imaging/dicom_seg_writer_operator/","title":"DICOM Segmentation Writer Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This operator writes segmentation results into DICOM Segmentation objects for medical imaging workflows.</p>","tags":["Medical Imaging","DICOM","Segmentation","Writer"]},{"location":"operators/medical_imaging/dicom_seg_writer_operator/#overview","title":"Overview","text":"<p>The <code>DICOMSegmentationWriterOperator</code> takes segmentation data and encodes it into DICOM-compliant segmentation objects, enabling interoperability and storage in clinical systems.</p>","tags":["Medical Imaging","DICOM","Segmentation","Writer"]},{"location":"operators/medical_imaging/dicom_seg_writer_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package</li> <li>pydicom</li> <li>highdicom</li> <li>SimpleITK (for image I/O, if using NIfTI or MHD files)</li> <li>numpy</li> </ul>","tags":["Medical Imaging","DICOM","Segmentation","Writer"]},{"location":"operators/medical_imaging/dicom_seg_writer_operator/#example-usage","title":"Example Usage","text":"<pre><code>from pathlib import Path\nfrom holoscan.core import Fragment\nfrom operators.medical_imaging.dicom_seg_writer_operator import DICOMSegmentationWriterOperator, SegmentDescription\nfrom highdicom import codes\n\nfragment = Fragment()\nseg_writer_op = DICOMSegmentationWriterOperator(\n    fragment,\n    name=\"seg_writer\",  # Optional operator name\n    segment_descriptions=[\n        SegmentDescription(\n            segment_label=\"Liver\",\n            segmented_property_category=codes.DCM.Organ,\n            segmented_property_type=codes.DCM.Liver,\n            algorithm_name=\"ExampleAlgorithm\",\n            algorithm_version=\"1.0.0\"\n        )\n    ],\n    output_folder=Path(\"output\"),  # Path to save the generated DICOM file(s)\n    custom_tags={\"PatientName\": \"DOE^JOHN\"},  # Optional: custom DICOM tags as a dict\n    omit_empty_frames=True  # Whether to omit frames with no segmentation\n)\n</code></pre>","tags":["Medical Imaging","DICOM","Segmentation","Writer"]},{"location":"operators/medical_imaging/dicom_series_selector_operator/","title":"DICOM Series Selector Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This operator selects specific DICOM series from a set of studies for further processing in medical imaging workflows.</p>","tags":["Medical Imaging","DICOM","Selector"]},{"location":"operators/medical_imaging/dicom_series_selector_operator/#overview","title":"Overview","text":"<p>The <code>DICOMSeriesSelectorOperator</code> enables filtering and selection of relevant DICOM series, streamlining downstream analysis and processing in Holoscan pipelines.</p>","tags":["Medical Imaging","DICOM","Selector"]},{"location":"operators/medical_imaging/dicom_series_selector_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package</li> <li>pydicom</li> </ul>","tags":["Medical Imaging","DICOM","Selector"]},{"location":"operators/medical_imaging/dicom_series_selector_operator/#example-usage","title":"Example Usage","text":"<pre><code>from holoscan.core import Fragment\nfrom operators.medical_imaging.dicom_series_selector_operator import DICOMSeriesSelectorOperator\n\nfragment = Fragment()\nselector_op = DICOMSeriesSelectorOperator(\n    fragment,\n    name=\"series_selector\",  # Optional operator name\n    rules=\"\"\"\n    {\n        \"Modality\": \"CT\",\n        \"SeriesDescription\": \"Axial\"\n    }\n    \"\"\",  # JSON string defining selection rules\n    all_matched=False  # Whether all rules must match (AND) or any rule can match (OR)\n)\n</code></pre>","tags":["Medical Imaging","DICOM","Selector"]},{"location":"operators/medical_imaging/dicom_series_to_volume_operator/","title":"DICOM Series to Volume Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: October 9, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This operator converts a DICOM series into a volumetric image for downstream analysis in medical imaging workflows.</p>","tags":["Converter","Medical Imaging","DICOM","Volume"]},{"location":"operators/medical_imaging/dicom_series_to_volume_operator/#overview","title":"Overview","text":"<p>The <code>DICOMSeriesToVolumeOperator</code> reads a DICOM series and constructs a volume image suitable for 3D processing and visualization in Holoscan pipelines.</p>","tags":["Converter","Medical Imaging","DICOM","Volume"]},{"location":"operators/medical_imaging/dicom_series_to_volume_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package</li> <li>pydicom</li> <li>numpy</li> </ul>","tags":["Converter","Medical Imaging","DICOM","Volume"]},{"location":"operators/medical_imaging/dicom_series_to_volume_operator/#example-usage","title":"Example Usage","text":"<pre><code>from holoscan.core import Fragment\nfrom operators.medical_imaging.dicom_series_to_volume_operator import DICOMSeriesToVolumeOperator\n\nfragment = Fragment()\nvol_op = DICOMSeriesToVolumeOperator(\n    fragment,\n    name=\"series_to_volume\"  # Optional operator name\n)\n</code></pre>","tags":["Converter","Medical Imaging","DICOM","Volume"]},{"location":"operators/medical_imaging/dicom_text_sr_writer_operator/","title":"DICOM Text SR Writer Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This operator writes DICOM Structured Report (SR) objects containing text results for medical imaging workflows.</p>","tags":["Medical Imaging","DICOM","SR","Text"]},{"location":"operators/medical_imaging/dicom_text_sr_writer_operator/#overview","title":"Overview","text":"<p>The <code>DICOMTextSRWriterOperator</code> encodes textual results into DICOM-compliant Structured Report objects, enabling standardized storage and interoperability in clinical systems.</p>","tags":["Medical Imaging","DICOM","SR","Text"]},{"location":"operators/medical_imaging/dicom_text_sr_writer_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package</li> <li>pydicom</li> <li>highdicom</li> </ul>","tags":["Medical Imaging","DICOM","SR","Text"]},{"location":"operators/medical_imaging/dicom_text_sr_writer_operator/#example-usage","title":"Example Usage","text":"<pre><code>from pathlib import Path\nfrom holoscan.core import Fragment\nfrom operators.medical_imaging.dicom_text_sr_writer_operator import DICOMTextSRWriterOperator\nfrom operators.medical_imaging.utils.dicom_utils import ModelInfo, EquipmentInfo\n\nfragment = Fragment()\nsr_writer_op = DICOMTextSRWriterOperator(\n    fragment,\n    name=\"sr_writer\",  # Optional operator name\n    output_folder=Path(\"output\"),  # Path to save the generated DICOM file(s)\n    model_info=ModelInfo(\n        creator=\"ExampleCreator\",\n        name=\"ExampleModel\",\n        version=\"1.0.0\",\n        uid=\"1.2.3.4.5.6.7.8.9\"\n    ),\n    equipment_info=EquipmentInfo(\n        manufacturer=\"ExampleManufacturer\",\n        manufacturer_model=\"ExampleModel\",\n        series_number=\"0000\",\n        software_version_number=\"1.0.0\"\n    ),\n    copy_tags=True,  # Set to True to copy tags from a DICOMSeries\n    custom_tags={\"PatientName\": \"DOE^JOHN\"}  # Optional: custom DICOM tags as a dict\n)\n</code></pre>","tags":["Medical Imaging","DICOM","SR","Text"]},{"location":"operators/medical_imaging/inference_operator/","title":"Inference Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This operator provides a base class for running inference in medical imaging pipelines.</p>","tags":["Medical Imaging","Inference","Base"]},{"location":"operators/medical_imaging/inference_operator/#overview","title":"Overview","text":"<p>The <code>InferenceOperator</code> serves as a foundation for building specialized inference operators, handling model loading, execution, and result management.</p>","tags":["Medical Imaging","Inference","Base"]},{"location":"operators/medical_imaging/inference_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package</li> <li>torch (optional, for deep learning models)</li> </ul>","tags":["Medical Imaging","Inference","Base"]},{"location":"operators/medical_imaging/inference_operator/#example-usage","title":"Example Usage","text":"<pre><code>from holoscan.core import Fragment\nfrom operators.medical_imaging.inference_operator import InferenceOperator\n\nclass MyInferenceOperator(InferenceOperator):\n    def __init__(self, fragment, *args, **kwargs):\n        super().__init__(fragment, *args, **kwargs)\n\n    def pre_process(self, data, *args, **kwargs):\n        # Implement preprocessing logic\n        return data\n\n    def predict(self, data, *args, **kwargs):\n        # Implement inference logic\n        return data\n\n    def post_process(self, data, *args, **kwargs):\n        # Implement postprocessing logic\n        return data\n\nfragment = Fragment()\ninference_op = MyInferenceOperator(\n    fragment,\n    name=\"my_inference\"  # Optional operator name\n)\n</code></pre>","tags":["Medical Imaging","Inference","Base"]},{"location":"operators/medical_imaging/monai_bundle_inference_operator/","title":"MONAI Bundle Inference Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This operator performs inference using MONAI Bundles for medical imaging tasks.</p>","tags":["Medical Imaging","MONAI","Bundle","Inference"]},{"location":"operators/medical_imaging/monai_bundle_inference_operator/#overview","title":"Overview","text":"<p>The <code>MonaiBundleInferenceOperator</code> loads a MONAI Bundle model and applies it to input medical images for inference, supporting flexible deployment in Holoscan pipelines.</p>","tags":["Medical Imaging","MONAI","Bundle","Inference"]},{"location":"operators/medical_imaging/monai_bundle_inference_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package</li> <li>MONAI</li> <li>torch</li> </ul>","tags":["Medical Imaging","MONAI","Bundle","Inference"]},{"location":"operators/medical_imaging/monai_bundle_inference_operator/#example-usage","title":"Example Usage","text":"<pre><code>from pathlib import Path\nfrom holoscan.core import Fragment\nfrom operators.medical_imaging.monai_bundle_inference_operator import MonaiBundleInferenceOperator\nfrom operators.medical_imaging.core import AppContext, IOMapping, IOType, Image\n\nfragment = Fragment()\napp_context = AppContext({})  # Initialize with empty args dict\n\nbundle_op = MonaiBundleInferenceOperator(\n    fragment,\n    name=\"monai_bundle\",  # Optional operator name\n    app_context=app_context,\n    input_mapping=[\n        IOMapping(\n            label=\"image\",\n            data_type=Image,\n            storage_type=IOType.IN_MEMORY\n        )\n    ],\n    output_mapping=[\n        IOMapping(\n            label=\"pred\",\n            data_type=Image,\n            storage_type=IOType.IN_MEMORY\n        )\n    ],\n    model_name=\"model\",  # Name of the model in the bundle\n    bundle_path=Path(\"model/model.ts\")  # Path to the MONAI bundle\n)\n</code></pre>","tags":["Medical Imaging","MONAI","Bundle","Inference"]},{"location":"operators/medical_imaging/monai_seg_inference_operator/","title":"MONAI Segmentation Inference Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This segmentation operator uses MONAI transforms and Sliding Window Inference to segment medical images.</p>","tags":["Medical Imaging","MONAI","Segmentation","Inference"]},{"location":"operators/medical_imaging/monai_seg_inference_operator/#overview","title":"Overview","text":"<p>The <code>MonaiSegInferenceOperator</code> performs pre-transforms on input images, runs segmentation inference using a specified model, and applies post-transforms. The segmentation result is returned as an in-memory image object and can optionally be saved to disk.</p>","tags":["Medical Imaging","MONAI","Segmentation","Inference"]},{"location":"operators/medical_imaging/monai_seg_inference_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package</li> <li>MONAI</li> <li>torch</li> </ul>","tags":["Medical Imaging","MONAI","Segmentation","Inference"]},{"location":"operators/medical_imaging/monai_seg_inference_operator/#example-usage","title":"Example Usage","text":"<pre><code>from pathlib import Path\nimport torch\nfrom monai.transforms import Compose, LoadImage, ScaleIntensity, EnsureChannelFirst\nfrom holoscan.core import Fragment\nfrom operators.medical_imaging.monai_segmentation_inference_operator import MonaiSegInferenceOperator\nfrom operators.medical_imaging.core import AppContext, IOMapping, IOType, Image\n\n# Initialize the fragment\nfragment = Fragment()\n\n# Create app context\napp_context = AppContext({})\n\n# Define transforms\npre_transforms = Compose([\n    LoadImage(image_only=True),\n    EnsureChannelFirst(),\n    ScaleIntensity(),\n])\n\npost_transforms = Compose([\n    # Add your post-processing transforms here\n])\n\n# Initialize the segmentation operator\nseg_op = MonaiSegInferenceOperator(\n    fragment,\n    roi_size=(96, 96, 96),  # Example ROI size for 3D images\n    pre_transforms=pre_transforms,\n    post_transforms=post_transforms,\n    app_context=app_context,\n    model_name=\"unet\",  # Example model name\n    overlap=0.25,\n    sw_batch_size=4,\n    model_path=Path(\"/path/to/your/model.pt\")  # Replace with your model path\n)\n</code></pre>","tags":["Medical Imaging","MONAI","Segmentation","Inference"]},{"location":"operators/medical_imaging/nii_data_loader_operator/","title":"NIfTI Data Loader Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This operator loads NIfTI (nii/nii.gz) medical images for use in Holoscan pipelines.</p>","tags":["Medical Imaging","NIfTI","Data Loading"]},{"location":"operators/medical_imaging/nii_data_loader_operator/#overview","title":"Overview","text":"<p>The <code>NIIDataLoaderOperator</code> reads NIfTI files and makes them available as in-memory images for downstream processing in medical imaging workflows.</p>","tags":["Medical Imaging","NIfTI","Data Loading"]},{"location":"operators/medical_imaging/nii_data_loader_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package (version &gt;= 1.0.3)</li> <li>SimpleITK</li> <li>numpy</li> </ul>","tags":["Medical Imaging","NIfTI","Data Loading"]},{"location":"operators/medical_imaging/nii_data_loader_operator/#example-usage","title":"Example Usage","text":"<pre><code>from pathlib import Path\nfrom holoscan.core import Fragment\nfrom operators.medical_imaging.nii_data_loader_operator import NiftiDataLoader\n\n# Create a fragment\nfragment = Fragment()\n\n# Initialize the NIfTI loader with a path to your NIfTI file\nnii_path = Path(\"path/to/your/image.nii\")  # or .nii.gz\nnii_loader = NiftiDataLoader(fragment, input_path=nii_path)\n\n# The operator can be used in a pipeline\n# The output port 'image' will contain the loaded image as a numpy array\n# You can connect it to other operators that expect image data\n</code></pre>","tags":["Medical Imaging","NIfTI","Data Loading"]},{"location":"operators/medical_imaging/nii_data_loader_operator/#inputoutput-ports","title":"Input/Output Ports","text":"<ul> <li>Input:</li> <li><code>image_path</code> (optional): Path to the NIfTI file. If not provided, uses the path specified during initialization.</li> <li>Output:</li> <li><code>image</code>: Numpy array containing the loaded image data.</li> </ul>","tags":["Medical Imaging","NIfTI","Data Loading"]},{"location":"operators/medical_imaging/nii_data_loader_operator/#notes","title":"Notes","text":"<ul> <li>The operator supports both .nii and .nii.gz file formats</li> <li>The output image is transposed to match the expected orientation (axes order: [2, 1, 0])</li> </ul>","tags":["Medical Imaging","NIfTI","Data Loading"]},{"location":"operators/medical_imaging/png_converter_operator/","title":"PNG Converter Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This operator converts medical images to PNG format for visualization or storage.</p>","tags":["Medical Imaging","PNG","Image Conversion"]},{"location":"operators/medical_imaging/png_converter_operator/#overview","title":"Overview","text":"<p>The <code>PNGConverterOperator</code> takes medical imaging data and outputs PNG images, facilitating integration with visualization tools and pipelines.</p>","tags":["Medical Imaging","PNG","Image Conversion"]},{"location":"operators/medical_imaging/png_converter_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package</li> <li>Pillow</li> </ul>","tags":["Medical Imaging","PNG","Image Conversion"]},{"location":"operators/medical_imaging/png_converter_operator/#example-usage","title":"Example Usage","text":"<pre><code>from pathlib import Path\nfrom holoscan.core import Fragment\nfrom operators.medical_imaging.png_converter_operator import PNGConverterOperator\nfrom operators.medical_imaging.core import Image\nimport numpy as np\n\n# Create a Fragment\nfragment = Fragment()\n\n# Create output directory\noutput_folder = Path(\"output_png\")\noutput_folder.mkdir(exist_ok=True)\n\n# Create the PNG converter operator\npng_op = PNGConverterOperator(\n    fragment,\n    output_folder=output_folder,\n    name=\"png_converter\"\n)\n\n# Example: Convert a 3D medical image to PNG slices\n# Assuming you have a 3D numpy array or Image object\n# For a 3D array of shape (slices, height, width)\nimage_data = np.random.randint(0, 255, (10, 512, 512), dtype=np.uint8)  # Example data\nmedical_image = Image(image_data)  # Create Image object\n\n# Convert and save the slices\npng_op.convert_and_save(medical_image, output_folder)\n</code></pre> <p>The operator will save individual PNG files for each slice in the specified output folder, named sequentially (0.png, 1.png, etc.).</p>","tags":["Medical Imaging","PNG","Image Conversion"]},{"location":"operators/medical_imaging/publisher_operator/","title":"Publisher Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This operator publishes medical imaging data to downstream consumers or external systems.</p>","tags":["Medical Imaging","Publisher"]},{"location":"operators/medical_imaging/publisher_operator/#overview","title":"Overview","text":"<p>The <code>PublisherOperator</code> enables flexible publishing of processed medical imaging data for visualization, storage, or further analysis in Holoscan pipelines.</p>","tags":["Medical Imaging","Publisher"]},{"location":"operators/medical_imaging/publisher_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package</li> </ul>","tags":["Medical Imaging","Publisher"]},{"location":"operators/medical_imaging/publisher_operator/#example-usage","title":"Example Usage","text":"<pre><code>from holoscan.core import Fragment\nfrom operators.medical_imaging.publisher_operator import PublisherOperator\nfrom pathlib import Path\n\n# Create a fragment\nfragment = Fragment()\n\n# Initialize the publisher operator with input and output folders\npub_op = PublisherOperator(\n    fragment,\n    input_folder=Path(\"path/to/input\"),  # Folder containing input and segment mask files\n    output_folder=Path(\"path/to/output\")  # Folder where published files will be saved\n)\n\n# Add the operator to the fragment\nfragment.add_operator(pub_op)\n</code></pre> <p>The operator expects:</p> <ul> <li>Input folder containing medical imaging files (nii, nii.gz, or mhd format)</li> <li>Output folder where the published files will be saved</li> <li>The operator will automatically find density and mask files in the input folder</li> <li>Published files will include the original images and configuration files for visualization</li> </ul>","tags":["Medical Imaging","Publisher"]},{"location":"operators/medical_imaging/stl_conversion_operator/","title":"STL Conversion Operator","text":"<p> Authors: Holoscan SDK Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: October 9, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.2.0, 3.2.0 Contribution metric: Level 2 - Trusted</p> <p>This operator converts medical imaging data to STL format for 3D visualization and printing.</p>","tags":["Converter","Medical Imaging","STL"]},{"location":"operators/medical_imaging/stl_conversion_operator/#overview","title":"Overview","text":"<p>The <code>STLConversionOperator</code> takes volumetric or surface data and outputs STL files, supporting workflows for 3D modeling and printing in medical imaging.</p>","tags":["Converter","Medical Imaging","STL"]},{"location":"operators/medical_imaging/stl_conversion_operator/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK Python package</li> <li>numpy</li> <li>numpy-stl</li> </ul>","tags":["Converter","Medical Imaging","STL"]},{"location":"operators/medical_imaging/stl_conversion_operator/#example-usage","title":"Example Usage","text":"<p>Here's a basic example of how to use the STLConversionOperator:</p> <pre><code>from holoscan.core import Fragment\nfrom operators.medical_imaging.stl_conversion_operator import STLConversionOperator\nfrom pathlib import Path\n\n# Create a fragment\nfragment = Fragment()\n\n# Initialize the STL conversion operator\nstl_operator = STLConversionOperator(\n    fragment,\n    output_file=\"output/surface_mesh.stl\",  # Path to save the STL file\n    is_smooth=True,  # Enable mesh smoothing\n    keep_largest_connected_component=True  # Keep only the largest connected component\n)\n\n# Setup the operator\nstl_operator.setup()\n\n# Example: Convert an image to STL\n# Assuming 'image' is an Image object with volumetric data\nstl_bytes = stl_operator._convert(image, Path(\"output/surface_mesh.stl\"))\n</code></pre> <p>For a complete workflow example that includes loading DICOM data and converting it to STL, please refer to the tutorial on Processing DICOM to USD with MONAI Deploy and Holoscan.</p>","tags":["Converter","Medical Imaging","STL"]},{"location":"operators/medical_imaging/stl_conversion_operator/#parameters","title":"Parameters","text":"<p>The STLConversionOperator accepts the following parameters:</p> <ul> <li><code>output_file</code> (Path or str): Path where the STL file will be saved</li> <li><code>class_id</code> (array, optional): Class label IDs to include in the conversion</li> <li><code>is_smooth</code> (bool, optional): Whether to apply mesh smoothing (default: True)</li> <li><code>keep_largest_connected_component</code> (bool, optional): Whether to keep only the largest connected component (default: True)</li> </ul>","tags":["Converter","Medical Imaging","STL"]},{"location":"operators/mesh_to_usd/","title":"Mesh to USD Operator","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 2 - Trusted</p> <p>The <code>SendMeshToUSDOp</code> operator converts 3D meshes in STL format to USD (Universal Scene Description) mesh format for integration into OpenUSD scenes.</p>","tags":["Converter","OpenUSD","STL"]},{"location":"operators/mesh_to_usd/#overview","title":"Overview","text":"<p>This operator takes STL mesh data (either from a file path or byte stream) and converts it to USD geometry format. It supports both file-based and in-memory STL data processing, making it flexible for various pipeline configurations.</p>","tags":["Converter","OpenUSD","STL"]},{"location":"operators/mesh_to_usd/#features","title":"Features","text":"<ul> <li>STL to USD Conversion: Converts STL mesh files to USD mesh format</li> <li>Dual Input Support: Accepts STL data from file paths or byte streams</li> <li>OpenUSD Integration: Seamlessly integrates with existing USD scenes</li> <li>Mesh Properties: Preserves mesh points, normals, and face information</li> <li>Component Hierarchy: Sets appropriate USD component kind for assembly selection</li> </ul>","tags":["Converter","OpenUSD","STL"]},{"location":"operators/mesh_to_usd/#usage","title":"Usage","text":"","tags":["Converter","OpenUSD","STL"]},{"location":"operators/mesh_to_usd/#basic-usage-with-file-path","title":"Basic Usage with File Path","text":"<pre><code>from holoscan.operators import SendMeshToUSDOp\n\n# Create operator with STL file path\nmesh_op = SendMeshToUSDOp(\n    fragment=fragment,\n    stl_file_path=\"/path/to/mesh.stl\",\n    g_stage=usd_stage\n)\n</code></pre>","tags":["Converter","OpenUSD","STL"]},{"location":"operators/mesh_to_usd/#usage-with-byte-stream-input","title":"Usage with Byte Stream Input","text":"<pre><code># Create operator for byte stream input\nmesh_op = SendMeshToUSDOp(\n    fragment=fragment,\n    g_stage=usd_stage\n)\n\n# Connect to upstream operator that provides STL bytes\n# The input name is \"stl_bytes\"\n</code></pre> <p>Please refer to the Processing DICOM to USD with MONAI Deploy and Holoscan Tutorial for an example usage.</p>","tags":["Converter","OpenUSD","STL"]},{"location":"operators/mesh_to_usd/#parameters","title":"Parameters","text":"<ul> <li><code>stl_file_path</code> (optional): Path to STL file to convert</li> <li><code>g_stage</code>: Existing USD stage where the mesh will be added</li> </ul>","tags":["Converter","OpenUSD","STL"]},{"location":"operators/mesh_to_usd/#inputoutput","title":"Input/Output","text":"","tags":["Converter","OpenUSD","STL"]},{"location":"operators/mesh_to_usd/#input","title":"Input","text":"<ul> <li><code>stl_bytes</code> (optional): Byte stream containing STL mesh data</li> <li>Required when <code>stl_file_path</code> is not provided</li> <li>Condition: <code>ConditionType.NONE</code> (optional input)</li> </ul>","tags":["Converter","OpenUSD","STL"]},{"location":"operators/mesh_to_usd/#output","title":"Output","text":"<ul> <li>USD Mesh: Creates a USD mesh primitive in the specified stage</li> <li>Mesh name: \"mesh\" (automatically made valid identifier)</li> <li>Location: Under the stage's default prim path</li> <li>Properties: Points, normals, face vertex indices, face vertex counts</li> <li>Kind: Component (for assembly hierarchy support)</li> </ul>","tags":["Converter","OpenUSD","STL"]},{"location":"operators/mesh_to_usd/#mesh-processing","title":"Mesh Processing","text":"<p>The operator performs the following conversions:</p> <ol> <li>STL Parsing: Reads STL file or byte stream using numpy-stl library</li> <li>Point Conversion: Extracts vertex coordinates and converts to USD format</li> <li>Normal Processing: Preserves surface normals from STL data</li> <li>Face Generation: Creates triangular faces with proper indexing</li> <li>Extent Calculation: Computes bounding box for the mesh</li> <li>USD Integration: Adds mesh to existing USD stage with proper metadata</li> </ol>","tags":["Converter","OpenUSD","STL"]},{"location":"operators/mesh_to_usd/#file-format-support","title":"File Format Support","text":"<ul> <li>Input: STL (Stereolithography) format</li> <li>Output: USD (Universal Scene Description) mesh format</li> <li>Compatibility: Works with OpenUSD pipeline and tools</li> </ul>","tags":["Converter","OpenUSD","STL"]},{"location":"operators/mesh_to_usd/#integration","title":"Integration","text":"<p>The operator is designed to work within Holoscan pipelines and can be connected to:</p> <ul> <li>STL file readers or generators</li> <li>USD stage management operators</li> <li>Visualization and rendering systems</li> <li>3D processing workflows</li> </ul>","tags":["Converter","OpenUSD","STL"]},{"location":"operators/npp_filter/","title":"NPP Filter","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: October 9, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>npp_filter</code> operator uses NPP to apply a filters to a Tensor or VideBuffer.</p>","tags":["Signal Processing","NPP"]},{"location":"operators/npp_filter/#holoscanopsnppfilter","title":"<code>holoscan::ops::NppFilter</code>","text":"<p>Operator class to apply a filter of the NPP library to a Tensor or VideBuffer.</p>","tags":["Signal Processing","NPP"]},{"location":"operators/npp_filter/#parameters","title":"Parameters","text":"<ul> <li><code>filter</code>: Name of the filter to apply (supported Gauss, SobelHoriz, SobelVert)</li> <li>type: <code>std::string</code></li> <li><code>mask_size</code>: Filter mask size (supported values 3, 5, 7, 9, 11, 13)</li> <li>type: <code>uint32_t</code></li> <li><code>allocator</code>: Allocator used to allocate the output data</li> <li>type: <code>std::shared_ptr&lt;Allocator&gt;</code></li> </ul>","tags":["Signal Processing","NPP"]},{"location":"operators/npp_filter/#inputs","title":"Inputs","text":"<ul> <li><code>input</code>: Input frame data</li> <li>type: <code>nvidia::gxf::Tensor</code> or <code>nvidia::gxf::VideoBuffer</code></li> </ul>","tags":["Signal Processing","NPP"]},{"location":"operators/npp_filter/#outputs","title":"Outputs","text":"<ul> <li><code>input</code>: Output frame data</li> <li>type: <code>nvidia::gxf::Tensor</code> or <code>nvidia::gxf::VideoBuffer</code></li> </ul>","tags":["Signal Processing","NPP"]},{"location":"operators/nvidia_video_codec/","title":"NVIDIA Video Codec Operators","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: October 9, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 3.3.0 Tested Holoscan SDK versions: 3.3.0, 3.4.0 Contribution metric: Level 2 - Trusted</p> <p>This directory includes the <code>nv_video_decoder</code>, <code>nv_video_encoder</code>, and <code>nv_video_reader</code> operators, which are based on the NVIDIA Video Codec SDK.</p> <p>These encoder and decoder operators are designed for streaming applications. The encoded frames are stored on the host (CPU)  memory, where they can be copied to another network streaming operator.</p> <p>[!IMPORTANT] By using the NVIDIA Video Codec operators, you agree to the NVIDIA Software Developer License Agreement. If you disagree with the EULA, please do not run this application.</p>","tags":["Streaming","NVIDIA Video Codec SDK","H.264","H.265","HEVC","Video Reading","File Reader","FFmpeg"]},{"location":"operators/nvidia_video_codec/#requirements","title":"Requirements","text":"<ul> <li>NVIDIA Driver Version &gt;= 570</li> <li>CUDA Version &gt;= 12.8</li> <li>x86 and SBSA platforms with dedicated GPU</li> </ul>","tags":["Streaming","NVIDIA Video Codec SDK","H.264","H.265","HEVC","Video Reading","File Reader","FFmpeg"]},{"location":"operators/nvidia_video_codec/#sample-applications","title":"Sample Applications","text":"<ul> <li>H.264 File Decoder</li> <li>Encode and Decode</li> <li>Video Writer</li> </ul>","tags":["Streaming","NVIDIA Video Codec SDK","H.264","H.265","HEVC","Video Reading","File Reader","FFmpeg"]},{"location":"operators/nvidia_video_codec/#licensing","title":"Licensing","text":"<p>Holohub applications and operators are licensed under Apache-2.0.</p> <p>NVIDIA Video Codec is governed by the terms of the NVIDIA Software Developer License Agreement, which you accept by cloning, running, or using the NVIDIA Video Codec sample applications and operators.</p>","tags":["Streaming","NVIDIA Video Codec SDK","H.264","H.265","HEVC","Video Reading","File Reader","FFmpeg"]},{"location":"operators/openigtlink/","title":"OpenIGTLink operator","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 2 - Trusted</p> <p>The <code>openigtlink</code> operator provides a way to send and receive imaging data using the OpenIGTLink library. The <code>openigtlink</code> operator contains separate operators for transmit and receive. Users may choose one or the other, or use both in applications requiring bidirectional traffic.</p> <p>The <code>openigtlink</code> operators use class names: <code>OpenIGTLinkTxOp</code> and <code>OpenIGTLinkRxOp</code></p>","tags":["Streaming","3D Slicer"]},{"location":"operators/openigtlink/#nvidiaholoscanopenigtlink","title":"<code>nvidia::holoscan::openigtlink</code>","text":"<p>Operator class to send and transmit data using the OpenIGTLink protocol.</p>","tags":["Streaming","3D Slicer"]},{"location":"operators/openigtlink/#receiver-configuration-parameters","title":"Receiver Configuration Parameters","text":"<ul> <li><code>port</code>: Port number of server</li> <li>type: <code>integer</code></li> <li><code>out_tensor_name</code>: Name of output tensor</li> <li>type: <code>string</code></li> <li><code>flip_width_height</code>: Flip width and height (necessary for receiving from 3D Slicer)</li> <li>type: <code>bool</code></li> </ul>","tags":["Streaming","3D Slicer"]},{"location":"operators/openigtlink/#transmitter-configuration-parameters","title":"Transmitter Configuration Parameters","text":"<ul> <li><code>device_name</code>: OpenIGTLink device name</li> <li>type: <code>string</code></li> <li><code>input_names</code>: Names of input messages</li> <li>type: <code>std::vector&lt;std::string&gt;</code></li> <li><code>host_name</code>: Host name</li> <li>type: <code>string</code></li> <li><code>port</code>: Port number of server</li> <li>type: <code>integer</code></li> </ul>","tags":["Streaming","3D Slicer"]},{"location":"operators/orsi/","title":"Orsi Academy Operators","text":"<p> Authors: Jasper Hofman (Orsi Academy) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 3 - Developmental</p> <p>A collection of specialized operators for medical imaging and surgical visualization applications, providing comprehensive tools for format conversion, segmentation processing, and 3D visualization.</p>"},{"location":"operators/orsi/#overview","title":"Overview","text":"<p>The Orsi Academy operators provide a complete pipeline for medical imaging applications, from data preprocessing to final visualization. These operators are designed for surgical guidance systems and medical AI workflows, supporting both CPU and GPU processing with CUDA acceleration.</p>"},{"location":"operators/orsi/#operators","title":"Operators","text":""},{"location":"operators/orsi/#1-format-converter-orsi_format_converter","title":"1. Format Converter (<code>orsi_format_converter</code>)","text":"<p>Converts between different image and tensor formats with support for resizing, normalization, and channel reordering.</p> <p>Key Features:</p> <ul> <li>Multiple format conversions (RGB888, RGBA8888, Float32, YUV420, NV12)</li> <li>Image resizing with different interpolation modes</li> <li>Channel reordering and normalization</li> <li>CUDA-accelerated processing using NPP libraries</li> <li>Support for ROI (Region of Interest) cropping</li> </ul> <p>Usage:</p> <pre><code>from holoscan.operators import OrsiFormatConverterOp\n\nconverter = OrsiFormatConverterOp(\n    fragment=fragment,\n    allocator=allocator,\n    out_dtype=\"float32\",\n    in_dtype=\"rgb888\",\n    scale_min=0.0,\n    scale_max=1.0,\n    resize_width=512,\n    resize_height=512\n)\n</code></pre> <p>Please refer to the following Holoscan reference applications for usage of this operator:</p> <ul> <li>In-Out Body Detection and Surgical Video Anonymization</li> <li>Multi AI and AR Visualization</li> <li>Surgical Tool Segmentation and AR Overlay</li> </ul>"},{"location":"operators/orsi/#2-segmentation-preprocessor-orsi_segmentation_preprocessor","title":"2. Segmentation Preprocessor (<code>orsi_segmentation_preprocessor</code>)","text":"<p>Prepares input data for segmentation neural networks with normalization and preprocessing.</p> <p>Key Features:</p> <ul> <li>Data format conversion (HWC/CHW)</li> <li>Mean/std normalization</li> <li>CUDA-accelerated preprocessing</li> <li>Support for different network input formats</li> <li>Flexible tensor handling</li> </ul> <p>Usage:</p> <pre><code>from holoscan.operators import OrsiSegmentationPreprocessorOp\n\npreprocessor = OrsiSegmentationPreprocessorOp(\n    fragment=fragment,\n    allocator=allocator,\n    data_format=\"hwc\",\n    normalize_means=[0.485, 0.456, 0.406],\n    normalize_stds=[0.229, 0.224, 0.225]\n)\n</code></pre> <p>Please refer to the following Holoscan reference applications for usage of this operator:</p> <ul> <li>In-Out Body Detection and Surgical Video Anonymization</li> <li>Multi AI and AR Visualization</li> <li>Surgical Tool Segmentation and AR Overlay</li> </ul>"},{"location":"operators/orsi/#3-segmentation-postprocessor-orsi_segmentation_postprocessor","title":"3. Segmentation Postprocessor (<code>orsi_segmentation_postprocessor</code>)","text":"<p>Processes neural network outputs to generate segmentation masks and visualizations.</p> <p>Key Features:</p> <ul> <li>Network output processing (softmax, sigmoid)</li> <li>Segmentation mask generation</li> <li>Image resizing and ROI handling</li> <li>CUDA-accelerated postprocessing</li> <li>Support for different output formats</li> </ul> <p>Usage:</p> <pre><code>from holoscan.operators import OrsiSegmentationPostprocessorOp\n\npostprocessor = OrsiSegmentationPostprocessorOp(\n    fragment=fragment,\n    allocator=allocator,\n    network_output_type=\"softmax\",\n    data_format=\"hwc\",\n    output_img_size=[512, 512]\n)\n</code></pre> <p>Please refer to the following Holkoscan reference applications for usage of this operator:</p> <ul> <li>Multi AI and AR Visualization</li> <li>Surgical Tool Segmentation and AR Overlay</li> </ul>"},{"location":"operators/orsi/#4-visualizer-orsi_visualizer","title":"4. Visualizer (<code>orsi_visualizer</code>)","text":"<p>Advanced 3D visualization system for surgical guidance with OpenGL rendering and VTK integration.</p> <p>Key Features:</p> <ul> <li>Real-time video frame rendering</li> <li>3D STL model visualization with VTK</li> <li>Surgical tool overlay effects</li> <li>Anonymization effects for privacy</li> <li>Interactive camera controls</li> <li>CUDA-OpenGL interop for performance</li> <li>Multi-window support</li> </ul> <p>Usage:</p> <pre><code>from holoscan.operators import OrsiVisualizationOp\n\nvisualizer = OrsiVisualizationOp(\n    fragment=fragment,\n    stl_file_path=\"/path/to/anatomy.stl\",\n    stl_names=[\"liver\", \"kidney\"],\n    stl_colors=[[255, 0, 0], [0, 255, 0]],\n    registration_params_path=\"/path/to/registration.json\"\n)\n</code></pre> <p>Please refer to the following Holkoscan reference applications for usage of this operator:</p> <ul> <li>In-Out Body Detection and Surgical Video Anonymization</li> <li>Multi AI and AR Visualization</li> <li>Surgical Tool Segmentation and AR Overlay</li> </ul>"},{"location":"operators/orsi/#common-parameters","title":"Common Parameters","text":""},{"location":"operators/orsi/#allocator","title":"Allocator","text":"<p>All operators require a shared allocator for memory management:</p> <pre><code>allocator = holoscan.resources.UnboundedAllocator(fragment)\n</code></pre>"},{"location":"operators/orsi/#cuda-stream-pool","title":"CUDA Stream Pool","text":"<p>For GPU-accelerated operations:</p> <pre><code>cuda_stream_pool = holoscan.resources.CudaStreamPool(fragment)\n</code></pre>"},{"location":"operators/orsi/#data-flow","title":"Data Flow","text":"<p>Typical pipeline configuration:</p> <pre><code>Video Source \u2192 Format Converter \u2192 Segmentation Preprocessor \u2192 AI Model \u2192 Segmentation Postprocessor \u2192 Visualizer\n</code></pre>"},{"location":"operators/orsi/#supported-formats","title":"Supported Formats","text":""},{"location":"operators/orsi/#input-formats","title":"Input Formats","text":"<ul> <li>RGB888, RGBA8888</li> <li>Float32 tensors</li> <li>YUV420, NV12 video formats</li> <li>Various tensor layouts (HWC, CHW)</li> </ul>"},{"location":"operators/orsi/#output-formats","title":"Output Formats","text":"<ul> <li>Processed video frames</li> <li>Segmentation masks</li> <li>3D visualizations</li> <li>Normalized tensors</li> </ul>"},{"location":"operators/orsi/#integration","title":"Integration","text":"<p>These operators are designed to work together in medical imaging pipelines:</p> <ul> <li>Preprocessing: Format conversion and normalization</li> <li>AI Processing: Segmentation model inference</li> <li>Postprocessing: Mask generation and refinement</li> <li>Visualization: Real-time surgical guidance display</li> </ul>"},{"location":"operators/prohawk_video_processing/","title":"Prohawk Video Processing Operator","text":"<p> Authors: Tim Wooldridge (Prohawk Technology Group) Supported platforms: aarch64 Language: C++ Last modified: August 5, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 0.5.1 Tested Holoscan SDK versions: 0.5.1, 0.6.0 Contribution metric: Level 4 - Experimental</p>","tags":["Video"]},{"location":"operators/prohawk_video_processing/#overview","title":"Overview","text":"<p>The Prohawk Video Processing Operator is a Holoscan SDK operator that integrates Prohawk Technology Group's video restoration and enhancement capabilities. This operator provides real-time video processing with multiple filter options for various applications including medical imaging, surveillance, and broadcast video enhancement.</p>","tags":["Video"]},{"location":"operators/prohawk_video_processing/#features","title":"Features","text":"<ul> <li>Real-time Video Processing: Process video streams in real-time with minimal latency</li> <li>Multiple Filter Presets:</li> <li>AFS (Adaptive Frame Stabilization) - Default filter with adaptive frame stabilization</li> <li>Low Light Enhancement - Optimized for low-light conditions</li> <li>Vascular Detail Enhancement - Specialized for medical imaging applications</li> <li>Vaper Filter - Advanced video enhancement algorithm</li> <li>Interactive Controls: Real-time filter switching and display options</li> <li>Side-by-Side Comparison: View original and processed video side-by-side</li> <li>OpenCV Integration: Seamless integration with OpenCV for image processing</li> <li>CUDA Support: GPU-accelerated processing for improved performance</li> </ul>","tags":["Video"]},{"location":"operators/prohawk_video_processing/#usage","title":"Usage","text":"<p>Please refer to Prohawk Video Processing for requirements and usage examples.</p>","tags":["Video"]},{"location":"operators/prohawk_video_processing/#basic-usage","title":"Basic Usage","text":"<p>The operator can be used in Holoscan applications to process video streams:</p> <pre><code>from holoscan.operators import ProhawkOp\n\n# Add the Prohawk operator\nprohawk_op = ProhawkOp(fragment, name=\"prohawk_processor\")\n\n# Connect to your video source and output\n# ... configure your pipeline\n</code></pre>","tags":["Video"]},{"location":"operators/prohawk_video_processing/#interactive-controls","title":"Interactive Controls","text":"<p>When running the operator, you can use the following keyboard controls:</p> <ul> <li>0: Enable AFS (Adaptive Frame Stabilization) filter</li> <li>1: Enable Low Light Enhancement filter</li> <li>2: Enable Vascular Detail Enhancement filter</li> <li>3: Enable Vaper filter</li> <li>d: Disable restoration (pass-through mode)</li> <li>v: Enable side-by-side view</li> <li>m: Display menu items</li> <li>q: Quit the application</li> </ul>","tags":["Video"]},{"location":"operators/prohawk_video_processing/#filter-descriptions","title":"Filter Descriptions","text":"","tags":["Video"]},{"location":"operators/prohawk_video_processing/#afs-adaptive-frame-stabilization-filter-0","title":"AFS (Adaptive Frame Stabilization) - Filter 0","text":"<ul> <li>Purpose: Adaptive frame stabilization with noise reduction</li> <li>Best For: General video enhancement and stabilization</li> <li>Parameters:</li> <li>Radius: 60x60 pixels</li> <li>Threshold: 8</li> <li>Accumulation: 128</li> </ul>","tags":["Video"]},{"location":"operators/prohawk_video_processing/#low-light-enhancement-filter-1","title":"Low Light Enhancement - Filter 1","text":"<ul> <li>Purpose: Optimized for low-light video conditions</li> <li>Best For: Surveillance, night vision, low-light recording</li> <li>Parameters:</li> <li>Radius: 60x60 pixels</li> <li>Threshold: 8</li> <li>Accumulation: 128</li> </ul>","tags":["Video"]},{"location":"operators/prohawk_video_processing/#vascular-detail-enhancement-filter-2","title":"Vascular Detail Enhancement - Filter 2","text":"<ul> <li>Purpose: Specialized for medical imaging applications</li> <li>Best For: Medical video processing, vascular imaging</li> <li>Parameters:</li> <li>Radius: 24x24 pixels</li> <li>Threshold: 16</li> <li>Accumulation: 12</li> </ul>","tags":["Video"]},{"location":"operators/prohawk_video_processing/#vaper-filter-filter-3","title":"Vaper Filter - Filter 3","text":"<ul> <li>Purpose: Advanced video enhancement algorithm</li> <li>Best For: High-quality video restoration</li> <li>Parameters:</li> <li>Radius: 161x161 pixels</li> <li>Threshold: 8</li> <li>Accumulation: 0</li> </ul>","tags":["Video"]},{"location":"operators/qt_video/","title":"Qt Video Operator","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: October 9, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>qt_video</code> operator is used to display a video in a QtQuick application.</p> <p>For more information on how to use this operator in an application see Qt video replayer example.</p>","tags":["Visualization","Qt","Video","UI"]},{"location":"operators/qt_video/#holoscanopsqtvideoop","title":"<code>holoscan::ops::QtVideoOp</code>","text":"<p>Operator class.</p>","tags":["Visualization","Qt","Video","UI"]},{"location":"operators/qt_video/#parameters","title":"Parameters","text":"<ul> <li><code>QtHoloscanVideo</code>: Instance of QtHoloscanVideo to be used<ul> <li>type: `QtHoloscanVideo</li> </ul> </li> </ul>","tags":["Visualization","Qt","Video","UI"]},{"location":"operators/qt_video/#inputs","title":"Inputs","text":"<ul> <li><code>input</code>: Input frame data</li> <li>type: <code>nvidia::gxf::Tensor</code> or <code>nvidia::gxf::VideoBuffer</code></li> </ul>","tags":["Visualization","Qt","Video","UI"]},{"location":"operators/realsense_camera/","title":"Intel RealSense Camera Operator","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: August 5, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 2.1.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 1 - Highly Reliable</p>"},{"location":"operators/realsense_camera/#overview","title":"Overview","text":"<p>The RealSense Camera Operator captures synchronized color and depth frames from Intel RealSense cameras using the RealSense SDK. This operator provides real-time streaming capabilities with configurable resolution and frame rates, making it ideal for computer vision applications, robotics, and 3D reconstruction tasks.</p>"},{"location":"operators/realsense_camera/#features","title":"Features","text":"<ul> <li>Dual Stream Capture: Simultaneously captures color (RGBA) and depth (Z16) video streams</li> <li>Synchronized Frames: Ensures temporal alignment between color and depth data</li> <li>GPU Memory Management: Efficient CUDA memory allocation and transfer</li> <li>Camera Intrinsics: Provides camera calibration parameters for both streams</li> <li>Configurable Resolution: Supports various resolution and frame rate combinations</li> <li>Real-time Processing: Optimized for low-latency streaming applications</li> </ul>"},{"location":"operators/realsense_camera/#usage","title":"Usage","text":""},{"location":"operators/realsense_camera/#basic-example","title":"Basic Example","text":"<pre><code>#include \"holoscan/holoscan.hpp\"\n#include \"realsense_camera.hpp\"\n\nclass MyApp : public holoscan::Application {\n public:\n  void compose() override {\n    auto realsense = make_operator&lt;holoscan::ops::RealsenseCameraOp&gt;(\n        \"realsense\",\n        Arg(\"allocator\", make_resource&lt;holoscan::UnboundedAllocator&gt;(\"pool\")));\n\n    add_operator(realsense);\n  }\n};\n</code></pre>"},{"location":"operators/realsense_camera/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>auto realsense = make_operator&lt;holoscan::ops::RealsenseCameraOp&gt;(\n    \"realsense\",\n    Arg(\"allocator\", make_resource&lt;holoscan::UnboundedAllocator&gt;(\"pool\")));\n</code></pre> <p>Please refer to the Intel RealSense Camera Visualizer reference application for an example usage.</p>"},{"location":"operators/realsense_camera/#outputs","title":"Outputs","text":"<p>The operator provides the following outputs:</p>"},{"location":"operators/realsense_camera/#video-buffers","title":"Video Buffers","text":"<ul> <li><code>color_buffer</code>: RGBA8 color video stream (1280x720 @ 30fps by default)</li> <li><code>depth_buffer</code>: Z16 depth video stream (1280x720 @ 30fps by default)</li> </ul>"},{"location":"operators/realsense_camera/#camera-models","title":"Camera Models","text":"<ul> <li><code>color_camera_model</code>: Intrinsic parameters for the color camera</li> <li><code>depth_camera_model</code>: Intrinsic parameters for the depth camera</li> </ul>"},{"location":"operators/realsense_camera/#camera-model-structure","title":"Camera Model Structure","text":"<p>Each camera model contains:</p> <ul> <li>Dimensions: Width and height in pixels</li> <li>Focal Length: fx, fy in pixels</li> <li>Principal Point: ppx, ppy in pixels</li> <li>Distortion Type: Currently set to Perspective</li> </ul>"},{"location":"operators/realsense_camera/#configuration","title":"Configuration","text":""},{"location":"operators/realsense_camera/#default-settings","title":"Default Settings","text":"<ul> <li>Color Stream: 1280x720, RGBA8 format, 30fps</li> <li>Depth Stream: 1280x720, Z16 format, 30fps</li> <li>Alignment: Depth frames aligned to color stream</li> </ul>"},{"location":"operators/realsense_camera/#supported-formats","title":"Supported Formats","text":"<ul> <li>Color: RGBA8, RGB8, BGR8, YUYV</li> <li>Depth: Z16, Y16, Y8</li> </ul>"},{"location":"operators/slang_shader/","title":"Slang Shader Operator","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: October 9, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 3.3.0 Tested Holoscan SDK versions: 3.3.0, 3.4.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>SlangShaderOp</code> is a Holoscan operator that enables execution of Slang shaders within Holoscan applications. It provides a bridge between the Slang shading language and Holoscan's data processing pipeline, allowing developers to write GPU-accelerated compute shaders that can process data flowing through Holoscan applications.</p>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#overview","title":"Overview","text":"<p>The SlangShaderOp compiles Slang shader source code into CUDA kernels and executes them on GPU devices. It supports dynamic parameter binding, automatic input/output port generation, and seamless integration with Holoscan's data flow model.</p>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#features","title":"Features","text":"<ul> <li>Slang Shader Compilation: Compiles Slang shader source code to CUDA PTX</li> <li>Dynamic Port Generation: Automatically creates input/output ports based on shader attributes</li> <li>Parameter Binding: Supports scalar parameter types (bool, int, float, etc.)</li> <li>Structured Buffer Support: Handles input/output structured buffers</li> <li>Grid Size Configuration: Configurable compute grid dimensions</li> <li>CUDA Stream Integration: Integrates with Holoscan's CUDA stream management</li> <li>Python and C++ APIs: Available in both Python and C++ interfaces</li> </ul>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#requirements","title":"Requirements","text":"<ul> <li>Holoscan SDK 3.3.0 or later</li> <li>CUDA-compatible GPU</li> <li>Slang compiler (automatically fetched during build)</li> <li>Supported platforms: x86_64, aarch64</li> </ul>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#installation","title":"Installation","text":"<p>The SlangShaderOp is included as part of the HoloHub operators. It will be automatically built when you build the HoloHub project.</p>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#usage","title":"Usage","text":"","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#basic-usage","title":"Basic Usage","text":"<p>The operator can be configured with either a shader source string or a shader source file:</p> <pre><code>from holoscan.operators import SlangShaderOp\n\n# Using shader source string\nshader_source = \"\"\"\nimport holoscan\n\n[holoscan::input(\"input_data\")]\nStructuredBuffer&lt;float&gt; input_buffer;\n\n[holoscan::output(\"output_data\")]\nRWStructuredBuffer&lt;float&gt; output_buffer;\n\n[holoscan::parameter(\"scale_factor\")]\nfloat scale;\n\n[numthreads(256, 1, 1)]\n[holoscan::invocations::size_of(\"input_data\")]\nvoid main(uint3 tid : SV_DispatchThreadID) {\n    output_buffer[tid.x] = input_buffer[tid.x] * scale;\n}\n\"\"\"\n\nop = SlangShaderOp(\n    fragment=app,\n    shader_source=shader_source,\n    name=\"my_shader\"\n)\n</code></pre> <p>Note that the data sent to the input of <code>SlangShaderOp</code> must be a data buffer (currently <code>holoscan::Tensor</code> and <code>nvidia::gxf::VideoBuffer</code> types are supported). For Python, any array-like objects implementing the <code>__dlpack__</code>, <code>__array_interface__</code> or <code>__cuda_array_interface__</code> are also supported.</p>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#shader-attributes","title":"Shader Attributes","text":"<p>The SlangShaderOp uses special attributes to define how shader parameters interact with Holoscan:</p>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#inputoutput-attributes","title":"Input/Output Attributes","text":"<ul> <li><code>[holoscan::input(\"port_name\")]</code>: Marks a structured buffer as an input port</li> <li><code>[holoscan::output(\"port_name\")]</code>: Marks a structured buffer as an output port</li> <li><code>[holoscan::alloc::size_of(\"port_name\")]</code>: Specifies allocation size based on input port</li> <li><code>[holoscan::alloc(x, y, z)]</code>: Specifies allocation size</li> <li><code>[holoscan::zeros()]</code>: Initializes a buffer to zero</li> </ul>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#parameter-attributes","title":"Parameter Attributes","text":"<ul> <li><code>[holoscan::parameter(\"param_name\")]</code>: Marks a scalar as a configurable parameter</li> <li><code>[holoscan::size_of(\"port_name\")]</code>: Provides size information from input port</li> <li><code>[holoscan::strides_of(\"port_name\")]</code>: Provides stride information from input port</li> </ul>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#compute-invocations","title":"Compute Invocations","text":"<ul> <li><code>[holoscan::invocations::size_of(\"port_name\")]</code>: Sets invocations based on input tensor dimensions</li> <li><code>[holoscan::invocations(x, y, z)]</code>: Sets fixed invocations</li> </ul>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#core-operator-parameters","title":"Core Operator Parameters","text":"<p>The operator registers these built-in parameters:</p> <ul> <li><code>shader_source</code>: The Slang shader source code as a string</li> <li><code>shader_source_file</code>: Path to a Slang shader source file (alternative to shader_source)</li> <li><code>preprocessor_macros</code>: Map of preprocessor macro names to values for shader compilation</li> <li><code>allocator</code>: Allocator resource for output buffers (defaults to RMMAllocator)</li> <li><code>cuda_stream</code>: CUDA stream pool resource</li> </ul>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#dynamic-parameter-generation","title":"Dynamic Parameter Generation","text":"<p>Based on shader reflection analysis, the operator automatically creates:</p> <ol> <li>Input Ports: For each <code>[holoscan::input(\"port_name\")]</code> attribute, creates an input port of type <code>gxf::Entity</code></li> <li>Output Ports: For each <code>[holoscan::output(\"port_name\")]</code> attribute, creates an output port of type <code>gxf::Entity</code></li> <li>Shader Parameters: For each <code>[holoscan::parameter(\"param_name\")]</code> attribute, creates a configurable parameter with the appropriate type</li> </ol>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#port-names","title":"Port names","text":"<p>Port names in Holoscan attributes follow a specific format to identify and bind resources to shader variables. The name string can take several forms:</p>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#1-simple-resource-name","title":"1. Simple Resource Name","text":"<ul> <li>Format: <code>\"resource_name\"</code></li> <li>Example: <code>\"input_buffer\"</code>, <code>\"output_tensor\"</code></li> <li>Usage: Used when referencing a single resource directly</li> </ul>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#2-tensor-map-reference","title":"2. Tensor Map Reference","text":"<ul> <li>Format: <code>\"tensor_map_name:tensor_name\"</code></li> <li>Example: <code>\"model:weights\"</code>, <code>\"data:input_image\"</code></li> <li>Usage: Used when the resource is part of a named tensor map, where the part before the colon identifies the tensor map and the part after identifies the specific tensor within that map</li> </ul>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#3-resource-with-swizzle-for-allocation-or-invocations-size-attributes","title":"3. Resource with Swizzle (for allocation or invocations size attributes)","text":"<ul> <li>Format: <code>\"resource_name.swizzle_string\"</code> or <code>\"tensor_map_name:tensor_name.swizzle_string\"</code></li> <li>Example: <code>\"input_tensor.cx\"</code>, <code>\"output_buffer.xy\"</code>, <code>\"data:input_image.xy\"</code></li> <li>Usage: The swizzle string selects specific dimensions of the resource for size matching</li> <li>Allowed characters: <code>\"x\"</code>, <code>\"y\"</code>, <code>\"z\"</code>, <code>\"c\"</code>, <code>\"0\"</code> - <code>\"9\"</code></li> <li><code>\"x\"</code>, <code>\"y\"</code>, <code>\"z\"</code>: Select specific dimensions</li> <li><code>\"c\"</code>: Component count</li> <li><code>\"0\"</code> - <code>\"9\"</code>: Static values</li> </ul>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#examples","title":"Examples","text":"<pre><code>[holoscan::input(\"input_data\")]           // Binds to a resource named \"input_data\"\n[holoscan::output(\"model:output\")]        // Binds to the \"output\" tensor in the \"model\" tensor map\n[holoscan::alloc::size_of(\"input_tensor.cx\")]  // Allocates based on the x dimension and component count of \"input_tensor\"\n[holoscan::alloc::size_of(\"buffer:coords.xyz\")]       // Allocates based on x, y, z dimensions of \"buffer:coords\"\n[holoscan::invocations::size_of(\"image.cxy\")]  // Sets invocations based on x, y dimensions and component count\n</code></pre>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#supported-data-types","title":"Supported Data Types","text":"","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#scalar-parameters","title":"Scalar Parameters","text":"<ul> <li><code>bool</code>, <code>int8</code>, <code>uint8</code>, <code>int16</code>, <code>uint16</code></li> <li><code>int32</code>, <code>uint32</code>, <code>int64</code>, <code>uint64</code></li> <li><code>float32</code>, <code>float64</code></li> </ul>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#buffer-types","title":"Buffer Types","text":"<ul> <li><code>StructuredBuffer&lt;T&gt;</code>: Input buffers</li> <li><code>RWStructuredBuffer&lt;T&gt;</code>: Output buffers</li> </ul>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#example-image-processing-shader","title":"Example: Image Processing Shader","text":"<pre><code>import holoscan\n\n// Simple image processing shader\n[holoscan::input(\"input_image\")]\nStructuredBuffer&lt;float4&gt; input_image;\n\n[holoscan::output(\"output_image\")]\nRWStructuredBuffer&lt;float4&gt; output_image;\n\n[holoscan::parameter(\"brightness\")]\nfloat brightness;\n\n[holoscan::size_of(\"input_image\")]\nint3 image_size;\n\n[numthreads(16, 16, 1)]\n[holoscan::invocations::size_of(\"input_image\")]\nvoid main(uint3 tid : SV_DispatchThreadID) {\n    uint index = tid.y * image_size.x + tid.x;\n    float4 pixel = input_image[index];\n\n    // Apply brightness adjustment\n    output_image[index] = pixel * brightness;\n}\n</code></pre>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#c-usage","title":"C++ Usage","text":"<pre><code>#include &lt;slang_shader/slang_shader.hpp&gt;\n\n// Create the operator with shader source from a string\nstd::string shader_source_string = R\"\ninclude holoscan;\n...\n\";\nauto shader_op_str  = make_operator&lt;holoscan::ops::SlangShaderOp&gt;(\"Slang\",\n    Arg(\"shader_source\", shader_source_string));\n\n// Or create the operator with a Slang shader source file\nauto shader_op_file = make_operator&lt;holoscan::ops::SlangShaderOp&gt;(\"Slang\",\n    Arg(\"shader_source_file\", \"my_shader.slang\"));\n</code></pre>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#architecture","title":"Architecture","text":"<p>The SlangShaderOp consists of several key components:</p>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#core-classes","title":"Core Classes","text":"<ul> <li><code>SlangShaderOp</code>: Main operator class that orchestrates shader execution</li> <li><code>SlangShader</code>: Manages shader compilation and CUDA kernel retrieval</li> <li><code>Command</code>: Command pattern implementation for various operations</li> <li><code>CommandWorkspace</code>: Centralized workspace for command execution</li> </ul>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#execution-flow","title":"Execution Flow","text":"<ol> <li>Setup Phase:</li> <li>Compiles Slang shader source to PTX</li> <li>Analyzes shader reflection to generate ports and parameters</li> <li> <p>Creates command sequences for pre-launch, launch, and post-launch operations</p> </li> <li> <p>Compute Phase:</p> </li> <li>Executes pre-launch commands (input handling, parameter setup)</li> <li>Launches CUDA kernels with configured grid/block dimensions</li> <li>Executes post-launch commands (output handling)</li> </ol>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#command-types","title":"Command Types","text":"<ul> <li><code>CommandInput</code>: Handles input port data reception</li> <li><code>CommandOutput</code>: Handles output port data emission</li> <li><code>CommandParameter</code>: Manages scalar parameter binding</li> <li><code>CommandSizeOf</code>: Provides size information to shaders</li> <li><code>CommandStrideOf</code>: Provides stride information to shaders</li> <li><code>CommandAlloc</code>: Handles resource allocation</li> <li><code>CommandLaunch</code>: Executes CUDA kernels</li> <li><code>CommandZeros</code>: Initializes a buffer with zeros</li> </ul>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#error-handling","title":"Error Handling","text":"<p>The operator provides comprehensive error handling:</p> <ul> <li>Compilation Errors: Detailed Slang compilation diagnostics</li> <li>Runtime Errors: CUDA execution error reporting</li> <li>Parameter Validation: Type checking and attribute validation</li> <li>Resource Management: Automatic cleanup of CUDA resources</li> </ul>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Kernel Compilation: Shaders are compiled once during setup</li> <li>Memory Management: Uses Holoscan's allocator system for buffer management</li> <li>Stream Management: Integrates with Holoscan's CUDA stream pool</li> <li>Parameter Updates: Efficient parameter updates without recompilation</li> </ul>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#limitations","title":"Limitations","text":"<ul> <li>Only compute shaders are supported (no vertex/fragment shaders)</li> <li>Structured buffers are the only supported buffer type</li> <li>Grid size must be specified via attributes</li> <li>Shader compilation happens at operator setup time</li> </ul>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#troubleshooting","title":"Troubleshooting","text":"","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#common-issues","title":"Common Issues","text":"<ol> <li>Compilation Errors: Check shader syntax and ensure all attributes are properly defined</li> <li>Parameter Type Mismatches: Verify parameter types match between shader and operator</li> <li>Grid Size Issues: Ensure grid size attributes are correctly specified</li> <li>Memory Errors: Verify buffer sizes and allocation parameters</li> </ol>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#debugging","title":"Debugging","text":"<p>Enable debug logging to see detailed execution information:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#testing","title":"Testing","text":"<p>The SlangShaderOp includes comprehensive testing to ensure reliability and correctness across different use cases and platforms.</p>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#running-tests","title":"Running Tests","text":"<pre><code>./holohub test slang_simple\n</code></pre>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#contributing","title":"Contributing","text":"<p>The SlangShaderOp is part of the HoloHub project. Contributions are welcome through the standard HoloHub contribution process.</p>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/slang_shader/#license","title":"License","text":"<p>This operator is licensed under the Apache License 2.0, same as the HoloHub project.</p>","tags":["Visualization","Slang","shading","rendering","shader","shadertoy","compute","raytracing","cuda"]},{"location":"operators/streaming_client/","title":"StreamingClient Operator","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: August 20, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 3.2.0 Tested Holoscan SDK versions: 3.2.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The StreamingClientOp class implements a Holoscan operator that provides bidirectional video streaming capabilities with the following key components:</p> <ul> <li>Configuration and Initialization:</li> <li>Configurable parameters for frame dimensions (width, height), frame rate (fps), server connection (IP, port)</li> <li>Input/output ports for frame data using GXF entities</li> <li>Support for both sending and receiving frames through separate flags</li> </ul> <p>Frame Processing Pipeline: - Input handling: Receives frames as GXF entities containing H.264 encoded video tensors - Frame conversion: Converts input tensors to VideoFrame objects with BGRA format - Memory management: Implements safe memory handling with bounds checking and zero-padding - Output generation: Creates GXF entities with properly configured tensors for downstream processing</p> <p>Streaming Protocol Implementation: - Bidirectional streaming support through StreamingClient class - Frame callback system for receiving frames - Frame source system for sending frames - Connection management with server including timeout handling</p>","tags":["Streaming","Video","Client","Real-time","Network"]},{"location":"operators/streaming_client/#dependencies","title":"Dependencies","text":"<p>In order to build the client operator, you must first download the client binaries from NGC and add to the <code>lib</code> directory for the <code>streaming_client</code> operator folder</p> <p>Download the Holoscan Client Cloud Streaming library from NGC: https://catalog.ngc.nvidia.com/orgs/nvidia/resources/holoscan_client_cloud_streaming</p> <pre><code># Download using NGC CLI\ncd &lt;your_holohub_path&gt;/operators/streaming_client\nngc registry resource download-version nvidia/holoscan_client_cloud_streaming:0.1\nunzip -o holoscan_client_cloud_streaming_v0.1/holoscan_client_cloud_streaming.zip\n\n# Copy the appropriate architecture libraries to lib/ directory\n# For x86_64 systems:\ncp lib/x86_64/* lib/\n# For aarch64 systems:\n# cp lib/aarch64/* lib/\n\n# Clean up architecture-specific directories and NGC download directory\nrm -rf lib/x86_64 lib/aarch64\nrm -rf holoscan_client_cloud_streaming_v0.1\n</code></pre> <p>After successful extraction and setup, your <code>operators/streaming_client</code> directory structure should look like this:</p> <pre><code>\u251c\u2500\u2500 CMakeLists.txt\n\u251c\u2500\u2500 FindHoloscanStreaming.cmake\n\u251c\u2500\u2500 include\n\u2502   \u251c\u2500\u2500 StreamingClient.h\n\u2502   \u2514\u2500\u2500 VideoFrame.h\n\u251c\u2500\u2500 lib\n\u2502   \u251c\u2500\u2500 libcrypto.so.3\n\u2502   \u251c\u2500\u2500 libcudart.so.12\n\u2502   \u251c\u2500\u2500 libcudart.so.12.0.107\n\u2502   \u251c\u2500\u2500 libNvStreamBase.so\n\u2502   \u251c\u2500\u2500 libNvStreamingSession.so\n\u2502   \u251c\u2500\u2500 libNvStreamServer.so\n\u2502   \u251c\u2500\u2500 libPoco.so\n\u2502   \u251c\u2500\u2500 libssl.so.3\n\u2502   \u251c\u2500\u2500 libStreamClientShared.so\n\u2502   \u2514\u2500\u2500 libStreamingClient.so\n\u251c\u2500\u2500 metadata.json\n\u251c\u2500\u2500 NOTICE.txt\n\u251c\u2500\u2500 python\n\u2502   \u251c\u2500\u2500 CMakeLists.txt\n\u2502   \u2514\u2500\u2500 streaming_client.cpp\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 streaming_client.cpp\n\u251c\u2500\u2500 streaming_client.hpp\n</code></pre> <p>All dependencies need to be properly installed in the operator directory structure.</p>","tags":["Streaming","Video","Client","Real-time","Network"]},{"location":"operators/streaming_client/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter build errors: - Make sure all required files are copied to the correct locations - Check that the libraries have appropriate permissions (644) - Ensure the directories exist inside the container environment </p>","tags":["Streaming","Video","Client","Real-time","Network"]},{"location":"operators/streaming_client/#supported-platforms","title":"Supported Platforms","text":"<ul> <li>Linux x86_64</li> <li>Linux aarch64</li> </ul>","tags":["Streaming","Video","Client","Real-time","Network"]},{"location":"operators/streaming_server/","title":"Streaming Server Operator","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: August 20, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 3.2.0 Tested Holoscan SDK versions: 3.2.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>streaming_server</code> operator provides a streaming server implementation that can receive and send video frames to connected clients. It wraps the StreamingServer interface to provide seamless integration with Holoscan applications.</p>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"operators/streaming_server/#holoscanopsstreamingserverop","title":"<code>holoscan::ops::StreamingServerOp</code>","text":"<p>This operator class implements a streaming server that can: - Accept incoming client connections - Receive video frames from clients - Send video frames to clients - Handle multiple client connections (optional) - Manage streaming events through callbacks</p>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"operators/streaming_server/#parameters","title":"Parameters","text":"<ul> <li><code>width</code>: Width of the video frames in pixels</li> <li>type: <code>uint32_t</code></li> <li> <p>default: 1920</p> </li> <li> <p><code>height</code>: Height of the video frames in pixels</p> </li> <li>type: <code>uint32_t</code></li> <li> <p>default: 1080</p> </li> <li> <p><code>fps</code>: Frame rate of the video</p> </li> <li>type: <code>uint32_t</code></li> <li> <p>default: 30</p> </li> <li> <p><code>port</code>: Port used for streaming server</p> </li> <li>type: <code>uint16_t</code></li> <li> <p>default: 8080</p> </li> <li> <p><code>multi_instance</code>: Allow multiple server instances</p> </li> <li>type: <code>bool</code></li> <li> <p>default: false</p> </li> <li> <p><code>server_name</code>: Name identifier for the server</p> </li> <li>type: <code>std::string</code></li> <li> <p>default: \"StreamingServer\"</p> </li> <li> <p><code>receive_frames</code>: Whether to receive frames from clients</p> </li> <li>type: <code>bool</code></li> <li> <p>default: true</p> </li> <li> <p><code>send_frames</code>: Whether to send frames to clients</p> </li> <li>type: <code>bool</code></li> <li> <p>default: false</p> </li> <li> <p><code>allocator</code>: Memory allocator for frame data</p> </li> <li>type: <code>std::shared_ptr&lt;Allocator&gt;</code></li> </ul>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"operators/streaming_server/#example-usage","title":"Example Usage","text":"<pre><code>// Create the operator with configuration\nauto streaming_server = make_operator&lt;ops::StreamingServerOp&gt;(\n    \"streaming_server\",\n    Arg(\"width\") = 1920,\n    Arg(\"height\") = 1080,\n    Arg(\"fps\") = 30,\n    Arg(\"port\") = 8080,\n    Arg(\"multi_instance\") = false,\n    Arg(\"server_name\") = \"MyStreamingServer\",\n    Arg(\"receive_frames\") = true,\n    Arg(\"send_frames\") = true,\n    Arg(\"allocator\") = make_resource&lt;UnboundedAllocator&gt;(\"pool\")\n);\n\n//add the streaming_server to the app\nadd_operator(streaming_server);\n</code></pre>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"operators/streaming_server/#building-the-operator","title":"Building the operator","text":"<p>In order to build the server operator, you must first download the server binaries form NGC and add to the <code>lib</code> directory in the <code>streaming_server</code> operator folder</p> <p>Download the Holoscan Server Cloud Streaming library from NGC: https://catalog.ngc.nvidia.com/orgs/nvidia/resources/holoscan_server_cloud_streaming</p> <pre><code>cd &lt;your_holohub_path&gt;/operators/streaming_server \nngc registry resource download-version \"nvidia/holoscan_server_cloud_streaming:0.1\"\nunzip -o holoscan_server_cloud_streaming_v0.1/holoscan_server_cloud_streaming.zip\n\n# Copy the appropriate architecture libraries to lib/ directory\n# For x86_64 systems:\ncp lib/x86_64/*.so* lib/\ncp -r lib/x86_64/plugins lib/\n# For aarch64 systems:\n# cp lib/aarch64/* lib/\n\n# Clean up architecture-specific directories and NGC download directory\nrm -rf lib/x86_64 lib/aarch64\nrm -rf holoscan_server_cloud_streaming_v0.1\n</code></pre>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"operators/streaming_server/#deployment-on-nvcf","title":"Deployment on NVCF","text":"<p>The Holoscan cloud steaming stack provides plugins with endpoints required to deploy the server docker container as a streaming function. You can push the container and create/update/deploy the streaming function from the web portal.</p>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"operators/streaming_server/#push-container","title":"Push Container","text":"<p>Note: You first must docker login to the NGC Container Registry before you can push containers to it: https://docs.nvidia.com/ngc/gpu-cloud/ngc-private-registry-user-guide/index.html#accessing-ngc-registry Tag the container and push it to the container registry:</p> <pre><code>docker tag simple-streamer:latest {registry}/{org-id}/{container-name}:{version}\ndocker push {registry}/{org-id}/{container-name}:{version}\n</code></pre> <p>For example, if your organization name/id is 0494839893562652 and you want to push a container to the prod container registry using the name my-simple-streamer at version 0.1.0 then run:</p> <pre><code>docker tag simple-streamer:latest nvcr.io/0494839893562652/my-simple-streamer:0.1.0\ndocker push nvcr.io/0494839893562652/my-simple-streamer:0.1.0\n</code></pre>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"operators/streaming_server/#set-variables","title":"Set Variables","text":"<p>All the helper scripts below depend on the following environment variables being set:</p> <pre><code># Required variables\nexport NGC_PERSONAL_API_KEY=&lt;get from https://nvcf.ngc.nvidia.com/functions -&gt; Generate Personal API Key&gt;\nexport STREAMING_CONTAINER_IMAGE=&lt;registry&gt;/&lt;org-id&gt;/&lt;container-name&gt;:&lt;version&gt;\nexport STREAMING_FUNCTION_NAME=&lt;my-simple-streamer-function-name&gt;\n\n# Optional variables (shown with default values)\nexport NGC_DOMAIN=api.ngc.nvidia.com\nexport NVCF_SERVER=grpc.nvcf.nvidia.com\nexport STREAMING_SERVER_PORT=49100\nexport HTTP_SERVER_PORT=8011\n</code></pre>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"operators/streaming_server/#create-the-cloud-streaming-function","title":"Create the Cloud Streaming Function","text":"<p>Create the streaming function by running the provided script after setting all the required variables: <pre><code>./nvcf/create_streaming_function.sh\n</code></pre></p> <p>Once the function is created, export the <code>FUNCTION_ID</code> as a variable:</p> <pre><code>export STREAMING_FUNCTION_ID={my-simple-streamer-function-id}\n</code></pre>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"operators/streaming_server/#update-function","title":"Update Function","text":"<p>Update an existing streaming function by running the provided script after setting all the required variables: <pre><code>./nvcf/update_streaming_function.sh\n</code></pre></p>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"operators/streaming_server/#deploy-function","title":"Deploy Function","text":"<p>Deploy the streaming function from the web portal: https://nvcf.ngc.nvidia.com/functions</p>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"operators/streaming_server/#pre-deployment-port-check","title":"Pre-deployment Port Check","text":"<p>Before starting HAProxy or deploying cloud functions, verify that the required ports are available:</p> <pre><code># Navigate to the holohub root directory\ncd /path/to/holohub\n\n# Check streaming server port (from STREAMING_SERVER_PORT variable, default: 49100)\n./check_port.sh ${STREAMING_SERVER_PORT:-49100}\n\n# Check HTTP server port (from HTTP_SERVER_PORT variable, default: 8011)  \n./check_port.sh ${HTTP_SERVER_PORT:-8011}\n\n# Check NVCF server port (typically 443 for grpc.nvcf.nvidia.com)\n./check_port.sh 443\n\n# Check any custom ports your application uses\n./check_port.sh [YOUR_CUSTOM_PORT]\n</code></pre> <p>Key ports to verify: - Streaming Server Port (<code>49100</code> by default): Main streaming communication port - HTTP Server Port (<code>8011</code> by default): HTTP endpoint for function management - HAProxy Ports: Any custom HAProxy configuration ports - NVCF gRPC Port (<code>443</code>): Communication with NVIDIA Cloud Functions</p> <p>The port checking script will help identify: - \ud83d\udeab Port conflicts: If ports are already in use by other processes - \u2705 Available ports: Confirmation that ports can be bound successfully - \ud83d\udd27 Process identification: What applications are using specific ports - \ud83d\udccb Port recommendations: Guidance on port selection</p> <p>If ports are in use, either: 1. Stop conflicting processes: <code>kill [PID]</code> (use caution) 2. Use different ports: Update environment variables 3. Configure around conflicts: Modify YAML configurations</p>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"operators/streaming_server/#test-function","title":"Test Function","text":"<p>Start the test intermediate haproxy by running the provided script after setting all the required variables:</p> <pre><code>./nvcf/start_test_intermediate_haproxy.sh\n</code></pre> <p>Please note that the test haproxy server should be running on a separate machine, either on the client machine or a separate one.</p> <p>Note: If the test haproxy is still running, and you wish to test the executable or docker file again you must first stop it:</p> <pre><code>./nvcf/stop_test_intermediate_haproxy.sh\n</code></pre>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"operators/streaming_server/#supported-platforms","title":"Supported Platforms","text":"<ul> <li>Linux x86_64</li> <li>NVCF Cloud instances </li> </ul> <p>For more information on NVCF Cloud functions, please refer to NVIDIA Cloud Functions documentation.</p>","tags":["Streaming","Video","Server","Real-time","Network"]},{"location":"operators/tensor_to_file/","title":"Tensor to File Operator","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: October 9, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 3.3.0 Tested Holoscan SDK versions: 3.3.0, 3.4.0 Contribution metric: Level 2 - Trusted</p> <p>A Holoscan operator that writes tensor data to a file. This operator is designed to stream input tensor data to a file, with the file opened at initialization and tensor binary data appended in the same order in which messages are received.</p>","tags":["Converter","Tensor","File","Writer"]},{"location":"operators/tensor_to_file/#overview","title":"Overview","text":"<p>The Tensor to File operator is a data persistence component that takes tensor data as input and writes it directly to a file. It's particularly useful for:</p> <ul> <li>Saving encoded video frames to elementary stream files (H.264/H.265)</li> <li>Persisting tensor data for later analysis or processing</li> <li>Creating data dumps for debugging purposes</li> <li>Building data pipelines that require file output</li> </ul>","tags":["Converter","Tensor","File","Writer"]},{"location":"operators/tensor_to_file/#features","title":"Features","text":"<ul> <li>Binary Data Writing: Writes tensor data as binary to maintain data integrity</li> <li>Performance Optimized: Configurable buffer size for optimal I/O performance</li> <li>File Validation: Validates output file paths and creates directories as needed</li> <li>Progress Tracking: Optional verbose mode with performance statistics</li> <li>Cross-Platform: Supports both x86_64 and aarch64 architectures</li> <li>Multi-Language: Available in both C++ and Python interfaces</li> </ul>","tags":["Converter","Tensor","File","Writer"]},{"location":"operators/tensor_to_file/#usage","title":"Usage","text":"","tags":["Converter","Tensor","File","Writer"]},{"location":"operators/tensor_to_file/#c-interface","title":"C++ Interface","text":"<pre><code>#include \"holoscan/operators/tensor_to_file/tensor_to_file.hpp\"\n\n// Create operator instance\nauto tensor_to_file = std::make_unique&lt;holoscan::ops::TensorToFileOp&gt;();\n\n// Configure parameters\ntensor_to_file-&gt;add_arg&lt;std::string&gt;(\"tensor_name\", \"input_tensor\");\ntensor_to_file-&gt;add_arg&lt;std::string&gt;(\"output_file\", \"/path/to/output.h264\");\ntensor_to_file-&gt;add_arg&lt;bool&gt;(\"verbose\", true);\ntensor_to_file-&gt;add_arg&lt;size_t&gt;(\"buffer_size\", 1024 * 1024); // 1MB buffer\n</code></pre>","tags":["Converter","Tensor","File","Writer"]},{"location":"operators/tensor_to_file/#python-interface","title":"Python Interface","text":"<pre><code>import holoscan as hs\nfrom holoscan.operators import TensorToFileOp\n\n# Create operator\ntensor_to_file = TensorToFileOp(\n    tensor_name=\"input_tensor\",\n    output_file=\"/path/to/output.h264\",\n    verbose=True,\n    buffer_size=1024 * 1024,  # 1MB buffer\n    name=\"tensor_writer\"\n)\n</code></pre>","tags":["Converter","Tensor","File","Writer"]},{"location":"operators/tensor_to_file/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>tensor_name</code> string \"\" Name of the tensor to write to the file <code>output_file</code> string \"\" Output file path for the data <code>allocator</code> Allocator - Allocator for output buffers <code>verbose</code> bool false Print detailed writer information including frame count and bytes written <code>buffer_size</code> size_t 1MB Buffer size for file I/O operations in bytes","tags":["Converter","Tensor","File","Writer"]},{"location":"operators/tensor_to_file/#inputoutput","title":"Input/Output","text":"","tags":["Converter","Tensor","File","Writer"]},{"location":"operators/tensor_to_file/#input","title":"Input","text":"<ul> <li>Entity: Contains tensor data to be written to file</li> <li>Tensor: Binary data (typically uint8) representing the content to be saved</li> </ul>","tags":["Converter","Tensor","File","Writer"]},{"location":"operators/tensor_to_file/#output","title":"Output","text":"<ul> <li>File: Binary file containing the tensor data in the order received</li> </ul>","tags":["Converter","Tensor","File","Writer"]},{"location":"operators/tensor_to_file/#file-format-support","title":"File Format Support","text":"<p>Any binary streaming file format is allowed. The operator does not check for validity of the specified output extension.</p> <p>Historically the operator has been used for writing video data with various file extensions:</p> <ul> <li><code>.h264</code>, <code>.264</code> - H.264 elementary stream</li> <li><code>.h265</code>, <code>.265</code>, <code>.hevc</code> - H.265/HEVC elementary stream  </li> <li><code>.mp4</code> - MP4 container format</li> </ul>","tags":["Converter","Tensor","File","Writer"]},{"location":"operators/tensor_to_file/#configuration-examples","title":"Configuration Examples","text":"","tags":["Converter","Tensor","File","Writer"]},{"location":"operators/tensor_to_file/#basic-usage","title":"Basic Usage","text":"<pre><code># Simple tensor to file writing\ntensor_to_file = TensorToFileOp(\n    tensor_name=\"encoded_frame\",\n    output_file=\"output.h264\"\n)\n</code></pre>","tags":["Converter","Tensor","File","Writer"]},{"location":"operators/tensor_to_file/#verbose-mode-with-custom-buffer","title":"Verbose Mode with Custom Buffer","text":"<pre><code># With detailed logging and custom buffer size\ntensor_to_file = TensorToFileOp(\n    tensor_name=\"encoded_frame\",\n    output_file=\"output.h265\",\n    verbose=True,\n    buffer_size=2 * 1024 * 1024  # 2MB buffer\n)\n</code></pre>","tags":["Converter","Tensor","File","Writer"]},{"location":"operators/tensor_to_file/#integration-in-pipeline","title":"Integration in Pipeline","text":"<pre><code># As part of a video processing pipeline\npipeline = holoscan.Pipeline()\n\n# Add operators to pipeline\nencoder = VideoEncoderOp(...)\ntensor_to_file = TensorToFileOp(\n    tensor_name=\"encoded_frame\",\n    output_file=\"processed_video.h264\",\n    verbose=True\n)\n\n# Connect operators\npipeline.add_operator(encoder)\npipeline.add_operator(tensor_to_file)\npipeline.add_flow(encoder, tensor_to_file)\n</code></pre>","tags":["Converter","Tensor","File","Writer"]},{"location":"operators/tensor_to_file/#error-handling","title":"Error Handling","text":"<p>The operator includes comprehensive error handling for:</p> <ul> <li>Invalid file paths</li> <li>File system permission issues</li> <li>Insufficient disk space</li> <li>Corrupted tensor data</li> <li>I/O operation failures</li> </ul>","tags":["Converter","Tensor","File","Writer"]},{"location":"operators/tensor_to_video_buffer/","title":"GXF Tensor to VideoBuffer Converter","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>tensor_to_video_buffer</code> converts GXF Tensor to VideoBuffer.</p>","tags":["Converter","Tensor","Video"]},{"location":"operators/tensor_to_video_buffer/#holoscanopstensortovideobufferop","title":"<code>holoscan::ops::TensorToVideoBufferOp</code>","text":"<p>Operator class to convert GXF Tensor to VideoBuffer. This operator is required for data transfer  between Holoscan operators that output GXF Tensor and the other Holoscan Wrapper Operators that understand only VideoBuffer. It receives GXF Tensor as input and outputs GXF VideoBuffer created from it.</p>","tags":["Converter","Tensor","Video"]},{"location":"operators/tensor_to_video_buffer/#parameters","title":"Parameters","text":"<ul> <li><code>data_in</code>: Data in GXF Tensor format</li> <li>type: <code>holoscan::IOSpec*</code></li> <li><code>data_out</code>: Data in GXF VideoBuffer format</li> <li>type: <code>holoscan::IOSpec*</code></li> <li><code>in_tensor_name</code>: Name of the input tensor</li> <li>type: <code>std::string</code></li> <li><code>video_format</code>: The video format, supported values: \"yuv420\", \"rgb\"</li> <li>type: <code>std::string</code></li> </ul>","tags":["Converter","Tensor","Video"]},{"location":"operators/tool_tracking_postprocessor/","title":"Tool Tracking Postprocessor Operator","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>tool_tracking_postprocessor</code> extension provides a codelet that converts inference output of <code>lstm_tensor_rt_inference</code> used in the endoscopy tool tracking pipeline to be consumed by the <code>holoviz</code> codelet.</p>","tags":["Visualization"]},{"location":"operators/tool_tracking_postprocessor/#nvidiaholoscantool_tracking_postprocessor","title":"<code>nvidia::holoscan::tool_tracking_postprocessor</code>","text":"<p>Tool tracking postprocessor codelet</p>","tags":["Visualization"]},{"location":"operators/tool_tracking_postprocessor/#parameters","title":"Parameters","text":"<ul> <li><code>in</code>: Input channel, type <code>gxf::Tensor</code></li> <li>type: <code>gxf::Handle&lt;gxf::Receiver&gt;</code></li> <li><code>out</code>: Output channel, type <code>gxf::Tensor</code></li> <li>type: <code>gxf::Handle&lt;gxf::Transmitter&gt;</code></li> <li><code>min_prob</code>: Minimum probability, (default: 0.5)</li> <li>type: <code>float</code></li> <li><code>overlay_img_colors</code>: Color of the image overlays, a list of RGB values with components between 0 and 1, (default: 12 qualitative classes color scheme from colorbrewer2)</li> <li>type: <code>std::vector&lt;std::vector&lt;float&gt;&gt;</code></li> <li><code>device_allocator</code>: Output Allocator</li> <li>type: <code>gxf::Handle&lt;gxf::Allocator&gt;</code></li> <li><code>cuda_stream_pool</code>: Instance of gxf::CudaStreamPool</li> <li>type: <code>gxf::Handle&lt;gxf::CudaStreamPool&gt;</code></li> </ul>","tags":["Visualization"]},{"location":"operators/unzip/","title":"Unzip Operator","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.1 Tested Holoscan SDK versions: 2.0.1 Contribution metric: Level 2 - Trusted</p> <p>The <code>unzip</code> operator decompresses a zip compressed file into its original contents.</p>"},{"location":"operators/unzip/#parameters","title":"Parameters","text":"<ul> <li><code>filter</code>: File filter for the decompressed files to be copied to the <code>output_path</code></li> <li>type: <code>str</code></li> <li><code>output_path</code>: The directory where the unzipped files will be stored.</li> <li>type: <code>str</code></li> </ul>"},{"location":"operators/velodyne_lidar/cpp/","title":"Velodyne Lidar Operator","text":"<p> Authors: Holoscan Team (NVIDIA), nvMap Team (NVIDIA), nvMap Embedded Team (NVIDIA), Tom Birdsong (NVIDIA), Julien Jomier (NVIDIA), Jiahao Yin (NVIDIA), Marlene Wan (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.0.0 Contribution metric: Level 4 - Experimental</p>","tags":["Robotics","Lidar","Point Cloud"]},{"location":"operators/velodyne_lidar/cpp/#overview","title":"Overview","text":"<p>A Holoscan operator to convert packets from the Velodyne VLP-16 Lidar sensor to a point cloud tensor format.</p>","tags":["Robotics","Lidar","Point Cloud"]},{"location":"operators/velodyne_lidar/cpp/#description","title":"Description","text":"<p>This operator receives packets from a Velodyne VLP-16 lidar and processes them into a point cloud of fixed size in Cartesian space.</p> <p>The operator performs the following steps: 1. Interpret a fixed-size UDP packet as a Velodyne VLP-16 lidar packet,    which contains 12 data blocks (azimuths) and 32 spherical data points per block. 2. Transform the spherical data points into Cartesian coordinates (x, y, z)    and add them to the output point cloud tensor, overwriting a previous cloud segment. 3. Output the point cloud tensor and update the tensor insertion pointer to prepare    for the next incoming packet.</p> <p>We recommend relying on HoloHub networking operators to receive Velodyne VLP-16 lidar packets over UDP/IP and forward them to this operator.</p>","tags":["Robotics","Lidar","Point Cloud"]},{"location":"operators/velodyne_lidar/cpp/#requirements","title":"Requirements","text":"<p>Hardware requirements: - Holoscan supported platform (x64 or NVIDIA IGX devkit); - Velodyne VLP-16 Lidar sensor</p>","tags":["Robotics","Lidar","Point Cloud"]},{"location":"operators/velodyne_lidar/cpp/#example-usage","title":"Example Usage","text":"<p>See the HoloHub Lidar Sample Application to get started.</p>","tags":["Robotics","Lidar","Point Cloud"]},{"location":"operators/velodyne_lidar/cpp/#acknowledgements","title":"Acknowledgements","text":"<p>This operator was developed in part with support from the NVIDIA nvMap team and adapts portions of the NVIDIA DeepMap SDK.</p>","tags":["Robotics","Lidar","Point Cloud"]},{"location":"operators/video_encoder/video_encoder_request/","title":"Video Encoder Request","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>video_encoder_request</code> handles the input for encoding YUV frames to H264 bit stream.</p>","tags":["Video","Encoder"]},{"location":"operators/video_encoder/video_encoder_request/#holoscanopsvideoencoderop","title":"<code>holoscan::ops::VideoEncoderOp</code>","text":"<p>Operator class to handle the input for encoding YUV frames to H264 bit stream.</p> <p>This implementation is based on <code>nvidia::gxf::VideoEncoderRequest</code>.</p>","tags":["Video","Encoder"]},{"location":"operators/video_encoder/video_encoder_request/#parameters","title":"Parameters","text":"<ul> <li><code>input_frame</code>: Receiver to get the input frame.</li> <li>type: <code>holoscan::IOSpec*</code></li> <li><code>videoencoder_context</code>: Encoder context Handle.</li> <li>type: <code>std::shared_ptr&lt;holoscan::ops::VideoEncoderContext&gt;</code></li> <li><code>inbuf_storage_type</code>: Input Buffer storage type, 0: kHost, 1: kDevice. Default: 1</li> <li>type: <code>uint32_t</code></li> <li><code>codec</code>: Video codec to use,  0: H264, only H264 supported. Default: 0.</li> <li>type: <code>int32_t</code></li> <li><code>input_height</code>: Input frame height.</li> <li>type: <code>uint32_t</code></li> <li><code>input_width</code>: Input image width.</li> <li>type: <code>uint32_t</code></li> <li><code>input_format</code>: Input color format, nv12,nv24,yuv420planar. Default: nv12.</li> <li>type: <code>nvidia::gxf::EncoderInputFormat</code></li> <li><code>profile</code>: Encode profile, 0: Baseline Profile, 1: Main, 2: High. Default: 2.</li> <li>type: <code>int32_t</code></li> <li><code>bitrate</code>: Bitrate of the encoded stream, in bits per second. Default: 20000000.</li> <li>type: <code>int32_t</code></li> <li><code>framerate</code>: Frame Rate, frames per second. Default: 30.</li> <li>type: <code>int32_t</code></li> <li><code>qp</code>: Encoder constant QP value. Default: 20.</li> <li>type: <code>uint32_t</code></li> <li><code>level</code>: Video H264 level. Maximum data rate and resolution, select from 0 to 14. Default: 14.</li> <li>type: <code>int32_t</code></li> <li><code>iframe_interval</code>: I Frame Interval, interval between two I frames. Default: 30.</li> <li>type: <code>int32_t</code></li> <li><code>rate_control_mode</code>: Rate control mode, 0: CQP[RC off], 1: CBR, 2: VBR. Default: 1.</li> <li>type: <code>int32_t</code></li> <li><code>config</code>: Preset of parameters, select from pframe_cqp, iframe_cqp, custom. Default: custom.</li> <li>type: <code>nvidia::gxf::EncoderConfig</code></li> </ul>","tags":["Video","Encoder"]},{"location":"operators/video_streaming/","title":"Video Streaming Operators","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: November 7, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 3.5.0 Tested Holoscan SDK versions: 3.5.0 Contribution metric: Level 2 - Trusted</p> <p>A unified package containing both streaming client and server operators for real-time video communication in Holoscan streaming applications.</p>","tags":["Video","Streaming","Client","Server","Networking","Cloud","Bidirectional"]},{"location":"operators/video_streaming/#overview","title":"Overview","text":"<p>This operator package combines <code>video_streaming_client</code> and <code>video_streaming_server</code> into a single, cohesive video streaming solution.</p>","tags":["Video","Streaming","Client","Server","Networking","Cloud","Bidirectional"]},{"location":"operators/video_streaming/#structure","title":"Structure","text":"<pre><code>video_streaming/\n\u251c\u2500\u2500 video_streaming_client/               # Complete streaming client operator\n\u2502   \u251c\u2500\u2500 video_streaming_client.cpp        # Main client implementation\n\u2502   \u251c\u2500\u2500 frame_saver.cpp                   # Frame saving utility\n\u2502   \u2514\u2500\u2500 holoscan_client_cloud_streaming/  # Client streaming binary once NGC download is complete\n\u251c\u2500\u2500 video_streaming_server/               # Complete streaming server operator\n\u2502   \u251c\u2500\u2500 video_streaming_server_*.cpp      # Server implementations\n\u2502   \u251c\u2500\u2500 frame_debug_utils.cpp             # Debug utilities\n\u2502   \u2514\u2500\u2500 holoscan_server_cloud_streaming/  # Server streaming binary once NGC download is complete \n\u251c\u2500\u2500 CMakeLists.txt                        # Unified build configuration\n\u251c\u2500\u2500 metadata.json                         # Combined metadata\n\u2514\u2500\u2500 README.md                             # This file\n</code></pre>","tags":["Video","Streaming","Client","Server","Networking","Cloud","Bidirectional"]},{"location":"operators/video_streaming/#components","title":"Components","text":"","tags":["Video","Streaming","Client","Server","Networking","Cloud","Bidirectional"]},{"location":"operators/video_streaming/#video-streaming-client-video_streaming_client","title":"Video Streaming Client (<code>video_streaming_client/</code>)","text":"<p>The client component provides bidirectional video streaming capabilities:</p> <ul> <li>VideoStreamingClientOp: Main operator for video streaming client functionality</li> <li>FrameSaverOp: Utility operator for saving frames to disk</li> <li>Features:</li> <li>Send and receive video frames</li> <li>V4L2 camera support</li> <li>Frame validation and debugging</li> </ul> <p>Documentation: See <code>video_streaming_client/README.md</code> for detailed information.</p>","tags":["Video","Streaming","Client","Server","Networking","Cloud","Bidirectional"]},{"location":"operators/video_streaming/#streaming-server-video_streaming_server","title":"Streaming Server (<code>video_streaming_server/</code>)","text":"<p>The server component provides comprehensive streaming server functionality:</p> <ul> <li>StreamingServerResource: Shared resource managing server connections</li> <li>StreamingServerUpstreamOp: Handles incoming video streams from clients</li> <li>StreamingServerDownstreamOp: Handles outgoing video streams to clients</li> <li>Features:</li> <li>Multi-client support</li> <li>Format conversion utilities</li> <li>Frame processing and validation</li> <li>Debug utilities for troubleshooting</li> </ul> <p>Documentation: See <code>video_streaming_server/README.md</code> for detailed information.</p>","tags":["Video","Streaming","Client","Server","Networking","Cloud","Bidirectional"]},{"location":"operators/video_streaming/#usage","title":"Usage","text":"","tags":["Video","Streaming","Client","Server","Networking","Cloud","Bidirectional"]},{"location":"operators/video_streaming/#in-applications","title":"In Applications","text":"","tags":["Video","Streaming","Client","Server","Networking","Cloud","Bidirectional"]},{"location":"operators/video_streaming/#cmakeliststxt","title":"CMakeLists.txt","text":"<pre><code>add_holohub_application(my_streaming_app DEPENDS OPERATORS video_streaming)\n</code></pre>","tags":["Video","Streaming","Client","Server","Networking","Cloud","Bidirectional"]},{"location":"operators/video_streaming/#c-applications","title":"C++ Applications","text":"<pre><code>// Client functionality\n#include \"video_streaming_client.hpp\"\n#include \"frame_saver.hpp\"\n\n// Server functionality  \n#include \"video_streaming_server_resource.hpp\"\n#include \"video_streaming_server_upstream_op.hpp\"\n#include \"video_streaming_server_downstream_op.hpp\"\n</code></pre>","tags":["Video","Streaming","Client","Server","Networking","Cloud","Bidirectional"]},{"location":"operators/video_streaming/#python-applications","title":"Python Applications","text":"<p>Both client and server operators have Python bindings available. To use them in Python:</p> <pre><code># Client functionality\nfrom holohub.video_streaming_client import VideoStreamingClientOp\n\n# Server functionality\nfrom holohub.video_streaming_server import (\n    StreamingServerResource,\n    StreamingServerUpstreamOp,\n    StreamingServerDownstreamOp,\n)\n</code></pre> <p>Building with Python support:</p> <pre><code>./holohub build video_streaming --language python\n</code></pre>","tags":["Video","Streaming","Client","Server","Networking","Cloud","Bidirectional"]},{"location":"operators/video_streaming/#running-the-applications","title":"Running the Applications","text":"<p>The video streaming demo provides both client and server applications. For complete documentation and setup instructions, see the Applications README.</p> <p>\u26a0\ufe0f Important: These applications are currently only compatible with CUDA 12.x. If your system uses CUDA 13.x, ensure you add the <code>--cuda 12</code> flag to all command-line invocations shown below.</p> <p>For complete Python application documentation, see:</p> <ul> <li>Server Application (C++ and Python)</li> <li>Client Application (C++ and Python)</li> </ul>","tags":["Video","Streaming","Client","Server","Networking","Cloud","Bidirectional"]},{"location":"operators/video_streaming/#dependencies","title":"Dependencies","text":"","tags":["Video","Streaming","Client","Server","Networking","Cloud","Bidirectional"]},{"location":"operators/video_streaming/#required","title":"Required","text":"<ul> <li>Holoscan SDK 3.5.0 or higher: Core framework</li> <li>CUDA 12.x: GPU acceleration support</li> </ul>","tags":["Video","Streaming","Client","Server","Networking","Cloud","Bidirectional"]},{"location":"operators/video_streaming/#cloud-streaming-binaries","title":"Cloud Streaming Binaries","text":"","tags":["Video","Streaming","Client","Server","Networking","Cloud","Bidirectional"]},{"location":"operators/video_streaming/#client-binary","title":"Client Binary","text":"<p>To build the client operator, first download the client binaries from NGC:</p> <pre><code># Download using NGC CLI\n\ncd &lt;your_holohub_path&gt;/operators/video_streaming/video_streaming_client\nngc registry resource download-version \"nvidia/holoscan_client_cloud_streaming:0.2\"\nunzip -o holoscan_client_cloud_streaming_v0.2/holoscan_client_cloud_streaming.zip -d holoscan_client_cloud_streaming\n\n# Clean up NGC download directory\nrm -rf ./holoscan_client_cloud_streaming_v0.2/\n</code></pre>","tags":["Video","Streaming","Client","Server","Networking","Cloud","Bidirectional"]},{"location":"operators/video_streaming/#server-binary","title":"Server Binary","text":"<p>To build the server operator, first download the server binaries from NGC:</p> <pre><code># Download using NGC CLI\n\ncd &lt;your_holohub_path&gt;/operators/video_streaming/video_streaming_server\nngc registry resource download-version \"nvidia/holoscan_server_cloud_streaming:0.2\"\nunzip -o holoscan_server_cloud_streaming_v0.2/holoscan_server_cloud_streaming.zip -d holoscan_server_cloud_streaming\n\n# Clean up NGC download directory\nrm -rf ./holoscan_server_cloud_streaming_v0.2/\n</code></pre> <p>All dependencies need to be properly installed in the operator directory structure.</p>","tags":["Video","Streaming","Client","Server","Networking","Cloud","Bidirectional"]},{"location":"operators/video_streaming/#testing","title":"Testing","text":"<p>This package includes comprehensive testing at multiple levels:</p>","tags":["Video","Streaming","Client","Server","Networking","Cloud","Bidirectional"]},{"location":"operators/video_streaming/#unit-tests","title":"Unit Tests","text":"<p>For comprehensive test output examples, expected results, and detailed test information, see UNIT_TESTS_SUMMARY.md.</p>","tags":["Video","Streaming","Client","Server","Networking","Cloud","Bidirectional"]},{"location":"operators/video_streaming/#run-instructions","title":"Run Instructions","text":"<p>Run all unit tests from the holohub root directory:</p> <pre><code># Run all unit tests with verbose output\n./holohub test video_streaming --ctest-options=\"-R unit_tests -V\"\n</code></pre>","tags":["Video","Streaming","Client","Server","Networking","Cloud","Bidirectional"]},{"location":"operators/video_streaming/#integration-tests","title":"Integration Tests","text":"<p>Please refer to the Integration Tests for end-to-end integration tests validate the complete streaming pipeline with actual server/client communication and frame transmission.</p>","tags":["Video","Streaming","Client","Server","Networking","Cloud","Bidirectional"]},{"location":"operators/video_streaming/#related-applications","title":"Related Applications","text":"<ul> <li>Streaming Client Demo: Example client application</li> <li>Streaming Server Demo: Example server application</li> </ul>","tags":["Video","Streaming","Client","Server","Networking","Cloud","Bidirectional"]},{"location":"operators/video_streaming/#performance-notes","title":"Performance Notes","text":"<ul> <li>Both components support GPU memory allocation for optimal performance</li> <li>Configure appropriate buffer sizes for your streaming requirements</li> <li>Monitor network bandwidth for remote streaming scenarios</li> <li>Use debug utilities to troubleshoot frame processing issues</li> </ul>","tags":["Video","Streaming","Client","Server","Networking","Cloud","Bidirectional"]},{"location":"operators/video_streaming/#license","title":"License","text":"<p>Apache-2.0 - See the LICENSE file for details.</p>","tags":["Video","Streaming","Client","Server","Networking","Cloud","Bidirectional"]},{"location":"operators/video_streaming/video_streaming_client/","title":"StreamingClient Operator","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: November 7, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 3.5.0 Tested Holoscan SDK versions: 3.5.0 Contribution metric: Level 2 - Trusted</p> <p>The VideoStreamingClientOp class implements a Holoscan operator that provides bidirectional video streaming capabilities with the following key components:</p> <ul> <li>Configuration and Initialization:</li> <li>Configurable parameters for frame dimensions (width, height), frame rate (fps), server connection (IP, port)</li> <li>Input/output ports for frame data using GXF entities</li> <li>Support for both sending and receiving frames through separate flags</li> </ul> <p>Frame Processing Pipeline:</p> <ul> <li>Input handling: Receives frames as GXF entities containing H.264 encoded video tensors</li> <li>Frame conversion: Converts input tensors to VideoFrame objects with BGRA format</li> <li>Memory management: Implements safe memory handling with bounds checking and zero-padding</li> <li>Output generation: Creates GXF entities with properly configured tensors for downstream processing</li> </ul> <p>Streaming Protocol Implementation:</p> <ul> <li>Bidirectional streaming support through StreamingClient class</li> <li>Frame callback system for receiving frames</li> <li>Frame source system for sending frames</li> <li>Connection management with server including timeout handling</li> </ul> <p>\ud83d\udcda Related Documentation:</p> <ul> <li>Main Operators README - Setup, dependencies, NGC downloads, and Python examples</li> <li>Client Application README - Complete client application with usage examples</li> <li>Server Operator README - Companion server operator documentation</li> <li>Testing Documentation - Integration testing and verification</li> </ul>","tags":["Streaming","Client","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_client/#architecture-overview","title":"Architecture Overview","text":"<p>The StreamingClient operator integrates with the Holoscan Client Cloud Streaming library to provide seamless video streaming capabilities:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                           Holoscan Application                                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502   Input Source  \u2502    \u2502              VideoStreamingClientOp                 \u2502 \u2502\n\u2502  \u2502                 \u2502    \u2502                                                     \u2502 \u2502\n\u2502  \u2502  \u2022 V4L2 Camera  \u2502\u2500\u2500\u2500\u25b6\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502\n\u2502  \u2502  \u2022 Video File   \u2502    \u2502  \u2502  Frame Convert  \u2502    \u2502    VideoFrame Object    \u2502 \u2502 \u2502\n\u2502  \u2502  \u2022 Tensor Data  \u2502    \u2502  \u2502  BGR \u2192 BGRA     \u2502\u2500\u2500\u2500\u25b6\u2502    \u2022 Width/Height       \u2502 \u2502 \u2502\n\u2502  \u2502                 \u2502    \u2502  \u2502  Validation     \u2502    \u2502    \u2022 Pixel Data         \u2502 \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502    \u2022 Format (BGRA)      \u2502 \u2502 \u2502\n\u2502                         \u2502                         \u2502    \u2022 Timestamp          \u2502 \u2502 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502\n\u2502  \u2502  Output Sink    \u2502\u25c0\u2500\u2500\u2500\u2524                                      \u2502              \u2502 \u2502\n\u2502  \u2502                 \u2502    \u2502                                      \u25bc              \u2502 \u2502\n\u2502  \u2502  \u2022 HoloViz      \u2502    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502\n\u2502  \u2502  \u2022 File Writer  \u2502    \u2502  \u2502         Holoscan Client Cloud Streaming        \u2502 \u2502 \u2502\n\u2502  \u2502  \u2022 Next Op      \u2502    \u2502  \u2502                                                \u2502 \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502\n\u2502                         \u2502  \u2502  \u2502 StreamingClient \u2502    \u2502   Network Protocol \u2502 \u2502 \u2502 \u2502\n\u2502                         \u2502  \u2502  \u2502                 \u2502    \u2502                    \u2502 \u2502 \u2502 \u2502\n\u2502                         \u2502  \u2502  \u2502 \u2022 sendFrame()   \u2502\u2500\u2500\u2500\u25b6\u2502  \u2022 Cloud Streaming \u2502 \u2502 \u2502 \u2502\n\u2502                         \u2502  \u2502  \u2502 \u2022 Callbacks     \u2502    \u2502  \u2022 Signaling       \u2502 \u2502 \u2502 \u2502\n\u2502                         \u2502  \u2502  \u2502 \u2022 Connection    \u2502    \u2502  \u2022 Media Transport \u2502 \u2502 \u2502 \u2502\n\u2502                         \u2502  \u2502  \u2502   Management    \u2502    \u2502  \u2022 Encryption      \u2502 \u2502 \u2502 \u2502\n\u2502                         \u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502\n\u2502                         \u2502  \u2502                                      \u2502         \u2502 \u2502 \u2502\n\u2502                         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502\n\u2502                         \u2502                                         \u2502           \u2502 \u2502\n\u2502                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                    \u2502\n                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                          \u2502                    Network                            \u2502\n                          \u2502                                                       \u2502\n                          \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n                          \u2502  \u2502              Streaming Server                  \u2502   \u2502\n                          \u2502  \u2502                                                \u2502   \u2502\n                          \u2502  \u2502  \u2022 Holoscan Server Cloud Streaming             \u2502   \u2502\n                          \u2502  \u2502  \u2022 Multi-client support                        \u2502   \u2502\n                          \u2502  \u2502  \u2022 Bidirectional communication                 \u2502   \u2502\n                          \u2502  \u2502  \u2022 Frame processing and relay                  \u2502   \u2502\n                          \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>","tags":["Streaming","Client","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_client/#component-interactions","title":"Component Interactions","text":"<ol> <li> <p>Input Processing: The operator receives video frames from upstream Holoscan operators (V4L2, video replayer, etc.)</p> </li> <li> <p>Frame Conversion: Input tensors are converted to VideoFrame objects with proper format validation and memory management</p> </li> <li> <p>Cloud Streaming Integration: The VideoFrame is passed to the Holoscan Client Cloud Streaming library via <code>StreamingClient::sendFrame()</code></p> </li> <li> <p>Network Transport: The cloud streaming library handles:</p> </li> <li>Protocol implementation</li> <li>Signaling and connection establishment</li> <li>Media encoding and transport</li> <li> <p>Security and encryption</p> </li> <li> <p>Bidirectional Communication: Frames received from the server are processed through callbacks and converted back to Holoscan tensors</p> </li> <li> <p>Output Generation: Processed frames are emitted as GXF entities for downstream operators (HoloViz, file writers, etc.)</p> </li> </ol>","tags":["Streaming","Client","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_client/#requirements-setup","title":"Requirements &amp; Setup","text":"<p>For complete setup instructions including:</p> <ul> <li>Holoscan SDK 3.5.0 or higher and CUDA 12.x requirements</li> <li>NGC binary downloads (client streaming binaries)</li> <li>Build troubleshooting</li> </ul> <p>See the Main Operators README for detailed setup instructions.</p>","tags":["Streaming","Client","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_client/#camera-setup-and-testing","title":"Camera Setup and Testing","text":"<p>This section provides detailed technical camera configuration for the StreamingClient operator. For application-level camera setup and quick start instructions, see the Application README.</p>","tags":["Streaming","Client","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_client/#testing-your-v4l2-camera","title":"Testing Your V4L2 Camera","text":"<p>Before using the streaming client with your camera, verify it's working properly:</p> <pre><code># Check available video devices\nls -la /dev/video*\n\n# Get camera information\nv4l2-ctl --device=/dev/video0 --info\n\n# List supported formats and resolutions\nv4l2-ctl --device=/dev/video0 --list-formats-ext\n\n# Test camera capture (replace resolution as needed)\nv4l2-ctl --device=/dev/video0 --set-fmt-video=width=1280,height=720,pixelformat=MJPG --stream-mmap --stream-count=10\n</code></pre>","tags":["Streaming","Client","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_client/#configuring-camera-resolution","title":"Configuring Camera Resolution","text":"<p>The streaming client applications use YAML configuration files to set camera parameters. Edit the appropriate config file:</p>","tags":["Streaming","Client","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_client/#for-video_streaming_client","title":"For video_streaming_client","text":"<p>Edit <code>../../../applications/video_streaming/video_streaming_client/cpp/video_streaming_client_demo.yaml</code>:</p> <pre><code># V4L2 camera configuration\nv4l2_source:\n  device: \"/dev/video0\"        # Camera device path\n  width: 1280                  # Camera resolution width\n  height: 720                  # Camera resolution height  \n  frame_rate: 30               # Camera frame rate\n  pixel_format: \"MJPG\"         # Pixel format (MJPG recommended for higher resolutions)\n  # Optional camera settings:\n  # exposure_time: 100         # Exposure time in multiples of 100\u03bcs\n  # gain: 10                   # Camera gain value\n</code></pre>","tags":["Streaming","Client","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_client/#recommended-settings-by-camera-type","title":"Recommended Settings by Camera Type","text":"<p>For Logitech HD Pro Webcam C920:</p> <ul> <li>1280x720 @ 30fps MJPG - Best balance of quality and performance</li> <li>1920x1080 @ 30fps MJPG - High quality (higher bandwidth)</li> <li>640x480 @ 30fps YUYV - Low bandwidth testing</li> </ul> <p>General Guidelines:</p> <ul> <li>Use MJPG format for resolutions above 640x480 for better performance</li> <li>Use YUYV format for lower resolutions or when uncompressed data is needed</li> <li>Start with 30 FPS and adjust based on your system performance</li> <li>Match the resolution between client and server applications</li> </ul>","tags":["Streaming","Client","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_client/#troubleshooting-camera-issues","title":"Troubleshooting Camera Issues","text":"<p>Camera not detected:</p> <pre><code># Check camera permissions\nsudo usermod -a -G video $USER\n# Log out and back in, then test again\n</code></pre> <p>Poor performance:</p> <ul> <li>Try lower resolution (e.g., 640x480)</li> <li>Switch from YUYV to MJPG format</li> <li>Reduce frame rate to 15 or 24 FPS</li> </ul> <p>Format not supported:</p> <pre><code># Check what formats your camera actually supports\nv4l2-ctl --device=/dev/video0 --list-formats-ext | grep -E \"Size:|Interval:\"\n</code></pre>","tags":["Streaming","Client","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_client/#framesaver-utility-class","title":"FrameSaver Utility Class","text":"<p>The <code>FrameSaverOp</code> is a utility operator that can save video frames to disk for debugging and analysis purposes. This operator is not integrated into the main streaming client but can be used as a standalone debugging tool.</p>","tags":["Streaming","Client","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_client/#features","title":"Features","text":"<ul> <li>Frame Capture: Saves individual video frames to disk</li> <li>Multiple Formats: Supports both raw binary (.raw) and BGR format (.bgr) output</li> <li>GPU/CPU Support: Automatically handles frames from both GPU and CPU memory</li> <li>Configurable Output: Customizable output directory and filename patterns</li> <li>Data Analysis: Includes frame content analysis and logging</li> </ul>","tags":["Streaming","Client","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_client/#usage","title":"Usage","text":"","tags":["Streaming","Client","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_client/#basic-configuration","title":"Basic Configuration","text":"<pre><code># Add FrameSaverOp to your Holoscan application\nframe_saver:\n  output_dir: \"debug_frames\"           # Directory to save frames\n  base_filename: \"frame_\"              # Base filename for saved frames\n  save_as_raw: false                   # false = .bgr format, true = .raw format\n</code></pre>","tags":["Streaming","Client","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_client/#integration-example","title":"Integration Example","text":"<pre><code>#include \"frame_saver.hpp\"\n\n// In your application setup\nauto frame_saver = make_operator&lt;holoscan::ops::FrameSaverOp&gt;(\n    \"frame_saver\",\n    holoscan::Arg(\"output_dir\", std::string(\"debug_frames\")),\n    holoscan::Arg(\"base_filename\", std::string(\"frame_\")),\n    holoscan::Arg(\"save_as_raw\", false)\n);\n\n// Connect to your frame source\nadd_flow(frame_saver, source, ('input_frames', 'output_frames'));\n</code></pre>","tags":["Streaming","Client","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_client/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>output_dir</code> string \"output_frames\" Directory where frames will be saved <code>base_filename</code> string \"frame_\" Base name for saved frame files <code>save_as_raw</code> bool false Whether to save as raw binary (.raw) or BGR format (.bgr)","tags":["Streaming","Client","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_client/#output-files","title":"Output Files","text":"<ul> <li>BGR Format (.bgr): Standard BGR pixel format, can be opened with image viewers</li> <li>Raw Format (.raw): Binary data, useful for debugging memory layouts</li> <li>Naming: <code>frame_000001.bgr</code>, <code>frame_000002.bgr</code>, etc.</li> </ul>","tags":["Streaming","Client","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_client/#debugging-features","title":"Debugging Features","text":"<p>The FrameSaver includes built-in debugging features:</p> <ul> <li>Content Analysis: Logs frame size, zero-pixel count, and data validity</li> <li>Memory Handling: Automatically copies GPU frames to CPU before saving</li> <li>Error Handling: Comprehensive error reporting for file operations</li> </ul>","tags":["Streaming","Client","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_client/#example-output","title":"Example Output","text":"<pre><code>Frame 0 data analysis: size=1843200, all_zeros=false, non_zero_count=95\nSaved frame 0 to debug_frames/frame_000001.bgr\n</code></pre>","tags":["Streaming","Client","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_client/#building-the-framesaver","title":"Building the FrameSaver","text":"<p>To use the FrameSaver in your application, you'll need to:</p> <ol> <li>Include the source files in your CMakeLists.txt:</li> </ol> <pre><code>add_library(frame_saver\n  frame_saver.cpp\n  frame_saver.hpp\n)\n</code></pre> <ol> <li>Link against required libraries:</li> </ol> <pre><code>target_link_libraries(frame_saver\n  holoscan::core\n  CUDA::cudart\n)\n</code></pre>","tags":["Streaming","Client","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_client/#use-cases","title":"Use Cases","text":"<ul> <li>Debugging: Save frames at specific points in your pipeline</li> <li>Analysis: Examine frame content and format</li> <li>Testing: Verify frame data integrity</li> <li>Development: Visual inspection of processed frames</li> </ul>","tags":["Streaming","Client","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_client/#python-bindings-applications","title":"Python Bindings &amp; Applications","text":"<p>For Python usage, application examples, and testing:</p> <ul> <li>Main Operators README - Python bindings overview and setup</li> <li>Client Application README - Complete Python client implementation</li> <li>Testing Documentation - Integration testing guide</li> </ul>","tags":["Streaming","Client","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/","title":"Streaming Server Operators","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: November 7, 2025 Latest version: 1.1.0 Minimum Holoscan SDK version: 3.5.0 Tested Holoscan SDK versions: 3.5.0 Contribution metric: Level 2 - Trusted</p> <p>The <code>video_streaming_server</code> operator provides a modular streaming server implementation with separate upstream, downstream, and resource components. This split architecture allows for better separation of concerns and more flexible streaming pipeline configurations.</p> <p>\ud83d\udcda Related Documentation:</p> <ul> <li>Main Operators README - Setup, dependencies, NGC downloads, and Python examples</li> <li>Server Application README - Complete server application with usage examples</li> <li>Client Operator README - Companion client operator documentation</li> <li>Testing Documentation - Integration testing and verification</li> </ul>","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/#architecture-overview","title":"Architecture Overview","text":"<p>The Streaming Server operators integrate with the Holoscan Server Cloud Streaming library to provide comprehensive multi-client streaming capabilities:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                           Holoscan Application                                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502                    StreamingServerResource                                  \u2502 \u2502\n\u2502  \u2502                                                                             \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502\n\u2502  \u2502  \u2502              Holoscan Server Cloud Streaming                            \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502                                                                         \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502 StreamingServer \u2502    \u2502         Network Protocol Stack              \u2502 \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502                 \u2502    \u2502                                             \u2502 \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502 \u2022 Multi-client  \u2502\u2500\u2500\u2500\u25b6\u2502  \u2022 Media Transport &amp; Encoding               \u2502 \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502   Management    \u2502    \u2502  \u2022 Connection Management                    \u2502 \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502 \u2022 Frame Routing \u2502    \u2502  \u2022 Security &amp; Authentication                \u2502 \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502  \u2502 \u2022 Callbacks     \u2502    \u2502                                             \u2502 \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502\n\u2502  \u2502                                      \u2502                                      \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                         \u2502                                        \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502                        Processing Pipeline                                  \u2502 \u2502\n\u2502  \u2502                                                                             \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502\n\u2502  \u2502  \u2502StreamingServer  \u2502    \u2502   Processing    \u2502    \u2502StreamingServer          \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502UpstreamOp       \u2502    \u2502   Operators     \u2502    \u2502DownstreamOp             \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502                 \u2502    \u2502                 \u2502    \u2502                         \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502 \u2022 Frame Receive \u2502\u2500\u2500\u2500\u25b6\u2502 \u2022 Format Conv   \u2502\u2500\u2500\u2500\u25b6\u2502 \u2022 Frame Processing      \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502 \u2022 Client Frames \u2502    \u2502 \u2022 AI/ML Ops     \u2502    \u2502 \u2022 Tensor \u2192 VideoFrame   \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502 \u2022 Tensor Output \u2502    \u2502 \u2022 Filtering     \u2502    \u2502 \u2022 Multi-client Send     \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502 \u2022 Validation    \u2502    \u2502 \u2022 Enhancement   \u2502    \u2502 \u2022 Optional Processing   \u2502 \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502\n\u2502  \u2502           \u2502                       \u2502                            \u2502            \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502              \u2502                       \u2502                            \u2502              \u2502\n\u2502              \u25bc                       \u2502                            \u25b2              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  Input Sources  \u2502                 \u2502                \u2502     Output Sinks        \u2502 \u2502\n\u2502  \u2502                 \u2502                 \u2502                \u2502                         \u2502 \u2502\n\u2502  \u2502 \u2022 Client Frames \u2502                 \u2502                \u2502 \u2022 Client Connections    \u2502 \u2502\n\u2502  \u2502 \u2022 Network Data  \u2502                 \u2502                \u2502 \u2022 Processed Frames      \u2502 \u2502\n\u2502  \u2502 \u2022 Remote Cams   \u2502                 \u2502                \u2502 \u2022 Multi-cast Streams    \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                      \u2502                                           \u2502\n\u2502                                      \u25bc                                           \u2502\n\u2502                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                            \u2502\n\u2502                          \u2502   Optional Processing   \u2502                            \u2502\n\u2502                          \u2502                         \u2502                            \u2502\n\u2502                          \u2502 \u2022 AI/ML Inference       \u2502                            \u2502\n\u2502                          \u2502 \u2022 Computer Vision       \u2502                            \u2502\n\u2502                          \u2502 \u2022 Frame Enhancement     \u2502                            \u2502\n\u2502                          \u2502 \u2022 Analytics             \u2502                            \u2502\n\u2502                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                         \u2502\n                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                          \u2502             Network         \u2502\n                          \u2502                             \u2502\n                          \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n                          \u2502  \u2502                Multiple Clients                 \u2502  \u2502\n                          \u2502  \u2502                                                 \u2502  \u2502\n                          \u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502  \u2502\n                          \u2502  \u2502  \u2502   Client 1  \u2502  \u2502   Client 2  \u2502  \u2502 Client N \u2502\u2502  \u2502\n                          \u2502  \u2502  \u2502             \u2502  \u2502             \u2502  \u2502          \u2502\u2502  \u2502\n                          \u2502  \u2502  \u2502 \u2022 Holoscan  \u2502  \u2502 \u2022 Holoscan  \u2502  \u2502\u2022 Holoscan\u2502\u2502  \u2502\n                          \u2502  \u2502  \u2502   Client    \u2502  \u2502   Client    \u2502  \u2502  Client  \u2502\u2502  \u2502\n                          \u2502  \u2502  \u2502   Streaming \u2502  \u2502   Streaming \u2502  \u2502 Streaming\u2502\u2502  \u2502\n                          \u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502  \u2502\n                          \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/#component-interactions","title":"Component Interactions","text":"<ol> <li> <p>Multi-Client Management: The StreamingServerResource manages multiple simultaneous client connections through the Holoscan Server Cloud Streaming library</p> </li> <li> <p>Upstream Processing: StreamingServerUpstreamOp receives frames from connected clients and converts them to Holoscan tensors for processing</p> </li> <li> <p>Pipeline Integration: Frames flow through the standard Holoscan processing pipeline (AI/ML, computer vision, analytics, etc.)</p> </li> <li> <p>Downstream Distribution: StreamingServerDownstreamOp takes processed tensors and distributes them to all connected clients</p> </li> <li> <p>Network Protocol Handling: The cloud streaming library manages:</p> </li> <li>Multi-client signaling and negotiation</li> <li>Media encoding/decoding and transport</li> <li>Load balancing and connection management</li> <li> <p>Security and authentication</p> </li> <li> <p>Bidirectional Communication: Supports simultaneous receiving from clients (upstream) and sending to clients (downstream)</p> </li> </ol>","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/#architecture-components","title":"Architecture Components","text":"","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/#holoscanopsstreamingserverresource","title":"<code>holoscan::ops::StreamingServerResource</code>","text":"<p>A shared resource that manages the underlying StreamingServer instance and provides:</p> <ul> <li>Centralized server lifecycle management</li> <li>Event handling and callback management</li> <li>Configuration management for server parameters</li> <li>Frame sending and receiving coordination between operators</li> </ul>","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/#holoscanopsstreamingserverupstreamop","title":"<code>holoscan::ops::StreamingServerUpstreamOp</code>","text":"<p>An operator that receives video frames from streaming clients and outputs them as Holoscan tensors:</p> <ul> <li>Receives frames from connected clients via the StreamingServerResource</li> <li>Converts received frames to <code>holoscan::Tensor</code> format</li> <li>Provides duplicate frame detection to ensure unique frame processing</li> <li>Outputs tensors to the Holoscan processing pipeline</li> </ul>","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/#holoscanopsstreamingserverdownstreamop","title":"<code>holoscan::ops::StreamingServerDownstreamOp</code>","text":"<p>An operator that receives Holoscan tensors and sends them to streaming clients:</p> <ul> <li>Takes <code>holoscan::Tensor</code> input from the processing pipeline</li> <li>Converts tensors back to video frame format</li> <li>Sends processed frames to connected clients via the StreamingServerResource</li> <li>Supports optional frame processing (mirroring, rotation, etc.)</li> </ul>","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/#benefits-of-split-architecture","title":"Benefits of Split Architecture","text":"<ul> <li>Modularity: Each component has a single responsibility (resource management, receiving, or sending)</li> <li>Flexibility: You can use only upstream, only downstream, or both depending on your pipeline needs</li> <li>Shared Resource: Multiple operators can share the same StreamingServerResource instance</li> <li>Better Testing: Each component can be tested independently</li> <li>Clear Data Flow: Explicit tensor-based input/output ports make data flow obvious</li> <li>Processing Integration: Seamless integration with Holoscan's tensor processing pipeline</li> </ul>","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/#parameters","title":"Parameters","text":"","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/#streamingserverresource-parameters","title":"StreamingServerResource Parameters","text":"<ul> <li><code>port</code>: Port used for streaming server</li> <li>type: <code>uint16_t</code></li> <li> <p>default: 48010</p> </li> <li> <p><code>is_multi_instance</code>: Allow multiple server instances</p> </li> <li>type: <code>bool</code></li> <li> <p>default: false</p> </li> <li> <p><code>server_name</code>: Name identifier for the server</p> </li> <li>type: <code>std::string</code></li> <li> <p>default: \"HoloscanStreamingServer\"</p> </li> <li> <p><code>width</code>: Width of the video frames in pixels</p> </li> <li>type: <code>uint32_t</code></li> <li> <p>default: 854</p> </li> <li> <p><code>height</code>: Height of the video frames in pixels</p> </li> <li>type: <code>uint32_t</code></li> <li> <p>default: 480</p> </li> <li> <p><code>fps</code>: Frame rate of the video</p> </li> <li>type: <code>uint16_t</code></li> <li> <p>default: 30</p> </li> <li> <p><code>enable_upstream</code>: Enable upstream (receiving) functionality</p> </li> <li>type: <code>bool</code></li> <li> <p>default: true</p> </li> <li> <p><code>enable_downstream</code>: Enable downstream (sending) functionality</p> </li> <li>type: <code>bool</code></li> <li>default: true</li> </ul>","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/#streamingserverupstreamop-parameters","title":"StreamingServerUpstreamOp Parameters","text":"<ul> <li><code>width</code>: Frame width (inherits from resource if not specified)</li> <li>type: <code>uint32_t</code></li> <li> <p>default: 854</p> </li> <li> <p><code>height</code>: Frame height (inherits from resource if not specified)</p> </li> <li>type: <code>uint32_t</code></li> <li> <p>default: 480</p> </li> <li> <p><code>fps</code>: Frame rate (inherits from resource if not specified)</p> </li> <li>type: <code>uint32_t</code></li> <li> <p>default: 30</p> </li> <li> <p><code>allocator</code>: Memory allocator for tensor data</p> </li> <li> <p>type: <code>std::shared_ptr&lt;Allocator&gt;</code></p> </li> <li> <p><code>video_streaming_server_resource</code>: Reference to StreamingServerResource</p> </li> <li>type: <code>std::shared_ptr&lt;StreamingServerResource&gt;</code></li> </ul>","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/#streamingserverdownstreamop-parameters","title":"StreamingServerDownstreamOp Parameters","text":"<ul> <li><code>width</code>: Frame width (inherits from resource if not specified)</li> <li>type: <code>uint32_t</code></li> <li> <p>default: 854</p> </li> <li> <p><code>height</code>: Frame height (inherits from resource if not specified)</p> </li> <li>type: <code>uint32_t</code></li> <li> <p>default: 480</p> </li> <li> <p><code>fps</code>: Frame rate (inherits from resource if not specified)</p> </li> <li>type: <code>uint32_t</code></li> <li> <p>default: 30</p> </li> <li> <p><code>enable_processing</code>: Enable frame processing (mirroring, etc.)</p> </li> <li>type: <code>bool</code></li> <li> <p>default: false</p> </li> <li> <p><code>processing_type</code>: Type of processing to apply</p> </li> <li>type: <code>std::string</code></li> <li>default: \"none\"</li> <li> <p>options: \"none\", \"mirror\", \"rotate\"</p> </li> <li> <p><code>allocator</code>: Memory allocator for tensor data</p> </li> <li> <p>type: <code>std::shared_ptr&lt;Allocator&gt;</code></p> </li> <li> <p><code>video_streaming_server_resource</code>: Reference to StreamingServerResource</p> </li> <li>type: <code>std::shared_ptr&lt;StreamingServerResource&gt;</code></li> </ul>","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/#inputoutput-ports","title":"Input/Output Ports","text":"","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/#streamingserverupstreamop-ports","title":"StreamingServerUpstreamOp Ports","text":"<p>Output Ports:</p> <ul> <li><code>output_frames</code>: Output port for frames received from clients</li> <li>type: <code>holoscan::Tensor</code></li> </ul>","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/#streamingserverdownstreamop-ports","title":"StreamingServerDownstreamOp Ports","text":"<p>Input Ports:</p> <ul> <li><code>input_frames</code>: Input port for frames to be sent to clients</li> <li>type: <code>holoscan::Tensor</code></li> </ul>","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/#example-usage","title":"Example Usage","text":"","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/#complete-pipeline-setup","title":"Complete Pipeline Setup","text":"<pre><code>// Create allocator resource\nauto allocator = make_resource&lt;UnboundedAllocator&gt;(\"allocator\");\n\n// Create the shared StreamingServerResource\nauto video_streaming_server_resource = make_resource&lt;ops::StreamingServerResource&gt;(\n    \"video_streaming_server_resource\",\n    Arg(\"port\") = 48010,\n    Arg(\"server_name\") = \"MyStreamingServer\",\n    Arg(\"width\") = 854,\n    Arg(\"height\") = 480,\n    Arg(\"fps\") = 30,\n    Arg(\"enable_upstream\") = true,\n    Arg(\"enable_downstream\") = true,\n    Arg(\"is_multi_instance\") = false\n);\n\n// Create upstream operator (receives frames from clients)\nauto upstream_op = make_operator&lt;ops::StreamingServerUpstreamOp&gt;(\n    \"streaming_upstream\",\n    Arg(\"allocator\") = allocator,\n    Arg(\"video_streaming_server_resource\") = video_streaming_server_resource\n);\n\n// Create downstream operator (sends frames to clients)\nauto downstream_op = make_operator&lt;ops::StreamingServerDownstreamOp&gt;(\n    \"streaming_downstream\",\n    Arg(\"enable_processing\") = false,\n    Arg(\"processing_type\") = \"none\",\n    Arg(\"allocator\") = allocator,\n    Arg(\"video_streaming_server_resource\") = video_streaming_server_resource\n);\n</code></pre>","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/#requirements-setup","title":"Requirements &amp; Setup","text":"<p>For complete setup instructions including:</p> <ul> <li>Holoscan SDK 3.5.0 or higher and CUDA 12.x requirements</li> <li>NGC binary downloads (server streaming binaries)</li> <li>Build troubleshooting</li> </ul> <p>See the Main Operators README for detailed setup instructions.</p>","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/#deployment-on-nvcf","title":"Deployment on NVCF","text":"<p>The Holoscan cloud streaming stack provides plugins with endpoints required to deploy the server docker container as a streaming function. You can push the container and create/update/deploy the streaming function from the web portal.</p>","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/#push-container","title":"Push Container","text":"<p>Note: You first must docker login to the NGC Container Registry before you can push containers to it: https://docs.nvidia.com/ngc/gpu-cloud/ngc-private-registry-user-guide/index.html#accessing-ngc-registry Tag the container and push it to the container registry:</p> <pre><code>docker tag simple-streamer:latest {registry}/{org-id}/{container-name}:{version}\ndocker push {registry}/{org-id}/{container-name}:{version}\n</code></pre> <p>For example, if your organization name/id is 0494839893562652 and you want to push a container to the prod container registry using the name my-simple-streamer at version 0.1.0 then run:</p> <pre><code>docker tag simple-streamer:latest nvcr.io/0494839893562652/my-simple-streamer:0.1.0\ndocker push nvcr.io/0494839893562652/my-simple-streamer:0.1.0\n</code></pre>","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/#set-variables","title":"Set Variables","text":"<p>All the helper scripts below depend on the following environment variables being set:</p> <pre><code># Required variables\nexport NGC_PERSONAL_API_KEY=&lt;get from https://nvcf.ngc.nvidia.com/functions -&gt; Generate Personal API Key&gt;\nexport STREAMING_CONTAINER_IMAGE=&lt;registry&gt;/&lt;org-id&gt;/&lt;container-name&gt;:&lt;version&gt;\nexport STREAMING_FUNCTION_NAME=&lt;my-simple-streamer-function-name&gt;\n\n# Optional variables (shown with default values)\nexport NGC_DOMAIN=api.ngc.nvidia.com\nexport NVCF_SERVER=grpc.nvcf.nvidia.com\nexport STREAMING_SERVER_PORT=49100\nexport HTTP_SERVER_PORT=8011\n</code></pre>","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/#create-the-cloud-streaming-function","title":"Create the Cloud Streaming Function","text":"<p>Create the streaming function by running the provided script after setting all the required variables:</p> <pre><code>./nvcf/create_streaming_function.sh\n</code></pre> <p>Once the function is created, export the <code>FUNCTION_ID</code> as a variable:</p> <pre><code>export STREAMING_FUNCTION_ID={my-simple-streamer-function-id}\n</code></pre>","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/#update-function","title":"Update Function","text":"<p>Update an existing streaming function by running the provided script after setting all the required variables:</p> <pre><code>./nvcf/update_streaming_function.sh\n</code></pre>","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/#deploy-function","title":"Deploy Function","text":"<p>Deploy the streaming function from the web portal: https://nvcf.ngc.nvidia.com/functions</p>","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/#test-function","title":"Test Function","text":"<p>Start the test intermediate haproxy by running the provided script after setting all the required variables:</p> <pre><code>./nvcf/start_test_intermediate_haproxy.sh\n</code></pre> <p>Please note that the test haproxy server should be running on a separate machine, either on the client machine or a separate one.</p> <p>Note: If the test haproxy is still running, and you wish to test the executable or docker file again you must first stop it:</p> <pre><code>./nvcf/stop_test_intermediate_haproxy.sh\n</code></pre>","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/#python-bindings-applications","title":"Python Bindings &amp; Applications","text":"<p>For Python usage, application examples, and testing:</p> <ul> <li>Main Operators README - Python bindings overview and setup</li> <li>Server Application README - Complete Python server implementation</li> <li>Testing Documentation - Integration testing guide</li> </ul>","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/video_streaming/video_streaming_server/#additional-resources","title":"Additional Resources","text":"<p>For more information on NVCF Cloud functions, please refer to NVIDIA Cloud Functions documentation.</p>","tags":["Streaming","Server","Networking","Cloud","Video"]},{"location":"operators/vita49_psd_packetizer/","title":"V49PsdPacketizer (latest)","text":"","tags":["Signal Processing"]},{"location":"operators/vita49_psd_packetizer/#vita49-psd-packetizer","title":"VITA49 PSD Packetizer","text":"<p> Authors: John Moon john.moon@vts-i.com (Valley Tech Systems, Inc.), Eric Ferrara eric.ferrara@vts-i.com (Valley Tech Systems, Inc.), Matthew Luckenbihl matthew.luckenbihl@vts-i.com (Valley Tech Systems, Inc.) Supported platforms: x86_64 Language: C++ Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.5.0 Tested Holoscan SDK versions: 2.5.0, 2.6.0, 2.7.0, 2.8.0, 2.9.0, 3.0.0, 3.1.0 Contribution metric: Level 3 - Developmental</p>","tags":["Signal Processing"]},{"location":"operators/vita49_psd_packetizer/#overview","title":"Overview","text":"<p>Generate VITA 49.2 spectral data packets from incoming data.</p>","tags":["Signal Processing"]},{"location":"operators/vita49_psd_packetizer/#description","title":"Description","text":"<p>This operator will take in PSD data computed by upstream operators and format it into VITA 49.2 Spectral Data packets.</p> <p>After creating the VRT packets, it will send the packets to the configured UDP IP/port.</p>","tags":["Signal Processing"]},{"location":"operators/vita49_psd_packetizer/#requirements","title":"Requirements","text":"<ul> <li>MatX (dependency - assumed to be installed on system)</li> <li>Rust (language dependency)</li> <li>vita49 (Rust library dependency)</li> </ul> <p>Note: this operator depends on a Rust component. The <code>Dockerfile</code> provided in this directory will install Rust in the dev container from the official Ubuntu repos.</p>","tags":["Signal Processing"]},{"location":"operators/vita49_psd_packetizer/#multiple-channels","title":"Multiple Channels","text":"<p>If multiple channels are configured, the packetizer will use the base port in the configuration and add the channel index. So, with <code>base_dest_port: 4991</code>, channel <code>0</code> would send data to <code>4991</code>, but channel <code>1</code> would send data to <code>4992</code>.</p> <p>The zero-indexed <code>channel_number</code> key will be looked up in <code>metadata()</code> on each <code>compute()</code> run. If no value is found, the default channel number is <code>0</code>.</p>","tags":["Signal Processing"]},{"location":"operators/vita49_psd_packetizer/#example-usage","title":"Example Usage","text":"<p>For an example of how to use this operator, see the <code>psd_pipeline</code> application.</p>","tags":["Signal Processing"]},{"location":"operators/vita49_psd_packetizer/#configuration","title":"Configuration","text":"<p>The packetizer takes the following parameters:</p> <pre><code>vita49_psd_packetizer:\n  burst_size: 1280\n  num_channels: 1\n  dest_host: 127.0.0.1\n  base_dest_port: 4991\n  manufacturer_oui: 0xFF5646\n  device_code: 0x80\n</code></pre> <ul> <li><code>burst_size</code>: Number of samples to process in each burst</li> <li><code>num_channels</code>: Number of channels for which to allocate memory</li> <li><code>dest_host</code>: Destination host</li> <li><code>base_dest_port</code>: Base destination UDP port</li> <li><code>manufacturer_oui</code>: Manufacturer identifier to embed in the context packets</li> <li><code>device_code</code>: Device code to embed in the context packets</li> </ul>","tags":["Signal Processing"]},{"location":"operators/volume_loader/","title":"Volume Loader","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: October 9, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0, 1.0.3, 2.0.0, 2.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>volume_loader</code> operator reads 3D volumes from the specified input file.</p>","tags":["Visualization","Volume"]},{"location":"operators/volume_loader/#supported-formats","title":"Supported Formats","text":"<p>The operator supports these medical volume file formats: * MHD (MetaImage)   * Detached-header format only (<code>.mhd</code> + <code>.raw</code>) * NIFTI * NRRD (Nearly Raw Raster Data)   * Attached-header format (<code>.nrrd</code>)   * Detached-header format (<code>.nhdr</code> + <code>.raw</code>)</p> <p>You must convert your data to one of these formats to load it with <code>VolumeLoaderOp</code>. Some third party open source tools for volume file format conversion include: - Command Line Tools   - the Insight Toolkit (ITK) (PyPI, Image IO Examples)   - SimpleITK (PyPI)   - Utah NRRD Utilities (unu) - GUI Applications   - 3D Slicer   - ImageJ</p>","tags":["Visualization","Volume"]},{"location":"operators/volume_loader/#api","title":"API","text":"","tags":["Visualization","Volume"]},{"location":"operators/volume_loader/#holoscanopsvolumeloaderop","title":"<code>holoscan::ops::VolumeLoaderOp</code>","text":"<p>Operator class to read a volume.</p>","tags":["Visualization","Volume"]},{"location":"operators/volume_loader/#parameters","title":"Parameters","text":"<ul> <li><code>file_name</code>: Volume data file name</li> <li>type: <code>std::string</code></li> <li><code>allocator</code>: Allocator used to allocate the volume data</li> <li>type: <code>std::shared_ptr&lt;Allocator&gt;</code></li> </ul>","tags":["Visualization","Volume"]},{"location":"operators/volume_loader/#outputs","title":"Outputs","text":"<ul> <li><code>volume</code>: Output volume data</li> <li>type: <code>nvidia::gxf::Tensor</code></li> <li><code>spacing</code>: Physical size of each volume element</li> <li>type: <code>std::array&lt;float, 3&gt;</code></li> <li><code>permute_axis</code>: Volume axis permutation of data space to world space, e.g. if x and y of a volume is swapped this is {1, 0, 2}</li> <li>type: <code>std::array&lt;uint32_t, 3&gt;</code></li> <li><code>flip_axes</code>: Volume axis flipping from data space to world space, e.g. if x is flipped this is {true, false, false}</li> <li>type: <code>std::array&lt;bool, 3&gt;</code></li> <li><code>extent</code>: Physical size of the the volume in world space</li> <li>type: <code>std::array&lt;float, 3&gt;</code></li> </ul>","tags":["Visualization","Volume"]},{"location":"operators/volume_renderer/","title":"Volume Renderer","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: October 9, 2025 Latest version: 2.0.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3, 2.0.0, 2.1.0, 3.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>volume_renderer</code> operator renders a volume using ClaraViz (https://github.com/NVIDIA/clara-viz).</p>","tags":["Visualization","Volume","Rendering"]},{"location":"operators/volume_renderer/#holoscanopsvolumerenderer","title":"<code>holoscan::ops::VolumeRenderer</code>","text":"<p>Operator class to render a volume.</p>","tags":["Visualization","Volume","Rendering"]},{"location":"operators/volume_renderer/#parameters","title":"Parameters","text":"<ul> <li><code>config_file</code>: Config file path. The content of the file is passed to <code>clara::viz::JsonInterface::SetSettings()</code> at initialization time. See Configuration for details.</li> <li>type: <code>std::string</code></li> <li><code>allocator</code>: Allocator used to allocate render buffer outputs when no pre-allocated color or depth buffer is passed to <code>color_buffer_in</code> or <code>depth_buffer_in</code>. Allocator needs to be capable to allocate device memory.</li> <li>type: <code>std::shared_ptr&lt;Allocator&gt;</code></li> <li><code>alloc_width</code>: Width of the render buffer to allocate when no pre-allocated buffers are provided.</li> <li>type: <code>uint32_t</code></li> <li><code>alloc_height</code>: Height of the render buffer to allocate when no pre-allocated buffers are provided.</li> <li>type: <code>uint32_t</code></li> </ul>","tags":["Visualization","Volume","Rendering"]},{"location":"operators/volume_renderer/#inputs","title":"Inputs","text":"<p>All inputs are optional.</p> <ul> <li><code>volume_pose</code>: Transform the volume.</li> <li>type: <code>nvidia::gxf::Pose3D</code></li> <li><code>crop_box</code>: Volume crop box. Each <code>nvidia::gxf::Vector2f</code> contains the min and max values in range <code>[0, 1]</code> of the x, y and z axes of the volume.</li> <li>type: <code>std::array&lt;nvidia::gxf::Vector2f, 3&gt;</code></li> <li><code>depth_range</code>: The distance to the near and far frustum planes.</li> <li>type: <code>nvidia::gxf::Vector2f</code></li> <li><code>left_camera_pose</code>: Camera pose for the left camera when rendering in stereo mode.</li> <li>type: <code>nvidia::gxf::Pose3D</code></li> <li><code>right_camera_pose</code>: Camera pose for the right camera when rendering in stereo mode.</li> <li>type: <code>nvidia::gxf::Pose3D</code></li> <li><code>left_camera_model</code>: Camera model for the left camera when rendering in stereo mode.</li> <li>type: <code>nvidia::gxf::CameraModel</code></li> <li><code>right_camera_model</code>: Camera model for the right camera when rendering in stereo mode.</li> <li>type: <code>nvidia::gxf::CameraModel</code></li> <li><code>camera_pose</code>: Camera pose when not rendering in stereo mode.</li> <li>type: <code>std::array&lt;float, 16&gt;</code> or <code>nvidia::gxf::Pose3D</code></li> <li><code>color_buffer_in</code>: Buffer to store the rendered color data to, format needs to be 8 bit per component RGBA and buffer needs to be in device memory.</li> <li>type: <code>nvidia::gxf::VideoBuffer</code></li> <li><code>depth_buffer_in</code>: Buffer to store the rendered depth data to, format needs to be 32 bit float single component buffer needs to be in device memory.</li> <li>type: <code>nvidia::gxf::VideoBuffer</code></li> <li><code>density_volume</code>: Density volume data. Needs to be a 3D single component array. Supported data types are signed|unsigned 8|16|32 bit integer and 32 bit floating point.</li> <li>type: <code>nvidia::gxf::Tensor</code></li> <li><code>density_spacing</code>: Physical size of each density volume element.</li> <li>type: <code>std::array&lt;float, 3&gt;</code></li> <li><code>density_permute_axis</code>: Density volume axis permutation of data space to world space, e.g. if x and y of a volume is swapped this is {1, 0, 2}.</li> <li>type: <code>std::array&lt;uint32_t, 3&gt;</code></li> <li><code>density_flip_axes</code>: Density volume axis flipping from data space to world space, e.g. if x is flipped this is {true, false, false}.</li> <li>type: <code>std::array&lt;bool, 3&gt;</code></li> <li><code>mask_volume</code>: Mask volume data. Needs to be a 3D single component array. Supported data types are signed|unsigned 8|16|32 bit integer and 32 bit floating point.</li> <li>type: <code>nvidia::gxf::Tensor</code></li> <li><code>mask_spacing</code>: Physical size of each mask volume element.</li> <li>type: <code>std::array&lt;float, 3&gt;</code></li> <li><code>mask_permute_axis</code>: Mask volume axis permutation of data space to world space, e.g. if x and y of a volume is swapped this is {1, 0, 2}.</li> <li>type: <code>std::array&lt;uint32_t, 3&gt;</code></li> <li><code>mask_flip_axes</code>: Mask volume axis flipping from data space to world space, e.g. if x is flipped this is {true, false, false}.</li> <li>type: <code>std::array&lt;bool, 3&gt;</code></li> </ul>","tags":["Visualization","Volume","Rendering"]},{"location":"operators/volume_renderer/#outputs","title":"Outputs","text":"<ul> <li><code>color_buffer_out</code>: Buffer with rendered color data, format is 8 bit per component RGBA and buffer is in device memory.</li> <li>type: <code>nvidia::gxf::VideoBuffer</code></li> <li><code>depth_buffer_out</code>: Buffer with rendered depth data, format is be 32 bit float single component and buffer is in device memory.</li> <li>type: <code>nvidia::gxf::VideoBuffer</code></li> </ul>","tags":["Visualization","Volume","Rendering"]},{"location":"operators/volume_renderer/#configuration","title":"Configuration","text":"<p>The renderer accepts a ClaraViz JSON configuration file at startup to control rendering settings, including - camera parameters; - transfer functions; - lighting; - and more.</p> <p>The ClaraViz JSON configuration file exists in addition to and independent of a Holoscan SDK <code>.yaml</code> configuration file that may be passed to an application.</p> <p>See the <code>volume_rendering_xr</code> application for a sample configuration file. Visit the ClaraViz <code>render_server.proto</code> gRPC specification for insight into configuration file field values.</p>","tags":["Visualization","Volume","Rendering"]},{"location":"operators/volume_renderer/#transfer-functions","title":"Transfer Functions","text":"<p>Usually CT datasets are stored in Hounsfield scale. The renderer maps these values in Hounsfield scale to opacity in order to display the volume. These mappings are called transfer functions. Multiple transfer functions for different input value regions can be defined. Transfer functions also include material properties like diffuse, specular and emissive color. The range of input values the transfer function is applied to is in normalized input range <code>[0, 1]</code>.</p>","tags":["Visualization","Volume","Rendering"]},{"location":"operators/volume_renderer/#segmentation-mask-volume","title":"Segmentation (Mask) Volume","text":"<p>Different organs often have very similar Hounsfield values, therefore additionally an segmentation volume is supported. The segmentation volume contains an integer index for each element of the volume. Transfer functions can be restricted on specific segmentation indices. The segmentation volume can, for example, be generated using TotalSegmentator.</p>","tags":["Visualization","Volume","Rendering"]},{"location":"operators/volume_renderer/#creating-a-configuration-file","title":"Creating a Configuration File","text":"<p>Configuration files are typically specific to a given dataset or modality, and are tailored to a specific voxel intensity range. It may be necessary to create a new configuration file when working with a new dataset in order to produce a meaningful rendering.</p> <p>There are two options to create a configuration file for a new dataset: - Copy from an existing configuration file as a reference and modify parameters manually. An example configuration file is available in the <code>volume_rendering_xr</code> application config folder. - Use <code>VolumeRendererOp</code> to deduce settings for the input dataset. Follow these steps:   1. Use the HoloHub <code>volume_rendering</code> app or a similar application that will load an input dataset and pass it to <code>VolumeRendererOp</code>.   2. Configure application settings via a Holoscan SDK YAML file or command line settings to run with the following values:     - Set the <code>VolumeRendererOp</code> <code>config_file</code> parameter to an empty string to indicate no default config file is present;     - Set the <code>VolumeRendererOp</code> <code>write_config_file</code> parameter to the desired output JSON configuration filepath.   3. Run the application with the desired input volume. The operator will deduce settings and write out the JSON file to reuse on subsequent runs via the <code>config_file</code> parameter.</p>","tags":["Visualization","Volume","Rendering"]},{"location":"operators/vtk_renderer/","title":"VTK Renderer Operator","text":"<p> Authors: Kitware Team (Kitware Inc) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>vtk_renderer</code> extension takes the output of the source video player and the output of the <code>tool_tracking_postprocessor</code> operator and renders the video stream with an overlay annotation of the label using VTK.</p> <p>VTK can be a useful addition to holohub stack since VTK is a industry leading visualization toolkit. It is important to mention that this renderer operator needs to copy the input from device memory to host due to limitations of VTK. While this is a strong limitation for VTK we believe that VTK can still be a good addition and VTK is an evolving project. Perhaps in the future we could overcome this limitation.</p>","tags":["Visualization"]},{"location":"operators/vtk_renderer/#how-to-build-this-operator","title":"How to build this operator","text":"<p>Using Holohub CLI, you can create and run a container, which includes VTK, by running the following command from the root directory of Holohub:</p> <pre><code>./holohub run-container vtk_renderer\n</code></pre> <p>This command will create and run a container based on the provided <code>Dockerfile</code>.</p> <p>[!NOTE] If you want to only build the docker image without running it, you can use the following command, which will only create the image and tag it as <code>holohub:vtk_renderer</code>.</p> <pre><code>./holohub build-container vtk_renderer\n</code></pre> <p>Inside the container you can build the holohub application with:</p> <pre><code>./holohub build &lt;application&gt; --build-with vtk_renderer\n</code></pre>","tags":["Visualization"]},{"location":"operators/vtk_renderer/#parameters","title":"Parameters","text":"<ul> <li><code>videostream</code>: Input channel for the videostream, type <code>gxf::Tensor</code></li> <li>type: <code>gxf::Handle&lt;gxf::Receiver&gt;</code></li> <li><code>annotations</code>: Input channel for the annotations, type <code>gxf::Tensor</code></li> <li>type: <code>gxf::Handle&lt;gxf::Receiver&gt;</code></li> <li><code>window_name</code>: Compositor window name.</li> <li>type: <code>std::string</code></li> <li><code>width</code>: width of the renderer window.</li> <li>type: <code>int</code></li> <li><code>height</code>: height of the renderer window.</li> <li>type: <code>int</code></li> <li><code>labels</code>: labels to be displayed on the rendered image.</li> <li>type: <code>std::vector&lt;std::string&gt;&gt;</code></li> </ul>","tags":["Visualization"]},{"location":"operators/webrtc_client/","title":"WebRTC Client Operator","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>webrtc_client</code> operator receives video frames through a WebRTC connection. The application using this operator needs to call the <code>offer</code> method of the operator when a new WebRTC connection is available.</p>","tags":["Networking and Distributed Computing","WebRTC","Video"]},{"location":"operators/webrtc_client/#methods","title":"Methods","text":"<ul> <li><code>async def offer(self, sdp, type) -&gt; (local_sdp, local_type)</code>   Start a connection between the local computer and the peer.</li> </ul> <p>Parameters   - sdp peer Session Description Protocol object   - type peer session type</p> <p>Return values   - sdp local Session Description Protocol object   - type local session type</p>","tags":["Networking and Distributed Computing","WebRTC","Video"]},{"location":"operators/webrtc_client/#outputs","title":"Outputs","text":"<ul> <li><code>output</code>: Tensor with 8 bit per component RGB data</li> <li>type: <code>Tensor</code></li> </ul>","tags":["Networking and Distributed Computing","WebRTC","Video"]},{"location":"operators/webrtc_server/","title":"WebRTC Server Operator","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The <code>webrtc_server</code> operator sends video frames through a WebRTC connection. The application using this operator needs to call the <code>offer</code> method of the operator when a new WebRTC connection is available.</p>","tags":["Networking and Distributed Computing","WebRTC","Video"]},{"location":"operators/webrtc_server/#methods","title":"Methods","text":"<ul> <li><code>async def offer(self, sdp, type) -&gt; (local_sdp, local_type)</code>   Start a connection between the local computer and the peer.</li> </ul> <p>Parameters   - sdp peer Session Description Protocol object   - type peer session type</p> <p>Return values   - sdp local Session Description Protocol object   - type local session type</p>","tags":["Networking and Distributed Computing","WebRTC","Video"]},{"location":"operators/webrtc_server/#inputs","title":"Inputs","text":"<ul> <li><code>input</code>: Tensor or numpy array with 8 bit per component RGB data</li> <li>type: <code>Tensor</code></li> </ul>","tags":["Networking and Distributed Computing","WebRTC","Video"]},{"location":"operators/xr/","title":"OpenXR Operators","text":"<p> Authors: Connor Smith (NVIDIA), Rafael Wiltz (NVIDIA), Mimi Liao (NVIDIA) Supported platforms: x86_64 Language: C++, Python Last modified: August 7, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 3.1.0 Tested Holoscan SDK versions: 3.1.0 Contribution metric: Level 4 - Experimental</p> <p>A collection of Holoscan operators for XR (Extended Reality) integration, enabling real-time XR applications with Vulkan graphics and CUDA compute capabilities.</p>","tags":["Extended Reality","Vulkan","Visualization"]},{"location":"operators/xr/#overview","title":"Overview","text":"<p>The XR operators provide a complete framework for building Holoscan applications that can render to XR headsets. The implementation leverages the OpenXR standard for cross-platform XR support, Vulkan for graphics, and CUDA for compute operations, enabling high-performance real-time XR applications.</p>","tags":["Extended Reality","Vulkan","Visualization"]},{"location":"operators/xr/#core-components","title":"Core Components","text":"<p>Below is a diagram showing how to use these operators in a typical XR workflow:</p> <p></p>","tags":["Extended Reality","Vulkan","Visualization"]},{"location":"operators/xr/#frame-management-operators","title":"Frame Management Operators","text":"","tags":["Extended Reality","Vulkan","Visualization"]},{"location":"operators/xr/#xrbeginframeop","title":"XrBeginFrameOp","text":"<ul> <li>Synchronizes application with XR device timing</li> <li>Calls OpenXR <code>xrWaitFrame</code> (blocks until the next frame is ready and returns a predicted display time)</li> <li>Calls OpenXR <code>xrBeginFrame</code> (signals the start of rendering for the new frame)</li> <li>Outputs frame state for downstream operators</li> </ul>","tags":["Extended Reality","Vulkan","Visualization"]},{"location":"operators/xr/#xrendframeop","title":"XrEndFrameOp","text":"<ul> <li>Calls OpenXR <code>xrEndFrame</code> to submit composition layers to XR device</li> </ul>","tags":["Extended Reality","Vulkan","Visualization"]},{"location":"operators/xr/#xrsession","title":"XrSession","text":"<p>The central resource that manages the OpenXR session lifecycle:</p> <ul> <li>Initialization: Creates OpenXR instance, system, session, and Vulkan graphics context</li> <li>Plugin System: Supports extensible plugins for additional XR functionality</li> <li>View Space Management: Provides reference and view spaces for pose tracking</li> <li>Graphics Integration: Manages Vulkan instance, device, and queue creation</li> </ul>","tags":["Extended Reality","Vulkan","Visualization"]},{"location":"operators/xr/#rendering-infrastructure","title":"Rendering Infrastructure","text":"","tags":["Extended Reality","Vulkan","Visualization"]},{"location":"operators/xr/#xrswapchaincuda","title":"XrSwapchainCuda","text":"<ul> <li>Manages OpenXR swapchains with CUDA interop</li> <li>Provides Vulkan/CUDA memory sharing via external memory</li> <li>Supports multiple formats: RGBA8, depth buffers</li> <li>Handles GPU synchronization between CUDA and Vulkan</li> </ul>","tags":["Extended Reality","Vulkan","Visualization"]},{"location":"operators/xr/#xrcompositionlayermanager","title":"XrCompositionLayerManager","text":"<ul> <li>Manages color and depth swapchains</li> <li>Creates composition layer storage for frame rendering</li> <li>Provides simplified interface for XR rendering</li> </ul>","tags":["Extended Reality","Vulkan","Visualization"]},{"location":"operators/xr/#xrcompositionlayerprojectionstorage","title":"XrCompositionLayerProjectionStorage","text":"<ul> <li>Stores projection layer views and depth information</li> <li>Supports stereo rendering with side-by-side layout</li> <li>Handles depth range and near/far plane configuration</li> </ul>","tags":["Extended Reality","Vulkan","Visualization"]},{"location":"operators/xr/#input-tracking","title":"Input Tracking","text":"","tags":["Extended Reality","Vulkan","Visualization"]},{"location":"operators/xr/#xrhandtracker","title":"XrHandTracker","text":"<ul> <li>Implements OpenXR hand tracking extension</li> <li>Provides joint locations for both hands</li> <li>Supports 26 hand joints per hand</li> <li>Real-time pose tracking with validation</li> </ul>","tags":["Extended Reality","Vulkan","Visualization"]},{"location":"operators/xr/#features","title":"Features","text":"<ul> <li>Session Management: Create and manage XR sessions, including initialization, lifecycle, and management of reference/view spaces.</li> <li>Frame Operations: Handle XR frame begin and end operations to synchronize rendering and presentation with the XR device.</li> <li>CUDA Swapchain: Enable direct GPU processing of OpenXR swapchain images using CUDA, supporting efficient zero-copy memory sharing between CUDA and Vulkan.</li> <li>Layer Composition and Stereo Rendering: Collect and manage all layers (color, depth, etc.) to be displayed, with built-in support for stereo rendering using side-by-side layouts.</li> <li>Input Systems: Provide 6DOF pose tracking and hand tracking (OpenXR EXT_hand_tracking).</li> <li>Performance and Efficiency: Leverage efficient CUDA-Vulkan interoperability, GPU synchronization, and external memory/semaphore primitives for high-performance XR rendering.</li> </ul>","tags":["Extended Reality","Vulkan","Visualization"]},{"location":"operators/xr/#dependencies","title":"Dependencies","text":"","tags":["Extended Reality","Vulkan","Visualization"]},{"location":"operators/xr/#external-libraries","title":"External Libraries","text":"<ul> <li>OpenXR SDK: Cross-platform XR API</li> <li>OpenXR-Hpp: C++ headers for OpenXR</li> <li>GLM: Mathematics library for graphics</li> <li>Vulkan: Graphics API for XR rendering</li> <li>CUDA: Compute API for GPU operations</li> </ul>","tags":["Extended Reality","Vulkan","Visualization"]},{"location":"operators/xr/#holoscan-requirements","title":"Holoscan Requirements","text":"<ul> <li>Minimum SDK Version: 3.3.0</li> <li>Tested Versions: 3.3.0</li> <li>Platforms: x86_64</li> </ul>","tags":["Extended Reality","Vulkan","Visualization"]},{"location":"operators/xr/#usage-example","title":"Usage Example","text":"<p>For practical usage examples, see the following applications that utilize these XR operators:</p> <ul> <li>xr_holoviz: Simple XR visualization using Holoviz.</li> <li>xr_gsplat: XR rendering of 3D scenes using Gaussian Splatting.</li> </ul>","tags":["Extended Reality","Vulkan","Visualization"]},{"location":"operators/yuan_qcap/","title":"YUAN QCAP Source Operator","text":"<p> Authors: David Su (Yuan) Supported platforms: x86_64, aarch64 Language: C++, Python Last modified: August 5, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 0.5.0 Tested Holoscan SDK versions: 0.5.0 Contribution metric: Level 2 - Trusted</p> <p>The <code>QCAPSourceOp</code> operator provides video stream capture from YUAN High-Tech capture cards, supporting various video formats and configurations for professional video acquisition.</p>","tags":["Camera"]},{"location":"operators/yuan_qcap/#overview","title":"Overview","text":"<p>This operator interfaces with YUAN High-Tech capture cards to acquire video streams with configurable resolution, frame rate, and pixel formats. It supports both C++ and Python implementations and provides flexible parameter configuration for different capture scenarios.</p>","tags":["Camera"]},{"location":"operators/yuan_qcap/#features","title":"Features","text":"<ul> <li>YUAN Capture Card Support: Direct integration with YUAN High-Tech capture hardware</li> <li>Configurable Resolution: Support for various video resolutions up to 4K</li> <li>Flexible Frame Rates: Adjustable frame rates for different applications</li> <li>Multiple Pixel Formats: Support for RGB, RGBA, and other color formats</li> <li>RDMA Support: Optional RDMA (Remote Direct Memory Access) for improved performance</li> <li>Multi-Platform: Available for both x86_64 and aarch64 architectures</li> </ul>","tags":["Camera"]},{"location":"operators/yuan_qcap/#usage","title":"Usage","text":"","tags":["Camera"]},{"location":"operators/yuan_qcap/#python-usage","title":"Python Usage","text":"<pre><code>from holoscan.operators import QCAPSourceOp\n\n# Create operator with default settings\nqcap_op = QCAPSourceOp(\n    fragment=fragment,\n    device=\"SC0710 PCI\",\n    channel=0,\n    width=3840,\n    height=2160,\n    framerate=60,\n    rdma=True,\n    pixel_format=\"bgr24\",\n    input_type=\"auto\"\n)\n</code></pre>","tags":["Camera"]},{"location":"operators/yuan_qcap/#c-usage","title":"C++ Usage","text":"<pre><code>#include \"qcap_source.hpp\"\n\nauto qcap_op = std::make_shared&lt;holoscan::ops::QCAPSourceOp&gt;(\n    Arg{\"device\", \"SC0710 PCI\"},\n    Arg{\"channel\", 0},\n    Arg{\"width\", 3840},\n    Arg{\"height\", 2160},\n    Arg{\"framerate\", 60},\n    Arg{\"rdma\", true},\n    Arg{\"pixel_format\", \"bgr24\"},\n    Arg{\"input_type\", \"auto\"}\n);\n</code></pre> <p>Please refer to the following Holoscan reference applications for usage of this operator:</p> <ul> <li>Endoscopy Tool Tracking</li> <li>H.264 Endoscopy Tool Tracking</li> </ul>","tags":["Camera"]},{"location":"operators/yuan_qcap/#parameters","title":"Parameters","text":"","tags":["Camera"]},{"location":"operators/yuan_qcap/#device-configuration","title":"Device Configuration","text":"<ul> <li><code>device</code> (string, default: \"SC0710 PCI\"): Device specifier for the capture card</li> <li><code>channel</code> (uint32_t, default: 0): Channel number to use for capture</li> </ul>","tags":["Camera"]},{"location":"operators/yuan_qcap/#video-settings","title":"Video Settings","text":"<ul> <li><code>width</code> (uint32_t, default: 3840): Width of the video stream in pixels</li> <li><code>height</code> (uint32_t, default: 2160): Height of the video stream in pixels</li> <li><code>framerate</code> (uint32_t, default: 60): Frame rate of the video stream in fps</li> </ul>","tags":["Camera"]},{"location":"operators/yuan_qcap/#performance-options","title":"Performance Options","text":"<ul> <li><code>rdma</code> (bool, default: false): Enable RDMA for improved memory transfer performance</li> <li><code>pixel_format</code> (string, default: \"bgr24\"): Pixel format of the video stream</li> <li><code>input_type</code> (string, default: \"auto\"): Input type configuration</li> <li><code>mst_mode</code> (uint32_t, default: 0): MST (Multi-Stream Transport) mode setting</li> <li><code>sdi12g_mode</code> (uint32_t, default: 0): SDI 12G mode configuration</li> </ul>","tags":["Camera"]},{"location":"operators/yuan_qcap/#inputoutput","title":"Input/Output","text":"","tags":["Camera"]},{"location":"operators/yuan_qcap/#output","title":"Output","text":"<ul> <li><code>video_buffer_output</code>: Video buffer containing the captured frame data</li> <li>Format: Video buffer with specified resolution and pixel format</li> <li>Rate: Matches the configured frame rate</li> <li>Type: GXF Entity with video buffer data</li> </ul>","tags":["Camera"]},{"location":"operators/yuan_qcap/#supported-configurations","title":"Supported Configurations","text":"","tags":["Camera"]},{"location":"operators/yuan_qcap/#video-resolutions","title":"Video Resolutions","text":"<ul> <li>4K: 3840x2160 (default)</li> <li>2K: 2048x1080</li> <li>1080p: 1920x1080</li> <li>720p: 1280x720</li> <li>Custom resolutions supported</li> </ul>","tags":["Camera"]},{"location":"operators/yuan_qcap/#pixel-formats","title":"Pixel Formats","text":"<ul> <li><code>bgr24</code>: 24-bit BGR format (default)</li> <li><code>rgb24</code>: 24-bit RGB format</li> <li><code>rgba32</code>: 32-bit RGBA format</li> <li>Additional formats based on hardware support</li> </ul>","tags":["Camera"]},{"location":"operators/yuan_qcap/#frame-rates","title":"Frame Rates","text":"<ul> <li>60 fps (default)</li> <li>30 fps</li> <li>25 fps</li> <li>24 fps</li> <li>Custom frame rates supported</li> </ul>","tags":["Camera"]},{"location":"operators/yuan_qcap/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>YUAN High-Tech capture card (e.g., SC0710 PCI)</li> <li>Compatible PCIe slot</li> <li>Sufficient bandwidth for video stream</li> <li>GPU memory for RDMA operations (if enabled)</li> </ul>","tags":["Camera"]},{"location":"operators/yuan_qcap/#integration","title":"Integration","text":"<p>The operator is designed to work within Holoscan pipelines and can be connected to:</p> <ul> <li>Video processing operators</li> <li>Encoding/compression operators</li> <li>Display/visualization operators</li> <li>Recording/storage operators</li> <li>AI inference pipelines</li> </ul>","tags":["Camera"]},{"location":"tutorials/async_buffer_deadline_tutorial/","title":"A Study using Asynchronous Lock-free Buffer with SCHED_DEADLINE","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Last modified: November 25, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 3.5.0 Tested Holoscan SDK versions: 3.5.0 Contribution metric: Level 3 - Developmental</p> <p>This tutorial demonstrates the impact of using an  asynchronous lock-free buffer  with <code>SCHED_DEADLINE</code> scheduling policy in Linux on the message latency in a Holoscan SDK application and compares it with the default buffer.</p>","tags":["Optimization","Real-Time","Performance","SCHED_DEADLINE","Buffer","Latency","Linux"]},{"location":"tutorials/async_buffer_deadline_tutorial/#application-configuration","title":"Application Configuration","text":"<p>The application source code is provided in the application directory.</p> <p>The application consists of two <code>PingTxOp</code> operators (<code>tx1</code> and <code>tx2</code>) and one <code>PingRxOp</code> operator (<code>rx</code>). Both <code>tx1</code> and <code>tx2</code> generate messages and send them to <code>rx</code>.</p> <pre><code>graph TD;\n    tx1(\"tx1\") --&gt;|out -&gt; in1| rx(\"rx\");\n    tx2(\"tx2\") --&gt;|out -&gt; in2| rx(\"rx\");</code></pre> <ul> <li><code>tx1</code>: Sends messages after busy-waiting for 5ms. The busy-wait is   representative of reading sensor data and a brief processing time.</li> <li><code>tx2</code>: Sends messages after busy-waiting for 10ms. The busy-wait is   representative of reading sensor data and a brief processing time.</li> <li><code>rx</code>: Receives messages from both <code>tx1</code> and <code>tx2</code>, and calculates   message latency and inter-message interval period.   Then, it waits for a brief   1 ms which could be representative of acutating a signal after a brief   processing time.</li> </ul> <p>The connection between the operators is configured to use either a default buffer (<code>DoubleBuffer</code>) or an async lock-free buffer.</p>","tags":["Optimization","Real-Time","Performance","SCHED_DEADLINE","Buffer","Latency","Linux"]},{"location":"tutorials/async_buffer_deadline_tutorial/#experiment-run-instructions","title":"Experiment Run Instructions","text":"<p>Application-specific run instructions are provided in the application directory.</p> <p>The following is an example application run instruction:</p> <pre><code>./holohub run async_buffer_deadline_tutorial --as-root --docker-opts='--ulimit rtprio=99 --cap-add=CAP_SYS_NICE'\n</code></pre> <p>We need to run the application with root privileges and other flags as <code>SCHED_DEADLINE</code> Linux scheduling policy requires those flags.</p>","tags":["Optimization","Real-Time","Performance","SCHED_DEADLINE","Buffer","Latency","Linux"]},{"location":"tutorials/async_buffer_deadline_tutorial/#experiment-scripts","title":"Experiment Scripts","text":"<p>To run all the experiments in this tutorial:</p> <pre><code>./tutorials/async_buffer_deadline_tutorial/run_experiment.sh\n</code></pre> <p>The above script will run the experiment and generate the plots in <code>period_variation</code> under this directory.</p>","tags":["Optimization","Real-Time","Performance","SCHED_DEADLINE","Buffer","Latency","Linux"]},{"location":"tutorials/async_buffer_deadline_tutorial/#experimental-setup","title":"Experimental Setup","text":"<p>The experiment demonstrates how an async lock-free buffer can allow Holoscan SDK operators to be run with different <code>SCHED_DEADLINE</code> periods independently without being affected by each other's periods or runtimes.</p> <p>We measure two key metrics: - Max Message Latency: The highest latency observed for messages being   generated at <code>tx1</code> (or at <code>tx2</code>) and then received at <code>rx</code>. - Max Message Interval: The longest time interval between two consecutive   messages from <code>tx1</code> (or from <code>tx2</code>) and then received at <code>rx</code>.</p> <p>We run two main scenarios with fixed <code>rx</code> period of 10ms:</p> <ol> <li> <p>Fixed <code>tx1</code> Period, Varying <code>tx2</code> Period:</p> <ul> <li><code>tx1</code> period is fixed at 20ms.</li> <li><code>tx2</code> period is varied from 20ms to 100ms.</li> <li>We measure the impact on <code>tx1</code>'s latency and message interval.</li> <li>Since the periods <code>tx1</code> and <code>rx</code> are fixed, the message timings of <code>tx1</code>   must ideally not be impacted with varying <code>tx2</code> periods.</li> </ul> </li> <li> <p>Fixed <code>tx2</code> Period, Varying <code>tx1</code> Period:</p> <ul> <li><code>tx2</code> period is fixed at 20ms.</li> <li><code>tx1</code> period is varied from 20ms to 100ms.</li> <li>We measure the impact on <code>tx2</code>'s latency and message interval.</li> <li>Since the periods <code>tx2</code> and <code>rx</code> are fixed, the message timings of <code>tx2</code>   must ideally not be impacted with varying <code>tx1</code> periods.</li> </ul> </li> </ol>","tags":["Optimization","Real-Time","Performance","SCHED_DEADLINE","Buffer","Latency","Linux"]},{"location":"tutorials/async_buffer_deadline_tutorial/#results","title":"Results","text":"<p>The results show that with the default buffer, the performance of one operator is heavily dependent on the other. However, with the async lock-free buffer, they are decoupled enabling true asynchronous execution of the operators.</p>","tags":["Optimization","Real-Time","Performance","SCHED_DEADLINE","Buffer","Latency","Linux"]},{"location":"tutorials/async_buffer_deadline_tutorial/#tx1-message-latency-vs-tx2-period","title":"TX1 Message Latency vs. TX2 Period","text":"<p>With the default buffer, as <code>tx2</code>'s period increases, <code>tx1</code>'s maximum latency also increases linearly. Since <code>rx</code> cannot run before both the upstream operators (<code>tx1</code> and <code>tx2</code>) have written messages for it, the latency of <code>tx1</code> is affected by <code>tx2</code>. With the async lock-free buffer, <code>tx1</code>'s latency  remains consistent and low, regardless of <code>tx2</code>'s period. Therefore, async lock-free buffer unlocks independent connection between <code>tx1</code> and <code>rx</code> irrespective of the behavior of <code>tx2</code>.</p>","tags":["Optimization","Real-Time","Performance","SCHED_DEADLINE","Buffer","Latency","Linux"]},{"location":"tutorials/async_buffer_deadline_tutorial/#in1-message-interval-vs-tx2-period","title":"IN1 Message Interval vs. TX2 Period","text":"<p>Similarly, the maximum message interval for <code>tx1</code> (at the <code>in1</code> port of <code>rx</code>) increases with <code>tx2</code>'s period when using the default buffer. The async lock-free buffer keeps the interval stable.</p>","tags":["Optimization","Real-Time","Performance","SCHED_DEADLINE","Buffer","Latency","Linux"]},{"location":"tutorials/async_buffer_deadline_tutorial/#tx2-message-latency-vs-tx1-period","title":"TX2 Message Latency vs. TX1 Period","text":"<p>The same trend is visible here. <code>tx2</code>'s message latency is affected by <code>tx1</code>'s period with the default buffer, but not with the async lock-free buffer.</p>","tags":["Optimization","Real-Time","Performance","SCHED_DEADLINE","Buffer","Latency","Linux"]},{"location":"tutorials/async_buffer_deadline_tutorial/#in2-message-interval-vs-tx1-period","title":"IN2 Message Interval vs. TX1 Period","text":"<p>The message interval for <code>tx2</code> (at the <code>in2</code> port of <code>rx</code>) remains stable with the async lock-free buffer, independent of <code>tx1</code>'s period.</p>","tags":["Optimization","Real-Time","Performance","SCHED_DEADLINE","Buffer","Latency","Linux"]},{"location":"tutorials/async_buffer_deadline_tutorial/#conclusion-and-guidance","title":"Conclusion and Guidance","text":"<p>The asynchronous lock-free buffer connection between operators enables true independent execution when using <code>SCHED_DEADLINE</code> Linux scheduling policy.  This buffer type allows each operator to maintain its specified runtime and  period without interference from other operators in the pipeline.</p> <p>Key Insights: - With async lock-free buffer: Operators run independently with their    configured <code>SCHED_DEADLINE</code> runtime and periods, achieving predictable real-time performance - With default buffer (DoubleBuffer): Operators become coupled, where one    operator's performance can be impacted by another operator's    behavior, even when <code>SCHED_DEADLINE</code> policy is applied</p> <p>Developer Recommendations: 1. Use async lock-free buffers when implementing soft real-time applications     with <code>SCHED_DEADLINE</code> scheduling to ensure predictable operator performance 2. Avoid default buffers in <code>SCHED_DEADLINE</code> scenarios where operator     independence is critical for meeting real-time constraints 3. Using default buffer with <code>SCHED_DEADLINE</code> policy means satisfying both    the constraints of the double buffer and periodic execution of    <code>SCHED_DEADLINE</code>. Depending on the application, this may not be desirable. 4. Using <code>SCHED_DEADLINE</code> for a chosen few operators (for example, source    operators in a DAG) along with default    buffer may provide a good balance because this provides predictable execution    for chosen few <code>SCHED_DEADLINE</code> operators while allowing applications to run    normally otherwise. 3. Test both buffer types during development to understand the performance     implications in your specific use case, especially when using <code>SCHED_DEADLINE</code>    policy 4. Monitor message latency and intervals to verify that operators maintain     their intended timing characteristics</p>","tags":["Optimization","Real-Time","Performance","SCHED_DEADLINE","Buffer","Latency","Linux"]},{"location":"tutorials/cloudxr_runtime_for_xr_applications/","title":"Setting up CloudXR Runtime with Holoscan XR Applications","text":"<p> Authors: Mimi Liao (NVIDIA) Supported platforms: x86_64 Language: N/A Last modified: October 9, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 3.3 Tested Holoscan SDK versions: 3.3, 3.4 Contribution metric: Level 1 - Highly Reliable</p> <p>This tutorial demonstrates how to leverage NVIDIA CloudXR Runtime to stream Holoscan XR applications to XR devices like Apple Vision Pro. </p> <p>CloudXR enables you to run computationally intensive applications (medical volume rendering, AI inference) on powerful server hardware while delivering high-quality, low-latency XR experiences to XR headsets.</p> <p>What you'll accomplish: - Set up CloudXR Runtime as a bridge between Holoscan applications and XR devices - Configure Apple Vision Pro as an XR client - Successfully stream and experience immersive 3D content from XR applications to Apple Vision Pro</p> <p> </p>","tags":["XR","CloudXR","Apple Vision Pro"]},{"location":"tutorials/cloudxr_runtime_for_xr_applications/#table-of-contents","title":"Table of Contents","text":"<ul> <li>System Requirements</li> <li>Run CloudXR Runtime Container with Docker</li> <li>Run Your XR Application</li> <li>Set up Apple Vision Pro</li> <li>Troubleshooting</li> </ul>","tags":["XR","CloudXR","Apple Vision Pro"]},{"location":"tutorials/cloudxr_runtime_for_xr_applications/#system-requirements","title":"System Requirements","text":"<ul> <li>A Linux host system (with an NVIDIA GPU) to run HoloHub XR applications and the CloudXR Runtime.</li> <li>For Apple Vision Pro and Mac setup instructions and requirements, please refer to:   https://isaac-sim.github.io/IsaacLab/main/source/how-to/cloudxr_teleoperation.html#system-requirements</li> </ul> <p>Note: The Mac is only required for a one-time setup to build and install the CloudXR client app for Apple Vision Pro.</p>","tags":["XR","CloudXR","Apple Vision Pro"]},{"location":"tutorials/cloudxr_runtime_for_xr_applications/#run-cloudxr-runtime-container-with-docker","title":"Run CloudXR Runtime Container with Docker","text":"<p>The CloudXR Runtime runs in a Docker container on the same machine as your application, serving as a bidirectional bridge that streams sensor data from the XR device to your application and display content back to the XR device.</p>","tags":["XR","CloudXR","Apple Vision Pro"]},{"location":"tutorials/cloudxr_runtime_for_xr_applications/#step-1-configure-firewall","title":"Step 1: Configure Firewall","text":"<p>Ensure your firewall allows connections to the ports used by CloudXR:</p> <pre><code>sudo ufw allow 47998:48000,48005,48008,48012/udp\nsudo ufw allow 48010/tcp\n</code></pre>","tags":["XR","CloudXR","Apple Vision Pro"]},{"location":"tutorials/cloudxr_runtime_for_xr_applications/#step-2-create-shared-directory","title":"Step 2: Create Shared Directory","text":"<p>Create a shared directory for communication between XR applications and the CloudXR Runtime:</p> <pre><code>mkdir -p $(pwd)/openxr\n</code></pre> <p>This directory will be used for temporary cache files and runtime communication.</p>","tags":["XR","CloudXR","Apple Vision Pro"]},{"location":"tutorials/cloudxr_runtime_for_xr_applications/#step-3-start-cloudxr-runtime-container","title":"Step 3: Start CloudXR Runtime Container","text":"<p>Launch the CloudXR Runtime container, mounting the shared directory:</p> <pre><code>docker run -it --rm --name cloudxr-runtime \\\n    --user $(id -u):$(id -g) \\\n    --gpus=all \\\n    -e \"ACCEPT_EULA=Y\" \\\n    --mount type=bind,src=$(pwd)/openxr,dst=/openxr \\\n    -p 48010:48010 \\\n    -p 47998:47998/udp \\\n    -p 47999:47999/udp \\\n    -p 48000:48000/udp \\\n    -p 48005:48005/udp \\\n    -p 48008:48008/udp \\\n    -p 48012:48012/udp \\\n    nvcr.io/nvidia/cloudxr-runtime:5.0.1\n</code></pre> <p>The container should start and wait for client connections. Example log output when CloudXR Runtime starts successfully: <pre><code>INFO [logServerInfo] Created CloudXR\u2122 Service\n    version: 5.0.1\n    logFile: /tmp/cxr_server.2025-08-04T195921Z.log\n    tag: 5.0.1\n    uses: Monado\u2122\n\nFurther logging is now being redirected to the file: `/tmp/cxr_streamsdk.2025-08-04T195921Z.000.log`\n(Error messages will appear both on the console and in that file.)\n</code></pre></p>","tags":["XR","CloudXR","Apple Vision Pro"]},{"location":"tutorials/cloudxr_runtime_for_xr_applications/#run-your-xr-application","title":"Run Your XR Application","text":"<p>This tutorial can work across XR applications in holohub, including volume_rendering_xr, xr_gsplat, and xr_holoviz.</p> <p>All these applications use OpenXR, an open standard that provides a unified API for XR development across different platforms and devices. CloudXR implements the OpenXR specification as a runtime, which allows any OpenXR-compatible application to work with CloudXR by configuring environment variables to redirect from local runtimes to CloudXR for remote streaming. For more information, visit the official OpenXR specification.</p>","tags":["XR","CloudXR","Apple Vision Pro"]},{"location":"tutorials/cloudxr_runtime_for_xr_applications/#quick-start","title":"Quick Start","text":"<p>You can build and run your chosen Holoscan XR application with CloudXR integration using this single command:</p> <pre><code>./holohub run &lt;app-name&gt; --docker-opts=\"-e XDG_RUNTIME_DIR=/workspace/holohub/openxr/run -e XR_RUNTIME_JSON=/workspace/holohub/openxr/share/openxr/1/openxr_cloudxr.json\"\n</code></pre> <p>The <code>holohub run</code> command automatically mounts the HoloHub directory by default, which makes the OpenXR sockets from <code>holohub/openxr</code> on the host available inside the application container. This is why we can directly access the CloudXR runtime configuration files that were created when we started the CloudXR Runtime container.</p> <p>The environment variables configure your application to use the CloudXR runtime instead of local OpenXR runtimes.</p> <p>Please refer to the specific application's README (for example, xr_gsplat) for any application-specific requirements or additional configuration options.</p>","tags":["XR","CloudXR","Apple Vision Pro"]},{"location":"tutorials/cloudxr_runtime_for_xr_applications/#set-up-apple-vision-pro","title":"Set up Apple Vision Pro","text":"","tags":["XR","CloudXR","Apple Vision Pro"]},{"location":"tutorials/cloudxr_runtime_for_xr_applications/#step-1-install-client-application","title":"Step 1: Install Client Application","text":"<ol> <li>Follow the instructions at isaac-xr-teleop-sample-client-apple to install the client app for Apple Vision Pro</li> <li>Build and deploy the app to your device using Xcode</li> </ol> <p>Note: The Isaac XR Teleop Sample Client app is primarily designed for teleoperation with Isaac applications. In this tutorial, we're using it solely as a CloudXR client to connect to and stream content from the host machine.</p>","tags":["XR","CloudXR","Apple Vision Pro"]},{"location":"tutorials/cloudxr_runtime_for_xr_applications/#step-2-connect-to-host","title":"Step 2: Connect to Host","text":"<ol> <li>Open the Isaac XR Teleop Sample Client on your Apple Vision Pro</li> <li>You should see a connection UI interface</li> <li>Enter the IP address of the host machine running your XR application</li> <li>Tap Connect</li> </ol>","tags":["XR","CloudXR","Apple Vision Pro"]},{"location":"tutorials/cloudxr_runtime_for_xr_applications/#step-3-experience-xr-content","title":"Step 3: Experience XR Content","text":"<p>Once connected successfully, you should see your 3D application content streaming live in the headset with full immersive experience.</p>","tags":["XR","CloudXR","Apple Vision Pro"]},{"location":"tutorials/cloudxr_runtime_for_xr_applications/#troubleshooting","title":"Troubleshooting","text":"","tags":["XR","CloudXR","Apple Vision Pro"]},{"location":"tutorials/cloudxr_runtime_for_xr_applications/#connection-issues","title":"Connection Issues","text":"<p>If you experience connection problems between CloudXR runtime and Apple Vision Pro, verify the following:</p> <p>Basic Checks: 1. CloudXR container is running on the host machine 2. After clicking <code>Connect</code> on Apple Vision Pro, check the CloudXR container logs for any error messages.</p> <p>Network Configuration: - Firewall settings: Ensure the host firewall is properly configured (see Step 1 above) - Network connectivity: Verify that Apple Vision Pro and the host machine are on the same network - IP address: Confirm the Apple Vision Pro teleop app is pointing to the correct host IP address</p>","tags":["XR","CloudXR","Apple Vision Pro"]},{"location":"tutorials/cloudxr_runtime_for_xr_applications/#common-error-messages","title":"Common Error Messages","text":"","tags":["XR","CloudXR","Apple Vision Pro"]},{"location":"tutorials/cloudxr_runtime_for_xr_applications/#out-of-buffer-space-when-trying-to-write-format-type-spacer","title":"\"Out of buffer space when trying to write format type spacer\"","text":"<ul> <li>Disconnect and reconnect WiFi on host machine</li> </ul>","tags":["XR","CloudXR","Apple Vision Pro"]},{"location":"tutorials/cloudxr_runtime_for_xr_applications/#references","title":"References","text":"<p>This tutorial is largely based on the Isaac Lab CloudXR Teleoperation Guide.</p>","tags":["XR","CloudXR","Apple Vision Pro"]},{"location":"tutorials/creating-multi-node-applications/","title":"Creating Multi Node Applications","text":"<p> Authors: Jin Li (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: October 9, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 2 - Trusted</p> <p>In this tutorial, we will walk through the process in two scenarios of creating a multi node application from existing applications. We will demonstrate the process with Python applications but it's similar for C++ applications as well.</p> <ol> <li> <p>When we would want to divide an application that was previously running on a single node into two fragments running on two nodes. This corresponds to use cases where we want to separate the compute and visualization workloads onto two different nodes, for example in the case of surgical robotics, the visualization node should be closest to the surgeon. For this purpose we choose the example of the <code>multiai_endoscopy</code> application.</p> </li> <li> <p>When we would want to connect and combine two previously independent applications into one application with two fragments. This corresponds to the use cases where we want to run time-critical task(s) on a node closest to the data stream, and non time-critical task(s) on another node that can have a bit more latency, for example saving the inbody video recording of the surgery to cloud can have a higher latency than the real-time visualization. For this purpose we choose the example of the <code>endoscopy_tool_tracking</code> application and the <code>endoscopy_out_of_body_detection</code> application.</p> </li> </ol> <p>The SDK documentation on Creating a Distributed Application contains the necessary core concepts and description for distributed applications, please familiarize yourself with the documentation before proceeding to this tutorial.</p> <p>Please also see two SDK examples ping_distributed and video_replayer_distributed on simple examples of creating distributed applications.</p> <p>In this tutorial, we will focus on modifying existing applications you have created into distributed applications.</p>","tags":["Networking and Distributed Computing","Distributed"]},{"location":"tutorials/creating-multi-node-applications/#scenario-1-divide-an-application-into-two-fragments","title":"Scenario 1 - Divide an application into two fragments","text":"<p>The <code>multiai_endoscopy</code> application has its app graph like below:</p> <p></p> <p>We will divide it into two fragments. The first fragment will include all operators excluding the visualizer and the second fragment will include the visualizer, as illustrated below:</p> <p></p>","tags":["Networking and Distributed Computing","Distributed"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-1-extra-imports","title":"Changes in scenario 1 - Extra Imports","text":"<p>To created a distributed application, we will need to import the Fragment object.  </p> <pre><code>from holoscan.core import Fragment\n</code></pre>","tags":["Networking and Distributed Computing","Distributed"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-1-changing-the-way-command-line-arguments-are-parsed","title":"Changes in scenario 1 - Changing the way command-line arguments are parsed","text":"<p>As seen in the documentation, it is recommended to parse user-defined arguments from the <code>argv ((C++/Python))</code> method/property of the application. To parse in user-defined command line arguments (such as <code>--data</code>, <code>--source</code>, <code>--labelfile</code> in this app), let's make sure to avoid arguments that are unique to the multi-fragment applications, such as  <code>--driver</code>, <code>--worker</code>, <code>--address</code>, <code>--worker-address</code>, <code>--fragments</code> (see the documentation for more details on using those arguments).  In the non-distributed application, we would have  <pre><code>if __name__ == \"__main__\":\n    parser = ArgumentParser(description=\"Multi-AI Detection Segmentation application.\")\n    parser.add_argument(...) # for the app config yaml file via --config \n    parser.add_argument(...) # for args needed in app init --source --data --labelfile\n    args = parser.parse_args()\n    # logic to define args (config_file, labelfile) needed to pass into application init and config\n    app = MultiAIDetectionSegmentation(source=args.source, data=args.data, labelfile=labelfile)\n    app.config(config_file)\n    app.run()\n</code></pre></p> <p>In the distributed application, we need to make the following changes mainly to <code>parser.parse_args</code>: <code>``python !5,6 if __name__ == \"__main__\":     parser = ArgumentParser(description=\"Multi-AI Detection Segmentation application.\")     parser.add_argument(...) # for the app config yaml file via --config      parser.add_argument(...) # for args needed in app init --source --data --labelfile     apps_argv = Application().argv # difference     args = parser.parse_args(apps_argv[1:]) # difference     # logic to define args (config_file, labelfile) needed to pass into application init and config     app = MultiAIDetectionSegmentation(source=args.source, data=args.data, labelfile=labelfile)     app.config(config_file)     app.run() <pre><code>### Changes in scenario 1 - Changing the application structure\nPreviously, we defined our non distributed applications with the `__init__()` and `compose()` methods. \n```python\nclass MultiAIDetectionSegmentation(Application):\n    def __init__(self, data, source=\"replayer\", labelfile=\"\"):\n        ...\n\n    def compose(self):\n        # define operators and add flow\n        ...\n</code></pre> Now we will define two fragments, and add and connect them in the application's</code>compose()` method: <pre><code>class Fragment1(Fragment):\n    # operators in fragment1 need the objects: sample_data_path, source, label_dict \n    def __init__(self, app, name, sample_data_path, source, label_dict):\n        super().__init__(app, name)\n        ...\n    def compose(self):\n        # define fragment1 operators\n        # add flow\n        ... \n\nclass Fragment2(Fragment):\n    # operators in fragment2 need the object: label_dict \n    def __init__(self, app, name, label_dict):\n        super().__init__(app, name)\n        ...\n    def compose(self):\n        # define the one operator in fragment2 (Holoviz)\n        # add operator\n        # no need to add_flow because there's only one operator in this fragment\n        ... \n\nclass MultiAIDetectionSegmentation(Application):\n    def __init__(self, data, source=\"replayer\", labelfile=\"\"):\n        super().__init__()\n        # set self.name\n        # get self.label_dict from labelfile,self.source, self.sample_data_path\n        ...\n\n    def compose(self):\n        # define the two fragments in this app: fragment1, fragment2\n        # pass in the objects needed to each fragment's operators when defining each fragment\n        # operators in fragment1 need the objects: sample_data_path, source, label_dict \n        fragment1 = Fragment1(self, name=\"fragment1\", source=self.source, sample_data_path=self.sample_data_path, label_dict=self.label_dict)\n        # operators in fragment2 need the object: label_dict \n        fragment2 = Fragment2(self, name=\"fragment2\", label_dict=self.label_dict)\n\n        # Connect the two fragments \n        # There are three connections between fragment1 and fragment2:\n        # (1) from the data source to Holoviz\n        source_output = self.source + \".video_buffer_output\" if self.source.lower() == \"aja\" else self.source + \".output\"\n        self.add_flow(fragment1, fragment2, {(source_output, \"holoviz.receivers\")})\n        # (2) from the detection postprocessor to Holoviz\n        self.add_flow(fragment1, fragment2, {(\"detection_postprocessor.out\" , \"holoviz.receivers\")})\n        # (3) from the segmentation postprocessor to Holoviz\n        self.add_flow(fragment1, fragment2, {(\"segmentation_postprocessor.out_tensor\" , \"holoviz.receivers\")})\n</code></pre></p>","tags":["Networking and Distributed Computing","Distributed"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-1-defining-objects-shared-among-fragments","title":"Changes in scenario 1 - Defining objects shared among fragments","text":"<p>When creating your fragments, first make a list of all the objects each fragment's operators will need. If there are objects that are needed across multiple fragments (such as <code>self.label_dict</code> in this case), before passing them into fragments in the application's <code>compose()</code> method, create such objects in the app's <code>__init()__</code> method ideally. In the non-distributed application, in the app's <code>compose()</code> method we define <code>label_dict</code> from <code>self.labelfile</code>, and continue using <code>label_dict</code> while composing the application. In the distributed application, we move the definition of <code>label_dict</code> from <code>self.labelfile</code> into the application's <code>__init()__</code> method, and refer to <code>self.label_dict</code> in the application's <code>compose()</code> method and each fragment's <code>__init__()</code>/<code>compose()</code> methods.</p> <p>Non distributed application: <pre><code>class MultiAIDetectionSegmentation(Application):\n    def compose(self):\n        # construct the labels dictionary if the commandline arg for labelfile isn't empty\n        label_dict = self.get_label_dict(self.labelfile)\n</code></pre></p> <p>Distributed application: <pre><code>class Fragment1(Fragment):\n    def __init__(self, app, name, sample_data_path, source, label_dict):\n        super().__init__(app, name)\n\n        self.source = source\n        self.label_dict = label_dict\n        self.sample_data_path = sample_data_path\n\n    def compose(self):\n        ...\n        # Use self.label_dict\n\nclass Fragment2(Fragment):\n    def __init__(self, app, name, label_dict):\n        super().__init__(app, name)\n\n        self.label_dict = label_dict\n\n    def compose(self):\n        ...\n        # Use self.label_dict\n\nclass MultiAIDetectionSegmentation(Application):\n    def __init__(self, data, source=\"replayer\", labelfile=\"\"):\n        ...\n        self.label_dict = self.get_label_dict(labelfile)\n    def compose(self):\n        fragment1 = Fragment1(self, name=\"fragment1\", source=self.source, sample_data_path=self.sample_data_path, label_dict=self.label_dict)\n        fragment2 = Fragment2(self, name=\"fragment2\", label_dict=self.label_dict)\n        ...\n</code></pre></p>","tags":["Networking and Distributed Computing","Distributed"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-1-adding-operators-to-app-graph","title":"Changes in scenario 1 - Adding Operators to App Graph","text":"<p>When composing a non-distributed application, operators are created in the <code>compose()</code> method, then added to the app graph one of two ways:    1. For applications with a single operator (rare), <code>add_operator()</code> should be called.    1. for applications with multiple operators, using <code>add_flow()</code> will take care of adding each operator to the app graph on top of connecting them. This applies to distributed applications as well: when composing multiple fragments, each of them are responsible for adding all their operators to the app graph in their <code>compose()</code> method. Calling <code>add_flow()</code> in the <code>compose()</code> method of the distributed application when connecting fragments together does not add the operators to the app graph. This is often relevant when breaking down a single fragment application in a multi fragments application for distributed use cases, as some fragments might end up owning a single operator, and the absence of <code>add_flow()</code> in that fragment should come with the addition of <code>add_operator()</code> instead.</p> <p>In the non distributed application: <pre><code>class MultiAIDetectionSegmentation(Application):\n    def compose(self):\n        # define operators\n        source = SourceClass(...)\n        detection_preprocessor = FormatConverterOp(...)\n        segmentation_preprocessor = FormatConverterOp(...)\n        multi_ai_inference = InferenceOp(...)\n        detection_postprocessor = DetectionPostprocessorOp(...)\n        segmentation_postprocessor = SegmentationPostprocessorOp(...)\n        holoviz = HolovizOp(...)\n\n        # add flow between operators\n        ...\n</code></pre></p> <p>In a distributed application: ```python #11-17 class Fragment1(Fragment):     def compose(self):         # define operators         source = SourceClass(...)         detection_preprocessor = FormatConverterOp(...)         segmentation_preprocessor = FormatConverterOp(...)         multi_ai_inference = InferenceOp(...)         detection_postprocessor = DetectionPostprocessorOp(...)         segmentation_postprocessor = SegmentationPostprocessorOp(...)</p> <pre><code>    # add flow between operators\n    ...\n</code></pre> <p>class Fragment2(Fragment):     def compose(self):         holoviz = HolovizOp(...)         self.add_operator(holoviz) <pre><code>### Changes in scenario 1 - Shared resources\nIn a non distributed application, there may be some shared resources defined in the application's `compose()` method, such as a `UnboundedAllocator` for various operators. When splitting the application into multiple fragments, remember to create those resources once for each fragment.\n\nNon distributed application:\n```python\nclass MultiAIDetectionSegmentation(Application):\n    def compose(self):\n        pool = UnboundedAllocator(self, name=\"pool\")\n</code></pre></p> <p>Distributed application: <pre><code>class Fragment1(Fragment):\n    def compose(self):\n        pool = UnboundedAllocator(self, name=\"pool\")\n        # ... operator definitions in fragment1 with parameter pool=pool,\n\nclass Fragment2(Fragment):\n    def compose(self):\n        pool = UnboundedAllocator(self, name=\"pool\")\n        # ... operator definitions in fragment2 with parameter pool=pool,\n</code></pre></p>","tags":["Networking and Distributed Computing","Distributed"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-1-running-the-application","title":"Changes in scenario 1 - Running the application","text":"<p>Previously for a non distributed application, the command to launch is <code>python3 multi_ai.py --data &lt;DATA_DIR&gt;</code>, now in the distributed application we will have the option to specify a few more things: <pre><code># To run fragment 1 on current node as driver and worker:\npython3 multi_ai.py --data /workspace/holohub/data/ --driver --worker --address &lt;node 1 IP address&gt;:&lt;port&gt; --fragments fragment1\n# To run fragment 2 on current node as worker:\npython3 multi_ai.py --data /workspace/holohub/data/  --worker --address &lt;node 1 IP address&gt;:&lt;port&gt; --fragments fragment2\n</code></pre></p> <p>For more details on the commandline arguments for multi fragment apps, see the documentation.</p> <p>To run the app we create in scenario 1, please see Running the Applications.</p>","tags":["Networking and Distributed Computing","Distributed"]},{"location":"tutorials/creating-multi-node-applications/#scenario-2-connect-two-applications-into-a-multi-node-application-with-two-fragments","title":"Scenario 2 - Connect two applications into a multi-node application with two fragments","text":"<p>In this scenario, we will combine the existing application endoscopy_tool_tracking and endoscopy_out_of_body_detection into a distributed application with 2 fragments.</p> <p>Since endoscopy_out_of_body_detection is implemented in C++, we will quickly implement the Python version of the app for our Fragment2.</p> <p>The app graph for <code>endoscopy_tool_tracking</code>:  The app graph for <code>endoscopy_out_of_body_detection</code>:  The distributed app graph we want to create: </p>","tags":["Networking and Distributed Computing","Distributed"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-2-extra-imports","title":"Changes in scenario 2  - Extra Imports","text":"<p>Similar to scenario 1, to created a distributed application, we will need to import the Fragment object. When combining two non distributed apps into a multi-fragment application, remember to import all prebuilt Holoscan operators needed in both fragments.</p> <pre><code>from holoscan.core import Fragment\nfrom holoscan.operators import (\n    AJASourceOp,\n    FormatConverterOp,\n    HolovizOp,\n    VideoStreamRecorderOp,\n    VideoStreamReplayerOp,\n    InferenceOp,\n    InferenceProcessorOp\n)\n</code></pre>","tags":["Networking and Distributed Computing","Distributed"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-2-changing-the-way-command-line-arguments-are-parsed","title":"Changes in scenario 2 - Changing the way command-line arguments are parsed","text":"<p>Similar to Changing the way command-line arguments are parse in scenario 1, instead of the following for the non distributed app: <pre><code>if __name__ == \"__main__\":\n    ...\n    args = parser.parse_args()\n    ...\n</code></pre> For the distributed app we will have: <pre><code>if __name__ == \"__main__\":\n    ...\n    apps_argv = Application().argv\n    args = parser.parse_args(apps_argv[1:])\n    ...\n</code></pre></p>","tags":["Networking and Distributed Computing","Distributed"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-2-modifying-non-distributed-applications-into-a-distributed-application","title":"Changes in scenario 2 - Modifying non-distributed application(s) into a distributed application","text":"<p>In the new distributed app, we will define the app graph in <code>endoscopy_tool_tracking</code> as the new app's fragment 1.  The non distributed app <code>endoscopy_tool_tracking</code> had its <code>__init__()</code> and <code>compose()</code> methods structured like following: <pre><code>class EndoscopyApp(Application):\n    def __init__(self, data, record_type=None, source=\"replayer\"):\n        super().__init__()\n        # set self.name\n        ...\n        # get parameters for app graph composition: \n        # self.record_type, self.source, self.sample_data_path \n        ...\n    def compose(self):\n        # Create operators including:\n        # source operator (video replayer, AJA, Yuan), \n        # format converters, recorder\n        # LSTMTensorRTInferenceOp, ToolTrackingPostprocessorOp\n        # HolovizOp\n        ...\n\n        # Add flow between operators\n        ...\n</code></pre></p> <p>We can define our fragment1 modified from <code>endoscopy_tool_tracking</code> app with the following structure.</p> <pre><code>class Fragment1(Fragment):\n    def __init__(self, app, name, sample_data_path, source, record_type):\n        super().__init__(app, name)\n\n        self.source = source\n        self.sample_data_path = sample_data_path\n        self.record_type = record_type\n    def compose(self):\n        # Create operators including:\n        # source operator (video replayer, AJA, Yuan), \n        # format converters, recorder\n        # LSTMTensorRTInferenceOp, ToolTrackingPostprocessorOp\n        # HolovizOp\n        ...\n\n        # Add flow between operators\n        ...\n</code></pre> <p>We will define the app graph in <code>endoscopy_out_of_body_detection</code> as the new app's fragment 2. <pre><code>class Fragment2(Fragment):\n    def __init__(self, app, name, source, model_path, record_type):\n        super().__init__(app, name)\n\n        self.source = source\n        self.record_type = record_type\n        self.model_path = model_path\n    def compose(self):\n\n        is_aja = self.source.lower() == \"aja\"\n\n        pool = UnboundedAllocator(self, name=\"fragment2_pool\")\n        in_dtype = \"rgba8888\" if is_aja else \"rgb888\"\n\n        out_of_body_preprocessor = FormatConverterOp(\n            self,\n            name=\"out_of_body_preprocessor\",\n            pool=pool,\n            in_dtype=in_dtype,\n            **self.kwargs(\"out_of_body_preprocessor\"),\n        )\n\n        model_path_map = {\"out_of_body\": os.path.join(self.model_path, \"out_of_body_detection.onnx\")}\n        for k, v in model_path_map.items():\n            if not os.path.exists(v):\n                raise RuntimeError(f\"Could not find model file: {v}\")\n        inference_kwargs = self.kwargs(\"out_of_body_inference\")\n        inference_kwargs[\"model_path_map\"] = model_path_map\n        out_of_body_inference = InferenceOp(\n            self,\n            name=\"out_of_body_inference\",\n            allocator=pool,\n            **inference_kwargs,\n        )\n        out_of_body_postprocessor = InferenceProcessorOp(\n            self,\n            name=\"out_of_body_postprocessor\",\n            allocator=pool,\n            disable_transmitter=True,\n            **self.kwargs(\"out_of_body_postprocessor\")\n        )\n\n        # add flow between operators\n        self.add_flow(out_of_body_preprocessor, out_of_body_inference, {(\"\", \"receivers\")})\n        self.add_flow(out_of_body_inference, out_of_body_postprocessor, {(\"transmitter\", \"receivers\")})\n</code></pre> We can then define our distributed application with the following structure. Notice how the objects <code>self.record_type</code>, <code>self.source</code>, and <code>self.sample_data_path</code> are passed into each fragment.</p> <pre><code>class EndoscopyDistributedApp(Application):\n    def __init__(self, data, record_type=None, source=\"replayer\"):\n        super().__init__()\n        # set self.name\n        ...\n        # get parameters for app graph composition: \n        # self.record_type, self.source, self.sample_data_path \n        ...\n    def compose(self):\n        is_aja = self.source.lower() == \"aja\"\n\n        fragment1 = Fragment1(self, \n                            name=\"fragment1\", \n                            source = self.source, \n                            sample_data_path=os.path.join(self.sample_data_path, \"endoscopy\"), \n                            record_type=self.record_type)\n        fragment2 = Fragment2(self, \n                            name=\"fragment2\", \n                            source = self.source, \n                            model_path=os.path.join(self.sample_data_path, \"endoscopy_out_of_body_detection\"),\n                            record_type = self.record_type)\n\n        self.add_flow(fragment1, fragment2, {(\"aja.video_buffer_output\" if is_aja else \"replayer.output\", \"out_of_body_preprocessor\")})\n</code></pre>","tags":["Networking and Distributed Computing","Distributed"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-2-combining-objects-needed-by-each-fragments-in-distributed-app-init-time","title":"Changes in scenario 2 - Combining objects needed by each fragments in distributed app init time","text":"<p>Note how the <code>__init__()</code> method in the distributed app structured above is now the place to get parameters for the app graph composition for both fragment1 and fragment 2. In this case, Fragment1 needs the following objects at <code>__init__()</code> time:  <pre><code>class Fragment1(Fragment):\n    def __init__(self, app, name, sample_data_path, source, record_type):\n        super().__init__(app, name)\n\n        self.source = source\n        self.sample_data_path = sample_data_path\n        self.record_type = record_type\n</code></pre> and Fragment 2 needs the following objects at <code>__init__()</code> time: <pre><code>class Fragment2(Fragment):\n    def __init__(self, app, name, source, model_path, record_type):\n        super().__init__(app, name)\n\n        self.source = source\n        self.record_type = record_type\n        self.model_path = model_path\n</code></pre> We need to make sure in the distributed app's <code>__init__()</code> method, we are creating the corresponding objects to pass in to each fragment's <code>__init__()</code> time.</p>","tags":["Networking and Distributed Computing","Distributed"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-2-configuration-file","title":"Changes in scenario 2 - Configuration File","text":"<p>If you had two <code>yaml</code> files for configuring each of the non distributed applications, now in the combined distributed application you will need to combine the content of both in a <code>yaml</code> file.</p>","tags":["Networking and Distributed Computing","Distributed"]},{"location":"tutorials/creating-multi-node-applications/#changes-in-scenario-2-running-the-application","title":"Changes in scenario 2 - Running the application","text":"<p>In the newly created distributed application we will launch the application like below: <pre><code># To run fragment 1 on current node as driver and worker:\npython3 /workspace/holohub/applications/distributed_app/python/endoscopy_tool_tracking.py --source replayer --data /workspace/holohub/data --driver --worker --fragments fragment1 --address &lt;node 1 IP address&gt;:&lt;port&gt;\n\n# To run fragment 2 on current node as worker:\npython3 /workspace/holohub/applications/distributed_app/python/endoscopy_tool_tracking.py --data /workspace/holohub/data --source replayer --worker --fragments fragment2 --address &lt;node 1 IP address&gt;:&lt;port&gt;\n\n# To run on a single node:\npython3 /workspace/holohub/applications/distributed_app/python/endoscopy_tool_tracking.py --source replayer --data /workspace/holohub/data --driver --worker --fragments fragment1,fragment2 \n</code></pre></p> <p>For more details on the commandline arguments for multi fragment apps, see the documentation.</p> <p>To run the app we create in scenario 2, please see Running the Applications.</p>","tags":["Networking and Distributed Computing","Distributed"]},{"location":"tutorials/creating-multi-node-applications/#configuration","title":"Configuration","text":"<p>You can run a distributed application across any combination of hardware that are compatible with the Holoscan SDK. Please see SDK Installation Prerequisites for a list of compatible hardware. </p> <p>If you would like the distributed application fragments to communicate through high speed ConnectX NICs, enable the ConnectX NIC for GPU Direct RDMA, for example the ConnectX-7 NIC on the IGX Orin Developer Kit, follow these instructions. Enabling ConnectX NICs could bring significant speedup to your distributed app connection, for example, the RJ45 ports on IGX Orin Developer Kit supports 1GbE while the QSFP28 ports connected to ConnectX-7 support up to 100 GbE.</p> <p>If connecting together two IGX Orin Developer Kits with dGPU, follow the above instructions on each devkit to enable GPU Direct RDMA through ConnectX-7, and make the hardware connection through the QSFP28 ports on the back panel. A QSFP-QSFP cable should be included in your devkit box alongside power cables etc in the inner small box. Use either one of the two port on the devkit, and make sure to find out the logical name of the port as detailed in the instructions.</p> <p></p> <p>On each machine, make sure to specify an address for the network logical name that has the QSFP cable connected, and when running your distributed application, make sure to specify that address for the <code>--driver</code> machine.</p>","tags":["Networking and Distributed Computing","Distributed"]},{"location":"tutorials/creating-multi-node-applications/#running-the-applications","title":"Running the Applications","text":"<p>Follow Container Build instructions to build and launch the HoloHub dev container.</p>","tags":["Networking and Distributed Computing","Distributed"]},{"location":"tutorials/creating-multi-node-applications/#scenario-1-application","title":"Scenario 1 Application","text":"<p>Before we start to launch the application, let's first run the original application. In the dev container, make sure to build and launch the <code>multiai_endoscopy</code> app, this will convert the downloaded ONNX model file into a TensorRT engine at first run. </p> <p>Build and run instructions may change in the future, please refer to the original application. <pre><code># on the node that runs fragment 1, or a node that runs the entire app\n./holohub build multiai_endoscopy\n./holohub run multiai_endoscopy --language python\n</code></pre> Now we're ready to launch the distributed application in scenario 1. <pre><code>cd scenario1/\n</code></pre> To launch the two fragments on separate nodes, launch fragment 1 on node 1 as the driver node with: <pre><code># with replayer as source:\npython3 multi_ai.py --source replayer --data /workspace/holohub/data/ --driver --worker --address &lt;node 1 IP address&gt;:&lt;port number&gt; --fragments fragment1\n# with AJA video capture card as source:\npython3 multi_ai.py --source aja  --data /workspace/holohub/data/ --driver --worker --address &lt;node 1 IP address&gt;:&lt;port number&gt; --fragments fragment1\n</code></pre> and launch fragment 2 with: <pre><code>python3 multi_ai.py --worker --address &lt;node 1 IP address&gt;:&lt;port number&gt; --fragments fragment2\n</code></pre> To launch the two fragments together on a single node, simply launch the application without specifying the additional parameters: <pre><code># with replayer as source:\npython3 multi_ai.py --source replayer --data /workspace/holohub/data/ \n# with AJA video capture card as source:\npython3 multi_ai.py --source aja  --data /workspace/holohub/data/ \n</code></pre></p>","tags":["Networking and Distributed Computing","Distributed"]},{"location":"tutorials/creating-multi-node-applications/#scenario-2-application","title":"Scenario 2 Application","text":"<p>Let's first run the original applications. In the dev container, make sure to build and launch the <code>endoscopy_tool_tracking</code> and <code>endoscopy_out_of_body_detection</code> apps, this will convert the downloaded ONNX models into TensorRT engines at first run. </p> <p>Build and run instructions may change in the future, please refer to the original applications. <pre><code># On the node that runs fragment 1 or a node that runs the entire app\n./holohub build endoscopy_tool_tracking\n./holohub run endoscopy_tool_tracking --language python\n\n# On the node that runs fragment 2 or a node that runs the entire app\n./holohub build endoscopy_out_of_body_detection\ncd build &amp;&amp; applications/endoscopy_out_of_body_detection/endoscopy_out_of_body_detection --data ../data/endoscopy_out_of_body_detection\n</code></pre> Now we're ready to launch the distributed application in scenario 2.  <pre><code># don't forget to do this on both machines\n# configure your PYTHONPATH environment variable \nexport PYTHONPATH=/opt/nvidia/holoscan/lib/cmake/holoscan/../../../python/lib:/workspace/holohub/build/python/lib\n# run in the build directory of Holohub in order to load extensions\ncd /workspace/holohub/build\n</code></pre></p> <p>To launch the two fragments on separate nodes, launch fragment 1 on node 1 as the driver node with: <pre><code># with replayer as source:\npython3 /workspace/holohub/tutorials/creating-multi-node-applications/scenario2/endoscopy_distributed_app.py --source replayer --data /workspace/holohub/data --driver --worker --fragments fragment1 --address &lt;node 1 IP address&gt;:&lt;port number&gt;\n# with AJA video capture card as source:\npython3 /workspace/holohub/tutorials/creating-multi-node-applications/scenario2/endoscopy_distributed_app.py --source aja --driver --worker --fragments fragment1 --address &lt;node 1 IP address&gt;:&lt;port number&gt;\n</code></pre> and launch fragment 2 with: <pre><code># with replayer as source:\npython3 /workspace/holohub/tutorials/creating-multi-node-applications/scenario2/endoscopy_distributed_app.py  --worker --fragments fragment2 --address &lt;node 1 IP address&gt;:&lt;port number&gt;\n# with AJA video capture card as source:\npython3 /workspace/holohub/tutorials/creating-multi-node-applications/scenario2/endoscopy_distributed_app.py  --source aja --worker --fragments fragment2 --address &lt;node 1 IP address&gt;:&lt;port number&gt;\n</code></pre> To launch the two fragments together on a single node, simply launch the application without specifying the additional parameters: <pre><code># with replayer as source:\npython3 /workspace/holohub/tutorials/creating-multi-node-applications/scenario2/endoscopy_distributed_app.py --source replayer --data /workspace/holohub/data --driver --worker --fragments fragment1,fragment2\n# with AJA video capture card as source:\npython3 /workspace/holohub/tutorials/creating-multi-node-applications/scenario2/endoscopy_distributed_app.py --source aja --driver --worker --fragments fragment1,fragment2 \n</code></pre></p>","tags":["Networking and Distributed Computing","Distributed"]},{"location":"tutorials/cuda_mps/","title":"CUDA MPS Tutorial for Holoscan Applications","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Last modified: October 9, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>CUDA MPS is NVIDIA's Multi-Process Service for CUDA applications. It allows multiple CUDA applications to share a single GPU, which can be useful for running more than one Holoscan application on a single machine featuring one or more GPUs. This tutorial describes the steps to enable CUDA MPS and demonstrate few performance benefits of using it.</p>","tags":["Optimization","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Steps to enable CUDA MPS</li> <li>Customization</li> <li>x86 System Performance</li> <li>IGX Orin<ol> <li>Model Benchmarking Application Setup</li> <li>Performance Benchmark Setup</li> <li>Performance Benefits on IGX Orin w/ Discrete GPU<ol> <li>Varying Number of Instances</li> <li>Varying Number of Parallel Inferences</li> </ol> </li> <li> IGX Orin w/ iGPU<ol> <li>MPS Setup on IGX-iGPU</li> <li>Performance Benefits on IGX Orin w/ Integrated GPU</li> </ol> </li> </ol> </li> </ol>","tags":["Optimization","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#steps-to-enable-cuda-mps","title":"Steps to enable CUDA MPS","text":"<p>Before enabling CUDA MPS, please check whether your system supports CUDA MPS.</p> <p>CUDA MPS can be enabled by running the <code>nvidia-cuda-mps-control -d</code> command and stopped by running  <code>echo quit | nvidia-cuda-mps-control</code> command. More control commands are described  here. </p> <p>CUDA MPS does not require any changes to an existing Holoscan application; even an already compiled application binary works as it is. Therefore, a Holoscan application can work with CUDA MPS without any  changes to its source code or binary. However, a machine learning model like a TRT engine file may need to be recompiled  for the first time after enabling CUDA MPS.</p> <p>We have included a helper script in this tutorial <code>start_mps_daemon.sh</code> to enable  CUDA MPS with necessary environment variables.</p> <pre><code>./start_mps_daemon.sh\n</code></pre>","tags":["Optimization","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#customization","title":"Customization","text":"<p>CUDA MPS provides many options to customize resource allocation for MPS clients. For example, it has an option to limit the maximum number of GPU threads that can  be used by every MPS client.  The <code>CUDA_MPS_ACTIVE_THREAD_PERCENTAGE</code> environment variable can be used to control this limit system-wide. This limit can also be configured by communicating the active thread percentage to the control daemon with <code>echo \"set_default_active_thread_percentage &lt;Thread Percentage&gt;\" | nvidia-cuda-mps-control</code>. Our <code>start_mps_daemon.sh</code> script takes this percentage as the first argument as well.</p> <pre><code>./start_mps_daemon.sh &lt;Active Thread Percentage&gt;\n</code></pre> <p>For different applications, one may want to set different limits on the number of GPU threads available to each of them. This can be done by setting the <code>CUDA_MPS_ACTIVE_THREAD_PERCENTAGE</code> environment variable separately for each application. It is elaborated in details here.</p> <p>There are other customizations available in CUDA MPS as well. Please refer to the CUDA MPS documentation to know more about them. Please note that concurrently running Holoscan applications may increase the GPU device memory footprint. Therefore, one needs to be careful about hitting the GPU memory size and potential delay due to page faults.</p> <p>CUDA MPS improves the performance for concurrently running Holoscan applications.  Since multiple applications can simultaneously execute more than one CUDA compute tasks with CUDA MPS, it can also improve the overall GPU utilization.</p>","tags":["Optimization","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#performance-benefits-on-x86-system","title":"Performance Benefits on x86 System","text":"<p>Note: Endoscopy Tool Tracking does not work with CUDA MPS after holohub/Holoscan-SDK-v2.6.0 due to the unavailability of CUDA dynamic parallelism implemented in this PR in CUDA MPS. In case endoscopy tool tracking needs to be tested with CUDA MPS, please use the <code>holoscan-sdk-v2.6.0</code> tag or earlier.</p> <p>Suppose, we want to run the endoscopy tool tracking and ultrasound segmentation applications concurrently on an x86 workstation with RTX A6000 GPU. The below table shows the maximum end-to-end latency performance without and with CUDA MPS, where the active thread percentage is set to 40\\% for each application. It demonstrates 18% and 50% improvement in the maximum end-to-end latency for the endoscopy tool tracking and ultrasound segmentation applications, respectively.</p> Application Without MPS (ms) With MPS (ms) Endoscopy Tool Tracking 115.38 94.20 Ultrasound Segmentation 121.48 60.94 <p>In another set of experiments, we concurrently run multiple instances of the endoscopy tool tracking application in different processes. We set the active thread percentage to be 20\\% for each MPS client. The below graph shows the maximum end-to-end latency with and without CUDA MPS. The experiment demonstrates up to 36% improvement with CUDA MPS.</p> <p></p> <p>Such experiments can easily be conducted with Holoscan Flow Benchmarking to retrieve various end-to-end latency performance metrics.</p>","tags":["Optimization","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#igx-orin","title":"IGX Orin","text":"<p>CUDA MPS is available on IGX Orin since CUDA 12.5. Please check you CUDA version and upgrade to CUDA 12.5+ to test CUDA MPS. We evaluate the benefits of MPS on IGX Orin with discrete and integrated GPUs. Please follow the steps outlined in Steps to enable CUDA MPS to start running the MPS server on IGX Orin. </p> <p>We use the model benchmarking application to demonstrate the benefits of CUDA MPS. In general, MPS improves performance by enabling multiple concurrent processes to share a CUDA context and scheduling resources. We show the benefits of using CUDA MPS along two dimensions: (a) increasing the workload per application instance (varying the number of parallel inferences for the same model) and (b) increasing the total number of instances. </p>","tags":["Optimization","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#model-benchmarking-application-setup","title":"Model Benchmarking Application Setup","text":"<p>Please follow the steps outlined in model benchmarking to ensure that the application builds and runs properly. </p> <p>Note that you need to run the video using v4l2loopback in a separate terminal while running the model benchmarking application.</p> <p>Make sure to change the device path in the <code>model_benchmarking/python/model_benchmarking.yaml</code> file to match the values you provided in the <code>modprobe</code> command when following the v4l2loopback instructions.</p>","tags":["Optimization","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#performance-benchmark-setup","title":"Performance Benchmark Setup","text":"<p>To gather performance metrics for the model benchmarking application, follow the steps outlined in Holoscan Flow Benchmarking. </p> <p>If you are running within a container, please complete Step-3 before launching the container</p> <p>We use the following steps:</p> <p>1. Patch Application:</p> <pre><code>./benchmarks/holoscan_flow_benchmarking/patch_application.sh model_benchmarking\n</code></pre> <p>2. Build Application for Benchmarking:</p> <pre><code>./holohub build --local model_benchmarking --language=python --configure-args=\"-DCMAKE_CXX_FLAGS=-I$PWD/benchmarks/holoscan_flow_benchmarking\"\n</code></pre> <p>3. Set Up V4l2Loopback Devices:</p> <p>i. Install <code>v4l2loopback</code> and <code>v4l2loopback</code>:</p> <pre><code>sudo apt-get install v4l2loopback-dkms ffmpeg\n</code></pre> <p>ii. Determine the number of instances you would like to benchmark and set that as the value of <code>devices</code>. Then, load the <code>v4l2loopback</code> kernel module on virtual devices <code>/dev/video[*]</code>. This enables each instance to get its input from a separate virtual device.</p> <p>Example: For 3 instances, the <code>v4l2loopback</code> kernel module can be loaded on <code>/dev/video1</code>, <code>/dev/video2</code> and <code>/dev/video3</code>:</p> <pre><code>sudo modprobe v4l2loopback devices=3 video_nr=1 max_buffers=4\n</code></pre> <p>Now open 3 separate terminals.</p> <p>In terminal-1, run: <pre><code>ffmpeg -stream_loop -1 -re -i /data/ultrasound_segmentation/ultrasound_256x256.avi -pix_fmt yuyv422 -f v4l2 /dev/video1\n</code></pre></p> <p>In terminal-2, run: <pre><code>ffmpeg -stream_loop -1 -re -i /data/ultrasound_segmentation/ultrasound_256x256.avi -pix_fmt yuyv422 -f v4l2 /dev/video2\n</code></pre></p> <p>In terminal-3, run: <pre><code>ffmpeg -stream_loop -1 -re -i /data/ultrasound_segmentation/ultrasound_256x256.avi -pix_fmt yuyv422 -f v4l2 /dev/video3\n</code></pre></p> <p>4. Benchmark Application:</p> <pre><code>python benchmarks/holoscan_flow_benchmarking/benchmark.py --run-command=\"python applications/model_benchmarking/python/model_benchmarking.py -l &lt;number of parallel inferences&gt; -i\"  --language python -i &lt;number of instances&gt; -r &lt;number of runs&gt; -m &lt;number of messages&gt; --sched greedy -d &lt;outputs folder&gt; -u\n</code></pre> <p>The command executes <code>&lt;number of runs&gt;</code> runs of <code>&lt;number of instances&gt;</code> instances of the model benchmarking application with <code>&lt;number of messages&gt;</code> messages. Each instance runs <code>&lt;number of parallel inferences&gt;</code> parallel model benchmarking inferences with no post-processing and visualization (<code>-i</code>).</p> <p>Please refer to Model benchmarking options and Holoscan flow benchmarking options for more information on the various command options.</p> <p>Example: After Step-3, to benchmark 3 instances for 10 runs with 1000 messages, run:</p> <pre><code>python benchmarks/holoscan_flow_benchmarking/benchmark.py --run-command=\"python applications/model_benchmarking/python/model_benchmarking.py -l 7 -i\"  --language python -i 3 -r 10 -m 1000 --sched greedy -d myoutputs -u`\n</code></pre>","tags":["Optimization","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#performance-benefits-on-igx-orin-w-discrete-gpu","title":"Performance Benefits on IGX Orin w/ Discrete GPU","text":"<p>We look at the performance benefits of MPS by varying the number of instances and number of inferences. We use the RTX A6000 GPU for our experiments. From our experiments, we observe that enabling MPS results in upto 12% improvement in maximum latency compared to the default setting.</p>","tags":["Optimization","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#varying-number-of-instances","title":"Varying Number of Instances","text":"<p>We fix the number of parallel inferences to 7, number of runs to 10 and number of messages to 1000 and vary the number of instances from 3 to 7 using the <code>-i</code> parameter. Please refer to Performance Benchmark Setup for benchmarking commands.</p> <p>The graph below shows the maximum end-to-end latency of model benchmarking application with and without CUDA MPS, where the active thread percentage was set to <code>80/(number of instances)</code>. For example, for 5 instances, we set the active thread percentage to <code>80/5 = 16</code>. By provisioning resources this way, we leave some resources idle in case a client should require to use it. Please refer to CUDA MPS Resource Provisioning for more details regarding this.</p> <p>The graph is missing a bar for the case of 7 instances and 7 parallel inferences as we were unable to get the baseline to execute. However, we were able to run when MPS was enabled, highlighting the advantage of using MPS for large workloads. We see that the maximum end-to-end latency improves when MPS is enabled and the improvement is more pronounced as the number of instances increases. This is because, as the number of concurrent processes increases, MPS confines CUDA workloads to a certain predefined set of SMs. MPS combines multiple CUDA contexts from multiple processes into one, while simultaneously running them together. It reduces the number of context switches and related inferences, resulting in improved GPU utilization.</p> <p>| Maximum end-to-end Latency | | :-------------------------:| .png)</p> <p>We also notice minor improvements in the 99.9<sup>th</sup> percentile latency and similar improvements in the 99<sup>th</sup> percentile latency.</p> 99.9<sup>th</sup> Percentile Latency 99<sup>th</sup> Percentile Latency .png) .png)","tags":["Optimization","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#varying-number-of-parallel-inferences","title":"Varying Number of Parallel Inferences","text":"<p>We vary the number of parallel inferences to show that MPS may not be beneficial if the workload is insufficient to offset the overhead of running the MPS server. The graph below shows the result of increasing the number of parallel inferences from 3 to 7 while the number of instances is constant. </p> <p>As the number of parallel inferences increases, so does the workload, and the benefit of MPS is more evident. However, when the workload is low, CUDA MPS may not be beneficial. </p> Maximum Latency for 5 Instances .png)","tags":["Optimization","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#igx-orin-w-integrated-gpu","title":"IGX Orin w/ Integrated GPU","text":"","tags":["Optimization","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#mps-setup-on-igx-igpu","title":"MPS Setup on IGX-iGPU","text":"<p>Note that we run all commands as root</p> <p>1. Please add cuda-12.5+ to <code>$PATH</code> and <code>$LD_LIBRARY_PATH</code></p> <p>If you have multiple CUDA installations, check it at <code>/usr/local/</code> directory.</p> <pre><code>echo $PATH\n/usr/local/cuda-12.6/compat:/usr/local/cuda-12.6/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin\n\necho $LD_LIBRARY_PATH\n/usr/local/cuda-12.6/compat/lib:/usr/local/cuda-12.6/compat:/usr/local/cuda-12.6/lib64:\n</code></pre> <p>2. Be sure to pass <code>-v /tmp/nvidia-mps:/tmp/nvidia-mps  -v /tmp/nvidia-log:/tmp/nvidia-log -v /usr/local/cuda-12.6:/usr/local/cuda-12.6</code> to <code>./holohub run</code> command to ensure that the container is connected to the MPS control and server</p> <p>Example: <pre><code>./holohub run --img holohub:v2.1 --docker-opts=\"-v /tmp/nvidia-mps:/tmp/nvidia-mps  -v /tmp/nvidia-log:/tmp/nvidia-log -v /usr/local/cuda-12.6:/usr/local/cuda-12.6\"\n</code></pre></p> <p>3. Inside the container, be sure to set the following environment variables: <pre><code>export CUDA_VISIBLE_DEVICES=0\nexport CUDA_MPS_PIPE_DIRECTORY=/tmp/nvidia-mps\nexport CUDA_MPS_LOG_DIRECTORY=/tmp/nvidia-log\n\nexport PATH=/usr/local/cuda-12.6/bin:$PATH\nexport PATH=/usr/local/cuda-12.6/compat:$PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.6/lib64:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.6/compat:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.6/compat/lib:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=/usr/lib/aarch64-linux-gnu/nvidia:$LD_LIBRARY_PATH\n</code></pre> Our <code>$PATH</code> and <code>$LD_LIBRARY_PATH</code> values inside the container are: <pre><code>echo $PATH\n/usr/local/cuda-12.6/bin:/opt/tensorrt/bin:/usr/local/mpi/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/ucx/bin:/opt/nvidia/holoscan/bin\n\necho $LD_LIBRARY_PATH\n/usr/local/cuda-12.6/compat/lib:/usr/local/cuda-12.6/compat:/usr/local/cuda-12.6/lib64:/usr/local/cuda/compat/lib.real:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/nvidia/holoscan/lib\n</code></pre></p> <p>4. Start MPS server and control <pre><code>sudo -i\nexport CUDA_MPS_ACTIVE_THREAD_PERCENTAGE=20\nnvidia-cuda-mps-control -d\n</code></pre></p> <p>5. After steps 1-4, follow the benchmark insructions to benchmark the application</p>","tags":["Optimization","CUDA","MPS"]},{"location":"tutorials/cuda_mps/#performance-benefits-on-igx-orin-w-integrated-gpu","title":"Performance Benefits on IGX Orin w/ Integrated GPU","text":"<p>We look at the performance benefits of MPS by varying the number of application instances. We run the model benchmarking application in a mode where the inputs are always available and being read from the disk with the video replayer operator. For every instance of the application, we run 1 inference (<code>-l 1</code>) as iGPU is a smaller GPU. In this experiment, we also oversubscribe the GPU to provide the instances more opportunity to utilize the available SMs (IGX Orin iGPU has 16 SMs).</p> <p>From our experiments, we observe that enabling MPS results in 22-50%, 13-49% and 6-37% improvement in maximum latency, 99.9 percentile latency and average latency, respectively. The graphs below capture the result. In the X-axis, number of instances is show to increase from 2 to 7, and the number in the parenthesis shows the number of SMs per instance enabled by <code>CUDA_ACTIVE_THREAD_PERCENTAGE</code>.</p> <p>| Maximum end-to-end Latency | | :-------------------------:| </p> 99.9 Percentile Latency Average Latency <p>We use different number of SMs for different instances to ensure the total number of SMs requested by all the instances exceed the number of available SMs.</p>","tags":["Optimization","CUDA","MPS"]},{"location":"tutorials/debugging/cli_debugging/","title":"Interactively Debugging a Holoscan Application","text":"<p> Authors: Tom Birdsong (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: October 9, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 2.2.0 Tested Holoscan SDK versions: 2.2.0, 2.3.0 Contribution metric: Level 1 - Highly Reliable</p> <p>Holoscan SDK is a platform for rapidly developing low-latency AI pipelines. As part of software development we often find it useful to inspect pipeline operations and data contexts during execution. This tutorial walks through a few common scenarios to illustrate how common command line interface tools can be used in debugging an application based on Holoscan SDK.</p>","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#tutorial-sections","title":"Tutorial Sections","text":"<ol> <li>Prerequisites</li> <li>Debugging a C++ or Python application with <code>gdb</code></li> <li>Debugging a Python application with <code>pdb</code></li> <li>Debugging Symbols for Legacy Holoscan SDK</li> <li>Logging</li> </ol>","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#background","title":"Background","text":"","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#what-is-debugging","title":"What is Debugging?","text":"<p>Software debugging is the process of identifying and rectifying issues in software. - Just-in-time debugging lets us pause execution and inspect the state while a program is running.   For a C++ library such as Holoscan SDK, we rely on debugging symbols generated by the compiler   at runtime to map the execution state back to human-readable source code, which lets us understand what the program was doing when paused. Tools such as <code>gdb</code> and <code>Visual Studio Code</code> allow us to manage application   execution by setting breakpoints and watches, as well as inspect the program state such as local variable   values when paused. - Logging is one process through which we record events that occur during real-time execution of   a pipeline without interrupting the flow of execution. Log messages can provide a view of the application   state and flow that is limited to the messages the developer chooses to log. Holoscan SDK supports   logging based on popular libraries such as <code>spdlog</code> and the Python <code>logging</code> module.</p> <p>In general, both logging and just-in-time debugging are useful tools for prototyping and general development, while logging is usually better suited for application deployment. In this guide, we focus on using just-in-time debugging tools to inspect applications built on Holoscan SDK.</p>","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#what-are-debugging-symbols","title":"What are debugging symbols?","text":"<p>Debugging symbols are generated by a C++ compiler at build time to provide additional information for application debugging such as file names and line numbers. Debugging symbols must be present for debugging tools to provide meaningful information to a developer when inspecting an application's execution state.</p> <p>Holoscan SDK uses the following build profiles in its <code>run</code> script: - <code>release</code>: Instructs the C++ compiler to optimize where possible and not generate debugging symbols. We use this build type to create minimum-footprint binaries such as those in the Holoscan SDK Debian package or Python wheel distributions. - <code>rel-debug</code>: Instructs the C++ compiler to optimize where possible and generate debugging symbols. We use this build type to create developer-friendly binaries packaged in the Holoscan SDK NGC containers. Debug symbols may occasionally have inaccuracies due to release optimizations. - <code>debug</code>: Instructs the C++ compiler to minimize optimizations and prioritize debugging symbol accuracy. As of v2.3, we do not release Holoscan SDK builds of this type, but we do provide a <code>run</code> script to help developers generate their own <code>debug</code> builds on demand.</p>","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#what-are-some-common-tools-i-can-use-for-debugging-my-application","title":"What are some common tools I can use for debugging my application?","text":"<p>There are a wide variety of free and/or open source software tools available for general C++ and Python debugging, including: - NVIDIA's debugging solutions, including NVIDIA NSight and CUDA-GDB; - The GNU project DeBugger (GDB); - The built-in Python Debugger (pdb) module; - Microsoft Visual Studio Code, with a wide variety of community extensions.</p> <p>In this tutorial we will focus on the <code>GDB</code> and <code>pdb</code> command line tools. These require minimal setup and can be run via a simple terminal without a dedicated display (\"headless\"). For advanced development we recommend reviewing Visual Studio Code Development Containers with custom launch profiles, with HoloHub support coming soon.</p>","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#references","title":"References","text":"<p>To get started with debugging your Holoscan SDK application, visit the Debugging user guide section for common topics, including: - Generating debug symbols with VSCode - Live debugging for C++ and Python applications - Inspecting application crashes - Profiling and code coverage</p> <p>Visit the Logging user guide section for a thorough overview on how to set up Holoscan SDK logging in your C++ or Python application.</p>","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#prerequisites","title":"Prerequisites","text":"<p>The steps for getting started with <code>gdb</code> depend on how you are consuming Holoscan SDK. - We encourage using Holoscan SDK containers from NGC for development and we take this approach for most of the tutorial. If you are using an NGC container for Holoscan SDK v2.3.0 or later, you already have access   to debug symbols and can get started right away. - If you are using an older Holoscan SDK container from NGC, or if you are consuming Holoscan SDK through another means such as Debian packages, Python wheels, or custom installation, you will need to build Holoscan SDK with debugging symbols in order to step through Holoscan SDK code during debugging. Jump to the Legacy Holoscan SDK section to get started.</p> <p>Review the HoloHub Prerequisites along with the Endoscopy Tool Tracking requirements C++, Python to get started before continuing.</p> <p>If you have previously built the Endoscopy Tool Tracking application, you should clear your build directory before proceeding. <pre><code>./holohub clear-cache\n</code></pre></p>","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#debugging-a-c-application-with-gdb","title":"Debugging a C++ Application with <code>gdb</code>","text":"","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#background_1","title":"Background","text":"<p>GDB (the GNU project DeBugger) is a widely used tool for headless just-in-time debugging of C++ applications. GDB distributions are available for most Holoscan SDK supported platforms and do not require a display to set up. Refer to the GDB User Manual to get started.</p>","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#getting-started","title":"Getting Started","text":"<p>For this tutorial we will debug the Endoscopy Tool Tracking application. The tutorial <code>debug_gdb.sh</code> script is a self-contained example that will build the C++ application and launch into a <code>gdb</code> debugging session.</p> <p>Run the script to get started: <pre><code>./tutorials/cli_debugging/debug_gdb.sh\n</code></pre></p> <p>The script runs through the following steps: 1. Builds the tutorial container environment with <code>gdb</code> based on the Holoscan SDK NGC container. 2. Builds the Endoscopy Tool Tracking application in the container environment. By default we use the   <code>debug</code> build mode to generate detailed debugging symbols for the Endoscopy Tool Tracking application.   Note that this does not regenerate build symbols for Holoscan SDK, which are already packaged in   Holoscan SDK binaries in <code>rel-debug</code> mode. 3. Launches the Endoscopy Tool Tracking application with <code>gdb</code>. This step prefixes the launch command   given by <code>./holohub run endoscopy_tool_tracking</code> to run with <code>gdb</code>. The command sets a few actions for <code>gdb</code> to take on startup:     - Sets a breakpoint in the <code>main</code> function of Endoscopy Tool Tracking;     - Runs the program with custom arguments until the breakpoint is hit;     - Sets a breakpoint in Holoscan SDK's <code>add_flow</code> function.</p> <p>At this point <code>gdb</code> enters an interactive session where we can inspect the Endoscopy Tool Tracking program state and advance execution.</p> <p>GDB can also be used to interactively debug C++ symbols underlying a Holoscan Python pipeline. Run the tutorial <code>debug_gdb.sh</code> script to inspect the Endoscopy Tool Tracking Python application:</p> <pre><code>./tutorials/cli_debugging/debug_gdb.sh debug python\n</code></pre> <p>When the <code>python</code> argument is provided, the script builds the Endoscopy Tool Tracking application with Python bindings and initiates debugging with GDB from the Python script entrypoint. Once symbols are loaded in GDB, we can set breakpoints and inspect the underlying state of Holoscan SDK C++ operator implementations at runtime.</p> <p>From this point we recommend referring to the GDB Manual or online tutorials to get started with interactive debugging commands.</p>","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#frequently-asked-questions-and-troubleshooting","title":"Frequently Asked Questions and Troubleshooting","text":"","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#how-can-i-verify-that-holoscan-sdk-debugging-symbols-have-been-loaded-in-gdb","title":"How can I verify that Holoscan SDK debugging symbols have been loaded in <code>gdb</code>?","text":"<p><code>gdb</code> loads debugging symbols for Holoscan SDK only when the application loads Holoscan SDK binaries. Before that time, we can set breakpoints in Holoscan SDK files, but <code>gdb</code> will not understand them yet.</p> <p>We can use <code>info sharedlibrary</code> to inspect the shared libraries that have been dynamically loaded for execution.</p> <pre><code>(gdb) info sharedlibrary\nFrom                To                  Syms Read   Shared Object Library\n0x00007ffff7fc5090  0x00007ffff7fee315  Yes         /lib64/ld-linux-x86-64.so.2\n0x00007ffff7ca3ba0  0x00007ffff7ec655d  Yes         /opt/nvidia/holoscan/lib/libholoscan_op_aja.so.2\n0x00007ffff7f90ac0  0x00007ffff7faee1f  Yes         /opt/nvidia/holoscan/lib/libholoscan_op_video_stream_replayer.so.2\n0x00007ffff7bd9810  0x00007ffff7bf4e3f  Yes         /opt/nvidia/holoscan/lib/libholoscan_op_video_stream_recorder.so.2\n...\n</code></pre> <p>We can use <code>info sources</code> to inspect the Holoscan SDK symbols are available. Note: Source paths are loaded from Holoscan SDK binaries and respect source file locations at the time the Holoscan SDK distribution was built. These paths may not reflect your filesystem if you have mounted <code>holoscan-sdk</code> somewhere other than <code>/workspace/holoscan-sdk</code>.</p> <pre><code>(gdb) info sources /workspace/holoscan-sdk/src/core\n/workspace/holoscan-sdk/src/core/application.cpp, /workspace/holoscan-sdk/src/core/services/generated/system_resource.grpc.pb.cc,\n/workspace/holoscan-sdk/src/core/services/generated/system_resource.pb.h, /workspace/holoscan-sdk/src/core/services/generated/system_resource.pb.cc,\n/workspace/holoscan-sdk/src/core/services/generated/result.grpc.pb.cc, /workspace/holoscan-sdk/src/core/services/generated/result.pb.h,\n...\n</code></pre>","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#how-can-i-pause-a-running-holoscan-sdk-application-for-debugging","title":"How can I pause a running Holoscan SDK application for debugging?","text":"<p>After launching the application with <code>gdb</code> as done in <code>debug_gdb.sh</code>, use <code>continue</code> to allow the application to run. Then, press <code>Ctrl+C</code> to force the application to pause when you want to enter interactive debugging. Use <code>backtrace</code> to view stack frames at the point where the application paused.</p>","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#how-can-i-manage-breakpoints-to-pause-the-application-sometime-in-the-future","title":"How can I manage breakpoints to pause the application sometime in the future?","text":"<ol> <li>To add a breakpoint <code>in holoscan/operators/format_converter/format_converter</code>: <pre><code>(gdb) break /workspace/holoscan-sdk/src/operators/format_converter/format_converter.cpp:compute\"\n</code></pre></li> <li>To list all current breakpoints: <pre><code>(gdb) info breakpoints\n</code></pre></li> <li>To remove breakpoints: <pre><code>(gdb) delete &lt;number(s) of breakpoint(s)&gt;\n</code></pre></li> </ol>","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#how-can-i-inspect-local-variables","title":"How can I inspect local variables?","text":"<p>Use the <code>info</code> command to inspect the values of local variables.</p> <pre><code>(gdb) info locals\n</code></pre>","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#how-can-i-attach-to-a-running-holoscan-sdk-session","title":"How can I attach to a running Holoscan SDK session?","text":"<p>Do the following to attach to a HoloHub application (C++ or Python):</p> <ol> <li> <p>Launch the container with root permissions and start the process in the background: <pre><code>./holohub run-container --img holohub:debugging --docker-opts=\"-u root\"\n\n# Run inside the container\n&gt;&gt;&gt; ./holohub run endoscopy_tool_tracking &amp;\n</code></pre></p> </li> <li> <p>Press <code>Ctrl+C</code> to return to your interactive shell</p> </li> <li> <p>Find the process ID (PID) of the running application: <pre><code># Inside the container\n&gt;&gt;&gt; ps -ef | grep endoscopy_tool_tracking\nuser+     292     203 28 13:17 pts/9    00:00:04 /workspace/holohub/build/endoscopy_tool_tracking/applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking --data /workspace/holohub/data/endoscopy\n</code></pre></p> </li> <li> <p>Attach to the running process with <code>gdb -p</code>: <pre><code># Inside the container\n&gt;&gt;&gt; gdb -p 292\n</code></pre></p> </li> </ol> <p>From this point you can use Ctrl+C to pause application execution, then use the interactive GDB console to set breakpoints and inspect application state as usual.</p>","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#i-see-a-gdb-python-exception-in-the-gdb-log","title":"I see a <code>gdb</code> Python Exception in the <code>gdb</code> log.","text":"<p><code>gdb</code> relies on a Python module for operations such as unwinding. If the Python module is not properly referenced in the container, you may see <code>gdb</code> errors appear in the console log such as the following: <pre><code>\"Python Exception &lt;class 'NameError'&gt;: Installation error: gdb._execute_unwinders function is missing\"\n</code></pre> To resolve the error, update your <code>PYTHONPATH</code> variable to include your GDB Python directory: <pre><code>PYTHONPATH=${PYTHONPATH}:/usr/share/gdb/python\n</code></pre></p>","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#debugging-a-python-application-with-pdb","title":"Debugging a Python application with <code>pdb</code>","text":"","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#background_2","title":"Background","text":"<p>The Python Debugger module is a built-in interactive debugging tool for Python programs. Similar to <code>gdb</code>, it supports setting breakpoints for interactive, headless, just-in-time debugging. You can use <code>pdb</code> to debug Holoscan SDK Python programs on any platform supported by Holoscan SDK.</p> <p>Holoscan SDK Python libraries serve as wrappers around Holoscan SDK C++ libraries. While <code>pdb</code> may load C++ symbols, it is not well suited for setting breakpoints or stepping into underlying C++ code. <code>pdb</code> is best suited for debugging Holoscan SDK operators whose implementation lies in a Python-native <code>compute</code> method.</p>","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#prerequisites_1","title":"Prerequisites","text":"<p>The <code>pdb</code> module is built in to modern Python versions. No additional installation is required.</p> <p>We will continue to use the Holoscan SDK v2.3 development container from NGC for this section, which includes pre-installed versions of Python and Holoscan SDK.</p>","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#getting-started_1","title":"Getting Started","text":"<p>We will continue to debug the Endoscopy Tool Tracking application. The tutorial <code>debug_pdb.sh</code> script is a self-contained example that will build the application and launch into a <code>pdb</code> debugging session.</p> <p>Run the script to get started: <pre><code>./tutorials/cli_debugging/debug_pdb.sh\n</code></pre></p> <p>The script runs through the following steps: 1. Builds the HoloHub container environment based on the Holoscan SDK NGC container. 2. Builds the Endoscopy Tool Tracking Python application in the container environment. By default we use the   <code>debug</code> build mode to generate detailed C++ debugging symbols for the Endoscopy Tool Tracking application. 3. Launches the Endoscopy Tool Tracking application with <code>pdb</code>, which is invoked via the Python interpreter: <pre><code>python3 -m pdb &lt;command --args ...&gt;\n</code></pre></p> <p>This command launches the Python version of the Endoscopy Tool Tracking application with a breakpoint set on the very first line. From this point you can set additional breakpoints, inspect application state, and control program execution.</p> <p>For instance, the following snippet sets a breakpoint and then inspects the value of <code>self.source.lower()</code> during pipeline setup in the app <code>compose</code> method: <pre><code>(Pdb) break endoscopy_tool_tracking.py:77\nBreakpoint 1 at /workspace/holohub/applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking.py:77\n(Pdb) continue\n...\n&gt; /workspace/holohub/applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking.py(77)compose()\n-&gt; if self.source.lower() == \"aja\":\n(Pdb) p self.source.lower()\n'replayer'\n(Pdb)\n...\n</code></pre></p> <p>You can also add a breakpoint directly in the <code>.py</code> source code before running a program by adding a <code>breakpoint()</code> statement.</p> <p>From here we recommend referring to <code>pdb</code> documentation for common commands and debugging strategies.</p>","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#debugging-symbols-for-legacy-holoscan-sdk-versions","title":"Debugging Symbols for Legacy Holoscan SDK Versions","text":"<p>Starting from Holoscan SDK v2.3, we package debugging symbols as part of the libraries distributed in NGC Holoscan Containers, with the goal of improving ease of development and debugging. This change comes at a small cost to size and performance of the Holoscan SDK binary distribution. But what about older versions of Holoscan SDK containers?</p> <p>If you are using a legacy Holoscan SDK container (earlier than v2.3) for your development, your container does not come with debugging symbols pre-packaged. However, you can rebuild the Holoscan SDK from its source code to enable interactive debugging. We provide utilities as part of Holoscan SDK open source code to help you generate debugging symbols that will allow you to use tools such as <code>gdb</code> and <code>pdb</code> with legacy Holoscan SDK.</p> <p>The following script provides the necessary steps to rebuild Holoscan SDK version with debugging symbols and then set up for debugging with <code>gdb</code>:</p> <pre><code>./tutorials/cli_debugging/debug_legacy.sh\n</code></pre> <p>The script does the following: 1. Builds the specified Holoscan SDK version with the specified build type in a temporary tutorial folder. Refer to   background discussion for an overview of the different build types. 2. Builds the Endoscopy Tool Tracking application against the custom Holoscan SDK debug build. 3. Runs the Endoscopy Tool Tracking application with <code>gdb</code> for interactive debugging.</p> <p>For convenience, the <code>debug_legacy.sh</code> script mounts your custom Holoscan SDK installation at <code>/opt/nvidia/holoscan</code>, the default library path in the Holoscan SDK container. This effectively hides the Holoscan SDK build otherwise distributed inside the container and instead makes your custom debugging build available to build downstream applications.</p>","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#frequently-asked-questions-and-troubleshooting_1","title":"Frequently Asked Questions and Troubleshooting","text":"","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#how-can-i-use-a-custom-container-path-for-my-holoscan-sdk-debugging-build-other-than-optnvidiaholoscan","title":"How can I use a custom container path for my Holoscan SDK debugging build other than <code>/opt/nvidia/holoscan</code>?","text":"<p>You can choose to mount your custom Holoscan SDK debugging build at another path in the container with the Docker <code>-v</code> option or the <code>holohub</code> script <code>--local-sdk-root</code> or <code>--mount-volume</code> options. If you are mounting your build at a custom path in the Holoscan SDK container for general development, consider the following details when building and debugging: - <code>LD_LIBRARY_PATH</code> is an environment variable with a list of locations to look up for dynamic loading. By default   the Holoscan SDK container sets <code>LD_LIBRARY_PATH</code> to include <code>/opt/nvidia/holoscan</code>, and then the HoloHub <code>run</code> script   sets it again when launching an application. Edit this variable and launch your application directly to load libraries   from your custom mount by default. - <code>RPATH</code> or <code>RUNPATH</code> is an ELF header field that embeds shared library lookup locations in an executable.   HoloHub applications set <code>RPATH</code> to include <code>/opt/nvidia/holoscan</code> by default. Edit the value of <code>CMAKE_INSTALL_RPATH</code> in  <code>CMakeLists.txt</code> to remove the reference to <code>/opt/nvidia/holoscan</code> or reference your preferred mount path.</p>","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#how-can-i-launch-the-tutorial-application","title":"How can I launch the tutorial application?","text":"<p>You can simply re-run the tutorial script to rebuild and relaunch the application:</p> <pre><code>./tutorials/cli_debugging/debug_legacy.sh\n</code></pre> <p>Alternatively, run the following to relaunch the application in the debugging container without rebuilding: <pre><code># Find the custom Holoscan SDK debugging build\nINSTALL_DIR=$(realpath $(find ./tutorials/cli_debugging/tmp -type d -name \"install-*\"))\n\n# Launch the debugging container\n./holohub run --docker-opts=\"-v $INSTALL_DIR:/opt/nvidia/holoscan --security-opt seccomp=unconfined\" --img holohub:debugging\n\n# Inside the container\n&gt;&gt;&gt; gdb -q \\\n    -ex \"break main\" \\\n    -ex \"run --data /workspace/holohub/data/endoscopy\" \\\n    -ex \"break /workspace/holoscan-sdk/src/core/application.cpp:add_flow\" \\\n    /workspace/holohub/build/endoscopy_tool_tracking/applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking\n</code></pre></p> <p>Refer to the <code>debug_legacy.sh</code> script for more details.</p>","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#logging","title":"Logging","text":"<p>Just-in-time debugging is not well suited to problems that require real-time performance analysis. Logging is usually the better choice to debug performance related issues in your Holoscan application.</p> <p>The Holoscan SDK User Guide Logging section presents a detailed overview of how to get started with logging from your application.</p>","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#logging-from-a-c-application","title":"Logging from a C++ application","text":"<p>C++ applications based on Holoscan should use the <code>HOLOSCAN_LOG_LEVEL</code> environment variable or <code>holoscan::set_log_level</code> function to set the global level of detail to log in the application. You can add inline macros such as <code>HOLOSCAN_LOG_INFO</code> AND <code>HOLOSCAN_LOG_TRACE</code> in your application code to print out log messages at runtime according to the current logging level of detail.</p> <pre><code>export HOLOSCAN_LOG_LEVEL=\"Debug\"\n./holohub run --local endoscopy_tool_tracking\n</code></pre>","tags":["Development","Debugging"]},{"location":"tutorials/debugging/cli_debugging/#logging-from-a-python-application","title":"Logging from a Python application","text":"<p>Python applications based on Holoscan should use the Python <code>logging</code> module. Holoscan observes the standard logging module interface with statements such as <code>logger.info</code> and <code>logger.debug</code>. Refer to the Python <code>logging</code> module for more information.</p>","tags":["Development","Debugging"]},{"location":"tutorials/debugging/holoscan_container_vscode/","title":"Holoscan SDK Visual Studio Code Dev Container Template","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Last modified: October 9, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.3.0 Tested Holoscan SDK versions: 2.3.0 Contribution metric: Level 1 - Highly Reliable</p>","tags":["Development","VS Code","Debugging"]},{"location":"tutorials/debugging/holoscan_container_vscode/#overview","title":"Overview","text":"<p>A Dev Container (short for Development Container) is a lightweight, isolated environment for developing software. It's a self-contained directory that contains all the dependencies and tools needed to develop a software project without polluting the host machine or interfering with other projects.</p> <p>This directory contains a pre-configured Dev Container designed for Holoscan SDK using the Holoscan NGC Container with Visual Studio Code. This Dev Container comes with the complete Holoscan SDK and source code, sample applications, and all the tools needed to build your next application using Holoscan SDK.</p> <p>Follow this step-by-step guide to get started!</p>","tags":["Development","VS Code","Debugging"]},{"location":"tutorials/debugging/holoscan_container_vscode/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker</li> <li>VS Code with the Dev Container Extension Pack</li> <li>Install Dev Container Extension Pack via command line     <pre><code>code --install-extension ms-vscode-remote.remote-containers\n</code></pre></li> <li>NVIDIA Container Toolkit</li> <li>NVIDIA CUDA Toolkit</li> <li>Holoscan SDK container 2.3 or later</li> </ul>","tags":["Development","VS Code","Debugging"]},{"location":"tutorials/debugging/holoscan_container_vscode/#dev-container-setup","title":"Dev Container Setup","text":"<ol> <li>Download everything in this directory to a folder of your choice. This folder can also be used to store your project(s) later. See Directory Structure.</li> <li>Open .devcontainer/devcontainer.json, find and change the value of <code>HOLOSCAN_SDK_IMAGE</code> to a version of Holoscan SDK container image you want to use.   \ud83d\udca1  Important: Holoscan SDK container v2.3 or later is required to step into Holoscan SDK source code in debug mode.</li> <li>Start VS Code, open the Command Palette by pressing <code>F1</code>, <code>Ctrl+Shift+P</code>, or navigating to <code>View &gt; Command Palette</code>, type to select <code>Dev Containers: Open Folder in Container...</code> and select the folder where you stored step 1.</li> <li>Wait for the Dev Container to start up. Building the container and installing all the required extensions will take a while. (Switch to the <code>Output</code> view and select the <code>Server</code> option from the dropdown to check the status of extension installations.)</li> </ol> <p>When everything is ready, you should see the following in the VS Code Explorer sidebar.</p> <p></p> <p>The <code>My Workspace</code> section contains all the files that you've copied. The <code>Holoscan SDK</code> section includes the Holoscan example applications from <code>~/examples</code>.</p>","tags":["Development","VS Code","Debugging"]},{"location":"tutorials/debugging/holoscan_container_vscode/#directory-structure","title":"Directory Structure","text":"<pre><code>/workspace\n   \u251c\u2500\u2500 .vscode/            # Visual Studio Code debugging configuration files\n   \u251c\u2500\u2500 holoscan-sdk/       # Holoscan SDK source code\n   \u2514\u2500\u2500 my/                 # Your workspace directory - this is the directory created and mounted from step 1 above.\n       \u251c\u2500\u2500 .devcontainer/  # Dev Container configurations\n       \u251c\u2500\u2500 src/            # A folder set up to store your project(s)\n       \u2514\u2500\u2500 README.md       # This file\n</code></pre> <p>\ud83d\udca1 Note: The Dev Container is configured with a default user account <code>holoscan</code> using user ID <code>1000</code> and group ID <code>1000</code> in the devcontainer.json file.   - To change the user name, find and replace <code>USERNAME</code> and <code>remoteUser</code>.  - To change the user ID, find and replace <code>USER_UID</code> and <code>userUid</code>.  - To change the group ID, find and replace <code>USER_GID</code> and <code>userGid</code>.</p>","tags":["Development","VS Code","Debugging"]},{"location":"tutorials/debugging/holoscan_container_vscode/#debugging-applications","title":"Debugging Applications","text":"","tags":["Development","VS Code","Debugging"]},{"location":"tutorials/debugging/holoscan_container_vscode/#debugging-a-c-application","title":"Debugging a C++ Application","text":"<p>\ud83d\udca1 Tip: Open this <code>README.md</code> file in the Dev Container will help you navigate to the linked files faster.      This file can be found inside the Dev Container under <code>/workspace/my/README.md</code>.</p> <p>This section will walk you through the steps to debug the Hello World application using VS Code.</p> <p>Open the hello_world.cpp file. (In VSCode, click on Explorer from the sidebar and expand the holoscan-sdk folder. Then, find and open <code>hello_world.cpp</code> under the <code>examples/hello_world/cpp</code> directory.) Let's put a breakpoint on this line:</p> <pre><code>auto app = holoscan::make_application&lt;HelloWorldApp&gt;();\n</code></pre> <p>Feel free to put more breakpoints wherever you want.</p> <p>Now, let's switch to the Run and Debug panel on the sidebar, and then click on the dropdown box to the right of the Start button.</p> <p>Select <code>(gdb) examples/hello_world/cpp</code> from the list of available launch configurations.</p> <p>Hit F5 on the keyboard or click the green arrow to start debugging. VS Code shall hit the breakpoint and stop as the screenshot shows below: </p> <p>\ud83d\udca1 Tip: What happens when you hit F5? VS Code looks up the launch profile for <code>(gdb) examples/hello_world/cpp</code> in the .vscode/launch.json file and starts the debugger with the appropriate configurations and arguments.</p>","tags":["Development","VS Code","Debugging"]},{"location":"tutorials/debugging/holoscan_container_vscode/#debugging-a-python-application","title":"Debugging a Python Application","text":"<p>There are a few options when debugging a Python application. In the .vscode/launch.json file, you may find the following options to debug the Hello World application:</p> <ul> <li>Python: Debug Current File: with this option selected, open hello_world.py file and hit F5. It shall stop at any breakpoints selected.</li> <li>Python C++ Debug: similar to the previous option, this launch profile allows you to debug both the Python and C++ code.   Open holoscan-sdk/src/core/application.cpp and find the <code>Application::run()</code> function. Let's put a breakpoint inside this function. Navigate back to the hello_world.py file and hit F5. When the debug session starts, it stops at the top of the main application file and brings up a prompt in the terminal asking for superuser access.</li> </ul> <pre><code>Superuser access is required to attach to a process. Attaching as superuser can potentially harm your computer. Do you want to continue? [y/N]\n</code></pre> <p>You may answer <code>Y</code> or <code>y</code> to continue the debug session. The debugger shall now stop at the breakpoint you've set in the <code>application.cpp</code> file.</p> <ul> <li>(gdb) examples/hello_world/python: this third launch profile option allows you to debug the C++ code only.  Put a breakpoint in the <code>Application::run()</code> function inside the holoscan-sdk/src/core/application.cpp file.</li> </ul> <p>\ud83d\udca1 Tip: you must open a Python file and make sure the file tab is active to debug when using the first two launch profiles.*</p>","tags":["Development","VS Code","Debugging"]},{"location":"tutorials/debugging/holoscan_container_vscode/#troubleshooting","title":"Troubleshooting","text":"","tags":["Development","VS Code","Debugging"]},{"location":"tutorials/debugging/holoscan_container_vscode/#cannot-start-debugger","title":"Cannot Start Debugger","text":"<ul> <li>Configured debug type 'cppdbg' is not supported.</li> <li>Configured debug type 'debugpy' is not supported.</li> <li>Configured debug type 'pythoncpp' is not supported.</li> <li>Configured debug type 'python' is not supported.</li> </ul> <p>If you encounter the above errors, please ensure all the required extensions are installed in VS Code. It may take a while to install them for the first time.</p>","tags":["Development","VS Code","Debugging"]},{"location":"tutorials/debugging/holoscan_container_vscode/#cannot-set-breakpoints","title":"Cannot Set Breakpoints","text":"<p>If you cannot set breakpoints, please ensure all the required extensions are installed in VS Code. It may take a while to install them for the first time.</p>","tags":["Development","VS Code","Debugging"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/","title":"Processing DICOM to USD with MONAI Deploy and Holoscan","text":"<p> Authors: Rahul Choudhury (NVIDIA), Andreas Heumann (NVIDIA), Cristiana Dinea (NVIDIA), Gregory Lee (NVIDIA), Jeroen Stinstra (NVIDIA), Ming Melvin Qi (NVIDIA), Tom Birdsong (NVIDIA), Wendell Hom (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 1.0.3 Contribution metric: Level 2 - Trusted</p> <p></p> <p>In this tutorial we demonstrate a method leveraging a combined MONAI Deploy and Holoscan pipeline to process DICOM input data and write a resulting mesh to disk in the OpenUSD file format.</p>","tags":["Interoperability","Medical Imaging","MONAI","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#demonstrated-technologies","title":"Demonstrated Technologies","text":"","tags":["Interoperability","Medical Imaging","MONAI","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#dicom","title":"DICOM","text":"<p>The Digital Imaging and Communications in Medicine (DICOM) standard is a comprehensive standard for medical imaging. DICOM covers a wide variety of medical imaging modalities and defines standards for both image storage and communications in medicine. DICOM data often represent 2D or 3D volumes or series of volumes.</p>","tags":["Interoperability","Medical Imaging","MONAI","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#openusd","title":"OpenUSD","text":"<p>Universal Scene Description (OpenUSD) is an extensible ecosystem of file formats, compositors, renderers, and other plugins for comprehensive 3D scene description.</p> <p>OpenUSD serves as the backbone of the NVIDIA Omniverse cloud computing platform. Omniverse includes a variety of applications such as USD Composer for viewing and manipulating OpenUSD scenes, with features such as: - State-of-the-art cloud rendering - Live collaborative scene editing - Multi-user mixed reality</p> <p>Download the NVIDIA Omniverse launcher to get started with Omniverse. See NVIDIA OpenUSD Tutorials for getting started with the OpenUSD Python libraries we use in this tutorial.</p>","tags":["Interoperability","Medical Imaging","MONAI","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#monai-deploy-holoscan-pipeline","title":"MONAI Deploy + Holoscan Pipeline","text":"<p>The MONAI Deploy App SDK provides a series of operators to load and select DICOM instances and then decode the pixel data into in-memory NumPy data objects. MONAI Deploy integrates seamlessly with Holoscan pipelines.</p> <ol> <li>The DICOM Loader operator parses a set of DICOM instance files, and loads them into a set of objects representing the logical hierarchical structure of DICOM Study, Series, and Instance. Key DICOM metadata is extracted, while the image pixel data is not loaded in memory or decoded.</li> <li>The DICOM Series Selector selects relevant DICOM Series, e.g, a MR T2 series, using simple configurable selection rules.</li> <li>The Series to Volume converter decodes and combines the pixel data of the instances to a 3D NumPy array with a set of metadata for spacing, orientation, etc.</li> <li>The MONAI Deploy AI Inference Operator accepts the 3D volume and runs AI inference to segment the region of interest, which in this case is pixels representing the spleen.</li> <li>The MONAI STL Conversion Operator converts the output label volume to a mesh in STL format.</li> <li>The Holoscan \"Send Mesh to USD\" operator writes the mesh to disk in the OpenUSD format.</li> </ol>","tags":["Interoperability","Medical Imaging","MONAI","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#requirements","title":"Requirements","text":"<p>Please review the HoloHub README to get started with HoloHub general requirements before continuing.</p>","tags":["Interoperability","Medical Imaging","MONAI","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#hardware","title":"Hardware","text":"<p>This tutorial may run on an <code>amd64</code> or <code>arm64</code> workstation. - For <code>amd64</code> we rely on <code>usd-core</code> Python wheels from PyPI for OpenUSD support. - For <code>arm64</code> we rely on NVIDIA Omniverse Python libraries for OpenUSD support.</p>","tags":["Interoperability","Medical Imaging","MONAI","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#host-software","title":"Host Software","text":"<p>This tutorial should run in a <code>docker</code> container:</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install docker\n</code></pre>","tags":["Interoperability","Medical Imaging","MONAI","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#building-the-tutorial-container","title":"Building the tutorial container","text":"<p>Run the command below from the top-level HoloHub directory to build the tutorial container on the host workstation:</p> <pre><code>export NGC_CONTAINER_IMAGE_PATH=\"nvcr.io/nvidia/clara-holoscan/holoscan:v1.0.3-dgpu\"\n./holohub build-container --docker-file tutorials/dicom_to_usd_with_monai_and_holoscan/Dockerfile --base-img ${NGC_CONTAINER_IMAGE_PATH} --img holohub:dicom-to-usd\n</code></pre>","tags":["Interoperability","Medical Imaging","MONAI","OpenUSD","STL"]},{"location":"tutorials/dicom_to_usd_with_monai_and_holoscan/#running-the-application","title":"Running the application","text":"<p>Run the commands below on the host workstation to launch the container and run the tutorial. The application will run the MONAI Deploy + Holoscan pipeline for AI segmentation and write results to the <code>.usd</code> output file. The mesh will also be available as a <code>mesh.stl</code> file on disk.</p> <pre><code>./holohub run-container --img holohub:dicom-to-usd # start the container\ncd ./tutorials/dicom_to_usd_with_monai_and_holoscan\npython tutorial.py --output ./output/spleen-segmentation.usd # run the tutorial\n</code></pre> <p>Download the NVIDIA Omniverse launcher to explore applications such as USD Composer for viewing and manipulating the OpenUSD output file.</p>","tags":["Interoperability","Medical Imaging","MONAI","OpenUSD","STL"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/","title":"Taking advantage of GPU Direct Storage on the latest NVIDIA Edge platform","text":"<p> Authors: Marcus Manos (NVIDIA) Supported platforms: aarch64 Language: Python Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.0.0 Tested Holoscan SDK versions: 2.1.0 Contribution metric: Level 4 - Experimental</p>","tags":["Interoperability","Optimization","GPUDirect"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#introduction","title":"Introduction","text":"<p>Modern-day edge accelerated computing solutions constantly push the boundaries of what is possible. The success of edge computing is mainly due to a new approach to accelerated computing problems, which are viewed from both a GPU and systems perspective. With innovations such as GPU Direct Storage (GDS) and GPU Direct Storage-over-Fabrics (GDS-OF), we can load data directly onto the GPU at lightning-fast speeds. GDS allows us to transform previously offline batch workloads into online streaming solutions, bringing them into the 21st century. This tutorial demonstrates this with a sample pipeline using the latest industrial-grade edge hardware (IGX) and A6000 workstation GPU. We\u2019ll start by providing an overview of the two main types of workflows, then discuss how to set up GDS, and finally, wrap up with an example application using Nvidia Holoscan and the Nvidia Rapids software suite.</p>","tags":["Interoperability","Optimization","GPUDirect"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#overview-of-gds-and-gds-of","title":"Overview of GDS and GDS-OF","text":"<p>To give a brief introduction to the GPU Direct Storage paradigm, we first need to understand the traditional file transfer pathway between the storage device and a GPU. Traditionally, to transfer a file from the storage device to the GPU, the CPU would create a temporary cache or bounce buffer out of the system memory (RAM). This buffer would hold data transferred from the storage device. Only after the data has been transferred to the bounce buffer or the buffer is full will the data transfer from the bounce buffer to the GPU. In the case of large file transfers to repetitive file transfers, this will result in increased latency. GPU Direct Storage sends the data directly from the storage device to the GPU without the need for the temporarily allocated CPU system memory to coordinate movement with the PCIE bus. For more information, please visit the blog below. -   Introduction to GPU Direct Storage</p> <p>Before diving into the technical details of setting up GDS, we will first cover the two prominent use cases of ultra-fast GDS. The first case is the consumer use case, those developers that will take advantage of the GDS to read data in real time. These cases include: - Running Digital Signal Processing models in real-time - Streaming Video data for live analytics - Streaming data from scientific instruments such as electron microscopes.</p>","tags":["Interoperability","Optimization","GPUDirect"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#producer-use-cases","title":"Producer Use Cases","text":"<p>The other GDS use case for developers includes producing or writing data to     a source. Examples of these workflows include: - Writing streams from multiple scientific equipment into a single stream. - Archiving raw or transformed data in real-time. - Writing data to a secondary workflow.</p>","tags":["Interoperability","Optimization","GPUDirect"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#overview-of-igx","title":"Overview of IGX","text":"<p>IGX is the latest generation of Industrial grade edge AI platform. It boasts an integrated 12-core ARM CPU and 2048 Cuda core + 64 Tensor Core integrated Ampere generation GPU. This powerful duo combines many other features, such as NVME support, RJ45 networking, and more, designed to optimize any edge AI workflow. The 2 PCIE slots connected to the ConnectX 7 chip onboard the IGX are critical. This ConnectX 7 chip uniquely facilitates GDS &amp; GDS-OF for the GPU and any PCI-E expansion card. For this reason, we added a PCIE riser card and compatible SSD to the second PCIE Gen5x8 slot. \u00a0  </p> <p></p>","tags":["Interoperability","Optimization","GPUDirect"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#setting-up-gds","title":"Setting Up GDS","text":"<p>As previously mentioned, we have two open slots onboard the IGX system. The first x16 slot is for our GPU, which will take advantage of all x16 lanes for communication. The second x8 slot, however, can be used for any PCIE expansion card; we chose to use a PCIE riser to add an NVME-compatible drive to take advantage of GDS. It\u2019s important to note that we couldn\u2019t use the existing M.2 NVME boot drive since it\u2019s not connected to the Connect 7 chip onboard the IGX.</p> <p></p>","tags":["Interoperability","Optimization","GPUDirect"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#hardware-requirements","title":"Hardware Requirements","text":"<p>The first step in this process is to obtain the correct hardware. In addition to the IGX system you have, you will also need a PCIE riser card and a compatible NVME SSD with the PCIE riser you chose (e.g., if you have a PCIE M.2 riser, then you\u2019ll need an M.2 NVME SSD).</p> <p></p>","tags":["Interoperability","Optimization","GPUDirect"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#software-installation","title":"Software Installation","text":"<p>The next step is to update the software inside your IGX. Note: You can skip this step if you have a new IGX system or one that has been freshly flashed. An explanation of each command will be available at the end of this post. * Install/update GCC and Linux Headers     * <code>sudo apt update</code> * Install MOFED drivers     * <code>wget --no-check-certificate https://content.mellanox.com/ofed/MLNX_OFED-24.01-0.3.3.1/MLNX_OFED_LINUX-24.01-0.3.3.1-ubuntu22.04-aarch64.iso</code>     * <code>mkdir /mnt/iso</code>     * <code>mount -o loop MLNX_OFED_LINUX-24.01-0.3.3.1-ubuntu22.04-x86_64.iso /mnt/iso</code>     * <code>/mnt/iso/mlnxofedinstall --with-nfsrdma --with-nvmf \u2013force</code>     * <code>update-initramfs -u -k $(uname -r)</code>     * <code>sudo reboot</code> * Install cuda toolkit, cuda drivers, and GPU drivers     * <code>sudo apt-get update</code>     * <code>sudo apt-get -y install cuda-toolkit-12-5</code>     * <code>sudo apt-get install -y nvidia-driver-535-open</code>     * <code>sudo apt-get install -y cuda-drivers-535</code></p> <p>After updating the essential software, follow the instructions below to install the Nvidia-GDS application and reboot your machine. * Install the nvidia-gds application     * <code>sudo apt-get install nvidia-gds</code> * Reboot     * <code>sudo reboot</code> * Mount a new directory to the SSD     * Find which SSD is the new one:         *   <code>Lsblk</code>     * Mount a compatible file system (In our case ext4) to the drive         * <code>sudo mount -t ext4 /dev/nvme1n1</code>         * <code>/mnt/nvme/ -o data=ordered</code></p> <p>Once you have completed the software setup process, there are some additional things you can do to ensure GDS is being used correctly:</p> <ul> <li>Modify your cufile.json only to be compatible with GDS file systems<ul> <li>Open <code>/usr/local/cuda/gds/cufile.json</code></li> <li>Change <code>\u201callow_compat_mode\u201d</code> to <code>\u201cfalse\u201d</code></li> </ul> </li> </ul> <p>After this, you can run several checks to ensure that GDS is working: * Run the gdscheck script     * <code>/usr/local/cuda/gds/tools/gdscheck -p</code> * Run the gdsio application to send sample data to the mounted directory     * <code>Sudo /usr/local/cuda/gds/tools/gdsio x 0 -i 1M -s 10M -d 0 -w 1 -I 1 -V -D /mnt/nvme</code></p>","tags":["Interoperability","Optimization","GPUDirect"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#example-application","title":"Example Application","text":"","tags":["Interoperability","Optimization","GPUDirect"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#using-nvidia-holoscan-and-rapids","title":"Using Nvidia Holoscan and RAPIDS","text":"<p>Kvikio is a part of the RAPIDS ecosystem of libraries. It gives you access to the GPU Direct Storage API, CuFile. Although you can access CuFile directly, Kvikio is a Python and C++ API that gives you easy access to the underlying CuFile functionality in a straightforward way. \u00a0</p> <p>Holoscan, a powerful sensor processing SDK, plays a crucial role in enabling scientists and engineers to harness accelerated computing for their needs. It provides the essential framework for hosting a scientific pipeline. In the example below, we demonstrate how Holoscan, in conjunction with CuPy and Kvikio, can be used to read electron microscope data and identify electrons in the image.</p> <p>Feel free to run the example application by either providing your own data, or using the data generation script. Once the data is generated you can run the application with the following command:  <code>python3 holoscan_gds.py</code></p>","tags":["Interoperability","Optimization","GPUDirect"]},{"location":"tutorials/gpu_direct_storage_on_holoscan/#conclusion","title":"Conclusion","text":"<p>GPU Direct Storage (GDS) is revolutionizing edge computing by enabling ultra-fast data transfer directly to GPUs. Following the steps outlined in this tutoril, you can set up GDS on your IGX system and leverage its real-time data processing capabilities. Try it out and see the difference it makes in your workflows!</p>","tags":["Interoperability","Optimization","GPUDirect"]},{"location":"tutorials/gui_for_python_applications/","title":"Adding a GUI to Holoscan Python Applications","text":"<p> Authors: Wendell Hom (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: October 9, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 1.0.3 Tested Holoscan SDK versions: 2.3.0 Contribution metric: Level 1 - Highly Reliable</p> <p>When developing Holoscan applications, incorporating a graphical user interface (GUI) can enhance usability and allow modification of the application's behavior at runtime.</p> <p>This tutorial demonstrates how GUI controls were integrated into the Florence-2 Python application using PySide6. This addition enables users to dynamically change the vision task performed by the application.</p> <p> </p>","tags":["Development","UI","Python"]},{"location":"tutorials/gui_for_python_applications/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview<ol> <li>Dockerfile</li> <li>Application Code</li> <li>GUI Code</li> </ol> </li> <li>Creating the GUI Widgets and Layout</li> <li>Starting the Holoscan Application Thread</li> <li>Adding a GUI to Your Own Application</li> </ol>","tags":["Development","UI","Python"]},{"location":"tutorials/gui_for_python_applications/#overview","title":"Overview","text":"<p>The Florence-2 application includes the typical components of a Holohub application, with the addition of a GUI component.  The main components are:</p> <ul> <li>Dockerfile: For installing additional dependencies.</li> <li>Application Code: Defines the Holoscan application and its operators.</li> <li>GUI Code: Utilizes PySide6 to add UI controls.</li> </ul>","tags":["Development","UI","Python"]},{"location":"tutorials/gui_for_python_applications/#dockerfile","title":"Dockerfile","text":"<p>The Dockerfile is used in Holohub when the application requires additional dependencies. For GUI functionality, this application's  Dockerfile installs:</p> <ul> <li><code>qt6-base-dev</code> for Qt6 framework</li> <li><code>PySide6</code> for Python bindings for Qt6 (as specified in requirements.txt)</li> </ul>","tags":["Development","UI","Python"]},{"location":"tutorials/gui_for_python_applications/#application-code","title":"Application Code","text":"<p>The Florence-2 application code is organized across several files:</p> <ul> <li><code>florence2_app.py</code>: Main application code.</li> <li><code>florence2_op.py</code>: Florence-2 model inference code.</li> <li><code>florence2_postprocessor_op.py</code>: Post-processing code to send overlays (e.g., bounding boxes, labels, segmentation masks) to Holoviz.</li> <li><code>config.yaml</code>: Default application parameters.</li> </ul> <p>The Florence-2 application can be run independently of the GUI code. E.g., the application can be run with <code>python application/florence-2-vision/florence2_app.py</code> inside the Florence-2 Docker container. This will run the application without the GUI controls.  The only code needed for GUI integration in the application code is the <code>set_parameters()</code> method in the <code>FlorenceApp</code> class. This method updates two fields in the Florence-2 operator:</p> <pre><code>class FlorenceApp(Application):\n    def set_parameters(self, task, prompt):\n        \"\"\"Set parameters for the Florence2Operator.\"\"\"\n        if self.florence_op:\n            self.florence_op.task = task\n            self.florence_op.prompt = prompt\n</code></pre> <p>These updated parameters are passed to the model during the next <code>compute()</code> method execution of the Florence-2 operator.</p>","tags":["Development","UI","Python"]},{"location":"tutorials/gui_for_python_applications/#gui-code","title":"GUI Code","text":"<p>The GUI code resides in qt_app.py. The code in this file defines a class for the main window which calls <code>setupUi()</code> and <code>runHoloscanApp()</code> when the instance is initialized.</p> <pre><code>class Window(QMainWindow):\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.setupUi()  # Setup the UI\n        self.runHoloscanApp()  # Run the Holoscan application\n</code></pre> <p>At a high level, this is all we need to launch a Python Holoscan application with a GUI. The <code>setupUi()</code> method defines the GUI widgets and layout, while <code>runHoloscanApp()</code> runs the Florence-2 application in a separate thread within the process. Details of these methods are explored in the following sections.</p>","tags":["Development","UI","Python"]},{"location":"tutorials/gui_for_python_applications/#creating-the-gui-widgets-and-layout","title":"Creating the GUI Widgets and Layout","text":"<p>The <code>setupUi()</code> method creates the GUI with a few simple widgets using PySide6 APIs.  For those unfamiliar with PySide6, this tutorial provides an introduction.</p> <pre><code>    def setupUi(self):\n        \"\"\"Setup the UI components.\"\"\"\n        self.setWindowTitle(\"Florence-2\")\n        self.resize(400, 150)\n        self.centralWidget = QWidget()\n        self.setCentralWidget(self.centralWidget)\n\n        layout = QVBoxLayout()\n\n        # Create and add dropdown for task selection\n        self.dropdown = QComboBox()\n        self.dropdown.addItems(\n            [\n                \"Object Detection\",\n                \"Caption\",\n                \"Detailed Caption\",\n                \"More Detailed Caption\",\n                \"Dense Region Caption\",\n                \"Region Proposal\",\n                \"Caption to Phrase Grounding\",\n                \"Referring Expression Segmentation\",\n                \"Open Vocabulary Detection\",\n                \"OCR\",\n                \"OCR with Region\",\n            ]\n        )\n        layout.addWidget(QLabel(\"Select an option:\"))\n        layout.addWidget(self.dropdown)\n\n        # Create and add text input for prompt\n        self.text_input = QLineEdit()\n        layout.addWidget(QLabel(\"Enter text:\"))\n        layout.addWidget(self.text_input)\n\n        # Create and add submit button\n        self.submit_button = QPushButton(\"Submit\")\n        self.submit_button.clicked.connect(self.on_submit)\n        layout.addWidget(self.submit_button)\n\n        self.centralWidget.setLayout(layout)\n</code></pre> <p>This code creates the following widgets:</p> <ul> <li>Drop-down Menu: Lists the vision tasks supported by Florence-2.</li> <li>Text input Widget: Allows text input for tasks such as Open Vocabulary Detection.</li> <li>Submit Button: Triggers the <code>on_submit()</code> method when clicked.</li> </ul> <p>When the application is running, the user selects a vision task, enters text (if  needed), and clicks \"Submit\" to change the task performed by the model. The <code>on_submit()</code> method is then invoked, calling the <code>set_parameters()</code> method  in the <code>FlorenceApp</code> class to update the operator's parameters.</p> <pre><code>    def on_submit(self):\n        \"\"\"Handle the submit button click event.\"\"\"\n        selected_option = self.dropdown.currentText()\n        entered_text = self.text_input.text()\n\n        # Set parameters in the Holoscan application\n        global gApp\n        if gApp:\n            gApp.set_parameters(selected_option, entered_text)\n</code></pre>","tags":["Development","UI","Python"]},{"location":"tutorials/gui_for_python_applications/#starting-the-holoscan-application-thread","title":"Starting the Holoscan Application Thread","text":"<p>The <code>runHoloscanApp()</code> method starts the Florence-2 application by creating an instance of <code>FlorenceWorker</code>  and running it in a thread.</p> <pre><code>    def runHoloscanApp(self):\n        \"\"\"Run the Holoscan application in a separate thread.\"\"\"\n        self.thread = QThread()\n        self.worker = FlorenceWorker()\n        self.worker.moveToThread(self.thread)\n        self.thread.started.connect(self.worker.run)\n        self.worker.finished.connect(self.thread.quit)\n        self.worker.finished.connect(self.worker.deleteLater)\n        self.thread.finished.connect(self.thread.deleteLater)\n        self.thread.start()\n</code></pre> <p>When the thread is started, it calls the <code>FlorenceWorker</code> class's <code>run()</code> method which creates and runs the Holoscan application.</p> <pre><code># Worker class to run the Holoscan application in a separate thread\nclass FlorenceWorker(QObject):\n    finished = Signal()  # Signal to indicate the worker has finished\n    progress = Signal(int)  # Signal to indicate progress (if needed)\n\n    def run(self):\n        \"\"\"Run the Holoscan application.\"\"\"\n        config_file = os.path.join(os.path.dirname(__file__), \"config.yaml\")\n        global gApp\n        gApp = app = FlorenceApp()\n        app.config(config_file)\n        app.run()\n</code></pre> <p>This covers the essential steps for creating a GUI to control your Python Holoscan applications.  To try out the application, follow the instructions provided here.</p>","tags":["Development","UI","Python"]},{"location":"tutorials/gui_for_python_applications/#adding-a-gui-to-your-own-application","title":"Adding a GUI to Your Own Application","text":"<p>To integrate a GUI into your Python application using PySide6, follow these steps:</p> <ol> <li>Ensure Qt and PySide6 dependencies are included in your Dockerfile. Verify that Qt and PySide6 package licenses meet your project requirements.</li> <li>Copy the <code>qt_app.py</code> file to your application directory.  Rename and modify the <code>FlorenceWorker</code> class to create an instance of your application. Update the import statement <code>from florence2_app import FlorenceApp</code> as necessary.</li> <li>Customize the <code>setupUi()</code> method to include the controls relevant to your application.</li> <li>Update <code>set_parameters()</code> methoed to reflect the parameters your application needs to update.</li> </ol>","tags":["Development","UI","Python"]},{"location":"tutorials/high_performance_networking/","title":"High Performance Networking with Holoscan","text":"<p> Authors: Alexis Girault (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: October 9, 2025 Version: latest (0.1.0) 0.1 <p> Minimum Holoscan SDK version: 3.0.0 Tested Holoscan SDK versions: 3.0.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This tutorial demonstrates how to use the Advanced Network library (referred to as <code>advanced_network</code> in HoloHub) for low latency and high throughput communication through NVIDIA SmartNICs. With a properly tuned system, the Advanced Network library can achieve hundreds of Gbps with latencies in the low microseconds.</p> <p>Note</p> <p>This solution is designed for users who want to create a Holoscan application that will interface with an external system or sensor over Ethernet.</p> <ul> <li>For high performance communication with systems also running Holoscan, refer to the Holoscan distributed application documentation instead.</li> <li>For JESD-compliant sensor without Ethernet support, consider the Holoscan Sensor Bridge for an FPGA-based interface to Holoscan.</li> </ul>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#prerequisites","title":"Prerequisites","text":"<p>Achieving High Performance Networking with Holoscan requires a system with an NVIDIA SmartNIC and a discrete GPU. That is the case of NVIDIA Data Center systems, or edge systems like the NVIDIA IGX platform and the NVIDIA Project DIGITS. <code>x86_64</code> systems equipped with these components are also supported, though the performance will vary greatly depending on the PCIe topology of the system (more on this below).</p> <p>In this tutorial, we will be developing on an NVIDIA IGX Orin platform with IGX SW 1.1 and an NVIDIA RTX 6000 ADA GPU, which is the configuration that is currently actively tested. The concepts should be applicable to other systems based on Ubuntu 22.04 as well. It should also work on other Linux distributions with a glibc version of 2.35 or higher by containerizing the dependencies and applications on top of an Ubuntu 22.04 image, but this is not actively tested at this time.</p> <p>Secure boot conflict</p> <p>If you have secure boot enabled on your system, you might need to disable it as a prerequisite to run some of the configurations below (switching the NIC link layers to Ethernet, updating the MRRS of your NIC ports, updating the BAR1 size of your GPU). Secure boot can be re-enabled after the configurations are completed.</p>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#background","title":"Background","text":"<p>Achieving high performance networking is a complex problem that involves many system components and configurations which we will cover in this tutorial. Two of the core concepts to achieve this are named Kernel Bypass, and GPUDirect.</p>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#kernel-bypass","title":"Kernel Bypass","text":"<p>In this context, Kernel Bypass refers to bypassing the operating system's kernel to directly communicate with the network interface (NIC), greatly reducing the latency and overhead of the Linux network stack. There are multiple technologies that achieve this in different fashions. They're all Ethernet-based, but differ in their implementation and features. The goal of the Advanced Network library in Holoscan Networking is to provide a common higher-level interface to all these backends:</p> <ul> <li>RDMA: Remote Direct Memory Access, using the open-source <code>rdma-core</code> library. It differs from the other Ethernet-based backends with its server/client model and RoCE (RDMA over Ethernet) protocol. Given the extra cost and complexity to setup on both ends, it offers a simpler user interface, orders packets on arrival, and is the only one to offer a high reliability mode.</li> <li>DPDK: the Data Plane Development Kit is an open-source project part of the Linux Foundation with a strong and long-lasting community support. Its RTE Flow capability is generally considered the most flexible solution to split packets ingress and egress data.</li> <li>DOCA GPUNetIO: This NVIDIA proprietary technology differs from the other backends by transmitting and receiving packets from the NIC using a GPU kernel instead of CPU code, which is highly beneficial for CPU-bound applications.</li> <li>NVIDIA Rivermax: NVIDIA's other proprietary kernel bypass technology. For a license fee, it should offer the lowest latency and lowest resource utilization for video streaming (RTP packets).</li> </ul> Work In Progress <p>The Holoscan Advanced Network library integration testing infrastructure is under active development. As such:</p> <ul> <li>The DPDK backend is supported and distributed with the <code>holoscan-networking</code> package, and is the only backend actively tested at this time.</li> <li>The DOCA GPUNetIO backend is supported and distributed with the <code>holoscan-networking</code> package, with testing infrastructure under development.</li> <li>The NVIDIA Rivermax backend is supported for Rx only when building from source, but not yet distributed nor actively tested. Tx support is under development.</li> <li>The RDMA backend is under active development and should be available soon.</li> </ul> <p>Which backend is best for your use case will depend on multiple factors, such as packet size, batch size, data type, and more. The goal of the Advanced Network library is to abstract the interface to these backends, allowing developers to focus on the application logic and experiment with different configurations to identify the best technology for their use case.</p>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#gpudirect","title":"GPUDirect","text":"<p><code>GPUDirect</code> allows the NIC to read and write data from/to a GPU without requiring to copy the data the system memory, decreasing CPU overheads and significantly reducing latency. An implementation of <code>GPUDirect</code> is supported by all the kernel bypass backends listed above.</p> <p>Warning</p> <p><code>GPUDirect</code> is only supported on Workstation/Quadro/RTX GPUs and Data Center GPUs. It is not supported on GeForce cards.</p> How does that relate to peermem or dma-buf? <p>There are two interfaces to enable <code>GPUDirect</code>:</p> <ul> <li>The <code>nvidia-peermem</code> kernel module, distributed with the NVIDIA DKMS GPU drivers.<ul> <li>Supported on Ubuntu kernels 5.4+, deprecated starting with kernel 6.8.</li> <li>Supported on NVIDIA optimized Linux kernels, including IGX OS and DGX OS.</li> <li>Supported by all MOFED drivers (requires rebuilding nvidia-dkms drivers afterwards).</li> </ul> </li> <li><code>DMA Buf</code>, supported on Linux kernels 5.12+ with NVIDIA open-source drivers 515+ and CUDA toolkit 11.7+.</li> </ul>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#1-installing-holoscan-networking","title":"1. Installing Holoscan Networking","text":"<p>We'll start with installing the <code>holoscan-networking</code> package, as it provides some utilities to help tune the system, and requires some dependencies which will help us with the system setup.</p> <p>First, add the DOCA apt repository which holds some of its dependencies:</p> IGX OS 1.1SBSA (Ubuntu 22.04)x86_64 (Ubuntu 22.04) <pre><code>export DOCA_URL=\"https://linux.mellanox.com/public/repo/doca/2.8.0/ubuntu22.04/arm64-sbsa/\"\nwget -qO- https://linux.mellanox.com/public/repo/doca/GPG-KEY-Mellanox.pub | gpg --dearmor - | sudo tee /etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub &gt; /dev/null\necho \"deb [signed-by=/etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub] $DOCA_URL ./\"  | sudo tee /etc/apt/sources.list.d/doca.list &gt; /dev/null\n\nsudo apt update\n</code></pre> <pre><code>export DOCA_URL=\"https://linux.mellanox.com/public/repo/doca/2.8.0/ubuntu22.04/arm64-sbsa/\"\nwget -qO- https://linux.mellanox.com/public/repo/doca/GPG-KEY-Mellanox.pub | gpg --dearmor - | sudo tee /etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub &gt; /dev/null\necho \"deb [signed-by=/etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub] $DOCA_URL ./\"  | sudo tee /etc/apt/sources.list.d/doca.list &gt; /dev/null\n\n# Also need the CUDA repository for holoscan: https://developer.nvidia.com/cuda-downloads?target_os=Linux\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\n\nsudo apt update\n</code></pre> <pre><code>export DOCA_URL=\"https://linux.mellanox.com/public/repo/doca/2.8.0/ubuntu22.04/x86_64/\"\nwget -qO- https://linux.mellanox.com/public/repo/doca/GPG-KEY-Mellanox.pub | gpg --dearmor - | sudo tee /etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub &gt; /dev/null\necho \"deb [signed-by=/etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub] $DOCA_URL ./\"  | sudo tee /etc/apt/sources.list.d/doca.list &gt; /dev/null\n\n# Also need the CUDA repository for holoscan: https://developer.nvidia.com/cuda-downloads?target_os=Linux\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\n\nsudo apt update\n</code></pre> <p>You can then install <code>holoscan-networking</code>:</p> Debian installationFrom source <pre><code>sudo apt install -y holoscan-networking\n</code></pre> <p>You can build the Holoscan Networking libraries and sample applications from source on HoloHub:</p> <pre><code>git clone git@github.com:nvidia-holoscan/holohub.git\ncd holohub\n./holohub install holoscan-networking   # Installed in ./install\n</code></pre> <p>If you'd like to generate the debian package from source and install it to ensure all dependencies are then present on your system, you can run:</p> <pre><code>./holohub install holoscan-networking\nsudo apt-get install ./holoscan-networking_*.deb        # Installed in /opt/nvidia/holoscan\n</code></pre> <p>Refer to the HoloHub README for more information.</p>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#2-required-system-setup","title":"2. Required System Setup","text":"","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#21-check-your-nic-drivers","title":"2.1 Check your NIC drivers","text":"<p>Ensure your NIC drivers are loaded:</p> <pre><code>lsmod | grep ib_core\n</code></pre> See an example output <p>This would be an expected output, where <code>ib_core</code> is listed on the left.</p> <pre><code>ib_core               442368  8 rdma_cm,ib_ipoib,iw_cm,ib_umad,rdma_ucm,ib_uverbs,mlx5_ib,ib_cm\nmlx_compat             20480  11 rdma_cm,ib_ipoib,mlxdevm,iw_cm,ib_umad,ib_core,rdma_ucm,ib_uverbs,mlx5_ib,ib_cm,mlx5_core\n</code></pre> <p>If this is empty, install the latest OFED drivers from DOCA (the DOCA APT repository should already be configured from the Holoscan Networking installation above), and reboot your system:</p> <pre><code>sudo apt update\nsudo apt install doca-ofed\nsudo reboot\n</code></pre> <p>Note</p> <p>If this is not empty, you can still install the newest OFED drivers from <code>doca-ofed</code> above. If you choose to keep your current drivers, install the following utilities for convenience later on. They include tools like <code>ibstat</code>, <code>ibv_devinfo</code>, <code>ibdev2netdev</code>, <code>mlxconfig</code>:</p> <pre><code>sudo apt update\nsudo apt install infiniband-diags ibverbs-utils mlnx-ofed-kernel-utils mft\n</code></pre> <p>Also upgrade the user space libraries to make sure your tools have all the symbols they need:</p> <pre><code>sudo apt install libibverbs1 librdmacm1 rdma-core\n</code></pre> <p>Running <code>ibstat</code> or <code>ibv_devinfo</code> will confirm your NIC interfaces are recognized by your drivers.</p>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#22-switch-your-nic-link-layers-to-ethernet","title":"2.2 Switch your NIC Link Layers to Ethernet","text":"<p>NVIDIA SmartNICs can function in two separate modes (called link layer):</p> <ul> <li>Ethernet (ETH)</li> <li>Infiniband (IB)</li> </ul> <p>To identify the current mode, run <code>ibstat</code> or <code>ibv_devinfo</code> and look for the <code>Link Layer</code> value.</p> <pre><code>ibv_devinfo\n</code></pre> Couldn't load driver 'libmlx5-rdmav34.so' <p>If you see an error like this, you might have different versions for your OFED tools and libraries. Attempt after upgrading your user space libraries to match the version of your OFED tools like so:</p> <pre><code>sudo apt update\nsudo apt install libibverbs1 librdmacm1 rdma-core\n</code></pre> See an example output <p>In the example below, the <code>mlx5_0</code> interface is in Ethernet mode, while the <code>mlx5_1</code> interface is in Infiniband mode. Do not pay attention to the <code>transport</code> value which is always <code>InfiniBand</code>.</p> <pre><code>hca_id: mlx5_0\n        transport:                      InfiniBand (0)\n        fw_ver:                         28.38.1002\n        node_guid:                      48b0:2d03:00f4:07fb\n        sys_image_guid:                 48b0:2d03:00f4:07fb\n        vendor_id:                      0x02c9\n        vendor_part_id:                 4129\n        hw_ver:                         0x0\n        board_id:                       NVD0000000033\n        phys_port_cnt:                  1\n                port:   1\n                        state:                  PORT_ACTIVE (4)\n                        max_mtu:                4096 (5)\n                        active_mtu:             4096 (5)\n                        sm_lid:                 0\n                        port_lid:               0\n                        port_lmc:               0x00\n                        link_layer:             Ethernet\n\nhca_id: mlx5_1\n        transport:                      InfiniBand (0)\n        fw_ver:                         28.38.1002\n        node_guid:                      48b0:2d03:00f4:07fc\n        sys_image_guid:                 48b0:2d03:00f4:07fb\n        vendor_id:                      0x02c9\n        vendor_part_id:                 4129\n        hw_ver:                         0x0\n        board_id:                       NVD0000000033\n        phys_port_cnt:                  1\n                port:   1\n                        state:                  PORT_ACTIVE (4)\n                        max_mtu:                4096 (5)\n                        active_mtu:             4096 (5)\n                        sm_lid:                 0\n                        port_lid:               0\n                        port_lmc:               0x00\n                        link_layer:             InfiniBand\n</code></pre> <p>For Holoscan Networking, we want the NIC to use the ETH link layer. To switch the link layer mode, there are two possible options:</p> <ol> <li>On IGX Orin developer kits, you can switch that setting through the BIOS: see IGX Orin documentation.</li> <li> <p>On any system with a NVIDIA NIC (including the IGX Orin developer kits), you can run the commands below from a terminal:</p> <ol> <li> <p>Identify the PCI address of your NVIDIA NIC</p> ibdev2netdevlspci <pre><code>nic_pci=$(sudo ibdev2netdev -v | awk '{print $1}' | head -n1)\n</code></pre> <pre><code># `0200` is the PCI-SIG class code for Ethernet controllers\n# `0207` is the PCI-SIG class code for Infiniband controllers\n# `15b3` is the Vendor ID for Mellanox\nnic_pci=$(lspci -n | awk '($2 == \"0200:\" || $2 == \"0207:\") &amp;&amp; $3 ~ /^15b3:/ {print $1; exit}')\n</code></pre> </li> <li> <p>Set both link layers to Ethernet. <code>LINK_TYPE_P1</code> and <code>LINK_TYPE_P2</code> are for <code>mlx5_0</code> and <code>mlx5_1</code> respectively. You can choose to only set one of them. <code>ETH</code> or <code>2</code> is Ethernet mode, and <code>IB</code> or <code>1</code> is for InfiniBand.</p> <pre><code>sudo mlxconfig -d $nic_pci set LINK_TYPE_P1=ETH LINK_TYPE_P2=ETH\n</code></pre> <p>Apply with <code>y</code>.</p> See an example output <pre><code>Device #1:\n----------\n\nDevice type:    ConnectX7\nName:           P3740-B0-QSFP_Ax\nDescription:    NVIDIA Prometheus P3740 ConnectX-7 VPI PCIe Switch Motherboard; 400Gb/s; dual-port QSFP; PCIe switch5.0 X8 SLOT0 ;X16 SLOT2; secure boot;\nDevice:         0005:03:00.0\n\nConfigurations:                                      Next Boot       New\n        LINK_TYPE_P1                                ETH(2)          ETH(2)\n        LINK_TYPE_P2                                IB(1)           ETH(2)\n\nApply new Configuration? (y/n) [n] :\ny\n\nApplying... Done!\n-I- Please reboot machine to load new configurations.\n</code></pre> <ul> <li><code>Next Boot</code> is the current value that was expected to be used at the next reboot.</li> <li><code>New</code> is the value you're about to set to override <code>Next Boot</code>.</li> </ul> ERROR: write counter to semaphore: Operation not permitted <p>Disable secure boot on your system ahead of changing the link type of your NIC ports. It can be re-enabled afterwards.</p> </li> <li> <p>Reboot your system.</p> <pre><code>sudo reboot\n</code></pre> </li> </ol> </li> </ol>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#23-configure-the-ip-addresses-of-the-nic-ports","title":"2.3 Configure the IP addresses of the NIC ports","text":"<p>First, we want to identify the logical names of your NIC interfaces. Connecting an SFP cable in just one of the ports of the NIC will help you identify which port is which. Run the following command once the cable is in place:</p> <pre><code>ibdev2netdev\n</code></pre> See an example output <p>In the example below, only <code>mlx5_1</code> has a cable connected (<code>Up</code>), and its logical ethernet name is <code>eth1</code>:</p> <pre><code>$ ibdev2netdev\nmlx5_0 port 1 ==&gt; eth0 (Down)\nmlx5_1 port 1 ==&gt; eth1 (Up)\n</code></pre> ibdev2netdev does not show the NIC <p>If you have a cable connected but it does not show Up/Down in the output of <code>ibdev2netdev</code>, you can try to parse the output of <code>dmesg</code> instead. The example below shows that <code>0005:03:00.1</code> is plugged, and that it is associated with <code>eth1</code>:</p> <pre><code>$ sudo dmesg | grep -w mlx5_core\n...\n[   11.512808] mlx5_core 0005:03:00.0 eth0: Link down\n[   11.640670] mlx5_core 0005:03:00.1 eth1: Link down\n...\n[ 3712.267103] mlx5_core 0005:03:00.1: Port module event: module 1, Cable plugged\n</code></pre> <p>The next step is to set a static IP on the interface you'd like to use so you can refer to it in your Holoscan applications. First, check if you already have any addresses configured using the ethernet interface names identified above (in our case, <code>eth0</code> and <code>eth1</code>):</p> <pre><code>ip -f inet addr show eth0\nip -f inet addr show eth1\n</code></pre> <p>If nothing appears, or you'd like to change the address, you can set an IP address through the Network Manager user interface, CLI (<code>nmcli</code>), or other IP configuration tools. In the example below, we configure the <code>eth0</code> interface with an address of <code>1.1.1.1/24</code>, and the <code>eth1</code> interface with an address of <code>2.2.2.2/24</code>.</p> One-timePersistent <pre><code>sudo ip addr add 1.1.1.1/24 dev eth0\nsudo ip addr add 2.2.2.2/24 dev eth1\n</code></pre> <p>Set these variables to your desired values:</p> <pre><code>if_name=eth0\nif_static_ip=1.1.1.1/24\n</code></pre> NetworkManagersystemd-networkd <p>Update the IP with <code>nmcli</code>:</p> <pre><code>sudo nmcli connection modify $if_name ipv4.addresses $if_static_ip\nsudo nmcli connection up $if_name\n</code></pre> <p>Create a network config file with the static IP:</p> <pre><code>cat &lt;&lt; EOF | sudo tee /etc/systemd/network/20-$if_name.network\n[Match]\nMACAddress=$(cat /sys/class/net/$if_name/address)\n\n[Network]\nAddress=$if_static_ip\nEOF\n</code></pre> <p>Apply now:</p> <pre><code>sudo systemctl restart systemd-networkd\n</code></pre> <p>Note</p> <p>If you are connecting the NIC to another NIC with an interconnect, do the same on the other system with an IP address on the same network segment. For example, to communicate with <code>1.1.1.1/24</code> above (<code>/24</code> -&gt; <code>255.255.255.0</code> submask), setup your other system with an IP between <code>1.1.1.2</code> and <code>1.1.1.254</code>, and the same <code>/24</code> submask.</p>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#24-enable-gpudirect","title":"2.4 Enable GPUDirect","text":"<p>Assuming you already have NVIDIA drivers installed, check if the <code>nvidia_peermem</code> kernel module is loaded:</p> tune_system.py Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check topo\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check topo\n</code></pre> See an example output <pre><code>2025-03-12 14:15:07 - INFO - GPU 0: NVIDIA RTX A6000 has GPUDirect support.\n2025-03-12 14:15:27 - INFO - nvidia-peermem module is loaded.\n</code></pre> <pre><code>lsmod | grep nvidia_peermem\n</code></pre> <p>If it's not loaded, run the following command, then check again:</p> One-timePersistent <pre><code>sudo modprobe nvidia_peermem\n</code></pre> <pre><code>sudo echo \"nvidia-peermem\" &gt;&gt; /etc/modules\nsudo systemctl restart systemd-modules-load.service\n</code></pre> Error loading the <code>nvidia-peermem</code> kernel module <p>If you run into an error loading the <code>nvidia-peermem</code> kernel module, follow these steps:</p> <ol> <li>Install the <code>doca-ofed</code> package to get the latest drivers for your NIC as documented above.</li> <li>Restart your system.</li> <li>Rebuild your NVIDIA drivers with DKMS like so:</li> </ol> <pre><code>peermem_ko=$(find /lib/modules/$(uname -r) -name \"*peermem*.ko\")\nnv_dkms=$(dpkg -S \"$peermem_ko\" | cut -d: -f1)\nsudo dpkg-reconfigure $nv_dkms\nsudo modprobe nvidia_peermem\n</code></pre> Why peermem and not dma buf? <p><code>peermem</code> is currently the only GPUDirect interface supported by all our networking backends. This section will therefore provide instructions for <code>peermem</code> and not <code>dma buf</code>.</p>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#3-optimal-system-configurations","title":"3. Optimal system configurations","text":"<p>Advanced</p> <p>The section below is for advanced users looking to extract more performance out of their system. You can choose to skip this section and return to it later if performance if your application is not satisfactory.</p> <p>While the configurations above are the minimum requirements to get a NIC and a NVIDIA GPU to communicate while bypassing the OS kernel stack, performance can be further improved in most scenarios by tuning the system as described below.</p> <p>Before diving in each of the setups below, we provide a utility script as part of the <code>holoscan-networking</code> package which provides an overview of the configurations that potentially need to be tuned on your system.</p> Work In Progress <p>This utility script is under active development and will be updated in future releases with additional checks, more actionable recommendations, and automated tuning.</p> Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check all\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check all\n</code></pre> See an example output <p>Our tuned-up IGX system with A6000 can optimize most settings:</p> <pre><code>2025-03-12 14:16:06 - INFO - CPU 0: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 1: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 2: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 3: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 4: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 5: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 6: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 7: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 8: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 9: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 10: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 11: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - cx7_0/0005:03:00.0: MRRS is correctly set to 4096.\n2025-03-12 14:16:06 - INFO - cx7_1/0005:03:00.1: MRRS is correctly set to 4096.\n2025-03-12 14:16:06 - WARNING - cx7_0/0005:03:00.0: PCIe Max Payload Size is not set to 256 bytes. Found: 128 bytes.\n2025-03-12 14:16:06 - WARNING - cx7_1/0005:03:00.1: PCIe Max Payload Size is not set to 256 bytes. Found: 128 bytes.\n2025-03-12 14:16:06 - INFO - HugePages_Total: 3\n2025-03-12 14:16:06 - INFO - HugePage Size: 1024.00 MB\n2025-03-12 14:16:06 - INFO - Total Allocated HugePage Memory: 3072.00 MB\n2025-03-12 14:16:06 - INFO - Hugepages are sufficiently allocated with at least 500 MB.\n2025-03-12 14:16:06 - INFO - GPU 0: SM Clock is correctly set to 1920 MHz (within 500 of the 2100 MHz theoretical Max).\n2025-03-12 14:16:06 - INFO - GPU 0: Memory Clock is correctly set to 8000 MHz.\n2025-03-12 14:16:06 - INFO - GPU 00000005:09:00.0: BAR1 size is 8192 MiB.\n2025-03-12 14:16:06 - INFO - GPU GPU0 has at least one PIX/PXB connection to a NIC\n2025-03-12 14:16:06 - INFO - isolcpus found in kernel boot line\n2025-03-12 14:16:06 - INFO - rcu_nocbs found in kernel boot line\n2025-03-12 14:16:06 - INFO - irqaffinity found in kernel boot line\n2025-03-12 14:16:06 - INFO - Interface cx7_0 has an acceptable MTU of 9000 bytes.\n2025-03-12 14:16:06 - INFO - Interface cx7_1 has an acceptable MTU of 9000 bytes.\n2025-03-12 14:16:06 - INFO - GPU 0: NVIDIA RTX A6000 has GPUDirect support.\n2025-03-12 14:16:06 - INFO - nvidia-peermem module is loaded.\n</code></pre> <p>Based on the results, you can figure out which of the sections below are appropriate to update configurations on your system.</p>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#31-ensure-ideal-pcie-topology","title":"3.1 Ensure ideal PCIe topology","text":"<p>Kernel bypass and GPUDirect rely on PCIe to communicate between the GPU and the NIC at high speeds. As-such, the topology of the PCIe tree on a system is critical to ensure optimal performance.</p> <p>Run the following command to check the GPUDirect communication matrix. You are looking for a <code>PXB</code> or <code>PIX</code> connection between the GPU and the NIC interfaces to get the best performance.</p> tune_system.pynvidia-smi Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check topo\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check topo\n</code></pre> See an example output <p>On IGX developer kits, the board's internal switch is designed to connect the GPU to the NIC interfaces with a <code>PXB</code> connection, offering great performance.</p> <pre><code>2025-03-06 12:07:45 - INFO - GPU GPU0 has at least one PIX/PXB connection to a NIC\n</code></pre> <pre><code>nvidia-smi topo -mp\n</code></pre> See an example output <p>On IGX developer kits, the board's internal switch is designed to connect the GPU to the NIC interfaces with a <code>PXB</code> connection, offering great performance. <pre><code>        GPU0    NIC0    NIC1    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PXB     PXB     0-11    0               N/A\nNIC0    PXB      X      PIX\nNIC1    PXB     PIX      X\n\nLegend:\n\nX    = Self\nSYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\nNODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\nPHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\nPXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\nPIX  = Connection traversing at most a single PCIe bridge\n\nNIC Legend:\n\nNIC0: mlx5_0\nNIC1: mlx5_1\n</code></pre></p> <p>If your connection is not optimal, you might be able to improve it by moving your NIC and/or GPU on a different PCIe port, so that they can share a branch and do not require going back to the Host Bridge (the CPU) to communicate. Refer to your system manufacturer for documentation, or run the following command to inspect the topology of your system:</p> <pre><code>lspci -tv\n</code></pre> See an example output <p>Here is the PCIe tree of an IGX system. Note how the ConnectX-7 and RTX A6000 are connected to the same branch. <pre><code>-+-[0007:00]---00.0-[01-ff]----00.0  Marvell Technology Group Ltd. 88SE9235 PCIe 2.0 x2 4-port SATA 6 Gb/s Controller\n+-[0005:00]---00.0-[01-ff]----00.0-[02-09]--+-00.0-[03]--+-00.0  Mellanox Technologies MT2910 Family [ConnectX-7]\n|                                           |            \\-00.1  Mellanox Technologies MT2910 Family [ConnectX-7]\n|                                           +-01.0-[04-06]----00.0-[05-06]----08.0-[06]--\n|                                           \\-02.0-[07-09]----00.0-[08-09]----00.0-[09]--+-00.0  NVIDIA Corporation GA102GL [RTX A6000]\n|                                                                                        \\-00.1  NVIDIA Corporation GA102 High Definition Audio Controller\n+-[0004:00]---00.0-[01-ff]----00.0  Sandisk Corp WD PC SN810 / Black SN850 NVMe SSD\n+-[0001:00]---00.0-[01-ff]----00.0-[02-fc]--+-01.0-[03-34]----00.0  Realtek Semiconductor Co., Ltd. RTL8111/8168/8411 PCI Express Gigabit Ethernet Controller\n|                                           +-02.0-[35-66]----00.0  Realtek Semiconductor Co., Ltd. RTL8111/8168/8411 PCI Express Gigabit Ethernet Controller\n|                                           +-03.0-[67-98]----00.0  Device 1c00:3450\n|                                           +-04.0-[99-ca]----00.0-[9a]--+-00.0  ASPEED Technology, Inc. ASPEED Graphics Family\n|                                           |                            \\-02.0  ASPEED Technology, Inc. Device 2603\n|                                           \\-05.0-[cb-fc]----00.0  Realtek Semiconductor Co., Ltd. RTL8822CE 802.11ac PCIe Wireless Network Adapter\n\\-[0000:00]-\n</code></pre></p> <p>x86_64 compatibility</p> <p>Most x86_64 systems are not designed for this topology as they lack a discrete PCIe switch. In that case, the best connection they can achieve is <code>NODE</code>.</p>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#32-check-the-nics-pcie-configuration","title":"3.2 Check the NIC's PCIe configuration","text":"<p>Understanding PCIe Configuration for Maximum Performance - May 27, 2022</p> <p>PCIe is used in any system for communication between different modules [including the NIC and the GPU]. This means that in order to process network traffic, the different devices communicating via the PCIe should be well configured. When connecting the network adapter to the PCIe, it auto-negotiates for the maximum capabilities supported between the network adapter and the CPU.</p> <p>The instructions below are meant to understand if your system is able to extract the maximum capabilities of your NIC, but they're not configurable. The two values that we are looking at here are the Max Payload Size (MPS - the maximum size of a PCIe packet) and the Speed (or PCIe generation).</p>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#max-payload-size-mps","title":"Max Payload Size (MPS)","text":"tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check mps\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check mps\n</code></pre> See an example output <p>The PCIe configuration on the IGX Orin developer kit is not able to leverage the max payload size of the NIC:</p> <pre><code>2025-03-10 16:15:54 - WARNING - cx7_0/0005:03:00.0: PCIe Max Payload Size is not set to 256 bytes. Found: 128 bytes.\n2025-03-10 16:15:54 - WARNING - cx7_1/0005:03:00.1: PCIe Max Payload Size is not set to 256 bytes. Found: 128 bytes.\n</code></pre> <p>Identify the PCIe address of your NVIDIA NIC:</p> ibdev2netdevlspci <pre><code>nic_pci=$(sudo ibdev2netdev -v | awk '{print $1}' | head -n1)\n</code></pre> <pre><code># `0200` is the PCI-SIG class code for NICs\n# `15b3` is the Vendor ID for Mellanox\nnic_pci=$(lspci -n | awk '$2 == \"0200:\" &amp;&amp; $3 ~ /^15b3:/ {print $1}' | head -n1)\n</code></pre> <p>Check current and max MPS:</p> <pre><code>sudo lspci -vv -s $nic_pci | awk '/DevCap/{s=1} /DevCtl/{s=0} /MaxPayload /{match($0, /MaxPayload [0-9]+/, m); if(s){print \"Max \" m[0]} else{print \"Current \" m[0]}}'\n</code></pre> See an example output <p>The PCIe configuration on the IGX Orin developer kit is not able to leverage the max payload size of the NIC:</p> <pre><code>Max MaxPayload 512\nCurrent MaxPayload 128\n</code></pre> <p>Note</p> <p>While your NIC might be capable of more, 256 bytes is generally the largest supported by any switch/CPU at this time.</p>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#pcie-speedgeneration","title":"PCIe Speed/Generation","text":"<p>Identify the PCIe address of your NVIDIA NIC:</p> ibdev2netdevlspci <pre><code>nic_pci=$(sudo ibdev2netdev -v | awk '{print $1}' | head -n1)\n</code></pre> <pre><code># `0200` is the PCI-SIG class code for NICs\n# `15b3` is the Vendor ID for Mellanox\nnic_pci=$(lspci -n | awk '$2 == \"0200:\" &amp;&amp; $3 ~ /^15b3:/ {print $1}' | head -n1)\n</code></pre> <p>Check current and max Speeds:</p> <pre><code>sudo lspci -vv -s $nic_pci | awk '/LnkCap/{s=1} /LnkSta/{s=0} /Speed /{match($0, /Speed [0-9]+GT\\/s/, m); if(s){print \"Max \" m[0]} else{print \"Current \" m[0]}}'\n</code></pre> See an example output <p>On IGX, the switch is able to maximize the NIC speed, both being PCIe 5.0:</p> <pre><code>Max Speed 32GT/s\nCurrent Speed 32GT/s\n</code></pre>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#33-maximize-the-nics-max-read-request-size-mrrs","title":"3.3 Maximize the NIC's Max Read Request Size (MRRS)","text":"<p>Understanding PCIe Configuration for Maximum Performance - May 27, 2022</p> <p>PCIe Max Read Request determines the maximal PCIe read request allowed. A PCIe device usually keeps track of the number of pending read requests due to having to prepare buffers for an incoming response. The size of the PCIe max read request may affect the number of pending requests (when using data fetch larger than the PCIe MTU).</p> <p>Unlike the PCIe properties queried in the previous section, the MRRS is configurable. We recommend maxing it to 4096 bytes. Run the following to check your current settings:</p> tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check mrrs\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check mrrs\n</code></pre> <p>Identify the PCIe address of your NVIDIA NIC:</p> ibdev2netdevlspci <pre><code>nic_pci=$(sudo ibdev2netdev -v | awk '{print $1}' | head -n1)\n</code></pre> <pre><code># `0200` is the PCI-SIG class code for NICs\n# `15b3` is the Vendor ID for Mellanox\nnic_pci=$(lspci -n | awk '$2 == \"0200:\" &amp;&amp; $3 ~ /^15b3:/ {print $1}' | head -n1)\n</code></pre> <p>Check current MRRS:</p> <pre><code>sudo lspci -vv -s $nic_pci | grep DevCtl: -A2 | grep -oE \"MaxReadReq [0-9]+\"\n</code></pre> <p>Update MRRS:</p> Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --set mrrs\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --set mrrs\n</code></pre> <p>Note</p> <p>This value is reset on reboot and needs to be set every time the system boots</p> ERROR: pcilib: sysfs_write: write failed: Operation not permitted <p>Disable secure boot on your system ahead of changing the MRRS of your NIC ports. It can be re-enabled afterwards.</p>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#34-enable-huge-pages","title":"3.4 Enable Huge pages","text":"<p>Huge pages are a memory management feature that allows the OS to allocate large blocks of memory (typically 2MB or 1GB) instead of the default 4KB pages. This reduces the number of page table entries and the amount of memory used for translation, improving cache performance and reducing TLB (Translation Lookaside Buffer) misses, which leads to lower latencies.</p> <p>While it is naturally beneficial for CPU packets, it is also needed when routing data packets to the GPU in order to handle metadata (mbufs) on the CPU.</p> hugeadmvanilla <p>We recommend installing the <code>libhugetlbfs-bin</code> package for the <code>hugeadm</code> utility:</p> <pre><code>sudo apt update\nsudo apt install -y libhugetlbfs-bin\n</code></pre> <p>Then, check your huge page pools:</p> <pre><code>hugeadm --pool-list\n</code></pre> See an example output <p>The example below shows that this system supports huge pages of 64K, 2M (default), 32M, and 1G, but that none of them are currently allocated.</p> <pre><code>      Size  Minimum  Current  Maximum  Default\n     65536        0        0        0\n   2097152        0        0        0        *\n  33554432        0        0        0\n1073741824        0        0        0\n</code></pre> <p>And your huge page mount points:</p> <pre><code>hugeadm --list-all-mounts\n</code></pre> See an example output <p>The default huge pages are mounted on <code>/dev/hugepages</code> with a page size of 2M.</p> <pre><code>Mount Point          Options\n/dev/hugepages       rw,relatime,pagesize=2M\n</code></pre> <p>First, check your huge page pools:</p> <pre><code>ls -1 /sys/kernel/mm/hugepages/\ngrep Huge /proc/meminfo\n</code></pre> See an example output <p>The example below shows that this system supports huge pages of 64K, 2M (default), 32M, and 1G, but that none of them are currently allocated.</p> <pre><code>hugepages-1048576kB\nhugepages-2048kB\nhugepages-32768kB\nhugepages-64kB\n</code></pre> <pre><code>HugePages_Total:       0\nHugePages_Free:        0\nHugePages_Rsvd:        0\nHugePages_Surp:        0\nHugepagesize:       2048 kB\nHugetlb:               0 kB\n</code></pre> <p>And your huge page mount points:</p> <pre><code>mount | grep huge\n</code></pre> See an example output <p>The default huge pages are mounted on <code>/dev/hugepages</code> with a page size of 2M.</p> <pre><code>hugetlbfs on /dev/hugepages type hugetlbfs (rw,relatime,pagesize=2M)\n</code></pre> <p>As a rule of thumb, we recommend to start with 3 to 4 GB of total huge pages, with an individual page size of 500 MB to 1 GB (per system availability).</p> <p>There are two ways to allocate huge pages:</p> <ul> <li>in the kernel bootline (recommended to ensure contiguous memory allocation) or</li> <li>dynamically at runtime (risk of fragmentation for large page sizes)</li> </ul> <p>The example below allocates 3 huge pages of 1GB each.</p> Kernel bootlineRuntime <p>Add the flags below to the <code>GRUB_CMDLINE_LINUX</code> variable in <code>/etc/default/grub</code>:</p> <pre><code>default_hugepagesz=1G hugepagesz=1G hugepages=3\n</code></pre> Show explanation <ul> <li><code>default_hugepagesz</code>: the default huge page size to use, making them available from the default mount point, <code>/dev/hugepages</code>.</li> <li><code>hugepagesz</code>: the size of the huge pages to allocate.</li> <li><code>hugepages</code>: the number of huge pages to allocate.</li> </ul> <p>Then rebuild your GRUB configuration and reboot:</p> <pre><code>sudo update-grub\nsudo reboot\n</code></pre> <p>Allocate the 3x 1GB huge pages:</p> hugeadmvanilla <pre><code>sudo hugeadm --pool-pages-min 1073741824:3\n</code></pre> <pre><code>echo 3 | sudo tee /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages\n</code></pre> <p>Create a mount point to access the 1GB huge pages pool since that is not the default size on that system. We will name it <code>/mnt/huge</code> here.</p> One-timePersistent <pre><code>sudo mkdir -p /mnt/huge\nsudo mount -t hugetlbfs -o pagesize=1G none /mnt/huge\n</code></pre> <pre><code>echo \"nodev /mnt/huge hugetlbfs pagesize=1G 0 0\" | sudo tee -a /etc/fstab\nsudo mount /mnt/huge\n</code></pre> <p>Note</p> <p>If you work with containers, remember to mount this directory in your container as well with <code>-v /mnt/huge:/mnt/huge</code>.</p> <p>Rerunning the initial commands should now list 3 hugepages of 1GB each. 1GB will be the default huge page size if updated in the kernel bootline only.</p>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#35-isolate-cpu-cores","title":"3.5 Isolate CPU cores","text":"<p>Note</p> <p>This optimization is less impactful when using the <code>gpunetio</code> backend since the GPU polls the NIC.</p> <p>The CPU interacting with the NIC to route packets is sensitive to perturbations, especially with smaller packet/batch sizes requiring more frequent work. Isolating a CPU in Linux prevents unwanted user or kernel threads from running on it, reducing context switching and latency spikes from noisy neighbors.</p> <p>We recommend isolating the CPU cores you will select to interact with the NIC (defined in the <code>advanced_network</code> configuration described later in this tutorial). This is done by setting additional flags on the kernel bootline.</p> <p>You can first check if any of the recommended flags were already set on the last boot:</p> tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check cmdline\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check cmdline\n</code></pre> <pre><code>cat /proc/cmdline | grep -e isolcpus -e irqaffinity -e nohz_full -e rcu_nocbs -e rcu_nocb_poll\n</code></pre> <p>Decide which cores to isolate based on your configuration. We recommend one core per queue as a rule of thumb. First, identify your core IDs:</p> <pre><code>cat /proc/cpuinfo | grep processor\n</code></pre> See an example output <p>This system has 12 cores, numbered 0 to 11: <pre><code>processor       # 0\nprocessor       # 1\nprocessor       # 2\nprocessor       # 3\nprocessor       # 4\nprocessor       # 5\nprocessor       # 6\nprocessor       # 7\nprocessor       # 8\nprocessor       # 9\nprocessor       # 10\nprocessor       # 11\n</code></pre></p> <p>As an example, the line below will isolate cores 9, 10 and 11, leaving cores 0-8 free for other tasks and hardware interrupts:</p> <pre><code>isolcpus=9-11 irqaffinity=0-8 nohz_full=9-11 rcu_nocbs=9-11 rcu_nocb_poll\n</code></pre> Show explanation Parameter Description <code>isolcpus</code> Isolates specific CPU cores from the Linux scheduler, preventing regular system tasks from running on them. This ensures dedicated cores are available exclusively for your networking tasks, reducing context switches and interruptions that can cause latency spikes. <code>irqaffinity</code> Controls which CPU cores can handle hardware interrupts. By directing network interrupts away from your isolated cores, you prevent networking tasks from being interrupted by hardware events, maintaining consistent processing time. <code>nohz_full</code> Disables regular kernel timer ticks on specified cores when they're running user space applications. This reduces overhead and prevents periodic interruptions, allowing your networking code to run with fewer disturbances. <code>rcu_nocbs</code> Offloads Read-Copy-Update (RCU) callback processing from specified cores. RCU is a synchronization mechanism in the Linux kernel that can cause periodic processing bursts. Moving this work away from your networking cores helps maintain consistent performance. <code>rcu_nocb_poll</code> Works with <code>rcu_nocbs</code> to improve how RCU callbacks are processed on non-callback CPUs. This can reduce latency spikes by changing how the kernel polls for RCU work. <p>Together, these parameters create an environment where specific CPU cores can focus exclusively on network packet processing with minimal interference from the operating system, resulting in lower and more consistent latency.</p> <p>Add these flags to the <code>GRUB_CMDLINE_LINUX</code> variable in <code>/etc/default/grub</code>, then rebuild your GRUB configuration and reboot:</p> <pre><code>sudo update-grub\nsudo reboot\n</code></pre> <p>Verify that the flags were properly set after boot by rerunning the check commands above.</p>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#36-prevent-cpu-cores-from-going-idle","title":"3.6 Prevent CPU cores from going idle","text":"<p>When a core goes idle/to sleep, coming back online to poll the NIC can cause latency spikes and dropped packets. To prevent this, we recommend setting the scaling governor to <code>performance</code> for these CPU cores.</p> <p>Note</p> <p>Cores from a single cluster will always share the same governor.</p> <p>Bug</p> <p>We have witnessed instances where setting the governor to <code>performance</code> on only the isolated cores (dedicated to polling the NIC) does not lead to the performance gains expected. As such, we currently recommend setting the governor to <code>performance</code> for all cores which has shown to be reliably effective.</p> <p>Check the current governor for each of your cores:</p> tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check cpu-freq\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check cpu-freq\n</code></pre> See an example output <pre><code>2025-03-06 12:20:27 - WARNING - CPU 0: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 1: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 2: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 3: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 4: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 5: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 6: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 7: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 8: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 9: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 10: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 11: Governor is set to 'powersave', not 'performance'.\n</code></pre> <pre><code>cat /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor\n</code></pre> See an example output <p>In this example, all cores were defaulted to <code>powersave</code> instead of the recommended <code>performance</code>.</p> <pre><code>powersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\n</code></pre> <p>Install <code>cpupower</code> to more conveniently set the governor:</p> <pre><code>sudo apt update\nsudo apt install -y linux-tools-$(uname -r)\n</code></pre> <p>Set the governor to <code>performance</code> for all cores:</p> One-timePersistent <pre><code>sudo cpupower frequency-set -g performance\n</code></pre> <pre><code>cat &lt;&lt; EOF | sudo tee /etc/systemd/system/cpu-performance.service\n[Unit]\nDescription=Set CPU governor to performance\nAfter=multi-user.target\n\n[Service]\nType=oneshot\nExecStart=/usr/bin/cpupower -c all frequency-set -g performance\n\n[Install]\nWantedBy=multi-user.target\nEOF\nsudo systemctl enable cpu-performance.service\nsudo systemctl start cpu-performance.service\n</code></pre> <p>Running the checks above should now list <code>performance</code> as the governor for all cores. You can also run <code>sudo cpupower -c all frequency-info</code> for more details.</p>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#37-prevent-the-gpu-from-going-idle","title":"3.7 Prevent the GPU from going idle","text":"<p>Similarly to the above, we want to maximize the GPU's clock speed and prevent it from going idle.</p> <p>Run the following command to check your current clocks and whether they're locked (persistence mode):</p> <pre><code>nvidia-smi -q | grep -i \"Persistence Mode\"\nnvidia-smi -q -d CLOCK\n</code></pre> See an example output <pre><code>    Persistence Mode: Enabled\n...\nAttached GPUs                             : 1\nGPU 00000005:09:00.0\n    Clocks\n        Graphics                          : 420 MHz\n        SM                                : 420 MHz\n        Memory                            : 405 MHz\n        Video                             : 1680 MHz\n    Applications Clocks\n        Graphics                          : 1800 MHz\n        Memory                            : 8001 MHz\n    Default Applications Clocks\n        Graphics                          : 1800 MHz\n        Memory                            : 8001 MHz\n    Deferred Clocks\n        Memory                            : N/A\n    Max Clocks\n        Graphics                          : 2100 MHz\n        SM                                : 2100 MHz\n        Memory                            : 8001 MHz\n        Video                             : 1950 MHz\n    ...\n</code></pre> <p>To lock the GPU's clocks to their max values:</p> One-timePersistent <pre><code>sudo nvidia-smi -pm 1\nsudo nvidia-smi -lgc=$(nvidia-smi --query-gpu=clocks.max.sm --format=csv,noheader,nounits)\nsudo nvidia-smi -lmc=$(nvidia-smi --query-gpu=clocks.max.mem --format=csv,noheader,nounits)\n</code></pre> <pre><code>cat &lt;&lt; EOF | sudo tee /etc/systemd/system/gpu-max-clocks.service\n[Unit]\nDescription=Max GPU clocks\nAfter=multi-user.target\n\n[Service]\nType=oneshot\nExecStart=/usr/bin/nvidia-smi -pm 1\nExecStart=/bin/bash -c '/usr/bin/nvidia-smi --lock-gpu-clocks=$(/usr/bin/nvidia-smi --query-gpu=clocks.max.sm --format=csv,noheader,nounits)'\nExecStart=/bin/bash -c '/usr/bin/nvidia-smi --lock-memory-clocks=$(/usr/bin/nvidia-smi --query-gpu=clocks.max.mem --format=csv,noheader,nounits)'\nRemainAfterExit=true\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\nsudo systemctl enable gpu-max-clocks.service\nsudo systemctl start gpu-max-clocks.service\n</code></pre> Show explanation <p>This queries the max clocks for the GPU SM (<code>clocks.max.sm</code>) and memory (<code>clocks.max.mem</code>) and sets them to the current clocks (<code>lock-gpu-clocks</code> and <code>lock-memory-clocks</code> respectively). <code>-pm 1</code> (or <code>--persistence-mode=1</code>) enables persistence mode to lock these values.</p> See an example output <pre><code>GPU clocks set to \"(gpuClkMin 2100, gpuClkMax 2100)\" for GPU 00000005:09:00.0\nAll done.\nMemory clocks set to \"(memClkMin 8001, memClkMax 8001)\" for GPU 00000005:09:00.0\nAll done.\n</code></pre> <p>You can confirm that the clocks are set to the max values by running <code>nvidia-smi -q -d CLOCK</code> again.</p> <p>Note</p> <p>Some max clocks might not be achievable in certain configurations, or due to boost clocks (SM) or rounding errors (Memory),  despite the lock commands indicating it worked. For example - on IGX - the max non-boot SM clock will be 1920 MHz, and the max memory clock will show 8000 MHz, which are satisfying compared to the initial mode.</p>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#38-maximize-gpu-bar1-size","title":"3.8 Maximize GPU BAR1 size","text":"<p>The GPU BAR1 memory is the primary resource consumed by <code>GPUDirect</code>. It allows other PCIe devices (like the CPU and the NIC) to access the GPU's memory space. The larger the BAR1 size, the more memory the GPU can expose to these devices in a single PCIe transaction, reducing the number of transactions needed and improving performance.</p> <p>We recommend a BAR1 size of 1GB or above. Check the current BAR1 size:</p> tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check bar1-size\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check bar1-size\n</code></pre> See an example output <pre><code>2025-03-06 12:22:53 - INFO - GPU 00000005:09:00.0: BAR1 size is 8192 MiB.\n</code></pre> <pre><code>nvidia-smi -q | grep -A 3 BAR1\n</code></pre> See an example output <p>For our RTX A6000, this shows a BAR1 size of 256 MiB:</p> <pre><code>    BAR1 Memory Usage\n    Total                             : 256 MiB\n    Used                              : 13 MiB\n    Free                              : 243 MiB\n</code></pre> <p>Warning</p> <p>Resizing the BAR1 size requires:</p> <ul> <li>A BIOS with resizable BAR support</li> <li>A GPU with physical resizable BAR</li> </ul> <p>If you attempt to go forward with the instructions below without meeting the above requirements, you might render your GPU unusable.</p>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#bios-resizable-bar-support","title":"BIOS Resizable BAR support","text":"<p>First, check if your system and BIOS support resizable BAR. Refer to your system's manufacturer documentation to access the BIOS. The Resizable BAR option is often categorized under <code>Advanced &gt; PCIe</code> settings. Enable this feature if found.</p> <p>Note</p> <p>The IGX Developer kit with IGX OS 1.1+ supports resizable BAR by default.</p>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#gpu-resizable-bar-support","title":"GPU Resizable BAR support","text":"<p>Next, you can check if your GPU has physical resizable BAR by running the following command:</p> <pre><code>sudo lspci -vv -s $(nvidia-smi --query-gpu=pci.bus_id --format=csv,noheader) | grep BAR\n</code></pre> See an example output <p>This RTX A6000 has a resizable BAR1, currently set to 256 MiB:</p> <pre><code>Capabilities: [bb0 v1] Physical Resizable BAR\n    BAR 0: current size: 16MB, supported: 16MB\n    BAR 1: current size: 256MB, supported: 64MB 128MB 256MB 512MB 1GB 2GB 4GB 8GB 16GB 32GB 64GB\n    BAR 3: current size: 32MB, supported: 32MB\n</code></pre> <p>If your GPU is listed on this page, you can download the <code>Display Mode Selector</code> to resize the BAR1 to 8GB.</p> <ol> <li>Press <code>Join Now</code>.</li> <li>Once approved, download the <code>Display Mode Selector</code> archive.</li> <li>Unzip the archive.</li> <li>Access your system without a X-server running, either through SSH or a Virtual Console (<code>Alt+F1</code>).</li> <li>Go down the right OS and architecture folder for your system (<code>linux/aarch64</code> or <code>linux/x64</code>).</li> <li>Run the <code>displaymodeselector</code> command like so:</li> </ol> <pre><code>chmod +x displaymodeselector\nsudo ./displaymodeselector --gpumode physical_display_enabled_8GB_bar1\n</code></pre> <p>Press <code>y</code> to confirm you'd like to continue, then <code>y</code> again to apply to all the eligible adapters.</p> See an example output <pre><code>NVIDIA Display Mode Selector Utility (Version 1.67.0)\nCopyright (C) 2015-2021, NVIDIA Corporation. All Rights Reserved.\n\nWARNING: This operation updates the firmware on the board and could make\n        the device unusable if your host system lacks the necessary support.\n\nAre you sure you want to continue?\nPress 'y' to confirm (any other key to abort):\ny\nSpecified GPU Mode \"physical_display_enabled_8GB_bar1\"\n\n\nUpdate GPU Mode of all adapters to \"physical_display_enabled_8GB_bar1\"?\nPress 'y' to confirm or 'n' to choose adapters or any other key to abort:\ny\n\nUpdating GPU Mode of all eligible adapters to \"physical_display_enabled_8GB_bar1\"\n\nApply GPU Mode &lt;6&gt; corresponds to \"physical_display_enabled_8GB_bar1\"\n\nReading EEPROM (this operation may take up to 30 seconds)\n\n[==================================================] 100 %\nReading EEPROM (this operation may take up to 30 seconds)\n\nSuccessfully updated GPU mode to \"physical_display_enabled_8GB_bar1\" ( Mode 6 ).\n\nA reboot is required for the update to take effect.\n</code></pre> Error: unload the NVIDIA kernel driver first <p>If you see this error:</p> <pre><code>ERROR: In order to avoid the irreparable damage to your graphics adapter it is necessary to unload the NVIDIA kernel driver first:\n\nrmmod nvidia_uvm nvidia_drm nvidia_modeset nvidia_peermem nvidia\n</code></pre> <p>Try to unload the NVIDIA kernel driver listed in the error message above (list may vary):</p> <pre><code>sudo rmmod nvidia_uvm nvidia_drm nvidia_modeset nvidia_peermem nvidia\n</code></pre> <p>If this fails because the drivers are in use, stop the X-server first before trying again:</p> <pre><code>sudo systemctl isolate multi-user\n</code></pre> /dev/mem: Operation not permitted. Access to physical memory denied <p>Disable secure boot on your system ahead of changing your GPU's BAR1 size. It can be re-enabled afterwards.</p> <p>Reboot your system, and check the BAR1 size again to confirm the change.</p> <pre><code>sudo reboot\n</code></pre>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#39-enable-jumbo-frames","title":"3.9 Enable Jumbo Frames","text":"<p>Jumbo frames are Ethernet frames that carry a payload larger than the standard 1500 bytes MTU (Maximum Transmission Unit). They can significantly improve network performance when transferring large amounts of data by reducing the overhead of packet headers and the number of packets that need to be processed.</p> <p>We recommend an MTU of 9000 bytes on all interfaces involved in the data path. You can check the current MTU of your interfaces:</p> tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check mtu\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check mtu\n</code></pre> See an example output <pre><code>2025-03-06 16:51:19 - INFO - Interface eth0 has an acceptable MTU of 9000 bytes.\n2025-03-06 16:51:19 - INFO - Interface eth1 has an acceptable MTU of 9000 bytes.\n</code></pre> <p>For a given <code>if_name</code> interface:</p> <pre><code>if_name=eth0\nip link show dev $if_name | grep -oE \"mtu [0-9]+\"\n</code></pre> See an example output <pre><code>mtu 1500\n</code></pre> <p>You can set the MTU for each interface like so, for a given <code>if_name</code> name identified above:</p> One-timePersistent <pre><code>sudo ip link set dev $if_name mtu 9000\n</code></pre> NetworkManagersystemd-networkd <pre><code>sudo nmcli connection modify $if_name ipv4.mtu 9000\nsudo nmcli connection up $if_name\n</code></pre> <p>Assuming you've set an IP address for the interface above, you can add the MTU to the interface's network configuration file like so:</p> <pre><code>sudo sed -i '/\\[Network\\]/a MTU=9000' /etc/systemd/network/20-$if_name.network\nsudo systemctl restart systemd-networkd\n</code></pre> Can I do more than 9000? <p>While your NIC might have a maximum MTU capability larger than 9000, we typically recommend setting the MTU to 9000 bytes, as that is the standard size for jumbo frames that's widely supported for compatibility with other network equipment. When using jumbo frames, all devices in the communication path must support the same MTU size. If any device in between has a smaller MTU, packets will be fragmented or dropped, potentially degrading performance.</p> <p>Example with the CX-7 NIC:</p> <pre><code>$ ip -d link show dev $if_name | grep -oE \"maxmtu [0-9]+\"\nmaxmtu 9978\n</code></pre>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#4-running-a-test-application","title":"4. Running a test application","text":"<p>Holoscan Networking provides a benchmarking application named <code>adv_networking_bench</code> that can be used to test the performance of the networking configuration. In this section, we'll walk you through the steps needed to configure the application for your NIC for Tx and Rx, and run a loopback test between the two interfaces with a physical SFP cable connecting them.</p> <p>Make sure to install <code>holoscan-networking</code> beforehand.</p>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#41-update-the-loopback-configuration","title":"4.1 Update the loopback configuration","text":"","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#find-the-application-files","title":"Find the application files","text":"<p>Identify the location of the <code>adv_networking_bench</code> executable, and of the configuration file named <code>adv_networking_bench_default_tx_rx.yaml</code>, for your installation:</p> Debian installationFrom source <p>Both located under <code>/opt/nvidia/holoscan/examples/adv_networking_bench/</code>:</p> <pre><code>ls -1 /opt/nvidia/holoscan/examples/adv_networking_bench/\nadv_networking_bench\nadv_networking_bench_default_rx_multi_q.yaml\nadv_networking_bench_default_tx_rx_hds.yaml\nadv_networking_bench_default_tx_rx.yaml\nadv_networking_bench_gpunetio_tx_rx.yaml\nadv_networking_bench_rivermax_rx.yaml\nCMakeLists.txt\ndefault_bench_op_rx.h\ndefault_bench_op_tx.h\ndoca_bench_op_rx.h\ndoca_bench_op_tx.h\nkernels.cu\nkernels.cuh\nmain.cpp\n</code></pre> <p>Both located under <code>./install/examples/adv_networking_bench/</code></p> <pre><code>ls -1 ./install/examples/adv_networking_bench\nadv_networking_bench\nadv_networking_bench_default_rx_multi_q.yaml\nadv_networking_bench_default_tx_rx_hds.yaml\nadv_networking_bench_default_tx_rx.yaml\nadv_networking_bench_gpunetio_tx_rx.yaml\nadv_networking_bench.py\nadv_networking_bench_rivermax_rx.yaml\nCMakeLists.txt\ndefault_bench_op_rx.h\ndefault_bench_op_tx.h\ndoca_bench_op_rx.h\ndoca_bench_op_tx.h\nkernels.cu\nkernels.cuh\nmain.cpp\n</code></pre> <p>Warning</p> <p>The configuration file is also located alongide the application source code at <code>applications/adv_networking_bench/adv_networking_bench_default_tx_rx.yaml</code>. However, modifying this file will not affect the configuration used by the application executable without rebuilding the application.</p> <p>For this reason, we recommend using the configuration file located in the install tree.</p> <p>Note</p> <p>The fields in this <code>yaml</code> file will be explained in more details in a section below. For now, we'll stick to modifying the strict minimum required fields to run the application as-is on your system.</p>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#identify-your-nics-pcie-addresses","title":"Identify your NIC's PCIe addresses","text":"<p>Retrieve the PCIe addresses of both ports of your NIC. We'll arbitrarily use the first for Tx and the second for Rx here:</p> ibdev2netdevlspci <pre><code>sudo ibdev2netdev -v | awk '{print $1}'\n</code></pre> <pre><code># `0200` is the PCI-SIG class code for NICs\n# `15b3` is the Vendor ID for Mellanox\nlspci -n | awk '$2 == \"0200:\" &amp;&amp; $3 ~ /^15b3:/ {print $1}'\n</code></pre> See an example output <pre><code>0005:03:00.0\n0005:03:00.1\n</code></pre>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#configure-the-nic-for-tx-and-rx","title":"Configure the NIC for Tx and Rx","text":"<p>Set the NIC addresses in the <code>interfaces</code> section of the <code>advanced_network</code> section, making sure to remove the template brackets <code>&lt; &gt;</code>. This configures your NIC independently of your application:</p> <ul> <li>Set the <code>address</code> field of the <code>tx_port</code> interface to one of these addresses. That interface will be able to transmit ethernet packets.</li> <li>Set the <code>address</code> field of the <code>rx_port</code> interface to the other address. This interface will be able to receive ethernet packets.</li> </ul> <pre><code>interfaces:\n    - name: \"tx_port\"\n    address: &lt;0000:00:00.0&gt;       # The BUS address of the interface doing Tx\n    tx:\n        ...\n    - name: \"rx_port\"\n    address: &lt;0000:00:00.0&gt;       # The BUS address of the interface doing Rx\n    rx:\n        ...\n</code></pre> See an example yaml <pre><code>interfaces:\n    - name: \"tx_port\"\n    address: 0005:03:00.0       # The BUS address of the interface doing Tx\n    tx:\n        ...\n    - name: \"rx_port\"\n    address: 0005:03:00.1       # The BUS address of the interface doing Rx\n    rx:\n        ...\n</code></pre>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#configure-the-application","title":"Configure the application","text":"<p>To run the benchmarking application to run a loopback on your system, you'll need to modify the <code>bench_tx</code> section which configures the application itself, to create the packet headers and direct the packets to the NIC. Make sure to remove the template brackets <code>&lt; &gt;</code>.</p> <ul> <li><code>eth_dst_addr</code> with the MAC address (and not the PCIe address) of the NIC interface you want to use for Rx. You can get the MAC address of your <code>if_name</code> interface with <code>cat /sys/class/net/$if_name/address</code>:</li> </ul> <pre><code>bench_tx:\n    interface_name: \"tx_port\" # Name of the TX port from the advanced_network config\n    ...\n    eth_dst_addr: &lt;00:00:00:00:00:00&gt; # Destination MAC address - required when Rx flow_isolation=true\n    ...\n</code></pre> See an example yaml <pre><code>bench_tx:\n    interface_name: \"tx_port\" # Name of the TX port from the advanced_network config\n    ...\n    eth_dst_addr: 48:b0:2d:ee:83:ad # Destination MAC address - required when Rx flow_isolation=true\n    ...\n</code></pre> Show explanation <ul> <li><code>eth_dst_addr</code> - the destination ethernet MAC address - will be embedded in the packet headers by the application. This is required here because the Rx interface above has <code>flow_isolation: true</code> (explained in more details below). In that configuration, only the packets listing the adequate destination MAC address will be accepted by the Rx interface.</li> <li>We ignore the IP fields (<code>ip_src_addr</code>, <code>ip_dst_addr</code>) for now, as we are testing on a layer 2 network by just connecting a cable between the two interfaces on our system, therefore having mock values has no impact.</li> <li>You might have noted the lack of a <code>eth_src_addr</code> field in this <code>bench_tx</code> section. This is because the source Ethernet MAC address can be inferred automatically by the Advanced Network library from the PCIe address of the Tx interface referenced above.</li> </ul>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#42-run-the-loopback-test","title":"4.2 Run the loopback test","text":"<p>After having modified the configuration file, ensure you have connected an SFP cable between the two interfaces of your NIC, then run the application with the command below:</p> Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/examples/adv_networking_bench/adv_networking_bench adv_networking_bench_default_tx_rx.yaml\n</code></pre> Bare MetalContainerized <p>This assumes you have the required dependencies (holoscan, doca, etc.) installed locally on your system.</p> <pre><code>sudo ./install/examples/adv_networking_bench/adv_networking_bench adv_networking_bench_default_tx_rx.yaml\n</code></pre> <pre><code>./holohub run-container \\\n  --img holohub:adv_networking_bench \\\n  --docker-opts \"-u 0 --privileged\" \\\n  -- bash -c \"./install/examples/adv_networking_bench/adv_networking_bench adv_networking_bench_default_tx_rx.yaml\"\n</code></pre> <p>The application will run indefinitely. You can stop it gracefully with <code>Ctrl-C</code>. You can also uncomment and set the <code>max_duration_ms</code> field in the <code>scheduler</code> section of the configuration file to limit the duration of the run automatically.</p> See an example output <pre><code>[info] [fragment.cpp:599] Loading extensions from configs...\n[info] [gxf_executor.cpp:264] Creating context\n[info] [main.cpp:35] Initializing advanced network operator\n[info] [main.cpp:40] Using ANO manager dpdk\n[info] [adv_network_rx.cpp:35] Adding output port bench_rx_out\n[info] [adv_network_rx.cpp:51] AdvNetworkOpRx::initialize()\n[info] [adv_network_common.h:607] Finished reading advanced network operator config\n[info] [adv_network_dpdk_mgr.cpp:373] Attempting to use 2 ports for high-speed network\n[info] [adv_network_dpdk_mgr.cpp:382] Setting DPDK log level to: Info\n[info] [adv_network_dpdk_mgr.cpp:402] DPDK EAL arguments: adv_net_operator --file-prefix=nwlrbbmqbh -l 3,11,9 --log-level=9 --log-level=pmd.net.mlx5:info -a 0005:03:00.0,txq_inline_max=0,dv_flow_en=2 -a 0005:03:00.1,txq_inline_max=0,dv_flow_en=2\nLog level 9 higher than maximum (8)\nEAL: Detected CPU lcores: 12\nEAL: Detected NUMA nodes: 1\nEAL: Detected shared linkage of DPDK\nEAL: Multi-process socket /var/run/dpdk/nwlrbbmqbh/mp_socket\nEAL: Selected IOVA mode 'VA'\nEAL: 1 hugepages of size 1073741824 reserved, but no mounted hugetlbfs found for that size\nEAL: Probe PCI driver: mlx5_pci (15b3:1021) device: 0005:03:00.0 (socket -1)\nmlx5_net: PCI information matches for device \"mlx5_0\"\nmlx5_net: enhanced MPS is enabled\nmlx5_net: port 0 MAC address is 48:B0:2D:EE:83:AC\nEAL: Probe PCI driver: mlx5_pci (15b3:1021) device: 0005:03:00.1 (socket -1)\nmlx5_net: PCI information matches for device \"mlx5_1\"\nmlx5_net: enhanced MPS is enabled\nmlx5_net: port 1 MAC address is 48:B0:2D:EE:83:AD\nTELEMETRY: No legacy callbacks, legacy socket not created\n[info] [adv_network_dpdk_mgr.cpp:298] Port 0 has no RX queues. Creating dummy queue.\n[info] [adv_network_dpdk_mgr.cpp:165] Adjusting buffer size to 9228 for headroom\n[info] [adv_network_dpdk_mgr.cpp:165] Adjusting buffer size to 9128 for headroom\n[info] [adv_network_dpdk_mgr.cpp:165] Adjusting buffer size to 9128 for headroom\n[info] [adv_network_mgr.cpp:116] Registering memory regions\n[info] [adv_network_mgr.cpp:178] Successfully allocated memory region MR_Unused_P0 at 0x100fa0000 type 2 with 9100 bytes (32768 elements @ 9228 bytes total 302383104)\n[info] [adv_network_mgr.cpp:178] Successfully allocated memory region Data_RX_GPU at 0xffff4fc00000 type 3 with 9000 bytes (51200 elements @ 9128 bytes total 467402752)\n[info] [adv_network_mgr.cpp:178] Successfully allocated memory region Data_TX_GPU at 0xffff33e00000 type 3 with 9000 bytes (51200 elements @ 9128 bytes total 467402752)\n[info] [adv_network_mgr.cpp:191] Finished allocating memory regions\n[info] [adv_network_dpdk_mgr.cpp:223] Successfully registered external memory for Data_TX_GPU\n[info] [adv_network_dpdk_mgr.cpp:223] Successfully registered external memory for Data_RX_GPU\n[info] [adv_network_dpdk_mgr.cpp:193] Mapped external memory descriptor for 0xffff4fc00000 to device 0\n[info] [adv_network_dpdk_mgr.cpp:193] Mapped external memory descriptor for 0xffff33e00000 to device 0\n[info] [adv_network_dpdk_mgr.cpp:193] Mapped external memory descriptor for 0xffff4fc00000 to device 1\n[info] [adv_network_dpdk_mgr.cpp:193] Mapped external memory descriptor for 0xffff33e00000 to device 1\n[info] [adv_network_dpdk_mgr.cpp:454] DPDK init (0005:03:00.0) -- RX: ENABLED TX: ENABLED\n[info] [adv_network_dpdk_mgr.cpp:464] Configuring RX queue: UNUSED_P0_Q0 (0) on port 0\n[info] [adv_network_dpdk_mgr.cpp:513] Created mempool RXP_P0_Q0_MR0 : mbufs=32768 elsize=9228 ptr=0x10041c380\n[info] [adv_network_dpdk_mgr.cpp:523] Max packet size needed for RX: 9100\n[info] [adv_network_dpdk_mgr.cpp:564] Configuring TX queue: ADC Samples (0) on port 0\n[info] [adv_network_dpdk_mgr.cpp:607] Created mempool TXP_P0_Q0_MR0 : mbufs=51200 elsize=9000 ptr=0x100c1fc00\n[info] [adv_network_dpdk_mgr.cpp:621] Max packet size needed with TX: 9100\n[info] [adv_network_dpdk_mgr.cpp:632] Setting port config for port 0 mtu:9082\n[info] [adv_network_dpdk_mgr.cpp:663] Initializing port 0 with 1 RX queues and 1 TX queues...\nmlx5_net: port 0 Tx queues number update: 0 -&gt; 1\nmlx5_net: port 0 Rx queues number update: 0 -&gt; 1\n[info] [adv_network_dpdk_mgr.cpp:679] Successfully configured ethdev\n[info] [adv_network_dpdk_mgr.cpp:689] Successfully set descriptors to 8192/8192\n[info] [adv_network_dpdk_mgr.cpp:704] Port 0 not in isolation mode\n[info] [adv_network_dpdk_mgr.cpp:713] Setting up port:0, queue:0, Num scatter:1 pool:0x10041c380\n[info] [adv_network_dpdk_mgr.cpp:734] Successfully setup RX port 0 queue 0\n[info] [adv_network_dpdk_mgr.cpp:756] Successfully set up TX queue 0/0\n[info] [adv_network_dpdk_mgr.cpp:761] Enabling promiscuous mode for port 0\nmlx5_net: [mlx5dr_cmd_query_caps]: Failed to query wire port regc value\nmlx5_net: port 0 Rx queues number update: 1 -&gt; 1\n[info] [adv_network_dpdk_mgr.cpp:775] Successfully started port 0\n[info] [adv_network_dpdk_mgr.cpp:778] Port 0, MAC address: 48:B0:2D:EE:83:AC\n[info] [adv_network_dpdk_mgr.cpp:1111] Applying tx_eth_src offload for port 0\n[info] [adv_network_dpdk_mgr.cpp:454] DPDK init (0005:03:00.1) -- RX: ENABLED TX: DISABLED\n[info] [adv_network_dpdk_mgr.cpp:464] Configuring RX queue: Data (0) on port 1\n[info] [adv_network_dpdk_mgr.cpp:513] Created mempool RXP_P1_Q0_MR0 : mbufs=51200 elsize=9128 ptr=0x125a5b940\n[info] [adv_network_dpdk_mgr.cpp:523] Max packet size needed for RX: 9000\n[info] [adv_network_dpdk_mgr.cpp:621] Max packet size needed with TX: 9000\n[info] [adv_network_dpdk_mgr.cpp:632] Setting port config for port 1 mtu:8982\n[info] [adv_network_dpdk_mgr.cpp:663] Initializing port 1 with 1 RX queues and 0 TX queues...\nmlx5_net: port 1 Rx queues number update: 0 -&gt; 1\n[info] [adv_network_dpdk_mgr.cpp:679] Successfully configured ethdev\n[info] [adv_network_dpdk_mgr.cpp:689] Successfully set descriptors to 8192/8192\n[info] [adv_network_dpdk_mgr.cpp:701] Port 1 in isolation mode\n[info] [adv_network_dpdk_mgr.cpp:713] Setting up port:1, queue:0, Num scatter:1 pool:0x125a5b940\n[info] [adv_network_dpdk_mgr.cpp:734] Successfully setup RX port 1 queue 0\n[info] [adv_network_dpdk_mgr.cpp:764] Not enabling promiscuous mode on port 1 since flow isolation is enabled\nmlx5_net: [mlx5dr_cmd_query_caps]: Failed to query wire port regc value\nmlx5_net: port 1 Rx queues number update: 1 -&gt; 1\n[info] [adv_network_dpdk_mgr.cpp:775] Successfully started port 1\n[info] [adv_network_dpdk_mgr.cpp:778] Port 1, MAC address: 48:B0:2D:EE:83:AD\n[info] [adv_network_dpdk_mgr.cpp:790] Adding RX flow ADC Samples\n[info] [adv_network_dpdk_mgr.cpp:998] Adding IPv4 length match for 1050\n[info] [adv_network_dpdk_mgr.cpp:1018] Adding UDP port match for src/dst 4096/4096\n[info] [adv_network_dpdk_mgr.cpp:814] Setting up RX burst pool with 8191 batches of size 81920\n[info] [adv_network_dpdk_mgr.cpp:833] Setting up RX burst pool with 8191 batches of size 20480\n[info] [adv_network_dpdk_mgr.cpp:875] Setting up TX ring TX_RING_P0_Q0\n[info] [adv_network_dpdk_mgr.cpp:901] Setting up TX burst pool TX_BURST_POOL_P0_Q0 with 10240 pointers at 0x125a0d4c0\n[info] [adv_network_dpdk_mgr.cpp:1186] Config validated successfully\n[info] [adv_network_dpdk_mgr.cpp:1199] Starting advanced network workers\n[info] [adv_network_dpdk_mgr.cpp:1278] Flushing packet on port 1\n[info] [adv_network_dpdk_mgr.cpp:1478] Starting RX Core 9, port 1, queue 0, socket 0\n[info] [adv_network_dpdk_mgr.cpp:1268] Done starting workers\n[info] [default_bench_op_tx.h:79] AdvNetworkingBenchDefaultTxOp::initialize()\n[info] [adv_network_dpdk_mgr.cpp:1637] Starting TX Core 11, port 0, queue 0 socket 0 using burst pool 0x125a0d4c0 ring 0x127690740\n[info] [default_bench_op_tx.h:113] Initialized 4 streams and events\n[info] [default_bench_op_tx.h:130] AdvNetworkingBenchDefaultTxOp::initialize() complete\n[info] [default_bench_op_rx.h:67] AdvNetworkingBenchDefaultRxOp::initialize()\n[info] [gxf_executor.cpp:1797] creating input IOSpec named 'burst_in'\n[info] [default_bench_op_rx.h:104] AdvNetworkingBenchDefaultRxOp::initialize() complete\n[info] [adv_network_tx.cpp:46] AdvNetworkOpTx::initialize()\n[info] [gxf_executor.cpp:1797] creating input IOSpec named 'burst_in'\n[info] [adv_network_common.h:607] Finished reading advanced network operator config\n[info] [gxf_executor.cpp:2208] Activating Graph...\n[info] [gxf_executor.cpp:2238] Running Graph...\n[info] [multi_thread_scheduler.cpp:300] MultiThreadScheduler started worker thread [pool name: default_pool, thread uid: 0]\n[info] [multi_thread_scheduler.cpp:300] MultiThreadScheduler started worker thread [pool name: default_pool, thread uid: 1]\n[info] [multi_thread_scheduler.cpp:300] MultiThreadScheduler started worker thread [pool name: default_pool, thread uid: 2]\n[info] [gxf_executor.cpp:2240] Waiting for completion...\n[info] [multi_thread_scheduler.cpp:300] MultiThreadScheduler started worker thread [pool name: default_pool, thread uid: 3]\n[info] [multi_thread_scheduler.cpp:300] MultiThreadScheduler started worker thread [pool name: default_pool, thread uid: 4]\n^C[info] [multi_thread_scheduler.cpp:636] Stopping multithread scheduler\n[info] [multi_thread_scheduler.cpp:694] Stopping all async jobs\n[info] [multi_thread_scheduler.cpp:218] Dispatcher thread has stopped checking jobs\n[info] [multi_thread_scheduler.cpp:679] Waiting to join all async threads\n[info] [multi_thread_scheduler.cpp:316] Worker Thread [pool name: default_pool, thread uid: 1] exiting.\n[info] [multi_thread_scheduler.cpp:702] *********************** DISPATCHER EXEC TIME : 476345.364000 ms\n\n[info] [multi_thread_scheduler.cpp:316] Worker Thread [pool name: default_pool, thread uid: 0] exiting.\n[info] [multi_thread_scheduler.cpp:316] Worker Thread [pool name: default_pool, thread uid: 3] exiting.\n[info] [multi_thread_scheduler.cpp:371] Event handler thread exiting.\n[info] [multi_thread_scheduler.cpp:703] *********************** DISPATCHER WAIT TIME : 47339.961000 ms\n\n[info] [multi_thread_scheduler.cpp:704] *********************** DISPATCHER COUNT : 197630449\n\n[info] [multi_thread_scheduler.cpp:316] Worker Thread [pool name: default_pool, thread uid: 2] exiting.\n[info] [multi_thread_scheduler.cpp:705] *********************** WORKER EXEC TIME : 983902.800000 ms\n\n[info] [multi_thread_scheduler.cpp:706] *********************** WORKER WAIT TIME : 1634522.159000 ms\n\n[info] [multi_thread_scheduler.cpp:707] *********************** WORKER COUNT : 11817369\n\n[info] [multi_thread_scheduler.cpp:316] Worker Thread [pool name: default_pool, thread uid: 4] exiting.\n[info] [multi_thread_scheduler.cpp:688] All async worker threads joined, deactivating all entities\n[info] [adv_network_rx.cpp:46] AdvNetworkOpRx::stop()\n[info] [adv_network_dpdk_mgr.cpp:1928] DPDK ANO shutdown called 2\n[info] [adv_network_tx.cpp:41] AdvNetworkOpTx::stop()\n[info] [adv_network_dpdk_mgr.cpp:1928] DPDK ANO shutdown called 1\n[info] [adv_network_dpdk_mgr.cpp:1133] Port 0:\n[info] [adv_network_dpdk_mgr.cpp:1135]  - Received packets:    0\n[info] [adv_network_dpdk_mgr.cpp:1136]  - Transmit packets:    6005066864\n[info] [adv_network_dpdk_mgr.cpp:1137]  - Received bytes:      0\n[info] [adv_network_dpdk_mgr.cpp:1138]  - Transmit bytes:      6389391347584\n[info] [adv_network_dpdk_mgr.cpp:1139]  - Missed packets:      0\n[info] [adv_network_dpdk_mgr.cpp:1140]  - Errored packets:     0\n[info] [adv_network_dpdk_mgr.cpp:1141]  - RX out of buffers:   0\n[info] [adv_network_dpdk_mgr.cpp:1143]    ** Extended Stats **\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_good_packets:          6005070000\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_good_bytes:            6389394480000\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_q0_packets:            6005070000\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_q0_bytes:              6389394480000\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_multicast_bytes:               9589\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_multicast_packets:             22\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_unicast_bytes:         6389394480000\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_multicast_bytes:               9589\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_unicast_packets:               6005070000\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_multicast_packets:             22\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_phy_packets:           6005070022\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_phy_packets:           24\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_phy_bytes:             6413414769677\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_phy_bytes:             9805\n[info] [adv_network_dpdk_mgr.cpp:1133] Port 1:\n[info] [adv_network_dpdk_mgr.cpp:1135]  - Received packets:    6004323692\n[info] [adv_network_dpdk_mgr.cpp:1136]  - Transmit packets:    0\n[info] [adv_network_dpdk_mgr.cpp:1137]  - Received bytes:      6388600255072\n[info] [adv_network_dpdk_mgr.cpp:1138]  - Transmit bytes:      0\n[info] [adv_network_dpdk_mgr.cpp:1139]  - Missed packets:      746308\n[info] [adv_network_dpdk_mgr.cpp:1140]  - Errored packets:     0\n[info] [adv_network_dpdk_mgr.cpp:1141]  - RX out of buffers:   5047027287\n[info] [adv_network_dpdk_mgr.cpp:1143]    ** Extended Stats **\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_good_packets:          6004323692\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_good_bytes:            6388600255072\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_missed_errors:         746308\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_mbuf_allocation_errors:                5047027287\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_q0_packets:            6004323692\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_q0_bytes:              6388600255072\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_q0_errors:             5047027287\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_unicast_bytes:         6389394480000\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_multicast_bytes:               9589\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_unicast_packets:               6005070000\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_multicast_packets:             22\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_multicast_bytes:               9589\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_multicast_packets:             22\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_phy_packets:           24\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_phy_packets:           6005070022\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_phy_bytes:             9805\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_phy_bytes:             6413414769677\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_out_of_buffer:         746308\n[info] [adv_network_dpdk_mgr.cpp:1935] ANO DPDK manager shutting down\n[info] [adv_network_dpdk_mgr.cpp:1622] Total packets received by application (port/queue 1/0): 6004323692\n[info] [adv_network_dpdk_mgr.cpp:1698] Total packets transmitted by application (port/queue 0/0): 6005070000\n[info] [multi_thread_scheduler.cpp:645] Multithread scheduler stopped.\n[info] [multi_thread_scheduler.cpp:664] Multithread scheduler finished.\n[info] [gxf_executor.cpp:2243] Deactivating Graph...\n[info] [multi_thread_scheduler.cpp:491] TOTAL EXECUTION TIME OF SCHEDULER : 523694.460857 ms\n\n[info] [gxf_executor.cpp:2251] Graph execution finished.\n[info] [adv_network_dpdk_mgr.cpp:1928] DPDK ANO shutdown called 0\n[info] [default_bench_op_tx.h:51] ANO benchmark TX op shutting down\n[info] [default_bench_op_rx.h:56] Finished receiver with 6388570603520/6004295680 bytes/packets received and 0 packets dropped\n[info] [default_bench_op_rx.h:61] ANO benchmark RX op shutting down\n[info] [default_bench_op_rx.h:108] AdvNetworkingBenchDefaultRxOp::freeResources() start\n[info] [default_bench_op_rx.h:116] AdvNetworkingBenchDefaultRxOp::freeResources() complete\n[info] [gxf_executor.cpp:294] Destroying context\n</code></pre> <p>To inspect the speed the data is moving through the NIC, run <code>mlnx_perf</code> on one of the interfaces in a separate terminal, concurrently with the application running:</p> <pre><code>sudo mlnx_perf -i $if_name\n</code></pre> See an example output <p>On IGX with RTX A6000, we are able to hit close to the 100 Gbps linerate with this configuration: <pre><code>  rx_vport_unicast_packets: 11,614,900\n    rx_vport_unicast_bytes: 12,358,253,600 Bps   = 98,866.2 Mbps\n            rx_packets_phy: 11,614,847\n              rx_bytes_phy: 12,404,657,664 Bps   = 99,237.26 Mbps\n rx_1024_to_1518_bytes_phy: 11,614,936\n            rx_prio0_bytes: 12,404,738,832 Bps   = 99,237.91 Mbps\n          rx_prio0_packets: 11,614,923\n</code></pre></p> Troubleshooting EAL: failed to parse device <p>Make sure to set valid PCIe addresses in the <code>address</code> fields in <code>interfaces</code>, per instructions above.</p> Invalid MAC address format <p>Make sure to set a valid MAC address in the <code>eth_dst_addr</code> field in <code>bench_tx</code>, per instructions above.</p> mlx5_common: Fail to create MR for address [...] Could not DMA map EXT memory <p>Example error:</p> <pre><code>mlx5_common: Fail to create MR for address (0xffff2fc00000)\nmlx5_common: Device 0005:03:00.0 unable to DMA map\n[critical] [adv_network_dpdk_mgr.cpp:188] Could not DMA map EXT memory: -1 err=Invalid argument\n[critical] [adv_network_dpdk_mgr.cpp:430] Failed to map MRs\n</code></pre> <p>Make sure that <code>nvidia-peermem</code> is loaded.</p> EAL: Couldn't get fd on hugepage file [..] error allocating rte services array <p>Example error:</p> <pre><code>EAL: get_seg_fd(): open '/mnt/huge/nwlrbbmqbhmap_0' failed: Permission denied\nEAL: Couldn't get fd on hugepage file\nEAL: error allocating rte services array\nEAL: FATAL: rte_service_init() failed\nEAL: rte_service_init() failed\n</code></pre> <p>Ensure you run as root, using <code>sudo</code>.</p> EAL: Cannot get hugepage information. <pre><code>EAL: x hugepages of size x reserved, no mounted hugetlbfs found for that size\n</code></pre> <p>Ensure your hugepages are mounted.</p> <pre><code>EAL: No free x kB hugepages reported on node 0\n</code></pre> <ul> <li>Ensure you have allocated hugepages.</li> <li> <p>If you have already, check if they are any free left with <code>grep Huge /proc/meminfo</code>.</p> See an example output <p>No more space here!</p> <pre><code>HugePages_Total:       2\nHugePages_Free:        0\nHugePages_Rsvd:        0\nHugePages_Surp:        0\nHugepagesize:    1048576 kB\nHugetlb:         2097152 kB\n</code></pre> </li> <li> <p>If not, you can delete dangling hugepages under your hugepage mount point. That happens when your previous application run crashes.</p> <pre><code>sudo rm -rf /dev/hugepages/* # default mount point\nsudo rm -rf /mnt/huge/*      # custom mount point\n</code></pre> </li> </ul> Could not allocate x MB of GPU memory [...] Failed to allocate GPU memory <p>Check your GPU utilization:</p> <pre><code>nvidia-smi pmon -c 1\n</code></pre> <p>You might need to kill some of the listed processes to free up GPU VRAM.</p>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#5-building-your-own-application","title":"5. Building your own application","text":"<p>This section will guide you through building your own application using the <code>adv_networking_bench</code> as an example. Make sure to install <code>holoscan-networking</code> first.</p>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#51-understand-the-configuration-parameters","title":"5.1 Understand the configuration parameters","text":"<p>Note</p> <p>The configuration below will be analyzed in the context of the application consuming it, as defined in the <code>main.cpp</code> file. You can look it up when the \"sample application code\" is referenced.</p> Debian installationFrom source <pre><code>/opt/nvidia/holoscan/examples/adv_networking_bench/main.cpp\n</code></pre> <pre><code>./applications/adv_networking_bench/cpp/main.cpp\n</code></pre> <p>If you are not yet familiar with how Holoscan applications are constructed, please refer to the Holoscan SDK documentation first.</p> <p>Let's look at the <code>adv_networking_bench_default_tx_rx.yaml</code> file below. Click on the (1) icons below to expand explanations for each annotated line.</p> <ol> <li>The cake is a lie </li> </ol> <pre><code>scheduler: # (1)!\n  check_recession_period_ms: 0\n  worker_thread_number: 5\n  stop_on_deadlock: true\n  stop_on_deadlock_timeout: 500\n  # max_duration_ms: 20000\n\nadvanced_network: # (2)!\n  cfg:\n    version: 1\n    manager: \"dpdk\" # (3)!\n    master_core: 3 # (4)!\n    debug: false\n    log_level: \"info\"\n\n    memory_regions: # (5)!\n    - name: \"Data_TX_GPU\" # (6)!\n      kind: \"device\" # (7)!\n      affinity: 0 # (8)!\n      num_bufs: 51200 # (9)!\n      buf_size: 1064 # (10)!\n    - name: \"Data_RX_GPU\"\n      kind: \"device\"\n      affinity: 0\n      num_bufs: 51200\n      buf_size: 1000\n    - name: \"Data_RX_CPU\"\n      kind: \"huge\"\n      affinity: 0\n      num_bufs: 51200\n      buf_size: 64\n\n    interfaces: # (11)!\n    - name: \"tx_port\" # (12)!\n      address: &lt;0000:00:00.0&gt; # (13)! # The BUS address of the interface doing Tx\n      tx: # (14)!\n        queues: # (15)!\n        - name: \"tx_q_0\" # (16)!\n          id: 0 # (17)!\n          batch_size: 10240 # (18)!\n          cpu_core: 11 # (19)!\n          memory_regions: # (20)!\n            - \"Data_TX_GPU\"\n          offloads: # (21)!\n            - \"tx_eth_src\"\n    - name: \"rx_port\"\n      address: &lt;0000:00:00.0&gt; # (22)! # The BUS address of the interface doing Rx\n      rx:\n        flow_isolation: true # (23)!\n        queues:\n        - name: \"rx_q_0\"\n          id: 0\n          cpu_core: 9\n          batch_size: 10240\n          memory_regions: # (24)!\n            - \"Data_RX_CPU\"\n            - \"Data_RX_GPU\"\n        flows: # (25)!\n        - name: \"flow_0\" # (26)!\n          id: 0 # (27)!\n          action: # (28)!\n            type: queue\n            id: 0\n          match: # (29)!\n            udp_src: 4096\n            udp_dst: 4096\n            ipv4_len: 1050\n\nbench_rx: # (30)!\n  interface_name: \"rx_port\" # Name of the RX port from the advanced_network config\n  gpu_direct: true          # Set to true if using a GPU region for the Rx queues.\n  split_boundary: true      # Whether header and data are split for Rx (Header to CPU)\n  batch_size: 10240\n  max_packet_size: 1064\n  header_size: 64\n\nbench_tx: # (31)!\n  interface_name: \"tx_port\" # Name of the TX port from the advanced_network config\n  gpu_direct: true          # Set to true if using a GPU region for the Tx queues.\n  split_boundary: 0         # Byte boundary where header and data are split for Tx, 0 if no split\n  batch_size: 10240\n  payload_size: 1000\n  header_size: 64\n  eth_dst_addr: &lt;00:00:00:00:00:00&gt; # Destination MAC address - required when Rx flow_isolation=true\n  ip_src_addr: &lt;1.2.3.4&gt;    # Source IP address - required on layer 3 network\n  ip_dst_addr: &lt;5.6.7.8&gt;    # Destination IP address - required on layer 3 network\n  udp_src_port: 4096        # UDP source port\n  udp_dst_port: 4096        # UDP destination port\n</code></pre> <ol> <li>The <code>scheduler</code> section is passed to the multi threaded scheduler we declare in the <code>main()</code> function of this application. See the holoscan SDK documentation and API docs for more details. This is related to the Holoscan core library and is not specific to Holoscan Networking.</li> <li>The <code>advanced_network</code> section is passed to the <code>advanced_network::adv_net_init</code> which is responsible for setting up the NIC. That function should be called in your <code>Application::compose()</code> function.</li> <li><code>manager</code> is the backend networking library. default: <code>dpdk</code>. Other: <code>gpunetio</code> (DOCA GPUNet IO + DOCA Ethernet &amp; Flow). Coming soon: <code>rivermax</code>, <code>rdma</code>.</li> <li><code>master_core</code> is the ID of the CPU core used for setup. It does not need to be isolated, and is recommended to differ differ from the <code>cpu_core</code> fields below used for polling the NIC.</li> <li>The <code>memory_regions</code> section lists where the NIC will write/read data from/to when bypassing the OS kernel. Tip: when using GPU buffer regions, keeping the sum of their buffer sizes lower than 80% of your BAR1 size is generally a good rule of thumb \ud83d\udc4d.</li> <li>A descriptive name for that memory region to refer to later in the <code>interfaces</code> section.</li> <li>The type of memory region. Best options are <code>device</code> (GPU), or <code>huge</code> (pages - CPU). Also supported but not recommended are <code>malloc</code> (CPU) and <code>pinned</code> (CPU).</li> <li>The GPU ID for <code>device</code> memory regions. The NUMA node ID for CPU memory regions.</li> <li>The number of buffers in the memory region. A higher value means more time to process the data, but it takes additional space on the GPU BAR1. Too low increases the risk of dropping packets from the NIC having nowhere to write (Rx) or the risk of higher latency from buffering (Tx). Need a rule of thumb \ud83d\udc4d? 5x the <code>batch_size</code> below is a good starting point.</li> <li>The size of each buffer in the memory region. These should be equal to your maximum packet size, or less if breaking down packets (ex: header data split, see the <code>rx</code> queue below).</li> <li>The <code>interfaces</code> section lists the NIC interfaces that will be configured for the application.</li> <li>A descriptive name for that interface, currently only used for logging.</li> <li>The PCIe/bus address of that interface, as identified in previous sections.</li> <li>Each interface can have a <code>tx</code> (transmitting) or <code>rx</code> (receiving) section, or both if you'd like to configure both Tx and Rx on the same interface.</li> <li>The <code>queues</code> section lists the queues for that interface. Queues are a core concept of NICs: they handle the actual receiving or transmitting of network packets. Rx queues buffer incoming packets until they can be processed by the application, while Tx queues hold outgoing packets waiting to be sent on the network. The simplest setup uses only one receive and one transmit queue. Using more queues allows multiple streams of network traffic to be processed in parallel, as each queue can be assigned to a specific CPU core, and are assigned their own memory regions that are not shared.</li> <li>A descriptive name for that queue, currently only used for logging.</li> <li>The ID of that queue, which can be referred to later in the <code>flows</code> section.</li> <li>The number of packets per batch (or burst). Your Rx operator will have access to packets from the NIC when it receives enough packets for a whole batch/burst. Your Tx operator needs to ensure it does not send more packets than this value on each <code>Operator::compute()</code> call.</li> <li>The ID of the CPU core that this queue will use to poll the NIC. Ideally one isolated core per queue.</li> <li>The list of memory regions where this queue will write/read packets from/to. The order matters: the first memory region will be used first to read/write from until it fills up one buffer (<code>buf_size</code>), after which it will move to the next region in the list and so on until the packet is fully written/read. See the <code>memory_regions</code> for the <code>rx</code> queue below for an example.</li> <li>The <code>offloads</code> section (Tx queues only) lists optional tasks that can be offloaded to the NIC. The only value currently supported is <code>tx_eth_src</code>, that lets the NIC insert the ethernet source mac address in the packet headers. Note: IP, UDP, and Ethernet Checksums or CRC are always done by the NIC currently and are not optional.</li> <li>Same as for <code>tx_port</code>. Each interface in this list should have a unique mac address. This one will do <code>rx</code> per config below.</li> <li>Whether to isolate the Rx flow. If true, any incoming packets that does not match the MAC address of this interface - or isn't directed to a queue when the <code>flows</code> section below is used - will be delegated back to Linux for processing (no kernel bypass). This is useful to let this interface handle ARP, ICMP, etc. Otherwise, any packets sent to this interface (ex: ping) will need to be processed (or dropped) by your application.</li> <li>This scenario is called HDS (Header-Data Split): the packet will first be written to a buffer in the <code>Data_RX_CPU</code> memory region, filling its <code>buf_size</code> of 64 bytes - which is consistent with the size of our header - then the rest of the packet will be written to the <code>Data_RX_GPU</code> memory region. Its <code>buf_size</code> of 1000 bytes is just what we need to write the payload size for our application, no byte wasted!</li> <li>The list of flows. Flows are responsible for routing packets to the correct queue based on various properties. If this field is missing, all packets will be routed to the first queue.</li> <li>The flow name, currently only used for logging.</li> <li>The flow <code>id</code> is used to tag the packets with what flow it arrived on. This is useful when sending multiple flows to a single queue, as the user application can differentiate which flow (i.e. rules) matched the packet based on this ID.</li> <li>What to do with packets that match this flow. The only supported action currently is <code>type: queue</code> to send the packet to a queue given its <code>id</code>.</li> <li>List of rules to match packets against. All rules must be met for a packet to match the flow. Currently supported rules include <code>udp_src</code> and <code>udp_dst</code> (port numbers), <code>ipv4_len</code> (#TODO#) etc.</li> <li>The <code>bench_rx</code> section is passed to the <code>AdvNetworkingBenchDefaultRxOp</code> operator in the <code>Application::compose()</code> function of the sample application. This operator is a custom operator implemented in <code>default_bench_op_rx.h</code> that pulls and aggregates packets received from the NIC, with parameters specific to its own implementation, which can be used as a reference for your own Rx operator. The first parameter, <code>interface_name</code>, is used to specify which NIC interface to use for the Rx operation. The following parameters are should align with how <code>memory_regions</code> and <code>queues</code> were configured for the <code>rx</code> interface.</li> <li>The <code>bench_tx</code> section is passed to the <code>AdvNetworkingBenchDefaultTxOp</code> operator in the <code>Application::compose()</code> function of the sample application. This operator is a custom operator implemented in <code>default_bench_op_tx.h</code> that generates dummy packets to send to the NIC, with parameters specific to its own implementation, which can be used as a reference for your own Tx operator. The first parameter, <code>interface_name</code>, is used to specify which NIC interface to use for the Tx operation. The following parameters up to <code>header_size</code> should align with how <code>memory_regions</code> and <code>queues</code> were configured for the <code>tx</code> interface. The remaining parameters are used to fill-in the ethernet header of the packets (ETH, IP, UDP).</li> </ol>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#52-create-your-own-rx-operator","title":"5.2 Create your own Rx operator","text":"<p>Under construction</p> <p>This section is under construction. Refer to the implementation of the <code>AdvNetworkingBenchDefaultRxOp</code> for an example.</p> Debian installationFrom source <pre><code>/opt/nvidia/holoscan/examples/adv_networking_bench/default_bench_op_rx.h\n</code></pre> <pre><code>./applications/adv_networking_bench/cpp/default_bench_op_rx.h\n</code></pre> <p>Note</p> <p>Design investigations are expected soon for a generic packet aggregator operator.</p>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#53-create-your-own-tx-operator","title":"5.3 Create your own Tx operator","text":"<p>Under construction</p> <p>This section is under construction. Refer to the implementation of the <code>AdvNetworkingBenchDefaultTxOp</code> for an example.</p> Debian installationFrom source <pre><code>/opt/nvidia/holoscan/examples/adv_networking_bench/default_bench_op_tx.h\n</code></pre> <pre><code>./applications/adv_networking_bench/cpp/default_bench_op_tx.h\n</code></pre> <p>Note</p> <p>Designs investigations are expected soon for a generic way to prepare packets to send to the NIC.</p>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/#54-build-with-cmake","title":"5.4 Build with CMake","text":"Debian installationFrom source <ol> <li>Create a source directory and write your source file(s) for your application and custom operators.</li> <li> <p>Create a <code>CMakeLists.txt</code> file in your source directory like this one:</p> <pre><code>cmake_minimum_required(VERSION 3.20)\nproject(my_app CXX) # Add CUDA if writing .cu kernels\n\nfind_package(holoscan 2.6 REQUIRED CONFIG PATHS \"/opt/nvidia/holoscan\")\nfind_package(holoscan-networking REQUIRED CONFIG PATHS \"/opt/nvidia/holoscan\")\n\n# Create an executable\nadd_executable(my_app\n    my_app.cpp\n    ...\n)\ntarget_include_directories(my_app\n    PRIVATE\n        my_include_dirs/\n        ...\n)\ntarget_link_libraries(my_app\n    PRIVATE\n        holoscan::core\n        holoscan::ops::advanced_network_rx\n        holoscan::ops::advanced_network_tx\n        my_other_dependencies\n        ...\n)\n\n# Copy the config file to the build directory for convenience referring to it\nadd_custom_target(my_app_config_yaml\n    COMMAND ${CMAKE_COMMAND} -E copy_if_different \"${CMAKE_CURRENT_SOURCE_DIR}/my_app_config.yaml\" ${CMAKE_CURRENT_BINARY_DIR}\n    DEPENDS \"${CMAKE_CURRENT_SOURCE_DIR}/my_app_config.yaml\"\n)\nadd_dependencies(my_app my_app_config_yaml)\n</code></pre> </li> <li> <p>Build your application like so:</p> <pre><code># Your chosen paths\nsrc_dir=\".\"\nbuild_dir=\"build\"\n\n# Configure the build\ncmake -S \"$src_dir\" -B \"$build_dir\"\n\n# Build the application\ncmake --build \"$build_dir\" -j\n</code></pre> Failed to detect a default CUDA architecture. <p>Add the path to your installation of <code>nvcc</code> to your <code>PATH</code>, or pass its to the cmake configuration command like so (adjust to your CUDA/nvcc installation path):</p> <pre><code>cmake -S \"$src_dir\" -B \"$build_dir\" -D CMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc\n</code></pre> </li> <li> <p>Run your application like so:</p> <pre><code>\"./$build_dir/my_app my_app_config.yaml\"\n</code></pre> </li> </ol> <ol> <li>Create an application directory under <code>applications/</code> in your clone of the HoloHub repository, and write your source file(s) for your application and custom operators.</li> <li> <p>Add the following to the <code>application/CMakeLists.txt</code> file:</p> <pre><code>add_holohub_application(my_app DEPENDS OPERATORS advanced_network)\n</code></pre> </li> <li> <p>Create a <code>CMakeLists.txt</code> file in your application directory like this one:</p> <pre><code>cmake_minimum_required(VERSION 3.20)\nproject(my_app CXX) # Add CUDA if writing .cu kernels\n\nfind_package(holoscan 2.6 REQUIRED CONFIG PATHS \"/opt/nvidia/holoscan\")\n\n# Create an executable\nadd_executable(my_app\n    my_app.cpp\n    ...\n)\ntarget_include_directories(my_app\n    PRIVATE\n        my_include_dirs/\n        ...\n)\ntarget_link_libraries(my_app\n    PRIVATE\n        holoscan::core\n        holoscan::ops::advanced_network_rx\n        holoscan::ops::advanced_network_tx\n        my_other_dependencies\n        ...\n)\n\n# Copy the config file to the build directory for convenience referring to it\nadd_custom_target(my_app_config_yaml\n    COMMAND ${CMAKE_COMMAND} -E copy_if_different \"${CMAKE_CURRENT_SOURCE_DIR}/my_app_config.yaml\" ${CMAKE_CURRENT_BINARY_DIR}\n    DEPENDS \"${CMAKE_CURRENT_SOURCE_DIR}/my_app_config.yaml\"\n)\nadd_dependencies(my_app my_app_config_yaml)\n</code></pre> </li> <li> <p>Build your application like so:</p> <pre><code>./holohub build my_app\n</code></pre> </li> <li> <p>Run your application like so:</p> <pre><code>./holohub run --img holohub:my_app --docker-opts \"-u 0 --privileged\" --bash -c \"./build/my_app/applications/my_app my_app_config.yaml\"\n</code></pre> <p>or, if you have set up a shortcut to run your application with its config file through its <code>metadata.json</code> (see other apps for examples):</p> <pre><code>./holohub run --no-local-build --container_args \" -u 0 --privileged\"\n</code></pre> </li> </ol>","tags":["Networking and Distributed Computing","DPDK","RDMA","GPUNetIO","GPUDirect","HPC"]},{"location":"tutorials/high_performance_networking/0.1/","title":"High Performance Networking with Holoscan","text":"<p> Authors: Alexis Girault (NVIDIA) Supported platforms: x86_64, SBSA, IGX Orin (dGPU) Language: C++ Last modified: March 17, 2025 Version: latest (0.1.0) 0.1 <p> Minimum Holoscan SDK version: 3.0.0 Tested Holoscan SDK versions: 3.0.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This tutorial demonstrates how to use the advanced networking Holoscan operator (often referred to as ANO or <code>advanced_network</code> in HoloHub) for low latency and high throughput communication through NVIDIA SmartNICs. With a properly tuned system, the advanced network operator can achieve hundreds of Gbps with latencies in the low microseconds.</p> <p>Note</p> <p>This solution is designed for users who want to create a Holoscan application that will interface with an external system or sensor over Ethernet.</p> <ul> <li>For high performance communication with systems also running Holoscan, refer to the Holoscan distributed application documentation instead.</li> <li>For JESD-compliant sensor without Ethernet support, consider the Holoscan Sensor Bridge for an FPGA-based interface to Holoscan.</li> </ul>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#prerequisites","title":"Prerequisites","text":"<p>Achieving High Performance Networking with Holoscan requires a system with an NVIDIA SmartNIC and a discrete GPU. That is the case of NVIDIA Data Center systems, or edge systems like the NVIDIA IGX platform and the NVIDIA Project DIGITS. <code>x86_64</code> systems equipped with these components are also supported, though the performance will vary greatly depending on the PCIe topology of the system (more on this below).</p> <p>In this tutorial, we will be developing on an NVIDIA IGX Orin platform with IGX SW 1.1 and an NVIDIA RTX 6000 ADA GPU, which is the configuration that is currently actively tested. The concepts should be applicable to other systems based on Ubuntu 22.04 as well. It should also work on other Linux distributions with a glibc version of 2.35 or higher by containerizing the dependencies and applications on top of an Ubuntu 22.04 image, but this is not actively tested at this time.</p> <p>Secure boot conflict</p> <p>If you have secure boot enabled on your system, you might need to disable it as a prerequisite to run some of the configurations below (switching the NIC link layers to Ethernet, updating the MRRS of your NIC ports, updating the BAR1 size of your GPU). Secure boot can be re-enabled after the configurations are completed.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#background","title":"Background","text":"<p>Achieving high performance networking is a complex problem that involves many system components and configurations which we will cover in this tutorial. Two of the core concepts to achieve this are named Kernel Bypass, and GPUDirect.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#kernel-bypass","title":"Kernel Bypass","text":"<p>In this context, Kernel Bypass refers to bypassing the operating system's kernel to directly communicate with the network interface (NIC), greatly reducing the latency and overhead of the Linux network stack. There are multiple technologies that achieve this in different fashions. They're all Ethernet-based, but differ in their implementation and features. The goal of the <code>advanced_network</code> operator in Holoscan Networking is to provide a common higher-level interface to all these backends:</p> <ul> <li>RDMA: Remote Direct Memory Access, using the open-source <code>rdma-core</code> library. It differs from the other Ethernet-based backends with its server/client model and RoCE (RDMA over Ethernet) protocol. Given the extra cost and complexity to setup on both ends, it offers a simpler user interface, orders packets on arrival, and is the only one to offer a high reliability mode.</li> <li>DPDK: the Data Plane Development Kit is an open-source project part of the Linux Foundation with a strong and long-lasting community support. Its RTE Flow capability is generally considered the most flexible solution to split packets ingress and egress data.</li> <li>DOCA GPUNetIO: This NVIDIA proprietary technology differs from the other backends by transmitting and receiving packets from the NIC using a GPU kernel instead of CPU code, which is highly beneficial for CPU-bound applications.</li> <li>NVIDIA Rivermax: NVIDIA's other proprietary kernel bypass technology. For a license fee, it should offer the lowest latency and lowest resource utilization for video streaming (RTP packets).</li> </ul> Work In Progress <p>The Holoscan Advanced Networking Operator integration testing infrastructure is under active development. As such:</p> <ul> <li>The DPDK backend is supported and distributed with the <code>holoscan-networking</code> package, and is the only backend actively tested at this time.</li> <li>The DOCA GPUNetIO backend is supported and distributed with the <code>holoscan-networking</code> package, with testing infrastructure under development.</li> <li>The NVIDIA Rivermax backend is supported for Rx only when building from source, but not yet distributed nor actively tested. Tx support is under development.</li> <li>The RDMA backend is under active development and should be available soon.</li> </ul> <p>Which backend is best for your use case will depend on multiple factors, such as packet size, batch size, data type, and more. The goal of the Advanced Networking Operator is to abstract the interface to these backends, allowing developers to focus on the application logic and experiment with different configurations to identify the best technology for their use case.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#gpudirect","title":"GPUDirect","text":"<p><code>GPUDirect</code> allows the NIC to read and write data from/to a GPU without requiring to copy the data the system memory, decreasing CPU overheads and significantly reducing latency. An implementation of <code>GPUDirect</code> is supported by all the kernel bypass backends listed above.</p> <p>Warning</p> <p><code>GPUDirect</code> is only supported on Workstation/Quadro/RTX GPUs and Data Center GPUs. It is not supported on GeForce cards.</p> How does that relate to peermem or dma-buf? <p>There are two interfaces to enable <code>GPUDirect</code>:</p> <ul> <li>The <code>nvidia-peermem</code> kernel module, distributed with the NVIDIA DKMS GPU drivers.<ul> <li>Supported on Ubuntu kernels 5.4+, deprecated starting with kernel 6.8.</li> <li>Supported on NVIDIA optimized Linux kernels, including IGX OS and DGX OS.</li> <li>Supported by all MOFED drivers (requires rebuilding nvidia-dkms drivers afterwards).</li> </ul> </li> <li><code>DMA Buf</code>, supported on Linux kernels 5.12+ with NVIDIA open-source drivers 515+ and CUDA toolkit 11.7+.</li> </ul>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#1-installing-holoscan-networking","title":"1. Installing Holoscan Networking","text":"<p>We'll start with installing the <code>holoscan-networking</code> package, as it provides some utilities to help tune the system, and requires some dependencies which will help us with the system setup.</p> <p>First, add the DOCA apt repository which holds some of its dependencies:</p> IGX OS 1.1SBSA (Ubuntu 22.04)x86_64 (Ubuntu 22.04) <pre><code>export DOCA_URL=\"https://linux.mellanox.com/public/repo/doca/2.10.0/ubuntu22.04/arm64-sbsa/\"\nwget -qO- https://linux.mellanox.com/public/repo/doca/GPG-KEY-Mellanox.pub | gpg --dearmor - | sudo tee /etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub &gt; /dev/null\necho \"deb [signed-by=/etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub] $DOCA_URL ./\"  | sudo tee /etc/apt/sources.list.d/doca.list &gt; /dev/null\n\nsudo apt update\n</code></pre> <pre><code>export DOCA_URL=\"https://linux.mellanox.com/public/repo/doca/2.10.0/ubuntu22.04/arm64-sbsa/\"\nwget -qO- https://linux.mellanox.com/public/repo/doca/GPG-KEY-Mellanox.pub | gpg --dearmor - | sudo tee /etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub &gt; /dev/null\necho \"deb [signed-by=/etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub] $DOCA_URL ./\"  | sudo tee /etc/apt/sources.list.d/doca.list &gt; /dev/null\n\n# Also need the CUDA repository for holoscan: https://developer.nvidia.com/cuda-downloads?target_os=Linux\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\n\nsudo apt update\n</code></pre> <pre><code>export DOCA_URL=\"https://linux.mellanox.com/public/repo/doca/2.10.0/ubuntu22.04/x86_64/\"\nwget -qO- https://linux.mellanox.com/public/repo/doca/GPG-KEY-Mellanox.pub | gpg --dearmor - | sudo tee /etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub &gt; /dev/null\necho \"deb [signed-by=/etc/apt/trusted.gpg.d/GPG-KEY-Mellanox.pub] $DOCA_URL ./\"  | sudo tee /etc/apt/sources.list.d/doca.list &gt; /dev/null\n\n# Also need the CUDA repository for holoscan: https://developer.nvidia.com/cuda-downloads?target_os=Linux\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\n\nsudo apt update\n</code></pre> <p>You can then install <code>holoscan-networking</code>:</p> Debian installationFrom source <pre><code>sudo apt install -y holoscan-networking\n</code></pre> <p>You can build the Holoscan Networking libraries and sample applications from source on HoloHub:</p> <pre><code>git clone git@github.com:nvidia-holoscan/holohub.git\ncd holohub\n./dev_container build_and_install holoscan-networking   # Installed in ./install\n</code></pre> <p>If you'd like to generate the debian package from source and install it to ensure all dependencies are then present on your system, you can run:</p> <pre><code>./dev_container build_and_package holoscan-networking\nsudo apt-get install ./holoscan-networking_*.deb        # Installed in /opt/nvidia/holoscan\n</code></pre> <p>Refer to the HoloHub README for more information.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#2-required-system-setup","title":"2. Required System Setup","text":"","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#21-check-your-nic-drivers","title":"2.1 Check your NIC drivers","text":"<p>Ensure your NIC drivers are loaded:</p> <pre><code>lsmod | grep ib_core\n</code></pre> See an example output <p>This would be an expected output, where <code>ib_core</code> is listed on the left.</p> <pre><code>ib_core               442368  8 rdma_cm,ib_ipoib,iw_cm,ib_umad,rdma_ucm,ib_uverbs,mlx5_ib,ib_cm\nmlx_compat             20480  11 rdma_cm,ib_ipoib,mlxdevm,iw_cm,ib_umad,ib_core,rdma_ucm,ib_uverbs,mlx5_ib,ib_cm,mlx5_core\n</code></pre> <p>If this is empty, install the latest OFED drivers from DOCA (the DOCA APT repository should already be configured from the Holoscan Networking installation above), and reboot your system:</p> <pre><code>sudo apt update\nsudo apt install doca-ofed\nsudo reboot\n</code></pre> <p>Note</p> <p>If this is not empty, you can still install the newest OFED drivers from <code>doca-ofed</code> above. If you choose to keep your current drivers, install the following utilities for convenience later on. They include tools like <code>ibstat</code>, <code>ibv_devinfo</code>, <code>ibdev2netdev</code>, <code>mlxconfig</code>:</p> <pre><code>sudo apt update\nsudo apt install infiniband-diags ibverbs-utils mlnx-ofed-kernel-utils mft\n</code></pre> <p>Also upgrade the user space libraries to make sure your tools have all the symbols they need:</p> <pre><code>sudo apt install libibverbs1 librdmacm1 rdma-core\n</code></pre> <p>Running <code>ibstat</code> or <code>ibv_devinfo</code> will confirm your NIC interfaces are recognized by your drivers.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#22-switch-your-nic-link-layers-to-ethernet","title":"2.2 Switch your NIC Link Layers to Ethernet","text":"<p>NVIDIA SmartNICs can function in two separate modes (called link layer):</p> <ul> <li>Ethernet (ETH)</li> <li>Infiniband (IB)</li> </ul> <p>To identify the current mode, run <code>ibstat</code> or <code>ibv_devinfo</code> and look for the <code>Link Layer</code> value.</p> <pre><code>ibv_devinfo\n</code></pre> Couldn't load driver 'libmlx5-rdmav34.so' <p>If you see an error like this, you might have different versions for your OFED tools and libraries. Attempt after upgrading your user space libraries to match the version of your OFED tools like so:</p> <pre><code>sudo apt update\nsudo apt install libibverbs1 librdmacm1 rdma-core\n</code></pre> See an example output <p>In the example below, the <code>mlx5_0</code> interface is in Ethernet mode, while the <code>mlx5_1</code> interface is in Infiniband mode. Do not pay attention to the <code>transport</code> value which is always <code>InfiniBand</code>.</p> <pre><code>hca_id: mlx5_0\n        transport:                      InfiniBand (0)\n        fw_ver:                         28.38.1002\n        node_guid:                      48b0:2d03:00f4:07fb\n        sys_image_guid:                 48b0:2d03:00f4:07fb\n        vendor_id:                      0x02c9\n        vendor_part_id:                 4129\n        hw_ver:                         0x0\n        board_id:                       NVD0000000033\n        phys_port_cnt:                  1\n                port:   1\n                        state:                  PORT_ACTIVE (4)\n                        max_mtu:                4096 (5)\n                        active_mtu:             4096 (5)\n                        sm_lid:                 0\n                        port_lid:               0\n                        port_lmc:               0x00\n                        link_layer:             Ethernet\n\nhca_id: mlx5_1\n        transport:                      InfiniBand (0)\n        fw_ver:                         28.38.1002\n        node_guid:                      48b0:2d03:00f4:07fc\n        sys_image_guid:                 48b0:2d03:00f4:07fb\n        vendor_id:                      0x02c9\n        vendor_part_id:                 4129\n        hw_ver:                         0x0\n        board_id:                       NVD0000000033\n        phys_port_cnt:                  1\n                port:   1\n                        state:                  PORT_ACTIVE (4)\n                        max_mtu:                4096 (5)\n                        active_mtu:             4096 (5)\n                        sm_lid:                 0\n                        port_lid:               0\n                        port_lmc:               0x00\n                        link_layer:             InfiniBand\n</code></pre> <p>For Holoscan Networking, we want the NIC to use the ETH link layer. To switch the link layer mode, there are two possible options:</p> <ol> <li>On IGX Orin developer kits, you can switch that setting through the BIOS: see IGX Orin documentation.</li> <li> <p>On any system with a NVIDIA NIC (including the IGX Orin developer kits), you can run the commands below from a terminal:</p> <ol> <li> <p>Identify the PCI address of your NVIDIA NIC</p> ibdev2netdevlspci <pre><code>nic_pci=$(sudo ibdev2netdev -v | awk '{print $1}' | head -n1)\n</code></pre> <pre><code># `0200` is the PCI-SIG class code for Ethernet controllers\n# `0207` is the PCI-SIG class code for Infiniband controllers\n# `15b3` is the Vendor ID for Mellanox\nnic_pci=$(lspci -n | awk '($2 == \"0200:\" || $2 == \"0207:\") &amp;&amp; $3 ~ /^15b3:/ {print $1; exit}')\n</code></pre> </li> <li> <p>Set both link layers to Ethernet. <code>LINK_TYPE_P1</code> and <code>LINK_TYPE_P2</code> are for <code>mlx5_0</code> and <code>mlx5_1</code> respectively. You can choose to only set one of them. <code>ETH</code> or <code>2</code> is Ethernet mode, and <code>IB</code> or <code>1</code> is for InfiniBand.</p> <pre><code>sudo mlxconfig -d $nic_pci set LINK_TYPE_P1=ETH LINK_TYPE_P2=ETH\n</code></pre> <p>Apply with <code>y</code>.</p> See an example output <pre><code>Device #1:\n----------\n\nDevice type:    ConnectX7\nName:           P3740-B0-QSFP_Ax\nDescription:    NVIDIA Prometheus P3740 ConnectX-7 VPI PCIe Switch Motherboard; 400Gb/s; dual-port QSFP; PCIe switch5.0 X8 SLOT0 ;X16 SLOT2; secure boot;\nDevice:         0005:03:00.0\n\nConfigurations:                                      Next Boot       New\n        LINK_TYPE_P1                                ETH(2)          ETH(2)\n        LINK_TYPE_P2                                IB(1)           ETH(2)\n\nApply new Configuration? (y/n) [n] :\ny\n\nApplying... Done!\n-I- Please reboot machine to load new configurations.\n</code></pre> <ul> <li><code>Next Boot</code> is the current value that was expected to be used at the next reboot.</li> <li><code>New</code> is the value you're about to set to override <code>Next Boot</code>.</li> </ul> ERROR: write counter to semaphore: Operation not permitted <p>Disable secure boot on your system ahead of changing the link type of your NIC ports. It can be re-enabled afterwards.</p> </li> <li> <p>Reboot your system.</p> <pre><code>sudo reboot\n</code></pre> </li> </ol> </li> </ol>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#23-configure-the-ip-addresses-of-the-nic-ports","title":"2.3 Configure the IP addresses of the NIC ports","text":"<p>First, we want to identify the logical names of your NIC interfaces. Connecting an SFP cable in just one of the ports of the NIC will help you identify which port is which. Run the following command once the cable is in place:</p> <pre><code>ibdev2netdev\n</code></pre> See an example output <p>In the example below, only <code>mlx5_1</code> has a cable connected (<code>Up</code>), and its logical ethernet name is <code>eth1</code>:</p> <pre><code>$ ibdev2netdev\nmlx5_0 port 1 ==&gt; eth0 (Down)\nmlx5_1 port 1 ==&gt; eth1 (Up)\n</code></pre> ibdev2netdev does not show the NIC <p>If you have a cable connected but it does not show Up/Down in the output of <code>ibdev2netdev</code>, you can try to parse the output of <code>dmesg</code> instead. The example below shows that <code>0005:03:00.1</code> is plugged, and that it is associated with <code>eth1</code>:</p> <pre><code>$ sudo dmesg | grep -w mlx5_core\n...\n[   11.512808] mlx5_core 0005:03:00.0 eth0: Link down\n[   11.640670] mlx5_core 0005:03:00.1 eth1: Link down\n...\n[ 3712.267103] mlx5_core 0005:03:00.1: Port module event: module 1, Cable plugged\n</code></pre> <p>The next step is to set a static IP on the interface you'd like to use so you can refer to it in your Holoscan applications. First, check if you already have any addresses configured using the ethernet interface names identified above (in our case, <code>eth0</code> and <code>eth1</code>):</p> <pre><code>ip -f inet addr show eth0\nip -f inet addr show eth1\n</code></pre> <p>If nothing appears, or you'd like to change the address, you can set an IP address through the Network Manager user interface, CLI (<code>nmcli</code>), or other IP configuration tools. In the example below, we configure the <code>eth0</code> interface with an address of <code>1.1.1.1/24</code>, and the <code>eth1</code> interface with an address of <code>2.2.2.2/24</code>.</p> One-timePersistent <pre><code>sudo ip addr add 1.1.1.1/24 dev eth0\nsudo ip addr add 2.2.2.2/24 dev eth1\n</code></pre> <p>Set these variables to your desired values:</p> <pre><code>if_name=eth0\nif_static_ip=1.1.1.1/24\n</code></pre> NetworkManagersystemd-networkd <p>Update the IP with <code>nmcli</code>:</p> <pre><code>sudo nmcli connection modify $if_name ipv4.addresses $if_static_ip\nsudo nmcli connection up $if_name\n</code></pre> <p>Create a network config file with the static IP:</p> <pre><code>cat &lt;&lt; EOF | sudo tee /etc/systemd/network/20-$if_name.network\n[Match]\nMACAddress=$(cat /sys/class/net/$if_name/address)\n\n[Network]\nAddress=$if_static_ip\nEOF\n</code></pre> <p>Apply now:</p> <pre><code>sudo systemctl restart systemd-networkd\n</code></pre> <p>Note</p> <p>If you are connecting the NIC to another NIC with an interconnect, do the same on the other system with an IP address on the same network segment. For example, to communicate with <code>1.1.1.1/24</code> above (<code>/24</code> -&gt; <code>255.255.255.0</code> submask), setup your other system with an IP between <code>1.1.1.2</code> and <code>1.1.1.254</code>, and the same <code>/24</code> submask.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#24-enable-gpudirect","title":"2.4 Enable GPUDirect","text":"<p>Assuming you already have NVIDIA drivers installed, check if the <code>nvidia_peermem</code> kernel module is loaded:</p> tune_system.py Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check topo\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check topo\n</code></pre> See an example output <pre><code>2025-03-12 14:15:07 - INFO - GPU 0: NVIDIA RTX A6000 has GPUDirect support.\n2025-03-12 14:15:27 - INFO - nvidia-peermem module is loaded.\n</code></pre> <pre><code>lsmod | grep nvidia_peermem\n</code></pre> <p>If it's not loaded, run the following command, then check again:</p> One-timePersistent <pre><code>sudo modprobe nvidia_peermem\n</code></pre> <pre><code>sudo echo \"nvidia-peermem\" &gt;&gt; /etc/modules\nsudo systemctl restart systemd-modules-load.service\n</code></pre> Error loading the <code>nvidia-peermem</code> kernel module <p>If you run into an error loading the <code>nvidia-peermem</code> kernel module, follow these steps:</p> <ol> <li>Install the <code>doca-ofed</code> package to get the latest drivers for your NIC as documented above.</li> <li>Restart your system.</li> <li>Rebuild your NVIDIA drivers with DKMS like so:</li> </ol> <pre><code>peermem_ko=$(find /lib/modules/$(uname -r) -name \"*peermem*.ko\")\nnv_dkms=$(dpkg -S \"$peermem_ko\" | cut -d: -f1)\nsudo dpkg-reconfigure $nv_dkms\nsudo modprobe nvidia_peermem\n</code></pre> Why peermem and not dma buf? <p><code>peermem</code> is currently the only GPUDirect interface supported by all our networking backends. This section will therefore provide instructions for <code>peermem</code> and not <code>dma buf</code>.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#3-optimal-system-configurations","title":"3. Optimal system configurations","text":"<p>Advanced</p> <p>The section below is for advanced users looking to extract more performance out of their system. You can choose to skip this section and return to it later if performance if your application is not satisfactory.</p> <p>While the configurations above are the minimum requirements to get a NIC and a NVIDIA GPU to communicate while bypassing the OS kernel stack, performance can be further improved in most scenarios by tuning the system as described below.</p> <p>Before diving in each of the setups below, we provide a utility script as part of the <code>holoscan-networking</code> package which provides an overview of the configurations that potentially need to be tuned on your system.</p> Work In Progress <p>This utility script is under active development and will be updated in future releases with additional checks, more actionable recommendations, and automated tuning.</p> Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check all\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check all\n</code></pre> See an example output <p>Our tuned-up IGX system with A6000 can optimize most settings:</p> <pre><code>2025-03-12 14:16:06 - INFO - CPU 0: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 1: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 2: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 3: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 4: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 5: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 6: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 7: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 8: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 9: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 10: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - CPU 11: Governor is correctly set to 'performance'.\n2025-03-12 14:16:06 - INFO - cx7_0/0005:03:00.0: MRRS is correctly set to 4096.\n2025-03-12 14:16:06 - INFO - cx7_1/0005:03:00.1: MRRS is correctly set to 4096.\n2025-03-12 14:16:06 - WARNING - cx7_0/0005:03:00.0: PCIe Max Payload Size is not set to 256 bytes. Found: 128 bytes.\n2025-03-12 14:16:06 - WARNING - cx7_1/0005:03:00.1: PCIe Max Payload Size is not set to 256 bytes. Found: 128 bytes.\n2025-03-12 14:16:06 - INFO - HugePages_Total: 3\n2025-03-12 14:16:06 - INFO - HugePage Size: 1024.00 MB\n2025-03-12 14:16:06 - INFO - Total Allocated HugePage Memory: 3072.00 MB\n2025-03-12 14:16:06 - INFO - Hugepages are sufficiently allocated with at least 500 MB.\n2025-03-12 14:16:06 - INFO - GPU 0: SM Clock is correctly set to 1920 MHz (within 500 of the 2100 MHz theoretical Max).\n2025-03-12 14:16:06 - INFO - GPU 0: Memory Clock is correctly set to 8000 MHz.\n2025-03-12 14:16:06 - INFO - GPU 00000005:09:00.0: BAR1 size is 8192 MiB.\n2025-03-12 14:16:06 - INFO - GPU GPU0 has at least one PIX/PXB connection to a NIC\n2025-03-12 14:16:06 - INFO - isolcpus found in kernel boot line\n2025-03-12 14:16:06 - INFO - rcu_nocbs found in kernel boot line\n2025-03-12 14:16:06 - INFO - irqaffinity found in kernel boot line\n2025-03-12 14:16:06 - INFO - Interface cx7_0 has an acceptable MTU of 9000 bytes.\n2025-03-12 14:16:06 - INFO - Interface cx7_1 has an acceptable MTU of 9000 bytes.\n2025-03-12 14:16:06 - INFO - GPU 0: NVIDIA RTX A6000 has GPUDirect support.\n2025-03-12 14:16:06 - INFO - nvidia-peermem module is loaded.\n</code></pre> <p>Based on the results, you can figure out which of the sections below are appropriate to update configurations on your system.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#31-ensure-ideal-pcie-topology","title":"3.1 Ensure ideal PCIe topology","text":"<p>Kernel bypass and GPUDirect rely on PCIe to communicate between the GPU and the NIC at high speeds. As-such, the topology of the PCIe tree on a system is critical to ensure optimal performance.</p> <p>Run the following command to check the GPUDirect communication matrix. You are looking for a <code>PXB</code> or <code>PIX</code> connection between the GPU and the NIC interfaces to get the best performance.</p> tune_system.pynvidia-smi Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check topo\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check topo\n</code></pre> See an example output <p>On IGX developer kits, the board's internal switch is designed to connect the GPU to the NIC interfaces with a <code>PXB</code> connection, offering great performance.</p> <pre><code>2025-03-06 12:07:45 - INFO - GPU GPU0 has at least one PIX/PXB connection to a NIC\n</code></pre> <pre><code>nvidia-smi topo -mp\n</code></pre> See an example output <p>On IGX developer kits, the board's internal switch is designed to connect the GPU to the NIC interfaces with a <code>PXB</code> connection, offering great performance. <pre><code>        GPU0    NIC0    NIC1    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PXB     PXB     0-11    0               N/A\nNIC0    PXB      X      PIX\nNIC1    PXB     PIX      X\n\nLegend:\n\nX    = Self\nSYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\nNODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\nPHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\nPXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\nPIX  = Connection traversing at most a single PCIe bridge\n\nNIC Legend:\n\nNIC0: mlx5_0\nNIC1: mlx5_1\n</code></pre></p> <p>If your connection is not optimal, you might be able to improve it by moving your NIC and/or GPU on a different PCIe port, so that they can share a branch and do not require going back to the Host Bridge (the CPU) to communicate. Refer to your system manufacturer for documentation, or run the following command to inspect the topology of your system:</p> <pre><code>lspci -tv\n</code></pre> See an example output <p>Here is the PCIe tree of an IGX system. Note how the ConnectX-7 and RTX A6000 are connected to the same branch. <pre><code>-+-[0007:00]---00.0-[01-ff]----00.0  Marvell Technology Group Ltd. 88SE9235 PCIe 2.0 x2 4-port SATA 6 Gb/s Controller\n+-[0005:00]---00.0-[01-ff]----00.0-[02-09]--+-00.0-[03]--+-00.0  Mellanox Technologies MT2910 Family [ConnectX-7]\n|                                           |            \\-00.1  Mellanox Technologies MT2910 Family [ConnectX-7]\n|                                           +-01.0-[04-06]----00.0-[05-06]----08.0-[06]--\n|                                           \\-02.0-[07-09]----00.0-[08-09]----00.0-[09]--+-00.0  NVIDIA Corporation GA102GL [RTX A6000]\n|                                                                                        \\-00.1  NVIDIA Corporation GA102 High Definition Audio Controller\n+-[0004:00]---00.0-[01-ff]----00.0  Sandisk Corp WD PC SN810 / Black SN850 NVMe SSD\n+-[0001:00]---00.0-[01-ff]----00.0-[02-fc]--+-01.0-[03-34]----00.0  Realtek Semiconductor Co., Ltd. RTL8111/8168/8411 PCI Express Gigabit Ethernet Controller\n|                                           +-02.0-[35-66]----00.0  Realtek Semiconductor Co., Ltd. RTL8111/8168/8411 PCI Express Gigabit Ethernet Controller\n|                                           +-03.0-[67-98]----00.0  Device 1c00:3450\n|                                           +-04.0-[99-ca]----00.0-[9a]--+-00.0  ASPEED Technology, Inc. ASPEED Graphics Family\n|                                           |                            \\-02.0  ASPEED Technology, Inc. Device 2603\n|                                           \\-05.0-[cb-fc]----00.0  Realtek Semiconductor Co., Ltd. RTL8822CE 802.11ac PCIe Wireless Network Adapter\n\\-[0000:00]-\n</code></pre></p> <p>x86_64 compatibility</p> <p>Most x86_64 systems are not designed for this topology as they lack a discrete PCIe switch. In that case, the best connection they can achieve is <code>NODE</code>.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#32-check-the-nics-pcie-configuration","title":"3.2 Check the NIC's PCIe configuration","text":"<p>Understanding PCIe Configuration for Maximum Performance - May 27, 2022</p> <p>PCIe is used in any system for communication between different modules [including the NIC and the GPU]. This means that in order to process network traffic, the different devices communicating via the PCIe should be well configured. When connecting the network adapter to the PCIe, it auto-negotiates for the maximum capabilities supported between the network adapter and the CPU.</p> <p>The instructions below are meant to understand if your system is able to extract the maximum capabilities of your NIC, but they're not configurable. The two values that we are looking at here are the Max Payload Size (MPS - the maximum size of a PCIe packet) and the Speed (or PCIe generation).</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#max-payload-size-mps","title":"Max Payload Size (MPS)","text":"tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check mps\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check mps\n</code></pre> See an example output <p>The PCIe configuration on the IGX Orin developer kit is not able to leverage the max payload size of the NIC:</p> <pre><code>2025-03-10 16:15:54 - WARNING - cx7_0/0005:03:00.0: PCIe Max Payload Size is not set to 256 bytes. Found: 128 bytes.\n2025-03-10 16:15:54 - WARNING - cx7_1/0005:03:00.1: PCIe Max Payload Size is not set to 256 bytes. Found: 128 bytes.\n</code></pre> <p>Identify the PCIe address of your NVIDIA NIC:</p> ibdev2netdevlspci <pre><code>nic_pci=$(sudo ibdev2netdev -v | awk '{print $1}' | head -n1)\n</code></pre> <pre><code># `0200` is the PCI-SIG class code for NICs\n# `15b3` is the Vendor ID for Mellanox\nnic_pci=$(lspci -n | awk '$2 == \"0200:\" &amp;&amp; $3 ~ /^15b3:/ {print $1}' | head -n1)\n</code></pre> <p>Check current and max MPS:</p> <pre><code>sudo lspci -vv -s $nic_pci | awk '/DevCap/{s=1} /DevCtl/{s=0} /MaxPayload /{match($0, /MaxPayload [0-9]+/, m); if(s){print \"Max \" m[0]} else{print \"Current \" m[0]}}'\n</code></pre> See an example output <p>The PCIe configuration on the IGX Orin developer kit is not able to leverage the max payload size of the NIC:</p> <pre><code>Max MaxPayload 512\nCurrent MaxPayload 128\n</code></pre> <p>Note</p> <p>While your NIC might be capable of more, 256 bytes is generally the largest supported by any switch/CPU at this time.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#pcie-speedgeneration","title":"PCIe Speed/Generation","text":"<p>Identify the PCIe address of your NVIDIA NIC:</p> ibdev2netdevlspci <pre><code>nic_pci=$(sudo ibdev2netdev -v | awk '{print $1}' | head -n1)\n</code></pre> <pre><code># `0200` is the PCI-SIG class code for NICs\n# `15b3` is the Vendor ID for Mellanox\nnic_pci=$(lspci -n | awk '$2 == \"0200:\" &amp;&amp; $3 ~ /^15b3:/ {print $1}' | head -n1)\n</code></pre> <p>Check current and max Speeds:</p> <pre><code>sudo lspci -vv -s $nic_pci | awk '/LnkCap/{s=1} /LnkSta/{s=0} /Speed /{match($0, /Speed [0-9]+GT\\/s/, m); if(s){print \"Max \" m[0]} else{print \"Current \" m[0]}}'\n</code></pre> See an example output <p>On IGX, the switch is able to maximize the NIC speed, both being PCIe 5.0:</p> <pre><code>Max Speed 32GT/s\nCurrent Speed 32GT/s\n</code></pre>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#33-maximize-the-nics-max-read-request-size-mrrs","title":"3.3 Maximize the NIC's Max Read Request Size (MRRS)","text":"<p>Understanding PCIe Configuration for Maximum Performance - May 27, 2022</p> <p>PCIe Max Read Request determines the maximal PCIe read request allowed. A PCIe device usually keeps track of the number of pending read requests due to having to prepare buffers for an incoming response. The size of the PCIe max read request may affect the number of pending requests (when using data fetch larger than the PCIe MTU).</p> <p>Unlike the PCIe properties queried in the previous section, the MRRS is configurable. We recommend maxing it to 4096 bytes. Run the following to check your current settings:</p> tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check mrrs\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check mrrs\n</code></pre> <p>Identify the PCIe address of your NVIDIA NIC:</p> ibdev2netdevlspci <pre><code>nic_pci=$(sudo ibdev2netdev -v | awk '{print $1}' | head -n1)\n</code></pre> <pre><code># `0200` is the PCI-SIG class code for NICs\n# `15b3` is the Vendor ID for Mellanox\nnic_pci=$(lspci -n | awk '$2 == \"0200:\" &amp;&amp; $3 ~ /^15b3:/ {print $1}' | head -n1)\n</code></pre> <p>Check current MRRS:</p> <pre><code>sudo lspci -vv -s $nic_pci | grep DevCtl: -A2 | grep -oE \"MaxReadReq [0-9]+\"\n</code></pre> <p>Update MRRS:</p> Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --set mrrs\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --set mrrs\n</code></pre> <p>Note</p> <p>This value is reset on reboot and needs to be set every time the system boots</p> ERROR: pcilib: sysfs_write: write failed: Operation not permitted <p>Disable secure boot on your system ahead of changing the MRRS of your NIC ports. It can be re-enabled afterwards.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#34-enable-huge-pages","title":"3.4 Enable Huge pages","text":"<p>Huge pages are a memory management feature that allows the OS to allocate large blocks of memory (typically 2MB or 1GB) instead of the default 4KB pages. This reduces the number of page table entries and the amount of memory used for translation, improving cache performance and reducing TLB (Translation Lookaside Buffer) misses, which leads to lower latencies.</p> <p>While it is naturally beneficial for CPU packets, it is also needed when routing data packets to the GPU in order to handle metadata (mbufs) on the CPU.</p> hugeadmvanilla <p>We recommend installing the <code>libhugetlbfs-bin</code> package for the <code>hugeadm</code> utility:</p> <pre><code>sudo apt update\nsudo apt install -y libhugetlbfs-bin\n</code></pre> <p>Then, check your huge page pools:</p> <pre><code>hugeadm --pool-list\n</code></pre> See an example output <p>The example below shows that this system supports huge pages of 64K, 2M (default), 32M, and 1G, but that none of them are currently allocated.</p> <pre><code>      Size  Minimum  Current  Maximum  Default\n     65536        0        0        0\n   2097152        0        0        0        *\n  33554432        0        0        0\n1073741824        0        0        0\n</code></pre> <p>And your huge page mount points:</p> <pre><code>hugeadm --list-all-mounts\n</code></pre> See an example output <p>The default huge pages are mounted on <code>/dev/hugepages</code> with a page size of 2M.</p> <pre><code>Mount Point          Options\n/dev/hugepages       rw,relatime,pagesize=2M\n</code></pre> <p>First, check your huge page pools:</p> <pre><code>ls -1 /sys/kernel/mm/hugepages/\ngrep Huge /proc/meminfo\n</code></pre> See an example output <p>The example below shows that this system supports huge pages of 64K, 2M (default), 32M, and 1G, but that none of them are currently allocated.</p> <pre><code>hugepages-1048576kB\nhugepages-2048kB\nhugepages-32768kB\nhugepages-64kB\n</code></pre> <pre><code>HugePages_Total:       0\nHugePages_Free:        0\nHugePages_Rsvd:        0\nHugePages_Surp:        0\nHugepagesize:       2048 kB\nHugetlb:               0 kB\n</code></pre> <p>And your huge page mount points:</p> <pre><code>mount | grep huge\n</code></pre> See an example output <p>The default huge pages are mounted on <code>/dev/hugepages</code> with a page size of 2M.</p> <pre><code>hugetlbfs on /dev/hugepages type hugetlbfs (rw,relatime,pagesize=2M)\n</code></pre> <p>As a rule of thumb, we recommend to start with 3 to 4 GB of total huge pages, with an individual page size of 500 MB to 1 GB (per system availability).</p> <p>There are two ways to allocate huge pages:</p> <ul> <li>in the kernel bootline (recommended to ensure contiguous memory allocation) or</li> <li>dynamically at runtime (risk of fragmentation for large page sizes)</li> </ul> <p>The example below allocates 3 huge pages of 1GB each.</p> Kernel bootlineRuntime <p>Add the flags below to the <code>GRUB_CMDLINE_LINUX</code> variable in <code>/etc/default/grub</code>:</p> <pre><code>default_hugepagesz=1G hugepagesz=1G hugepages=3\n</code></pre> Show explanation <ul> <li><code>default_hugepagesz</code>: the default huge page size to use, making them available from the default mount point, <code>/dev/hugepages</code>.</li> <li><code>hugepagesz</code>: the size of the huge pages to allocate.</li> <li><code>hugepages</code>: the number of huge pages to allocate.</li> </ul> <p>Then rebuild your GRUB configuration and reboot:</p> <pre><code>sudo update-grub\nsudo reboot\n</code></pre> <p>Allocate the 3x 1GB huge pages:</p> hugeadmvanilla <pre><code>sudo hugeadm --pool-pages-min 1073741824:3\n</code></pre> <pre><code>echo 3 | sudo tee /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages\n</code></pre> <p>Create a mount point to access the 1GB huge pages pool since that is not the default size on that system. We will name it <code>/mnt/huge</code> here.</p> One-timePersistent <pre><code>sudo mkdir -p /mnt/huge\nsudo mount -t hugetlbfs -o pagesize=1G none /mnt/huge\n</code></pre> <pre><code>echo \"nodev /mnt/huge hugetlbfs pagesize=1G 0 0\" | sudo tee -a /etc/fstab\nsudo mount /mnt/huge\n</code></pre> <p>Note</p> <p>If you work with containers, remember to mount this directory in your container as well with <code>-v /mnt/huge:/mnt/huge</code>.</p> <p>Rerunning the initial commands should now list 3 hugepages of 1GB each. 1GB will be the default huge page size if updated in the kernel bootline only.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#35-isolate-cpu-cores","title":"3.5 Isolate CPU cores","text":"<p>Note</p> <p>This optimization is less impactful when using the <code>gpunetio</code> backend since the GPU polls the NIC.</p> <p>The CPU interacting with the NIC to route packets is sensitive to perturbations, especially with smaller packet/batch sizes requiring more frequent work. Isolating a CPU in Linux prevents unwanted user or kernel threads from running on it, reducing context switching and latency spikes from noisy neighbors.</p> <p>We recommend isolating the CPU cores you will select to interact with the NIC (defined in the <code>advanced_network</code> configuration described later in this tutorial). This is done by setting additional flags on the kernel bootline.</p> <p>You can first check if any of the recommended flags were already set on the last boot:</p> tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check cmdline\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check cmdline\n</code></pre> <pre><code>cat /proc/cmdline | grep -e isolcpus -e irqaffinity -e nohz_full -e rcu_nocbs -e rcu_nocb_poll\n</code></pre> <p>Decide which cores to isolate based on your configuration. We recommend one core per queue as a rule of thumb. First, identify your core IDs:</p> <pre><code>cat /proc/cpuinfo | grep processor\n</code></pre> See an example output <p>This system has 12 cores, numbered 0 to 11: <pre><code>processor       # 0\nprocessor       # 1\nprocessor       # 2\nprocessor       # 3\nprocessor       # 4\nprocessor       # 5\nprocessor       # 6\nprocessor       # 7\nprocessor       # 8\nprocessor       # 9\nprocessor       # 10\nprocessor       # 11\n</code></pre></p> <p>As an example, the line below will isolate cores 9, 10 and 11, leaving cores 0-8 free for other tasks and hardware interrupts:</p> <pre><code>isolcpus=9-11 irqaffinity=0-8 nohz_full=9-11 rcu_nocbs=9-11 rcu_nocb_poll\n</code></pre> Show explanation Parameter Description <code>isolcpus</code> Isolates specific CPU cores from the Linux scheduler, preventing regular system tasks from running on them. This ensures dedicated cores are available exclusively for your networking tasks, reducing context switches and interruptions that can cause latency spikes. <code>irqaffinity</code> Controls which CPU cores can handle hardware interrupts. By directing network interrupts away from your isolated cores, you prevent networking tasks from being interrupted by hardware events, maintaining consistent processing time. <code>nohz_full</code> Disables regular kernel timer ticks on specified cores when they're running user space applications. This reduces overhead and prevents periodic interruptions, allowing your networking code to run with fewer disturbances. <code>rcu_nocbs</code> Offloads Read-Copy-Update (RCU) callback processing from specified cores. RCU is a synchronization mechanism in the Linux kernel that can cause periodic processing bursts. Moving this work away from your networking cores helps maintain consistent performance. <code>rcu_nocb_poll</code> Works with <code>rcu_nocbs</code> to improve how RCU callbacks are processed on non-callback CPUs. This can reduce latency spikes by changing how the kernel polls for RCU work. <p>Together, these parameters create an environment where specific CPU cores can focus exclusively on network packet processing with minimal interference from the operating system, resulting in lower and more consistent latency.</p> <p>Add these flags to the <code>GRUB_CMDLINE_LINUX</code> variable in <code>/etc/default/grub</code>, then rebuild your GRUB configuration and reboot:</p> <pre><code>sudo update-grub\nsudo reboot\n</code></pre> <p>Verify that the flags were properly set after boot by rerunning the check commands above.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#36-prevent-cpu-cores-from-going-idle","title":"3.6 Prevent CPU cores from going idle","text":"<p>When a core goes idle/to sleep, coming back online to poll the NIC can cause latency spikes and dropped packets. To prevent this, we recommend setting the scaling governor to <code>performance</code> for these CPU cores.</p> <p>Note</p> <p>Cores from a single cluster will always share the same governor.</p> <p>Bug</p> <p>We have witnessed instances where setting the governor to <code>performance</code> on only the isolated cores (dedicated to polling the NIC) does not lead to the performance gains expected. As such, we currently recommend setting the governor to <code>performance</code> for all cores which has shown to be reliably effective.</p> <p>Check the current governor for each of your cores:</p> tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check cpu-freq\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check cpu-freq\n</code></pre> See an example output <pre><code>2025-03-06 12:20:27 - WARNING - CPU 0: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 1: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 2: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 3: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 4: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 5: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 6: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 7: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 8: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 9: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 10: Governor is set to 'powersave', not 'performance'.\n2025-03-06 12:20:27 - WARNING - CPU 11: Governor is set to 'powersave', not 'performance'.\n</code></pre> <pre><code>cat /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor\n</code></pre> See an example output <p>In this example, all cores were defaulted to <code>powersave</code> instead of the recommended <code>performance</code>.</p> <pre><code>powersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\npowersave\n</code></pre> <p>Install <code>cpupower</code> to more conveniently set the governor:</p> <pre><code>sudo apt update\nsudo apt install -y linux-tools-$(uname -r)\n</code></pre> <p>Set the governor to <code>performance</code> for all cores:</p> One-timePersistent <pre><code>sudo cpupower frequency-set -g performance\n</code></pre> <pre><code>cat &lt;&lt; EOF | sudo tee /etc/systemd/system/cpu-performance.service\n[Unit]\nDescription=Set CPU governor to performance\nAfter=multi-user.target\n\n[Service]\nType=oneshot\nExecStart=/usr/bin/cpupower -c all frequency-set -g performance\n\n[Install]\nWantedBy=multi-user.target\nEOF\nsudo systemctl enable cpu-performance.service\nsudo systemctl start cpu-performance.service\n</code></pre> <p>Running the checks above should now list <code>performance</code> as the governor for all cores. You can also run <code>sudo cpupower -c all frequency-info</code> for more details.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#37-prevent-the-gpu-from-going-idle","title":"3.7 Prevent the GPU from going idle","text":"<p>Similarly to the above, we want to maximize the GPU's clock speed and prevent it from going idle.</p> <p>Run the following command to check your current clocks and whether they're locked (persistence mode):</p> <pre><code>nvidia-smi -q | grep -i \"Persistence Mode\"\nnvidia-smi -q -d CLOCK\n</code></pre> See an example output <pre><code>    Persistence Mode: Enabled\n...\nAttached GPUs                             : 1\nGPU 00000005:09:00.0\n    Clocks\n        Graphics                          : 420 MHz\n        SM                                : 420 MHz\n        Memory                            : 405 MHz\n        Video                             : 1680 MHz\n    Applications Clocks\n        Graphics                          : 1800 MHz\n        Memory                            : 8001 MHz\n    Default Applications Clocks\n        Graphics                          : 1800 MHz\n        Memory                            : 8001 MHz\n    Deferred Clocks\n        Memory                            : N/A\n    Max Clocks\n        Graphics                          : 2100 MHz\n        SM                                : 2100 MHz\n        Memory                            : 8001 MHz\n        Video                             : 1950 MHz\n    ...\n</code></pre> <p>To lock the GPU's clocks to their max values:</p> One-timePersistent <pre><code>sudo nvidia-smi -pm 1\nsudo nvidia-smi -lgc=$(nvidia-smi --query-gpu=clocks.max.sm --format=csv,noheader,nounits)\nsudo nvidia-smi -lmc=$(nvidia-smi --query-gpu=clocks.max.mem --format=csv,noheader,nounits)\n</code></pre> <pre><code>cat &lt;&lt; EOF | sudo tee /etc/systemd/system/gpu-max-clocks.service\n[Unit]\nDescription=Max GPU clocks\nAfter=multi-user.target\n\n[Service]\nType=oneshot\nExecStart=/usr/bin/nvidia-smi -pm 1\nExecStart=/bin/bash -c '/usr/bin/nvidia-smi --lock-gpu-clocks=$(/usr/bin/nvidia-smi --query-gpu=clocks.max.sm --format=csv,noheader,nounits)'\nExecStart=/bin/bash -c '/usr/bin/nvidia-smi --lock-memory-clocks=$(/usr/bin/nvidia-smi --query-gpu=clocks.max.mem --format=csv,noheader,nounits)'\nRemainAfterExit=true\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\nsudo systemctl enable gpu-max-clocks.service\nsudo systemctl start gpu-max-clocks.service\n</code></pre> Show explanation <p>This queries the max clocks for the GPU SM (<code>clocks.max.sm</code>) and memory (<code>clocks.max.mem</code>) and sets them to the current clocks (<code>lock-gpu-clocks</code> and <code>lock-memory-clocks</code> respectively). <code>-pm 1</code> (or <code>--persistence-mode=1</code>) enables persistence mode to lock these values.</p> See an example output <pre><code>GPU clocks set to \"(gpuClkMin 2100, gpuClkMax 2100)\" for GPU 00000005:09:00.0\nAll done.\nMemory clocks set to \"(memClkMin 8001, memClkMax 8001)\" for GPU 00000005:09:00.0\nAll done.\n</code></pre> <p>You can confirm that the clocks are set to the max values by running <code>nvidia-smi -q -d CLOCK</code> again.</p> <p>Note</p> <p>Some max clocks might not be achievable in certain configurations, or due to boost clocks (SM) or rounding errors (Memory),  despite the lock commands indicating it worked. For example - on IGX - the max non-boot SM clock will be 1920 MHz, and the max memory clock will show 8000 MHz, which are satisfying compared to the initial mode.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#38-maximize-gpu-bar1-size","title":"3.8 Maximize GPU BAR1 size","text":"<p>The GPU BAR1 memory is the primary resource consumed by <code>GPUDirect</code>. It allows other PCIe devices (like the CPU and the NIC) to access the GPU's memory space. The larger the BAR1 size, the more memory the GPU can expose to these devices in a single PCIe transaction, reducing the number of transactions needed and improving performance.</p> <p>We recommend a BAR1 size of 1GB or above. Check the current BAR1 size:</p> tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check bar1-size\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check bar1-size\n</code></pre> See an example output <pre><code>2025-03-06 12:22:53 - INFO - GPU 00000005:09:00.0: BAR1 size is 8192 MiB.\n</code></pre> <pre><code>nvidia-smi -q | grep -A 3 BAR1\n</code></pre> See an example output <p>For our RTX A6000, this shows a BAR1 size of 256 MiB:</p> <pre><code>    BAR1 Memory Usage\n    Total                             : 256 MiB\n    Used                              : 13 MiB\n    Free                              : 243 MiB\n</code></pre> <p>Warning</p> <p>Resizing the BAR1 size requires:</p> <ul> <li>A BIOS with resizable BAR support</li> <li>A GPU with physical resizable BAR</li> </ul> <p>If you attempt to go forward with the instructions below without meeting the above requirements, you might render your GPU unusable.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#bios-resizable-bar-support","title":"BIOS Resizable BAR support","text":"<p>First, check if your system and BIOS support resizable BAR. Refer to your system's manufacturer documentation to access the BIOS. The Resizable BAR option is often categorized under <code>Advanced &gt; PCIe</code> settings. Enable this feature if found.</p> <p>Note</p> <p>The IGX Developer kit with IGX OS 1.1+ supports resizable BAR by default.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#gpu-resizable-bar-support","title":"GPU Resizable BAR support","text":"<p>Next, you can check if your GPU has physical resizable BAR by running the following command:</p> <pre><code>sudo lspci -vv -s $(nvidia-smi --query-gpu=pci.bus_id --format=csv,noheader) | grep BAR\n</code></pre> See an example output <p>This RTX A6000 has a resizable BAR1, currently set to 256 MiB:</p> <pre><code>Capabilities: [bb0 v1] Physical Resizable BAR\n    BAR 0: current size: 16MB, supported: 16MB\n    BAR 1: current size: 256MB, supported: 64MB 128MB 256MB 512MB 1GB 2GB 4GB 8GB 16GB 32GB 64GB\n    BAR 3: current size: 32MB, supported: 32MB\n</code></pre> <p>If your GPU is listed on this page, you can download the <code>Display Mode Selector</code> to resize the BAR1 to 8GB.</p> <ol> <li>Press <code>Join Now</code>.</li> <li>Once approved, download the <code>Display Mode Selector</code> archive.</li> <li>Unzip the archive.</li> <li>Access your system without a X-server running, either through SSH or a Virtual Console (<code>Alt+F1</code>).</li> <li>Go down the right OS and architecture folder for your system (<code>linux/aarch64</code> or <code>linux/x64</code>).</li> <li>Run the <code>displaymodeselector</code> command like so:</li> </ol> <pre><code>chmod +x displaymodeselector\nsudo ./displaymodeselector --gpumode physical_display_enabled_8GB_bar1\n</code></pre> <p>Press <code>y</code> to confirm you'd like to continue, then <code>y</code> again to apply to all the eligible adapters.</p> See an example output <pre><code>NVIDIA Display Mode Selector Utility (Version 1.67.0)\nCopyright (C) 2015-2021, NVIDIA Corporation. All Rights Reserved.\n\nWARNING: This operation updates the firmware on the board and could make\n        the device unusable if your host system lacks the necessary support.\n\nAre you sure you want to continue?\nPress 'y' to confirm (any other key to abort):\ny\nSpecified GPU Mode \"physical_display_enabled_8GB_bar1\"\n\n\nUpdate GPU Mode of all adapters to \"physical_display_enabled_8GB_bar1\"?\nPress 'y' to confirm or 'n' to choose adapters or any other key to abort:\ny\n\nUpdating GPU Mode of all eligible adapters to \"physical_display_enabled_8GB_bar1\"\n\nApply GPU Mode &lt;6&gt; corresponds to \"physical_display_enabled_8GB_bar1\"\n\nReading EEPROM (this operation may take up to 30 seconds)\n\n[==================================================] 100 %\nReading EEPROM (this operation may take up to 30 seconds)\n\nSuccessfully updated GPU mode to \"physical_display_enabled_8GB_bar1\" ( Mode 6 ).\n\nA reboot is required for the update to take effect.\n</code></pre> Error: unload the NVIDIA kernel driver first <p>If you see this error:</p> <pre><code>ERROR: In order to avoid the irreparable damage to your graphics adapter it is necessary to unload the NVIDIA kernel driver first:\n\nrmmod nvidia_uvm nvidia_drm nvidia_modeset nvidia_peermem nvidia\n</code></pre> <p>Try to unload the NVIDIA kernel driver listed in the error message above (list may vary):</p> <pre><code>sudo rmmod nvidia_uvm nvidia_drm nvidia_modeset nvidia_peermem nvidia\n</code></pre> <p>If this fails because the drivers are in use, stop the X-server first before trying again:</p> <pre><code>sudo systemctl isolate multi-user\n</code></pre> /dev/mem: Operation not permitted. Access to physical memory denied <p>Disable secure boot on your system ahead of changing your GPU's BAR1 size. It can be re-enabled afterwards.</p> <p>Reboot your system, and check the BAR1 size again to confirm the change.</p> <pre><code>sudo reboot\n</code></pre>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#39-enable-jumbo-frames","title":"3.9 Enable Jumbo Frames","text":"<p>Jumbo frames are Ethernet frames that carry a payload larger than the standard 1500 bytes MTU (Maximum Transmission Unit). They can significantly improve network performance when transferring large amounts of data by reducing the overhead of packet headers and the number of packets that need to be processed.</p> <p>We recommend an MTU of 9000 bytes on all interfaces involved in the data path. You can check the current MTU of your interfaces:</p> tune_system.pymanual Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/bin/tune_system.py --check mtu\n</code></pre> <pre><code>cd holohub\nsudo ./operators/advanced_network/python/tune_system.py --check mtu\n</code></pre> See an example output <pre><code>2025-03-06 16:51:19 - INFO - Interface eth0 has an acceptable MTU of 9000 bytes.\n2025-03-06 16:51:19 - INFO - Interface eth1 has an acceptable MTU of 9000 bytes.\n</code></pre> <p>For a given <code>if_name</code> interface:</p> <pre><code>if_name=eth0\nip link show dev $if_name | grep -oE \"mtu [0-9]+\"\n</code></pre> See an example output <pre><code>mtu 1500\n</code></pre> <p>You can set the MTU for each interface like so, for a given <code>if_name</code> name identified above:</p> One-timePersistent <pre><code>sudo ip link set dev $if_name mtu 9000\n</code></pre> NetworkManagersystemd-networkd <pre><code>sudo nmcli connection modify $if_name ipv4.mtu 9000\nsudo nmcli connection up $if_name\n</code></pre> <p>Assuming you've set an IP address for the interface above, you can add the MTU to the interface's network configuration file like so:</p> <pre><code>sudo sed -i '/\\[Network\\]/a MTU=9000' /etc/systemd/network/20-$if_name.network\nsudo systemctl restart systemd-networkd\n</code></pre> Can I do more than 9000? <p>While your NIC might have a maximum MTU capability larger than 9000, we typically recommend setting the MTU to 9000 bytes, as that is the standard size for jumbo frames that's widely supported for compatibility with other network equipment. When using jumbo frames, all devices in the communication path must support the same MTU size. If any device in between has a smaller MTU, packets will be fragmented or dropped, potentially degrading performance.</p> <p>Example with the CX-7 NIC:</p> <pre><code>$ ip -d link show dev $if_name | grep -oE \"maxmtu [0-9]+\"\nmaxmtu 9978\n</code></pre>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#4-running-a-test-application","title":"4. Running a test application","text":"<p>Holoscan Networking provides a benchmarking application named <code>adv_networking_bench</code> that can be used to test the performance of the networking configuration. In this section, we'll walk you through the steps needed to configure the application for your NIC for Tx and Rx, and run a loopback test between the two interfaces with a physical SFP cable connecting them.</p> <p>Make sure to install <code>holoscan-networking</code> beforehand.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#41-update-the-loopback-configuration","title":"4.1 Update the loopback configuration","text":"","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#find-the-application-files","title":"Find the application files","text":"<p>Identify the location of the <code>adv_networking_bench</code> executable, and of the configuration file named <code>adv_networking_bench_default_tx_rx.yaml</code>, for your installation:</p> Debian installationFrom source <p>Both located under <code>/opt/nvidia/holoscan/examples/adv_networking_bench/</code>:</p> <pre><code>ls -1 /opt/nvidia/holoscan/examples/adv_networking_bench/\nadv_networking_bench\nadv_networking_bench_default_rx_multi_q.yaml\nadv_networking_bench_default_tx_rx_hds.yaml\nadv_networking_bench_default_tx_rx.yaml\nadv_networking_bench_gpunetio_tx_rx.yaml\nadv_networking_bench_rmax_rx.yaml\nCMakeLists.txt\ndefault_bench_op_rx.h\ndefault_bench_op_tx.h\ndoca_bench_op_rx.h\ndoca_bench_op_tx.h\nkernels.cu\nkernels.cuh\nmain.cpp\n</code></pre> <p>Both located under <code>./install/examples/adv_networking_bench/</code></p> <pre><code>ls -1 ./install/examples/adv_networking_bench\nadv_networking_bench\nadv_networking_bench_default_rx_multi_q.yaml\nadv_networking_bench_default_tx_rx_hds.yaml\nadv_networking_bench_default_tx_rx.yaml\nadv_networking_bench_gpunetio_tx_rx.yaml\nadv_networking_bench.py\nadv_networking_bench_rmax_rx.yaml\nCMakeLists.txt\ndefault_bench_op_rx.h\ndefault_bench_op_tx.h\ndoca_bench_op_rx.h\ndoca_bench_op_tx.h\nkernels.cu\nkernels.cuh\nmain.cpp\n</code></pre> <p>Warning</p> <p>The configuration file is also located alongide the application source code at <code>applications/adv_networking_bench/adv_networking_bench_default_tx_rx.yaml</code>. However, modifying this file will not affect the configuration used by the application executable without rebuilding the application.</p> <p>For this reason, we recommend using the configuration file located in the install tree.</p> <p>Note</p> <p>The fields in this <code>yaml</code> file will be explained in more details in a section below. For now, we'll stick to modifying the strict minimum required fields to run the application as-is on your system.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#identify-your-nics-pcie-addresses","title":"Identify your NIC's PCIe addresses","text":"<p>Retrieve the PCIe addresses of both ports of your NIC. We'll arbitrarily use the first for Tx and the second for Rx here:</p> ibdev2netdevlspci <pre><code>sudo ibdev2netdev -v | awk '{print $1}'\n</code></pre> <pre><code># `0200` is the PCI-SIG class code for NICs\n# `15b3` is the Vendor ID for Mellanox\nlspci -n | awk '$2 == \"0200:\" &amp;&amp; $3 ~ /^15b3:/ {print $1}'\n</code></pre> See an example output <pre><code>0005:03:00.0\n0005:03:00.1\n</code></pre>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#configure-the-nic-for-tx-and-rx","title":"Configure the NIC for Tx and Rx","text":"<p>Set the NIC addresses in the <code>interfaces</code> section of the <code>advanced_network</code> section, making sure to remove the template brackets <code>&lt; &gt;</code>. This configures your NIC independently of your application:</p> <ul> <li>Set the <code>address</code> field of the <code>tx_port</code> interface to one of these addresses. That interface will be able to transmit ethernet packets.</li> <li>Set the <code>address</code> field of the <code>rx_port</code> interface to the other address. This interface will be able to receive ethernet packets.</li> </ul> <pre><code>interfaces:\n    - name: \"tx_port\"\n    address: &lt;0000:00:00.0&gt;       # The BUS address of the interface doing Tx\n    tx:\n        ...\n    - name: \"rx_port\"\n    address: &lt;0000:00:00.0&gt;       # The BUS address of the interface doing Rx\n    rx:\n        ...\n</code></pre> See an example yaml <pre><code>interfaces:\n    - name: \"tx_port\"\n    address: 0005:03:00.0       # The BUS address of the interface doing Tx\n    tx:\n        ...\n    - name: \"rx_port\"\n    address: 0005:03:00.1       # The BUS address of the interface doing Rx\n    rx:\n        ...\n</code></pre>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#configure-the-application","title":"Configure the application","text":"<p>Modify the <code>bench_tx</code> section which configures the application itself, to create the packet headers and direct them to the NIC. Make sure to remove the template brackets <code>&lt; &gt;</code>.</p> <ul> <li><code>eth_dst_addr</code> with the MAC address (and not the PCIe address) of the NIC interface you want to use for Rx. You can get the MAC address of your <code>if_name</code> interface with <code>cat /sys/class/net/$if_name/address</code>:</li> <li>Replacing <code>address</code> with the PCIe address of the NIC interface you want to use for Tx (same as <code>tx_port</code>'s address above).</li> </ul> <pre><code>bench_tx:\n    ...\n    eth_dst_addr: &lt;00:00:00:00:00:00&gt; # Destination MAC address - required when Rx flow_isolation=true\n    ip_src_addr: &lt;1.2.3.4&gt;  # Source IP address - required on layer 3 network\n    ip_dst_addr: &lt;5.6.7.8&gt;  # Destination IP address - required on layer 3 network\n    udp_src_port: 4096      # UDP source port\n    udp_dst_port: 4096      # UDP destination port\n    address: &lt;0000:00:00.0&gt; # Source NIC Bus ID. Should match the address of the Tx interface above\n</code></pre> See an example yaml <pre><code>bench_tx:\n    ...\n    eth_dst_addr: 48:b0:2d:ee:83:ad # Destination MAC address - required when Rx flow_isolation=true\n    ip_src_addr: &lt;1.2.3.4&gt;  # Source IP address - required on layer 3 network\n    ip_dst_addr: &lt;5.6.7.8&gt;  # Destination IP address - required on layer 3 network\n    udp_src_port: 4096      # UDP source port\n    udp_dst_port: 4096      # UDP destination port\n    address: 0005:03:00.0  # Source NIC Bus ID. Should match the address of the Tx interface above\n</code></pre> Show explanation <ul> <li><code>eth_dst_addr</code> - the destination ethernet MAC address - will be embedded in the packet headers by the application. This is required here because the Rx interface above has <code>flow_isolation: true</code> (explained in more details below). In that configuration, only the packets listing the adequate destination MAC address will be accepted by the Rx interface.</li> <li>We ignore the IP fields (<code>ip_src_addr</code>, <code>ip_dst_addr</code>) for now, as we are testing on a layer 2 network by just connecting a cable between the two interfaces on our system, therefore having mock values has no impact.</li> <li><code>address</code> - the source PCIe address - needs to be defined again to tell the application itself to route the packets to the NIC interface we have configured previously for Tx.</li> <li>You might have noted the lack of a <code>eth_src_addr</code> field in the <code>bench_tx</code> section. This is because the source Ethernet MAC address can be inferred automatically from the PCIe address of the Tx interface (below).</li> </ul>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#42-run-the-loopback-test","title":"4.2 Run the loopback test","text":"<p>After having modified the configuration file, ensure you have connected an SFP cable between the two interfaces of your NIC, then run the application with the command below:</p> Debian installationFrom source <pre><code>sudo /opt/nvidia/holoscan/examples/adv_networking_bench/adv_networking_bench adv_networking_bench_default_tx_rx.yaml\n</code></pre> Bare MetalContainerized <p>This assumes you have the required dependencies (holoscan, doca, etc.) installed locally on your system.</p> <pre><code>sudo ./install/examples/adv_networking_bench/adv_networking_bench adv_networking_bench_default_tx_rx.yaml\n</code></pre> <pre><code>./dev_container launch \\\n  --img holohub:adv_networking_bench \\\n  --docker_opts \"-u 0 --privileged\" \\\n  -- bash -c \"./install/examples/adv_networking_bench/adv_networking_bench adv_networking_bench_default_tx_rx.yaml\"\n</code></pre> <p>The application will run indefinitely. You can stop it gracefully with <code>Ctrl-C</code>. You can also uncomment and set the <code>max_duration_ms</code> field in the <code>scheduler</code> section of the configuration file to limit the duration of the run automatically.</p> See an example output <pre><code>[info] [fragment.cpp:599] Loading extensions from configs...\n[info] [gxf_executor.cpp:264] Creating context\n[info] [main.cpp:35] Initializing advanced network operator\n[info] [main.cpp:40] Using ANO manager dpdk\n[info] [adv_network_rx.cpp:35] Adding output port bench_rx_out\n[info] [adv_network_rx.cpp:51] AdvNetworkOpRx::initialize()\n[info] [adv_network_common.h:607] Finished reading advanced network operator config\n[info] [adv_network_dpdk_mgr.cpp:373] Attempting to use 2 ports for high-speed network\n[info] [adv_network_dpdk_mgr.cpp:382] Setting DPDK log level to: Info\n[info] [adv_network_dpdk_mgr.cpp:402] DPDK EAL arguments: adv_net_operator --file-prefix=nwlrbbmqbh -l 3,11,9 --log-level=9 --log-level=pmd.net.mlx5:info -a 0005:03:00.0,txq_inline_max=0,dv_flow_en=2 -a 0005:03:00.1,txq_inline_max=0,dv_flow_en=2\nLog level 9 higher than maximum (8)\nEAL: Detected CPU lcores: 12\nEAL: Detected NUMA nodes: 1\nEAL: Detected shared linkage of DPDK\nEAL: Multi-process socket /var/run/dpdk/nwlrbbmqbh/mp_socket\nEAL: Selected IOVA mode 'VA'\nEAL: 1 hugepages of size 1073741824 reserved, but no mounted hugetlbfs found for that size\nEAL: Probe PCI driver: mlx5_pci (15b3:1021) device: 0005:03:00.0 (socket -1)\nmlx5_net: PCI information matches for device \"mlx5_0\"\nmlx5_net: enhanced MPS is enabled\nmlx5_net: port 0 MAC address is 48:B0:2D:EE:83:AC\nEAL: Probe PCI driver: mlx5_pci (15b3:1021) device: 0005:03:00.1 (socket -1)\nmlx5_net: PCI information matches for device \"mlx5_1\"\nmlx5_net: enhanced MPS is enabled\nmlx5_net: port 1 MAC address is 48:B0:2D:EE:83:AD\nTELEMETRY: No legacy callbacks, legacy socket not created\n[info] [adv_network_dpdk_mgr.cpp:298] Port 0 has no RX queues. Creating dummy queue.\n[info] [adv_network_dpdk_mgr.cpp:165] Adjusting buffer size to 9228 for headroom\n[info] [adv_network_dpdk_mgr.cpp:165] Adjusting buffer size to 9128 for headroom\n[info] [adv_network_dpdk_mgr.cpp:165] Adjusting buffer size to 9128 for headroom\n[info] [adv_network_mgr.cpp:116] Registering memory regions\n[info] [adv_network_mgr.cpp:178] Successfully allocated memory region MR_Unused_P0 at 0x100fa0000 type 2 with 9100 bytes (32768 elements @ 9228 bytes total 302383104)\n[info] [adv_network_mgr.cpp:178] Successfully allocated memory region Data_RX_GPU at 0xffff4fc00000 type 3 with 9000 bytes (51200 elements @ 9128 bytes total 467402752)\n[info] [adv_network_mgr.cpp:178] Successfully allocated memory region Data_TX_GPU at 0xffff33e00000 type 3 with 9000 bytes (51200 elements @ 9128 bytes total 467402752)\n[info] [adv_network_mgr.cpp:191] Finished allocating memory regions\n[info] [adv_network_dpdk_mgr.cpp:223] Successfully registered external memory for Data_TX_GPU\n[info] [adv_network_dpdk_mgr.cpp:223] Successfully registered external memory for Data_RX_GPU\n[info] [adv_network_dpdk_mgr.cpp:193] Mapped external memory descriptor for 0xffff4fc00000 to device 0\n[info] [adv_network_dpdk_mgr.cpp:193] Mapped external memory descriptor for 0xffff33e00000 to device 0\n[info] [adv_network_dpdk_mgr.cpp:193] Mapped external memory descriptor for 0xffff4fc00000 to device 1\n[info] [adv_network_dpdk_mgr.cpp:193] Mapped external memory descriptor for 0xffff33e00000 to device 1\n[info] [adv_network_dpdk_mgr.cpp:454] DPDK init (0005:03:00.0) -- RX: ENABLED TX: ENABLED\n[info] [adv_network_dpdk_mgr.cpp:464] Configuring RX queue: UNUSED_P0_Q0 (0) on port 0\n[info] [adv_network_dpdk_mgr.cpp:513] Created mempool RXP_P0_Q0_MR0 : mbufs=32768 elsize=9228 ptr=0x10041c380\n[info] [adv_network_dpdk_mgr.cpp:523] Max packet size needed for RX: 9100\n[info] [adv_network_dpdk_mgr.cpp:564] Configuring TX queue: ADC Samples (0) on port 0\n[info] [adv_network_dpdk_mgr.cpp:607] Created mempool TXP_P0_Q0_MR0 : mbufs=51200 elsize=9000 ptr=0x100c1fc00\n[info] [adv_network_dpdk_mgr.cpp:621] Max packet size needed with TX: 9100\n[info] [adv_network_dpdk_mgr.cpp:632] Setting port config for port 0 mtu:9082\n[info] [adv_network_dpdk_mgr.cpp:663] Initializing port 0 with 1 RX queues and 1 TX queues...\nmlx5_net: port 0 Tx queues number update: 0 -&gt; 1\nmlx5_net: port 0 Rx queues number update: 0 -&gt; 1\n[info] [adv_network_dpdk_mgr.cpp:679] Successfully configured ethdev\n[info] [adv_network_dpdk_mgr.cpp:689] Successfully set descriptors to 8192/8192\n[info] [adv_network_dpdk_mgr.cpp:704] Port 0 not in isolation mode\n[info] [adv_network_dpdk_mgr.cpp:713] Setting up port:0, queue:0, Num scatter:1 pool:0x10041c380\n[info] [adv_network_dpdk_mgr.cpp:734] Successfully setup RX port 0 queue 0\n[info] [adv_network_dpdk_mgr.cpp:756] Successfully set up TX queue 0/0\n[info] [adv_network_dpdk_mgr.cpp:761] Enabling promiscuous mode for port 0\nmlx5_net: [mlx5dr_cmd_query_caps]: Failed to query wire port regc value\nmlx5_net: port 0 Rx queues number update: 1 -&gt; 1\n[info] [adv_network_dpdk_mgr.cpp:775] Successfully started port 0\n[info] [adv_network_dpdk_mgr.cpp:778] Port 0, MAC address: 48:B0:2D:EE:83:AC\n[info] [adv_network_dpdk_mgr.cpp:1111] Applying tx_eth_src offload for port 0\n[info] [adv_network_dpdk_mgr.cpp:454] DPDK init (0005:03:00.1) -- RX: ENABLED TX: DISABLED\n[info] [adv_network_dpdk_mgr.cpp:464] Configuring RX queue: Data (0) on port 1\n[info] [adv_network_dpdk_mgr.cpp:513] Created mempool RXP_P1_Q0_MR0 : mbufs=51200 elsize=9128 ptr=0x125a5b940\n[info] [adv_network_dpdk_mgr.cpp:523] Max packet size needed for RX: 9000\n[info] [adv_network_dpdk_mgr.cpp:621] Max packet size needed with TX: 9000\n[info] [adv_network_dpdk_mgr.cpp:632] Setting port config for port 1 mtu:8982\n[info] [adv_network_dpdk_mgr.cpp:663] Initializing port 1 with 1 RX queues and 0 TX queues...\nmlx5_net: port 1 Rx queues number update: 0 -&gt; 1\n[info] [adv_network_dpdk_mgr.cpp:679] Successfully configured ethdev\n[info] [adv_network_dpdk_mgr.cpp:689] Successfully set descriptors to 8192/8192\n[info] [adv_network_dpdk_mgr.cpp:701] Port 1 in isolation mode\n[info] [adv_network_dpdk_mgr.cpp:713] Setting up port:1, queue:0, Num scatter:1 pool:0x125a5b940\n[info] [adv_network_dpdk_mgr.cpp:734] Successfully setup RX port 1 queue 0\n[info] [adv_network_dpdk_mgr.cpp:764] Not enabling promiscuous mode on port 1 since flow isolation is enabled\nmlx5_net: [mlx5dr_cmd_query_caps]: Failed to query wire port regc value\nmlx5_net: port 1 Rx queues number update: 1 -&gt; 1\n[info] [adv_network_dpdk_mgr.cpp:775] Successfully started port 1\n[info] [adv_network_dpdk_mgr.cpp:778] Port 1, MAC address: 48:B0:2D:EE:83:AD\n[info] [adv_network_dpdk_mgr.cpp:790] Adding RX flow ADC Samples\n[info] [adv_network_dpdk_mgr.cpp:998] Adding IPv4 length match for 1050\n[info] [adv_network_dpdk_mgr.cpp:1018] Adding UDP port match for src/dst 4096/4096\n[info] [adv_network_dpdk_mgr.cpp:814] Setting up RX burst pool with 8191 batches of size 81920\n[info] [adv_network_dpdk_mgr.cpp:833] Setting up RX burst pool with 8191 batches of size 20480\n[info] [adv_network_dpdk_mgr.cpp:875] Setting up TX ring TX_RING_P0_Q0\n[info] [adv_network_dpdk_mgr.cpp:901] Setting up TX burst pool TX_BURST_POOL_P0_Q0 with 10240 pointers at 0x125a0d4c0\n[info] [adv_network_dpdk_mgr.cpp:1186] Config validated successfully\n[info] [adv_network_dpdk_mgr.cpp:1199] Starting advanced network workers\n[info] [adv_network_dpdk_mgr.cpp:1278] Flushing packet on port 1\n[info] [adv_network_dpdk_mgr.cpp:1478] Starting RX Core 9, port 1, queue 0, socket 0\n[info] [adv_network_dpdk_mgr.cpp:1268] Done starting workers\n[info] [default_bench_op_tx.h:79] AdvNetworkingBenchDefaultTxOp::initialize()\n[info] [adv_network_dpdk_mgr.cpp:1637] Starting TX Core 11, port 0, queue 0 socket 0 using burst pool 0x125a0d4c0 ring 0x127690740\n[info] [default_bench_op_tx.h:113] Initialized 4 streams and events\n[info] [default_bench_op_tx.h:130] AdvNetworkingBenchDefaultTxOp::initialize() complete\n[info] [default_bench_op_rx.h:67] AdvNetworkingBenchDefaultRxOp::initialize()\n[info] [gxf_executor.cpp:1797] creating input IOSpec named 'burst_in'\n[info] [default_bench_op_rx.h:104] AdvNetworkingBenchDefaultRxOp::initialize() complete\n[info] [adv_network_tx.cpp:46] AdvNetworkOpTx::initialize()\n[info] [gxf_executor.cpp:1797] creating input IOSpec named 'burst_in'\n[info] [adv_network_common.h:607] Finished reading advanced network operator config\n[info] [gxf_executor.cpp:2208] Activating Graph...\n[info] [gxf_executor.cpp:2238] Running Graph...\n[info] [multi_thread_scheduler.cpp:300] MultiThreadScheduler started worker thread [pool name: default_pool, thread uid: 0]\n[info] [multi_thread_scheduler.cpp:300] MultiThreadScheduler started worker thread [pool name: default_pool, thread uid: 1]\n[info] [multi_thread_scheduler.cpp:300] MultiThreadScheduler started worker thread [pool name: default_pool, thread uid: 2]\n[info] [gxf_executor.cpp:2240] Waiting for completion...\n[info] [multi_thread_scheduler.cpp:300] MultiThreadScheduler started worker thread [pool name: default_pool, thread uid: 3]\n[info] [multi_thread_scheduler.cpp:300] MultiThreadScheduler started worker thread [pool name: default_pool, thread uid: 4]\n^C[info] [multi_thread_scheduler.cpp:636] Stopping multithread scheduler\n[info] [multi_thread_scheduler.cpp:694] Stopping all async jobs\n[info] [multi_thread_scheduler.cpp:218] Dispatcher thread has stopped checking jobs\n[info] [multi_thread_scheduler.cpp:679] Waiting to join all async threads\n[info] [multi_thread_scheduler.cpp:316] Worker Thread [pool name: default_pool, thread uid: 1] exiting.\n[info] [multi_thread_scheduler.cpp:702] *********************** DISPATCHER EXEC TIME : 476345.364000 ms\n\n[info] [multi_thread_scheduler.cpp:316] Worker Thread [pool name: default_pool, thread uid: 0] exiting.\n[info] [multi_thread_scheduler.cpp:316] Worker Thread [pool name: default_pool, thread uid: 3] exiting.\n[info] [multi_thread_scheduler.cpp:371] Event handler thread exiting.\n[info] [multi_thread_scheduler.cpp:703] *********************** DISPATCHER WAIT TIME : 47339.961000 ms\n\n[info] [multi_thread_scheduler.cpp:704] *********************** DISPATCHER COUNT : 197630449\n\n[info] [multi_thread_scheduler.cpp:316] Worker Thread [pool name: default_pool, thread uid: 2] exiting.\n[info] [multi_thread_scheduler.cpp:705] *********************** WORKER EXEC TIME : 983902.800000 ms\n\n[info] [multi_thread_scheduler.cpp:706] *********************** WORKER WAIT TIME : 1634522.159000 ms\n\n[info] [multi_thread_scheduler.cpp:707] *********************** WORKER COUNT : 11817369\n\n[info] [multi_thread_scheduler.cpp:316] Worker Thread [pool name: default_pool, thread uid: 4] exiting.\n[info] [multi_thread_scheduler.cpp:688] All async worker threads joined, deactivating all entities\n[info] [adv_network_rx.cpp:46] AdvNetworkOpRx::stop()\n[info] [adv_network_dpdk_mgr.cpp:1928] DPDK ANO shutdown called 2\n[info] [adv_network_tx.cpp:41] AdvNetworkOpTx::stop()\n[info] [adv_network_dpdk_mgr.cpp:1928] DPDK ANO shutdown called 1\n[info] [adv_network_dpdk_mgr.cpp:1133] Port 0:\n[info] [adv_network_dpdk_mgr.cpp:1135]  - Received packets:    0\n[info] [adv_network_dpdk_mgr.cpp:1136]  - Transmit packets:    6005066864\n[info] [adv_network_dpdk_mgr.cpp:1137]  - Received bytes:      0\n[info] [adv_network_dpdk_mgr.cpp:1138]  - Transmit bytes:      6389391347584\n[info] [adv_network_dpdk_mgr.cpp:1139]  - Missed packets:      0\n[info] [adv_network_dpdk_mgr.cpp:1140]  - Errored packets:     0\n[info] [adv_network_dpdk_mgr.cpp:1141]  - RX out of buffers:   0\n[info] [adv_network_dpdk_mgr.cpp:1143]    ** Extended Stats **\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_good_packets:          6005070000\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_good_bytes:            6389394480000\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_q0_packets:            6005070000\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_q0_bytes:              6389394480000\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_multicast_bytes:               9589\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_multicast_packets:             22\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_unicast_bytes:         6389394480000\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_multicast_bytes:               9589\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_unicast_packets:               6005070000\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_multicast_packets:             22\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_phy_packets:           6005070022\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_phy_packets:           24\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_phy_bytes:             6413414769677\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_phy_bytes:             9805\n[info] [adv_network_dpdk_mgr.cpp:1133] Port 1:\n[info] [adv_network_dpdk_mgr.cpp:1135]  - Received packets:    6004323692\n[info] [adv_network_dpdk_mgr.cpp:1136]  - Transmit packets:    0\n[info] [adv_network_dpdk_mgr.cpp:1137]  - Received bytes:      6388600255072\n[info] [adv_network_dpdk_mgr.cpp:1138]  - Transmit bytes:      0\n[info] [adv_network_dpdk_mgr.cpp:1139]  - Missed packets:      746308\n[info] [adv_network_dpdk_mgr.cpp:1140]  - Errored packets:     0\n[info] [adv_network_dpdk_mgr.cpp:1141]  - RX out of buffers:   5047027287\n[info] [adv_network_dpdk_mgr.cpp:1143]    ** Extended Stats **\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_good_packets:          6004323692\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_good_bytes:            6388600255072\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_missed_errors:         746308\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_mbuf_allocation_errors:                5047027287\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_q0_packets:            6004323692\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_q0_bytes:              6388600255072\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_q0_errors:             5047027287\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_unicast_bytes:         6389394480000\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_multicast_bytes:               9589\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_unicast_packets:               6005070000\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_multicast_packets:             22\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_multicast_bytes:               9589\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_multicast_packets:             22\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_phy_packets:           24\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_phy_packets:           6005070022\n[info] [adv_network_dpdk_mgr.cpp:1173]       tx_phy_bytes:             9805\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_phy_bytes:             6413414769677\n[info] [adv_network_dpdk_mgr.cpp:1173]       rx_out_of_buffer:         746308\n[info] [adv_network_dpdk_mgr.cpp:1935] ANO DPDK manager shutting down\n[info] [adv_network_dpdk_mgr.cpp:1622] Total packets received by application (port/queue 1/0): 6004323692\n[info] [adv_network_dpdk_mgr.cpp:1698] Total packets transmitted by application (port/queue 0/0): 6005070000\n[info] [multi_thread_scheduler.cpp:645] Multithread scheduler stopped.\n[info] [multi_thread_scheduler.cpp:664] Multithread scheduler finished.\n[info] [gxf_executor.cpp:2243] Deactivating Graph...\n[info] [multi_thread_scheduler.cpp:491] TOTAL EXECUTION TIME OF SCHEDULER : 523694.460857 ms\n\n[info] [gxf_executor.cpp:2251] Graph execution finished.\n[info] [adv_network_dpdk_mgr.cpp:1928] DPDK ANO shutdown called 0\n[info] [default_bench_op_tx.h:51] ANO benchmark TX op shutting down\n[info] [default_bench_op_rx.h:56] Finished receiver with 6388570603520/6004295680 bytes/packets received and 0 packets dropped\n[info] [default_bench_op_rx.h:61] ANO benchmark RX op shutting down\n[info] [default_bench_op_rx.h:108] AdvNetworkingBenchDefaultRxOp::freeResources() start\n[info] [default_bench_op_rx.h:116] AdvNetworkingBenchDefaultRxOp::freeResources() complete\n[info] [gxf_executor.cpp:294] Destroying context\n</code></pre> <p>To inspect the speed the data is moving through the NIC, run <code>mlnx_perf</code> on one of the interfaces in a separate terminal, concurrently with the application running:</p> <pre><code>sudo mlnx_perf -i $if_name\n</code></pre> See an example output <p>On IGX with RTX A6000, we are able to hit close to the 100 Gbps linerate with this configuration: <pre><code>  rx_vport_unicast_packets: 11,614,900\n    rx_vport_unicast_bytes: 12,358,253,600 Bps   = 98,866.2 Mbps\n            rx_packets_phy: 11,614,847\n              rx_bytes_phy: 12,404,657,664 Bps   = 99,237.26 Mbps\n rx_1024_to_1518_bytes_phy: 11,614,936\n            rx_prio0_bytes: 12,404,738,832 Bps   = 99,237.91 Mbps\n          rx_prio0_packets: 11,614,923\n</code></pre></p> Troubleshooting EAL: failed to parse device <p>Make sure to set valid PCIe addresses in the <code>address</code> fields in <code>interfaces</code>, per instructions above.</p> Invalid MAC address format <p>Make sure to set a valid MAC address in the <code>eth_dst_addr</code> field in <code>bench_tx</code>, per instructions above.</p> mlx5_common: Fail to create MR for address [...] Could not DMA map EXT memory <p>Example error:</p> <pre><code>mlx5_common: Fail to create MR for address (0xffff2fc00000)\nmlx5_common: Device 0005:03:00.0 unable to DMA map\n[critical] [adv_network_dpdk_mgr.cpp:188] Could not DMA map EXT memory: -1 err=Invalid argument\n[critical] [adv_network_dpdk_mgr.cpp:430] Failed to map MRs\n</code></pre> <p>Make sure that <code>nvidia-peermem</code> is loaded.</p> EAL: Couldn't get fd on hugepage file [..] error allocating rte services array <p>Example error:</p> <pre><code>EAL: get_seg_fd(): open '/mnt/huge/nwlrbbmqbhmap_0' failed: Permission denied\nEAL: Couldn't get fd on hugepage file\nEAL: error allocating rte services array\nEAL: FATAL: rte_service_init() failed\nEAL: rte_service_init() failed\n</code></pre> <p>Ensure you run as root, using <code>sudo</code>.</p> EAL: Cannot get hugepage information. <pre><code>EAL: x hugepages of size x reserved, no mounted hugetlbfs found for that size\n</code></pre> <p>Ensure your hugepages are mounted.</p> <pre><code>EAL: No free x kB hugepages reported on node 0\n</code></pre> <ul> <li>Ensure you have allocated hugepages.</li> <li> <p>If you have already, check if they are any free left with <code>grep Huge /proc/meminfo</code>.</p> See an example output <p>No more space here!</p> <pre><code>HugePages_Total:       2\nHugePages_Free:        0\nHugePages_Rsvd:        0\nHugePages_Surp:        0\nHugepagesize:    1048576 kB\nHugetlb:         2097152 kB\n</code></pre> </li> <li> <p>If not, you can delete dangling hugepages under your hugepage mount point. That happens when your previous application run crashes.</p> <pre><code>sudo rm -rf /dev/hugepages/* # default mount point\nsudo rm -rf /mnt/huge/*      # custom mount point\n</code></pre> </li> </ul> Could not allocate x MB of GPU memory [...] Failed to allocate GPU memory <p>Check your GPU utilization:</p> <pre><code>nvidia-smi pmon -c 1\n</code></pre> <p>You might need to kill some of the listed processes to free up GPU VRAM.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#5-building-your-own-application","title":"5. Building your own application","text":"<p>This section will guide you through building your own application using the <code>adv_networking_bench</code> as an example. Make sure to install <code>holoscan-networking</code> first.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#51-understand-the-configuration-parameters","title":"5.1 Understand the configuration parameters","text":"<p>Note</p> <p>The configuration below will be analyzed in the context of the application consuming it, as defined in the <code>main.cpp</code> file. You can look it up when the \"sample application code\" is referenced.</p> Debian installationFrom source <pre><code>/opt/nvidia/holoscan/examples/adv_networking_bench/main.cpp\n</code></pre> <pre><code>./applications/adv_networking_bench/cpp/main.cpp\n</code></pre> <p>If you are not yet familiar with how Holoscan applications are constructed, please refer to the Holoscan SDK documentation first.</p> <p>Let's look at the <code>adv_networking_bench_default_tx_rx.yaml</code> file below. Click on the (1) icons below to expand explanations for each annotated line.</p> <ol> <li>The cake is a lie </li> </ol> <pre><code>scheduler: # (1)!\n  check_recession_period_ms: 0\n  worker_thread_number: 5\n  stop_on_deadlock: true\n  stop_on_deadlock_timeout: 500\n  # max_duration_ms: 20000\n\nadvanced_network: # (2)!\n  cfg:\n    version: 1\n    manager: \"dpdk\" # (3)!\n    master_core: 3 # (4)!\n    debug: false\n    log_level: \"info\"\n\n    memory_regions: # (5)!\n    - name: \"Data_TX_GPU\" # (6)!\n      kind: \"device\" # (7)!\n      affinity: 0 # (8)!\n      num_bufs: 51200 # (9)!\n      buf_size: 1064 # (10)!\n    - name: \"Data_RX_GPU\"\n      kind: \"device\"\n      affinity: 0\n      num_bufs: 51200\n      buf_size: 1000\n    - name: \"Data_RX_CPU\"\n      kind: \"huge\"\n      affinity: 0\n      num_bufs: 51200\n      buf_size: 64\n\n    interfaces: # (11)!\n    - name: \"tx_port\" # (12)!\n      address: &lt;0000:00:00.0&gt; # (13)! # The BUS address of the interface doing Tx\n      tx: # (14)!\n        queues: # (15)!\n        - name: \"tx_q_0\" # (16)!\n          id: 0 # (17)!\n          batch_size: 10240 # (18)!\n          cpu_core: 11 # (19)!\n          memory_regions: # (20)!\n            - \"Data_TX_GPU\"\n          offloads: # (21)!\n            - \"tx_eth_src\"\n    - name: \"rx_port\"\n      address: &lt;0000:00:00.0&gt; # (22)! # The BUS address of the interface doing Rx\n      rx:\n        flow_isolation: true # (23)!\n        queues:\n        - name: \"rx_q_0\"\n          id: 0\n          cpu_core: 9\n          batch_size: 10240\n          output_port: \"bench_rx_out\" # (24)!\n          memory_regions: # (25)!\n            - \"Data_RX_CPU\"\n            - \"Data_RX_GPU\"\n        flows: # (26)!\n        - name: \"flow_0\" # (27)!\n          id: 0 # (28)!\n          action: # (29)!\n            type: queue\n            id: 0\n          match: # (30)!\n            udp_src: 4096\n            udp_dst: 4096\n            ipv4_len: 1050\n\nbench_rx: # (31)!\n  gpu_direct: true       # Set to true if using a GPU region for the Rx queues.\n  split_boundary: true   # Whether header and data are split for Rx (Header to CPU)\n  batch_size: 10240\n  max_packet_size: 1064\n  header_size: 64\n\nbench_tx: # (32)!\n  gpu_direct: true        # Set to true if using a GPU region for the Tx queues.\n  split_boundary: 0       # Byte boundary where header and data are split for Tx, 0 if no split\n  batch_size: 10240\n  payload_size: 1000\n  header_size: 64\n  eth_dst_addr: &lt;00:00:00:00:00:00&gt; # Destination MAC address - required when Rx flow_isolation=true\n  ip_src_addr: &lt;1.2.3.4&gt;  # Source IP address - required on layer 3 network\n  ip_dst_addr: &lt;5.6.7.8&gt;  # Destination IP address - required on layer 3 network\n  udp_src_port: 4096      # UDP source port\n  udp_dst_port: 4096      # UDP destination port\n  address: &lt;0000:00:00.0&gt; # Source NIC Bus ID. Should match the address of the Tx interface above\n</code></pre> <ol> <li>The <code>scheduler</code> section is passed to the multi threaded scheduler we declare in the <code>main()</code> function of this application. See the holoscan SDK documentation and API docs for more details. This is related to the Holoscan core library and is not specific to Holoscan Networking.</li> <li>The <code>advanced_network</code> section is passed to the <code>AdvNetworkOpRx</code> and <code>AdvNetworkOpTx</code> operators which are responsible for setting up the NIC.</li> <li><code>manager</code> is the backend networking library. default: <code>dpdk</code>. Other: <code>gpunetio</code> (DOCA GPUNet IO + DOCA Ethernet &amp; Flow). Coming soon: <code>rivermax</code>, <code>rdma</code>.</li> <li><code>master_core</code> is the ID of the CPU core used for setup. It does not need to be isolated, and is recommended to differ differ from the <code>cpu_core</code> fields below used for polling the NIC.</li> <li>The <code>memory_regions</code> section lists where the NIC will write/read data from/to when bypassing the OS kernel. Tip: when using GPU buffer regions, keeping the sum of their buffer sizes lower than 80% of your BAR1 size is generally a good rule of thumb \ud83d\udc4d.</li> <li>A descriptive name for that memory region to refer to later in the <code>interfaces</code> section.</li> <li>The type of memory region. Best options are <code>device</code> (GPU), or <code>huge</code> (pages - CPU). Also supported but not recommended are <code>malloc</code> (CPU) and <code>pinned</code> (CPU).</li> <li>The GPU ID for <code>device</code> memory regions. The NUMA node ID for CPU memory regions.</li> <li>The number of buffers in the memory region. A higher value means more time to process the data, but it takes additional space on the GPU BAR1. Too low increases the risk of dropping packets from the NIC having nowhere to write (Rx) or the risk of higher latency from buffering (Tx). Need a rule of thumb \ud83d\udc4d? 5x the <code>batch_size</code> below is a good starting point.</li> <li>The size of each buffer in the memory region. These should be equal to your maximum packet size, or less if breaking down packets (ex: header data split, see the <code>rx</code> queue below).</li> <li>The <code>interfaces</code> section lists the NIC interfaces that will be configured for the application.</li> <li>A descriptive name for that interface, currently only used for logging.</li> <li>The PCIe/bus address of that interface, as identified in previous sections.</li> <li>Each interface can have a <code>tx</code> (transmitting) or <code>rx</code> (receiving) section, or both if you'd like to configure both Tx and Rx on the same interface.</li> <li>The <code>queues</code> section lists the queues for that interface. Queues are a core concept of NICs: they handle the actual receiving or transmitting of network packets. Rx queues buffer incoming packets until they can be processed by the application, while Tx queues hold outgoing packets waiting to be sent on the network. The simplest setup uses only one receive and one transmit queue. Using more queues allows multiple streams of network traffic to be processed in parallel, as each queue can be assigned to a specific CPU core, and are assigned their own memory regions that are not shared.</li> <li>A descriptive name for that queue, currently only used for logging.</li> <li>The ID of that queue, which can be referred to later in the <code>flows</code> section.</li> <li>The number of packets per batch. The <code>advanced_network</code> Rx operator will forward packets on a timer, or when the NIC receives enough packets for a whole batch per this number. The <code>advanced_network</code> Tx operator needs to ensure it does not send more packets than this value on each <code>Operator::compute()</code> call.</li> <li>The ID of the CPU core that this queue will use to poll the NIC. Ideally one isolated core per queue.</li> <li>The list of memory regions where this queue will write/read packets from/to. The order matters: the first memory region will be used first to read/write from until it fills up one buffer (<code>buf_size</code>), after which it will move to the next region in the list and so on until the packet is fully written/read. See the <code>memory_regions</code> for the <code>rx</code> queue below for an example.</li> <li>The <code>offloads</code> section (Tx queues only) lists optional tasks that can be offloaded to the NIC. The only value currently supported is <code>tx_eth_src</code>, that lets the NIC insert the ethernet source mac address in the packet headers. Note: IP, UDP, and Ethernet Checksums or CRC are always done by the NIC currently and are not optional.</li> <li>Same as for <code>tx_port</code>. Each interface in this list should have a unique mac address. This one will do <code>rx</code> per config below.</li> <li>Whether to isolate the Rx flow. If true, any incoming packets that does not match the MAC address of this interface - or isn't directed to a queue when the <code>flows</code> section below is used - will be delegated back to Linux for processing (no kernel bypass). This is useful to let this interface handle ARP, ICMP, etc. Otherwise, any packets sent to this interface (ex: ping) will need to be processed (or dropped) by your application.</li> <li><code>rx</code> queues have an <code>output_port</code> parameter so you can attach a downstream operator to receive data from this specific queue, as can be seen in the <code>Application::compose()</code> function of the sample application. Multiple <code>rx</code> queues can share the same <code>output_port</code>. In contrast, <code>tx</code> queues have a single non-configurable port (name: <code>burst_in</code>) to which upstream operators will send all packets, which are then routed to the correct queue based on the port/queue in the burst header.</li> <li>This scenario is called HDS (Header-Data Split): the packet will first be written to a buffer in the <code>Data_RX_CPU</code> memory region, filling its <code>buf_size</code> of 64 bytes - which is consistent with the size of our header - then the rest of the packet will be written to the <code>Data_RX_GPU</code> memory region. Its <code>buf_size</code> of 1000 bytes is just what we need to write the payload size for our application, no byte wasted!</li> <li>The list of flows. Flows are responsible for routing packets to the correct queue based on various properties. If this field is missing, all packets will be routed to the first queue.</li> <li>The flow name, currently only used for logging.</li> <li>The flow <code>id</code> is used to tag the packets with what flow it arrived on. This is useful when sending multiple flows to a single queue, as the user application can differentiate which flow (i.e. rules) matched the packet based on this ID.</li> <li>What to do with packets that match this flow. The only supported action currently is <code>type: queue</code> to send the packet to a queue given its <code>id</code>.</li> <li>List of rules to match packets against. All rules must be met for a packet to match the flow. Currently supported rules include <code>udp_src</code> and <code>udp_dst</code> (port numbers), <code>ipv4_len</code> (#TODO#) etc.</li> <li>The <code>bench_rx</code> section is passed to the <code>AdvNetworkingBenchDefaultRxOp</code> operator in the <code>Application::compose()</code> function of the sample application. This operator is a custom operator implemented in <code>default_bench_op_rx.h</code> that aggregates packets received from the NIC. The parameters in this section are specific to this operator, and should align with how <code>memory_regions</code> and <code>queues</code> were configured for the <code>rx</code> interface.</li> <li>The <code>bench_tx</code> section is passed to the <code>AdvNetworkingBenchDefaultTxOp</code> operator in the <code>Application::compose()</code> function of the sample application. This operator is a custom operator implemented in <code>default_bench_op_tx.h</code> that generates dummy packets to send to the NIC. The parameters in this section up to <code>header_size</code> should align with how <code>memory_regions</code> and <code>queues</code> were configured for the <code>tx</code> interface. The following parameters up to <code>udp_dst_port</code> are used to fill-in the ethernet header of the packets. The last parameter, <code>address</code>, is used to specify which NIC interface to use for the Tx operation.</li> </ol>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#52-create-your-own-rx-operator","title":"5.2 Create your own Rx operator","text":"<p>Under construction</p> <p>This section is under construction. Refer to the implementation of the <code>AdvNetworkingBenchDefaultRxOp</code> for an example.</p> Debian installationFrom source <pre><code>/opt/nvidia/holoscan/examples/adv_networking_bench/default_bench_op_rx.h\n</code></pre> <pre><code>./applications/adv_networking_bench/cpp/default_bench_op_rx.h\n</code></pre> <p>Note</p> <p>Design investigations are expected soon for a generic packet aggregator operator.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#53-create-your-own-tx-operator","title":"5.3 Create your own Tx operator","text":"<p>Under construction</p> <p>This section is under construction. Refer to the implementation of the <code>AdvNetworkingBenchDefaultTxOp</code> for an example.</p> Debian installationFrom source <pre><code>/opt/nvidia/holoscan/examples/adv_networking_bench/default_bench_op_tx.h\n</code></pre> <pre><code>./applications/adv_networking_bench/cpp/default_bench_op_tx.h\n</code></pre> <p>Note</p> <p>Designs investigations are expected soon for a generic way to prepare packets to send to the NIC.</p>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/high_performance_networking/0.1/#54-build-with-cmake","title":"5.4 Build with CMake","text":"Debian installationFrom source <ol> <li>Create a source directory and write your source file(s) for your application (and custom operators if needed)</li> <li> <p>Create a <code>CMakeLists.txt</code> file in your source directory like this one:</p> <pre><code>cmake_minimum_required(VERSION 3.20)\nproject(my_app CXX) # Add CUDA if writing .cu kernels\n\nfind_package(holoscan 2.6 REQUIRED CONFIG PATHS \"/opt/nvidia/holoscan\")\nfind_package(holoscan-networking REQUIRED CONFIG PATHS \"/opt/nvidia/holoscan\")\n\n# Create an executable\nadd_executable(my_app\n    my_app.cpp\n    ...\n)\ntarget_include_directories(my_app\n    PRIVATE\n        my_include_dirs/\n        ...\n)\ntarget_link_libraries(my_app\n    PRIVATE\n        holoscan::core\n        holoscan::ops::advanced_network_rx\n        holoscan::ops::advanced_network_tx\n        my_other_dependencies\n        ...\n)\n\n# Copy the config file to the build directory for convenience referring to it\nadd_custom_target(my_app_config_yaml\n    COMMAND ${CMAKE_COMMAND} -E copy_if_different \"${CMAKE_CURRENT_SOURCE_DIR}/my_app_config.yaml\" ${CMAKE_CURRENT_BINARY_DIR}\n    DEPENDS \"${CMAKE_CURRENT_SOURCE_DIR}/my_app_config.yaml\"\n)\nadd_dependencies(my_app my_app_config_yaml)\n</code></pre> </li> <li> <p>Build your application like so:</p> <pre><code># Your chosen paths\nsrc_dir=\".\"\nbuild_dir=\"build\"\n\n# Configure the build\ncmake -S \"$src_dir\" -B \"$build_dir\"\n\n# Build the application\ncmake --build \"$build_dir\" -j\n</code></pre> Failed to detect a default CUDA architecture. <p>Add the path to your installation of <code>nvcc</code> to your <code>PATH</code>, or pass its to the cmake configuration command like so (adjust to your CUDA/nvcc installation path):</p> <pre><code>cmake -S \"$src_dir\" -B \"$build_dir\" -D CMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc\n</code></pre> </li> <li> <p>Run your application like so:</p> <pre><code>\"./$build_dir/my_app my_app_config.yaml\"\n</code></pre> </li> </ol> <ol> <li>Create an application directory under <code>applications/</code> in your clone of the HoloHub repository, and write your source file(s) for your application (and custom operators if needed).</li> <li> <p>Add the following to the <code>application/CMakeLists.txt</code> file:</p> <pre><code>add_holohub_application(my_app DEPENDS OPERATORS advanced_network)\n</code></pre> </li> <li> <p>Create a <code>CMakeLists.txt</code> file in your application directory like this one:</p> <pre><code>cmake_minimum_required(VERSION 3.20)\nproject(my_app CXX) # Add CUDA if writing .cu kernels\n\nfind_package(holoscan 2.6 REQUIRED CONFIG PATHS \"/opt/nvidia/holoscan\")\n\n# Create an executable\nadd_executable(my_app\n    my_app.cpp\n    ...\n)\ntarget_include_directories(my_app\n    PRIVATE\n        my_include_dirs/\n        ...\n)\ntarget_link_libraries(my_app\n    PRIVATE\n        holoscan::core\n        holoscan::ops::advanced_network_rx\n        holoscan::ops::advanced_network_tx\n        my_other_dependencies\n        ...\n)\n\n# Copy the config file to the build directory for convenience referring to it\nadd_custom_target(my_app_config_yaml\n    COMMAND ${CMAKE_COMMAND} -E copy_if_different \"${CMAKE_CURRENT_SOURCE_DIR}/my_app_config.yaml\" ${CMAKE_CURRENT_BINARY_DIR}\n    DEPENDS \"${CMAKE_CURRENT_SOURCE_DIR}/my_app_config.yaml\"\n)\nadd_dependencies(my_app my_app_config_yaml)\n</code></pre> </li> <li> <p>Build your application like so:</p> <pre><code>./dev_container build_and_run my_app --no_run\n</code></pre> </li> <li> <p>Run your application like so:</p> <pre><code>./dev_container launch --img holohub:my_app --docker_opts \"-u 0 --privileged\" --bash -c \"./build/my_app/applications/my_app my_app_config.yaml\"\n</code></pre> <p>or, if you have set up a shortcut to run your application with its config file through its <code>metadata.json</code> (see other apps for examples):</p> <pre><code>./dev_container build_and_run --no_build --container_args \" -u 0 --privileged\"\n</code></pre> </li> </ol>","tags":["DPDK","RDMA","Rivermax","GPUNetIO","GPUDirect","ConnectX","Networking","NIC","HPC"]},{"location":"tutorials/holohub_operators_external_applications/","title":"Using Holohub Operators in External Applications","text":"<p> Authors: Julien Jomier (NVIDIA) Supported platforms: x86_64, aarch64 Language: C++ Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 3.5.0 Tested Holoscan SDK versions: 3.5.0 Contribution metric: Level 1 - Highly Reliable</p> <p>This tutorial demonstrates how to import and use Holohub operators in your own external applications. You'll learn how to fetch specific operators from the Holohub repository and integrate them into your Holoscan-based applications.</p>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#overview","title":"Overview","text":"<p>Holohub provides a collection of pre-built operators that you can easily integrate into your applications. This tutorial shows you how to:</p> <ol> <li>Set up a CMake project that uses Holohub operators</li> <li>Fetch specific operators using the <code>FetchHolohubOperator.cmake</code> utility</li> <li>Link against the required Holohub libraries</li> <li>Use the operators in your application code</li> </ol>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#prerequisites","title":"Prerequisites","text":"<ul> <li>CMake 3.18 or higher</li> <li>Holoscan SDK installed</li> <li>Git</li> <li>C++ compiler (GCC, Clang, or MSVC)</li> </ul>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#project-structure","title":"Project Structure","text":"<pre><code>your_external_app/\n\u251c\u2500\u2500 CMakeLists.txt\n\u251c\u2500\u2500 main.cpp\n\u251c\u2500\u2500 main.py (optional - for Python applications)\n\u2514\u2500\u2500 build/\n    \u2514\u2500\u2500 python/\n        \u2514\u2500\u2500 lib/\n            \u2514\u2500\u2500 holohub/ (Python modules)\n</code></pre>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#step-by-step-guide","title":"Step-by-Step Guide","text":"","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#1-create-your-cmakeliststxt","title":"1. Create Your CMakeLists.txt","text":"<p>Create a <code>CMakeLists.txt</code> file in your project root. The file includes two different approaches for including the <code>FetchHolohubOperator.cmake</code> utility (plus an internal-only option):</p> <pre><code>cmake_minimum_required(VERSION 3.18)\nproject(your_app_name)\n\n# Find the Holoscan package\nfind_package(holoscan REQUIRED)\n\n# =============================================================================\n# FETCH HOLOHUB OPERATOR UTILITY - TWO APPROACHES AVAILABLE\n# =============================================================================\n# Choose one of the following two approaches by uncommenting the desired option:\n\n# INTERNAL ONLY: Repository Include (for Holohub repository internal use only)\n# This approach is only available when the application is within the Holohub repository structure\n# For external applications, use one of the two approaches below\ninclude(../../cmake/FetchHolohubOperator.cmake)\n\n# APPROACH 1: Download from GitHub (for external applications with internet access)\n# Pros: Independent of repository structure, automatic updates, no manual copying\n# Cons: Requires internet connection, depends on GitHub availability\n# Uncomment the following lines to use this approach (and comment out the internal include above):\n# set(FETCH_HOLOHUB_OPERATOR_URL \"https://raw.githubusercontent.com/nvidia-holoscan/holohub/refs/heads/main/cmake/FetchHolohubOperator.cmake\")\n# set(FETCH_HOLOHUB_OPERATOR_LOCAL_PATH \"${CMAKE_CURRENT_BINARY_DIR}/FetchHolohubOperator.cmake\")\n# if(NOT EXISTS ${FETCH_HOLOHUB_OPERATOR_LOCAL_PATH})\n#      file(DOWNLOAD\n#          ${FETCH_HOLOHUB_OPERATOR_URL}\n#          ${FETCH_HOLOHUB_OPERATOR_LOCAL_PATH}\n#          SHOW_PROGRESS\n#          TLS_VERIFY ON\n#      )\n#      if(NOT EXISTS ${FETCH_HOLOHUB_OPERATOR_LOCAL_PATH})\n#          message(FATAL_ERROR \"Failed to download FetchHolohubOperator.cmake from ${FETCH_HOLOHUB_OPERATOR_URL}\")\n#      endif()\n# endif()\n# include(${FETCH_HOLOHUB_OPERATOR_LOCAL_PATH})\n\n# APPROACH 2: Local Copy (for offline environments or version control)\n# Pros: Complete independence, no internet dependency, full version control\n# Cons: Requires manual file copying, need to manually update\n# Uncomment the following line to use this approach (requires FetchHolohubOperator.cmake in same directory):\n# include(${CMAKE_CURRENT_SOURCE_DIR}/FetchHolohubOperator.cmake)\n\n# Fetch the specific operator you need\nfetch_holohub_operator(aja_source)\n\n# Add your executable\nadd_executable(${PROJECT_NAME} main.cpp)\n\n# Link against Holohub libraries\ntarget_link_libraries(${PROJECT_NAME} \n   PRIVATE \n   holoscan::core\n   holoscan::aja\n   )\n</code></pre>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#2-understanding-the-cmakeliststxt","title":"2. Understanding the CMakeLists.txt","text":"<p>Let's break down each section:</p>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#project-setup","title":"Project Setup","text":"<pre><code>cmake_minimum_required(VERSION 3.18)\nproject(your_app_name)\n</code></pre> <ul> <li>Sets the minimum CMake version required</li> <li>Defines your project name</li> </ul>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#holoscan-integration","title":"Holoscan Integration","text":"<pre><code>find_package(holoscan REQUIRED)\n</code></pre> <ul> <li>Locates and configures the Holoscan SDK</li> <li>Makes Holoscan targets available for linking</li> </ul>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#operator-fetching-two-approaches-available","title":"Operator Fetching - Two Approaches Available","text":"<p>The CMakeLists.txt provides two different approaches for including the <code>FetchHolohubOperator.cmake</code> utility:</p> <p>Internal Only: Repository Include <pre><code>include(../../cmake/FetchHolohubOperator.cmake)\n</code></pre></p> <ul> <li>\u2705 Simple and straightforward</li> <li>\u2705 Always uses the latest version from the repository</li> <li>\u2705 No additional files needed</li> <li>\u274c INTERNAL USE ONLY - Requires the application to be within the Holohub repository structure</li> <li>\u274c Not suitable for external applications</li> </ul> <p>Approach 1: Download from GitHub <pre><code> set(FETCH_HOLOHUB_OPERATOR_URL \"https://raw.githubusercontent.com/nvidia-holoscan/holohub/refs/heads/main/cmake/FetchHolohubOperator.cmake\")\n set(FETCH_HOLOHUB_OPERATOR_LOCAL_PATH \"${CMAKE_CURRENT_BINARY_DIR}/FetchHolohubOperator.cmake\")\n if(NOT EXISTS ${FETCH_HOLOHUB_OPERATOR_LOCAL_PATH})\n      file(DOWNLOAD\n          ${FETCH_HOLOHUB_OPERATOR_URL}\n          ${FETCH_HOLOHUB_OPERATOR_LOCAL_PATH}\n          SHOW_PROGRESS\n          TLS_VERIFY ON\n      )\n      if(NOT EXISTS ${FETCH_HOLOHUB_OPERATOR_LOCAL_PATH})\n          message(FATAL_ERROR \"Failed to download FetchHolohubOperator.cmake from ${FETCH_HOLOHUB_OPERATOR_URL}\")\n      endif()\n endif()\n include(${FETCH_HOLOHUB_OPERATOR_LOCAL_PATH})\n</code></pre></p> <ul> <li>\u2705 Completely independent of repository structure</li> <li>\u2705 Works for truly external applications</li> <li>\u2705 Always gets the latest version from the main branch</li> <li>\u274c Requires internet connection during build</li> <li>\u274c Depends on GitHub availability</li> </ul> <p>Approach 2: Local Copy <pre><code> include(${CMAKE_CURRENT_SOURCE_DIR}/FetchHolohubOperator.cmake)\n</code></pre></p> <ul> <li>\u2705 Complete independence from repository structure</li> <li>\u2705 No internet dependency during build</li> <li>\u2705 Full version control over the utility file</li> <li>\u274c Requires manual file copying</li> <li>\u274c Need to manually update when new versions are released</li> </ul> <p>Usage: <pre><code>fetch_holohub_operator(aja_source)\n</code></pre></p> <ul> <li>Downloads the <code>aja_source</code> operator from Holohub using sparse checkout</li> </ul>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#application-building","title":"Application Building","text":"<pre><code>add_executable(${PROJECT_NAME} main.cpp)\ntarget_link_libraries(${PROJECT_NAME} \n   PRIVATE \n   holoscan::core\n   holoscan::aja\n   )\n</code></pre> <ul> <li>Creates your executable from <code>main.cpp</code></li> <li>Links against the required Holohub libraries</li> </ul>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#3-the-fetchholohuboperatorcmake-utility","title":"3. The FetchHolohubOperator.cmake Utility","text":"<p>The <code>FetchHolohubOperator.cmake</code> file provides a convenient way to fetch specific operators from the Holohub repository. It uses Git sparse checkout to download only the required operator, making the process efficient.</p>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#function-signature","title":"Function Signature","text":"<pre><code>fetch_holohub_operator(OPERATOR_NAME [PATH path] [REPO_URL url] [BRANCH branch] [DEPTH depth] [DISABLE_PYTHON] [PATCH_COMMAND command])\n</code></pre>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#parameters","title":"Parameters","text":"<ul> <li><code>OPERATOR_NAME</code>: The name of the operator to fetch</li> <li><code>PATH</code> (optional): The path to the operator within the Holohub repository (defaults to OPERATOR_NAME)</li> <li><code>REPO_URL</code> (optional): The URL of the Holohub repository (defaults to the official Holohub repo)</li> <li><code>BRANCH</code> (optional): The branch to checkout (defaults to \"main\")</li> <li><code>DEPTH</code> (optional): Git clone depth (defaults to 1 for shallow clone, use 0 for full history)</li> <li><code>DISABLE_PYTHON</code> (optional): Flag to disable Python bindings build (Python bindings are enabled by default)</li> <li><code>PATCH_COMMAND</code> (optional): Custom command to run after checkout (e.g., to apply patches)</li> </ul>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#examples","title":"Examples","text":"<pre><code># Fetch the aja_source operator\nfetch_holohub_operator(aja_source)\n\n# Fetch an operator with a custom path\nfetch_holohub_operator(dds_operator_base PATH dds/base)\n\n# Fetch from a custom repository\nfetch_holohub_operator(custom_operator REPO_URL \"https://github.com/custom/holohub.git\")\n\n# Fetch from a specific branch\nfetch_holohub_operator(custom_operator BRANCH \"dev\")\n\n# Fetch with full git history\nfetch_holohub_operator(custom_operator DEPTH 0)\n\n# Fetch an operator without Python bindings\nfetch_holohub_operator(aja_source DISABLE_PYTHON)\n\n# Fetch and apply a patch\nfetch_holohub_operator(aja_source PATCH_COMMAND git apply ${CMAKE_CURRENT_SOURCE_DIR}/fix.patch)\n\n# Combine multiple parameters\nfetch_holohub_operator(custom_operator\n    PATH operators/custom\n    REPO_URL \"https://github.com/my-org/custom-holohub.git\"\n    BRANCH \"dev\"\n    DEPTH 0\n    DISABLE_PYTHON\n    PATCH_COMMAND git apply ${CMAKE_CURRENT_SOURCE_DIR}/fix1.patch\n                  &amp;&amp; git apply ${CMAKE_CURRENT_SOURCE_DIR}/fix2.patch\n)\n</code></pre>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#4-choosing-the-right-approach","title":"4. Choosing the Right Approach","text":"<p>When deciding which approach to use for including <code>FetchHolohubOperator.cmake</code>, consider your specific requirements:</p> <p>Use Repository Include (Internal Only) when:</p> <ul> <li>Your application is part of the Holohub repository</li> <li>You want to always use the latest version</li> <li>You're developing within the Holohub ecosystem</li> <li>Note: This approach is only available for internal Holohub repository use</li> </ul> <p>Use Approach 1 (Download from GitHub) when:</p> <ul> <li>Your application is external to the Holohub repository</li> <li>You have reliable internet connectivity during builds</li> <li>You want automatic updates from the main branch</li> <li>You need independence from the repository structure</li> </ul> <p>Use Approach 2 (Local Copy) when:</p> <ul> <li>Your application needs to work offline</li> <li>You require version stability and reproducibility</li> <li>You need complete control over the utility file</li> <li>You're building for production environments</li> </ul>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#5-create-your-application-code","title":"5. Create Your Application Code","text":"<p>Create a <code>main.cpp</code> file that uses the fetched operator:</p> <pre><code>#include \"holoscan/holoscan.hpp\"\n#include \"aja_source.hpp\"\n\nclass App : public holoscan::Application {\n public:\n  void compose() override {\n    using namespace holoscan;\n\n    // Create an instance of the AJA source operator\n    auto aja_source = make_operator&lt;ops::AJASourceOp&gt;(\"aja\");\n\n    // Add the operator to your application\n    add_operator(aja_source);\n  }\n};\n\nint main(int argc, char** argv) {\n  auto app = holoscan::make_application&lt;App&gt;();\n  app-&gt;run();\n\n  return 0;\n}\n</code></pre>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#6-building-your-application","title":"6. Building Your Application","text":"<pre><code># Create a build directory\nmkdir build &amp;&amp; cd build\n\n# Configure the project\ncmake ..\n\n# Build the project\nmake -j$(nproc)\n</code></pre>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#7-using-python-bindings","title":"7. Using Python Bindings","text":"<p>Many Holohub operators provide Python bindings, allowing you to use them in Python applications. Here's how to work with Python bindings:</p>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#building-with-python-bindings","title":"Building with Python Bindings","text":"<p>By default, Python bindings are automatically built when you fetch an operator. If you want to disable Python bindings, you can use the <code>DISABLE_PYTHON</code> flag:</p> <pre><code># Disable Python bindings for this operator\nfetch_holohub_operator(aja_source DISABLE_PYTHON)\n</code></pre>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#running-python-applications","title":"Running Python Applications","text":"<p>After building your project, Python modules are typically installed in the <code>python/lib</code> directory within your build directory. To run Python applications that use the Holohub operators:</p> <pre><code># Navigate to your build directory\ncd build\n\n# Set the PYTHONPATH to include the built Python modules\nexport PYTHONPATH=$PYTHONPATH:$(pwd)/python/lib\n\n# Run your Python application\npython3 ../main.py\n</code></pre>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#python-example","title":"Python Example","text":"<p>Create a <code>main.py</code> file that uses the fetched operator:</p> <pre><code>from holoscan.core import Application\nfrom holohub.aja_source import AJASourceOp\n\nclass App(Application):\n    def compose(self):\n        # Create an instance of the AJA source operator\n        aja_source = AJASourceOp(self, name=\"aja\")\n\n        # Add the operator to your application\n        self.add_operator(aja_source)\n\ndef main():\n    app = App()\n    app.run()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#python-bindings-location","title":"Python Bindings Location","text":"<p>The Python modules are built and installed in the following structure: <pre><code>build/\n\u251c\u2500\u2500 python/\n\u2502   \u2514\u2500\u2500 lib/\n\u2502       \u2514\u2500\u2500 holohub/\n\u2502           \u251c\u2500\u2500 __init__.py\n\u2502           \u251c\u2500\u2500 aja_source.py\n\u2502           \u2514\u2500\u2500 ... (other operator modules)\n</code></pre></p>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#troubleshooting-python-bindings","title":"Troubleshooting Python Bindings","text":"<ol> <li>Module not found errors: Ensure <code>PYTHONPATH</code> is set correctly</li> <li>Import errors: Verify that Python bindings were built (check the <code>python/lib</code> directory)</li> <li>Version compatibility: Make sure your Python version is compatible with the Holoscan SDK</li> </ol>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#available-operators","title":"Available Operators","text":"<p>Holohub provides many operators that you can fetch and use. Some popular ones include:</p> <ul> <li><code>aja_source</code> - AJA video capture</li> <li><code>aja_sink</code> - AJA video output</li> <li><code>realsense_camera</code> - Intel RealSense camera</li> <li><code>dds_operator_base</code> - DDS communication</li> <li><code>tensor_rt_inference</code> - TensorRT inference</li> <li><code>format_converter</code> - Format conversion utilities</li> </ul> <p>To find more operators, check the Holohub operators directory.</p>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#advanced-usage","title":"Advanced Usage","text":"","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#fetching-multiple-operators","title":"Fetching Multiple Operators","text":"<p>You can fetch multiple operators in the same project:</p> <pre><code># Fetch multiple operators\nfetch_holohub_operator(aja_source)\nfetch_holohub_operator(format_converter)\nfetch_holohub_operator(tensor_rt_inference)\n\n# Link against all required libraries\ntarget_link_libraries(${PROJECT_NAME} \n   PRIVATE \n   holoscan::core\n   holoscan::aja\n   holoscan::format_converter\n   holoscan::tensor_rt_inference\n   )\n</code></pre>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#custom-operator-paths","title":"Custom Operator Paths","text":"<p>If an operator is located in a subdirectory within the Holohub repository:</p> <pre><code>fetch_holohub_operator(dds_operator_base PATH dds/base)\n</code></pre>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#using-different-branches","title":"Using Different Branches","text":"<p>To use operators from a specific branch:</p> <pre><code>fetch_holohub_operator(experimental_operator BRANCH \"experimental\")\n</code></pre>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#applying-patches-to-operators","title":"Applying Patches to Operators","text":"<p>You can apply custom patches to operators using the <code>PATCH_COMMAND</code> parameter:</p> <pre><code># Apply a patch file\nfetch_holohub_operator(aja_source \n    PATCH_COMMAND git apply ${CMAKE_CURRENT_SOURCE_DIR}/fix.patch\n)\n</code></pre> <p>Note: If applying patches created from older commits, you may need to fetch full git history:</p> <pre><code>fetch_holohub_operator(aja_source \n    DEPTH 0\n    PATCH_COMMAND git apply ${CMAKE_CURRENT_SOURCE_DIR}/fix.patch\n)\n</code></pre>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#troubleshooting","title":"Troubleshooting","text":"","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#common-issues","title":"Common Issues","text":"<ol> <li>CMake can't find Holoscan</li> <li>Ensure Holoscan SDK is properly installed</li> <li> <p>Set <code>CMAKE_PREFIX_PATH</code> to point to your Holoscan installation</p> </li> <li> <p>Operator not found</p> </li> <li>Verify the operator name exists in the Holohub repository</li> <li> <p>Check the correct path if the operator is in a subdirectory</p> </li> <li> <p>Linking errors</p> </li> <li>Ensure you're linking against the correct Holohub libraries</li> <li>Check that the operator dependencies are satisfied</li> </ol>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#debug-information","title":"Debug Information","text":"<p>To see what's being fetched, you can enable CMake verbose output:</p> <pre><code>cmake -DCMAKE_VERBOSE_MAKEFILE=ON ..\n</code></pre>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#best-practices","title":"Best Practices","text":"<ol> <li>Version Pinning: Consider using specific branches or tags for production applications</li> <li>Dependency Management: Only fetch the operators you actually need</li> <li>Error Handling: Always check if the <code>find_package(holoscan REQUIRED)</code> succeeds</li> <li>Documentation: Document which operators your application depends on</li> </ol>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#example-complete-project","title":"Example Complete Project","text":"<p>See the <code>main.cpp</code>, <code>main.py</code>, and <code>CMakeLists.txt</code> files in this directory for complete working examples that demonstrate how to use the AJA source operator from Holohub in both C++ and Python applications.</p>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#using-holohub-cli-in-external-projects","title":"Using Holohub CLI in External Projects","text":"<p>The Holohub CLI provides convenient command-line tools for managing Holohub applications, including building, running, and testing operators and applications. You can easily integrate the CLI into your external projects to leverage these functionalities.</p>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#adding-holohubcli_wrapper-to-your-project","title":"Adding holohubCLI_wrapper to Your Project","text":"<p>To use the Holohub CLI in your external project, simply copy the <code>holohubCLI_wrapper</code> script from this tutorial to the root of your project.:</p> <pre><code># Download the CLI script to your project root\nwget -O /path/to/your/project/proj_cli https://raw.githubusercontent.com/nvidia-holoscan/holohub/main/tutorials/holohub_operators_external_applications/holohubCLI_wrapper\n\n# Make it executable\nchmod +x /path/to/your/project/proj_cli\n</code></pre> <p>Add the following to your <code>.gitignore</code> file to exclude the downloaded Holohub CLI utilities:</p> <pre><code># Holohub CLI\nutilities\ncmake\n.local\n</code></pre>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#project-structure-with-cli","title":"Project Structure with CLI","text":"<p>Your project structure will look like this:</p> <pre><code>your_external_app/\n\u251c\u2500\u2500 myprojectCLI           # Holohub CLI script\n\u251c\u2500\u2500 CMakeLists.txt\n\u251c\u2500\u2500 main.cpp\n\u251c\u2500\u2500 main.py (optional)\n\u2514\u2500\u2500 build/\n</code></pre>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#using-the-cli","title":"Using the CLI","text":"<p>Once you have the <code>proj_cli</code> script in your project, you can use it to access various Holohub CLI commands:</p> <pre><code># Run your application\n./proj_cli run &lt;application&gt;\n\n# List available commands\n./proj_cli --help\n</code></pre> <p>Note: The first time you run the script, it will automatically download the Holohub CLI scripts locally from the Holohub repository. Therefore, an internet connection is required for the initial run.</p> <p>Please refer to the Holohub CLI help command for more information.</p>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#cli-features","title":"CLI Features","text":"<p>The Holohub CLI provides several useful features for external projects:</p> <ul> <li>Build Management: Automated building of C++ and Python applications</li> <li>Docker Integration: Run applications in Docker containers with proper environment setup</li> <li>Testing: Run tests and validation for your operators</li> <li>Development Tools: Various utilities for Holohub development</li> </ul>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#environment-setup","title":"Environment Setup","text":"<p>The CLI script automatically handles: - Setting up the correct Python path - Managing Docker options (if using Docker) - Configuring environment variables - Fetching necessary utilities from the Holohub repository</p>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#benefits-for-external-projects","title":"Benefits for External Projects","text":"<p>Using the Holohub CLI in your external project provides:</p> <ol> <li>Consistency: Same build and run processes as Holohub applications</li> <li>Automation: Automated setup and configuration</li> <li>Docker Support: Easy containerization of your applications</li> <li>Testing: Built-in testing capabilities</li> <li>Documentation: Access to Holohub's documentation and examples</li> </ol>","tags":["Development","Interoperability"]},{"location":"tutorials/holohub_operators_external_applications/#additional-resources","title":"Additional Resources","text":"<ul> <li>Holohub Repository</li> <li>Holoscan Documentation</li> <li>Holohub Operators Documentation</li> </ul>","tags":["Development","Interoperability"]},{"location":"tutorials/holoscan-bootcamp/","title":"NVIDIA Holoscan Bootcamp","text":"<p> Authors: Denis Leshchev (NVIDIA), Adam Thompson (NVIDIA), Nicolas Lebovitz (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: October 9, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 2.2.0 Tested Holoscan SDK versions: 2.3.0 Contribution metric: Level 1 - Highly Reliable</p>","tags":["Development"]},{"location":"tutorials/holoscan-bootcamp/#overview","title":"Overview","text":"<p>This is a meta-tutorial for deploying and running NVIDIA Holoscan Bootcamp lab materials.</p>","tags":["Development"]},{"location":"tutorials/holoscan-bootcamp/#description","title":"Description","text":"<p>NVIDIA Holoscan Bootcamp was conducted by the NVIDIA team in collaboration with OpenHackathons on September 24th, 2024 (event link). The bootcamp included a lecture and a hands-on Python-based lab. The lab materials are freely available for downloading at OpenHackathons repo.</p> <p>NVIDIA Holoscan Bootcamp lab materials include a Jupyter notebook for self-paced studying of the core Holoscan concepts.</p> <p>The notebook covers the following topics: - Creating custom Holoscan operators - Connecting operators to form an application - Build Holoscan applications with GPU-accelerated Python packages - How to build a sensor to visualization pipeline - Measuring and optimizing application performance - How to build complex sensor processing workflows using multiple fragments and the advanced schedulers</p> <p>The notebook guides users through the topics with example code and diagrams, and includes exercises to help them practice the concepts they've learned.</p> <p>To run the lab materials, clone the repo and follow the deployment guide.</p>","tags":["Development"]},{"location":"tutorials/holoscan-playground-on-aws/","title":"Holoscan Playground on AWS","text":"<p> Authors: Jin Li (NVIDIA) Supported platforms: x86_64, aarch64 Last modified: October 9, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p>","tags":["Deployment","Cloud","AWS"]},{"location":"tutorials/holoscan-playground-on-aws/#overview","title":"Overview","text":"<p>The Holoscan on AWS EC2 experience is an easy way for having a first try at the Holoscan SDK. The Holoscan SDK documentation lists out the hardware prerequisites. If you have a compatible hardware at hand, please get started with the SDK on your hardware. Otherwise, you could utilize an AWS EC2 instance to have a first look at the Holoscan SDK by following this guide.</p> <p>We estimate the time needed to follow this guide is around 1 hour, after which you could feel free to explore more of the SDK examples and applications. Please note that for the g5.xlarge instance type utilized, the cost is $1.006/hour.</p> <ol> <li> <p>The AWS experience is intended as a trial environment of the Holoscan SDK, not as a full time development environment. Some limitations of running the SDK on an EC2 instance are:</p> </li> <li> <p>An EC2 instance does not have the capability of live input sources, including video capture cards like AJA and Deltacast, or the onboard HDMI input port on devkits. An EC2 instance does not have ConnectX networking capabilities available on devkits.</p> </li> <li> <p>Display forwarding from EC2 to your local machine depends on internet connectivity and results in heavy latency, so if you would like to develop applications with display, it is not ideal.</p> </li> </ol>","tags":["Deployment","Cloud","AWS"]},{"location":"tutorials/holoscan-playground-on-aws/#launch-ec2-instance","title":"Launch EC2 Instance","text":"<p>Type in the name that you want to give to the instance.</p> <p></p> <p>In the <code>Application and OS Images (Amazon Machine Image)</code> window, search for <code>NVIDIA</code>.</p> <p></p> <p>From the results, switch to <code>AWS Marketplace AMIs</code> and choose <code>NVIDIA GPU-Optimized AMI</code>.</p> <p></p> <p>Select <code>Continue</code> after viewing the details of this AMI.</p> <p></p> <p>The selected AMI should look like this in the view to create an instance:</p> <p></p> <p>For <code>Instance type</code>, select <code>g5.xlarge</code>. Note: If you see an error similar to <code>The selected instance type is not supported in the zone (us-west-2d). Please select a different instance type or subnet.</code>, go down to <code>Network settings</code>, click on <code>Edit</code>, and try changing the <code>Subnet</code> selection. Note: If <code>g5.xlarge</code> is not available in any region/subnet accessible to you, <code>p3.2xlarge</code> should also work.</p> <p></p> <p>For <code>Key pair</code>, create a new key pair, enter the key pair name as you like, and store the file as prompted. After clicking on <code>Create key pair</code>, the file <code>your-name.pem</code> will be automatically downloaded by the browser to the Downloads folder. Then select the key pair in the view to create an instance.</p> <p></p> <p>Configure the <code>Network settings</code>. Click on <code>Edit</code> to start. * If you got an error in the <code>Instance type</code> selection about <code>g5.xlarge</code> being unavailable, try changing your <code>Subnet</code> selection in <code>Network settings</code>. Otherwise, there is no need to change the <code>Subnet</code> selection. * Make sure your <code>Auto-assign public IP</code> is enabled, otherwise you would have issues ssh\u2019ing into the instance. * Select a security group with a public IP address where you plan to ssh from, if one doesn\u2019t exist yet, select <code>create security group</code>.     * If you\u2019re already on the local machine you plan to ssh from, select <code>My IP</code> under <code>Source Type</code>.     * To add other machines that you plan to ssh from, select <code>Custom</code> under <code>Source Type</code> and enter your public IP address under <code>Source</code>. You can find the public IP address of the machine by going to https://www.whatsmyip.org/ from the machine.</p> <p></p> <p>Keep the default 128 GB specification in <code>Configure storage</code>.</p> <p></p> <p>Your Summary on the right side should look like this:</p> <p></p> <p>Please note that with a different instance type, Storage (volumes) may look different too.</p> <p>Click <code>Launch instance</code>, and you should see a <code>Success</code> notification.</p> <p></p> <p>Now go back to the <code>Instances</code> window to view the <code>Status check</code> of the instance we had just launched. It should show <code>Initializing</code> for a few minutes:</p> <p></p> <p>And later it should show <code>2/2 checks passed</code>:</p> <p></p> <p>Now we\u2019re ready to ssh into the instance.</p>","tags":["Deployment","Cloud","AWS"]},{"location":"tutorials/holoscan-playground-on-aws/#ssh-into-ec2-instance","title":"SSH into EC2 Instance","text":"<p>Click on the instance ID, and you should see this layout for instance details. Click on the <code>Connect</code> button on the top right.</p> <p></p> <p>Under the <code>SSH client</code> tab there are the SSH instructions. Note that the username <code>root</code> is guessed, and for the AMI we chose, it should be <code>ubuntu</code>. The private key file that you saved from when you were configuring the instance should be on the machine that you are ssh\u2019ing from.</p> <p>Add <code>-X</code> to the ssh command to enable display forwarding.</p> <p></p>","tags":["Deployment","Cloud","AWS"]},{"location":"tutorials/holoscan-playground-on-aws/#setting-up-display-forwarding-from-ec2-instance","title":"Setting up Display Forwarding from EC2 Instance","text":"<p>Holoscan SDK has examples and applications that depend on seeing a display. For this experience, we will do X11 forwarding.</p>","tags":["Deployment","Cloud","AWS"]},{"location":"tutorials/holoscan-playground-on-aws/#on-ec2-instance","title":"On EC2 Instance","text":"<p>First,install the package needed for a simple forwarding test, xeyes.</p> <pre><code>sudo apt install -y x11-apps\n</code></pre> <p>Next, run \u201cxeyes\u201d in the terminal, and you should get a display window popping up on the machine you\u2019re ssh\u2019ing from: <pre><code>xeyes\n</code></pre></p> <p>https://github.com/jin-nvidia/holohub/assets/60405124/57c76bed-ca16-458b-8740-1e4351ca63f7</p> <p>If you run into display issues, ensure the machine you\u2019re ssh\u2019ing from has X11 forwarding enabled. Please see the Troubleshooting section.</p>","tags":["Deployment","Cloud","AWS"]},{"location":"tutorials/holoscan-playground-on-aws/#in-a-docker-container-on-ec2-instance","title":"In a Docker Container On EC2 Instance","text":"<p>Now you have enabled display forwarding on the EC2 instance bare metal, let\u2019s take it one step further to enable display forwarding from a Docker container on the EC2 instance.</p> <pre><code>XSOCK=/tmp/.X11-unix\nXAUTH=/tmp/.docker.xauth\n# the error \u201cfile does not exist\u201d is expected at the next command\nxauth nlist $DISPLAY | sed -e 's/^..../ffff/' | sudo xauth -f $XAUTH nmerge -\nsudo chmod 777 $XAUTH\ndocker run -ti -e DISPLAY=$DISPLAY -v $XSOCK:$XSOCK -v $XAUTH:$XAUTH -e XAUTHORITY=$XAUTH --net host ubuntu:latest\n</code></pre> <p>Within the container:</p> <p><pre><code>apt update &amp;&amp; apt install -y x11-apps\nxeyes\n</code></pre> Press  <code>ctrl + D</code> to exit the Docker container. Now we have enabled display forwarding from both EC2 bare metal and containerized environments!</p>","tags":["Deployment","Cloud","AWS"]},{"location":"tutorials/holoscan-playground-on-aws/#run-holoscan","title":"Run Holoscan","text":"","tags":["Deployment","Cloud","AWS"]},{"location":"tutorials/holoscan-playground-on-aws/#install-holoscan","title":"Install Holoscan","text":"<p>There are several ways to install the Holoscan SDK. For the quickest way to get started, we will choose the Holoscan Docker container that already has all dependencies set up. We run nvidia-smi in the EC2 instance to check that there are drivers installed:</p> <p></p> <p>Follow the overview of https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/containers/holoscan. Some modifications are made to the original commands due to the EC2 environment, <code>--gpus all</code> and <code>-v $XAUTH:$XAUTH -e XAUTHORITY=$XAUTH</code>. <pre><code># install xhost util\nsudo apt install -y x11-xserver-utils\n\nxhost +local:docker\n\nnvidia_icd_json=$(find /usr/share /etc -path '*/vulkan/icd.d/nvidia_icd.json' -type f,l -print -quit 2&gt;/dev/null | grep .) || (echo \"nvidia_icd.json not found\" &gt;&amp;2 &amp;&amp; false)\n\nexport NGC_CONTAINER_IMAGE_PATH=\"nvcr.io/nvidia/clara-holoscan/holoscan:v0.6.0-dgpu\"\n\ndocker run -it --rm --net host \\\n  --gpus all \\\n   -v $XAUTH:$XAUTH -e XAUTHORITY=$XAUTH \\\n  -v /tmp/.X11-unix:/tmp/.X11-unix \\\n  -v $nvidia_icd_json:$nvidia_icd_json:ro \\\n  -e NVIDIA_DRIVER_CAPABILITIES=graphics,video,compute,utility,display \\\n  -e DISPLAY=$DISPLAY \\\n  --ipc=host \\\n  --cap-add=CAP_SYS_PTRACE \\\n  --ulimit memlock=-1 \\\n  ${NGC_CONTAINER_IMAGE_PATH}\n</code></pre></p>","tags":["Deployment","Cloud","AWS"]},{"location":"tutorials/holoscan-playground-on-aws/#sanity-check-with-holoscan-hello-world","title":"Sanity Check with Holoscan Hello World","text":"<pre><code>/opt/nvidia/holoscan/examples/hello_world/cpp/hello_world\n</code></pre>","tags":["Deployment","Cloud","AWS"]},{"location":"tutorials/holoscan-playground-on-aws/#examples","title":"Examples","text":"<p>Refer to each one of the Holoscan SDK examples on GitHub. You will find these examples installed under <code>/opt/nvidia/holoscan/examples/</code>.</p>","tags":["Deployment","Cloud","AWS"]},{"location":"tutorials/holoscan-playground-on-aws/#video-replayer-example","title":"Video Replayer Example","text":"<p>Let\u2019s take a look at the video replayer example which is a basic video player app. Since we are in the Docker container, there\u2019s no need to manually download data as it already exists in the container.</p> <p>Run the video_replayer example</p> <p><pre><code>cd /opt/nvidia/holoscan\n./examples/video_replayer/cpp/video_replayer\n</code></pre> You should see a window like below</p> <p>https://github.com/jin-nvidia/holohub/assets/60405124/7ae99409-ca42-4c38-b495-84a59648b671</p> <p>Please note that it is normal for the video stream to be lagging behind since it is forwarded from a docker container on a EC2 instance to your local machine. How much the forwarded video will lag heavily depends on the internet connection. When running Holoscan applications on the edge, we should have significantly less latency lag.</p> <p>You can close the sample application by pressing ctrl +C.</p> <p>Now that we have run a simple video replayer, let\u2019s explore the examples a little more.</p>","tags":["Deployment","Cloud","AWS"]},{"location":"tutorials/holoscan-playground-on-aws/#tensor-interoperability-example","title":"Tensor Interoperability Example","text":"","tags":["Deployment","Cloud","AWS"]},{"location":"tutorials/holoscan-playground-on-aws/#the-c-tensor-interop-example","title":"The C++ Tensor Interop example","text":"<p>Since we used the Debian package install, run the C++ tensor interopability example by</p> <pre><code>/opt/nvidia/holoscan/examples/tensor_interop/cpp/tensor_interop\n</code></pre> <p>Please refer to the README and the source file to see how we can have interoperability between a native operator (<code>ProcessTensorOp</code>) and two wrapped GXF Codelets (<code>SendTensor</code> and <code>ReceiveTensor</code>). For the Holoscan documentation on tensor interop in the C++ API, please see Interoperability between GXF and native C++ operators.</p>","tags":["Deployment","Cloud","AWS"]},{"location":"tutorials/holoscan-playground-on-aws/#the-python-tensor-interop-example","title":"The Python Tensor Interop example","text":"<p>The Python Tensor Interop example demonstrates interoperability between a native Python operator (<code>ImageProcessingOp</code>) and two operators that wrap existing C++ based operators,  (<code>VideoStreamReplayerOp</code> and <code>HolovizOp</code>) through the Holoscan Tensor object.</p> <p>Run the Python example by</p> <p><pre><code>python3 /opt/nvidia/holoscan/examples/tensor_interop/python/tensor_interop.py\n</code></pre> This example applies a Gaussian filtering to each frame of an endoscopy video stream and displays the filtered (blurred) video stream. You should see a window like below</p> <p>https://github.com/jin-nvidia/holohub/assets/60405124/b043637b-5fd9-4ee1-abc5-0dae069e785f</p> <p>The native Python operator is defined at tensor_interop.py#L37. We can see in the initialization <code>__init__()</code> of the operator, <code>self.count</code> was initialize to 1. In the <code>setup()</code> method, the input message, output message and the parameter <code>sigma</code> is defined. The <code>compute()</code> method is what gets called every time. In the <code>compute()</code> method, first we receive the upstream tensor by</p> <pre><code>in_message = op_input.receive(\"input_tensor\")\n</code></pre> <p>Please note that <code>input_tensor</code> is the name defined in <code>setup()</code>.</p> <p><code>cp_array</code> is the CuPy array that holds the output value after the Gaussian filter, and we can see that the way the CuPy array gets transmitted downstream is <pre><code>out_message = dict()\n\u2026\n# add each CuPy array to the out_message dictionary\nout_message[key] = cp_array\n\u2026\nop_output.emit(out_message, \"output_tensor\")\n</code></pre></p> <p>Please note that <code>output_tensor</code> is the name defined in <code>setup()</code>.</p> <p>Since there is only one input and one output port, when connecting the native Python operator <code>ImageProcessingOp</code> to its upstream and downstream operators, we do not need to specify the in/out name for <code>ImageProcessingOp</code>: <pre><code>self.add_flow(source, image_processing)\nself.add_flow(image_processing, visualizer, {(\"\", \"receivers\")})\n</code></pre></p> <p>Otherwise, with each <code>add_flow()</code>, the input and output port names need to be specified when multiple ports are present.</p> <p>For more information on tensor interop in Python API, please see the Holoscan documentation Interoperability between wrapped and native Python operators.</p> <p>Now that we have seen an example of tensor interop for single tensors per port, let\u2019s look at the next example where there are multiple tensors in the native operator\u2019s output port.</p>","tags":["Deployment","Cloud","AWS"]},{"location":"tutorials/holoscan-playground-on-aws/#holoviz-example","title":"Holoviz Example","text":"<p>Let\u2019s take a look at the Holoviz example. Run the example <pre><code>python3 /opt/nvidia/holoscan/examples/holoviz/python/holoviz_geometry.py\n</code></pre></p> <p>You should get something like below on the display</p> <p>https://github.com/jin-nvidia/holohub/assets/60405124/6d79845a-66bd-4448-9646-284b90c5e5f3</p> <p>Please take your time to look through holoviz_geometry.py for how each one of the shapes and text in the native Holoscan Python operator is defined.</p> <p>Let\u2019s also dive into how we can add to <code>out_message</code> and pass to Holoviz various tensors at the same time, including the frame itself, <code>box_coords</code>,  <code>triangle_coords</code>, <code>cross_coords</code>, <code>oval_coords</code>, the time varying <code>point_coords</code>, and <code>label_coords</code>.</p> <pre><code># define the output message dictionary where box_coords is a numpy array and \u201cboxes\u201d is the tensor name\nout_message = {\n            \"boxes\": box_coords,\n            \"triangles\": triangle_coords,\n            \"crosses\": cross_coords,\n            \"ovals\": oval_coords,\n            \"points\": point_coords,\n            \"label_coords\": label_coords,\n            \"dynamic_text\": dynamic_text,\n}\n\n# emit the output message\nop_output.emit(out_message, \"outputs\")\n</code></pre> <p>We can also see that each tensor name is referenced by the <code>tensors</code> parameter in the instantiation of a Holoviz operator at line 249.</p> <p>This is a great example and reference not only for passing different shapes to Holoviz, but also creating and passing multiple tensors within one message from a native Holoscan Python operator to the downstream operators.</p> <p>For more information on the Holoviz module, please see the Holoscan documentation.</p> <p>Exit from the Docker container by ctrl+D.</p>","tags":["Deployment","Cloud","AWS"]},{"location":"tutorials/holoscan-playground-on-aws/#applications","title":"Applications","text":"<p>To run the reference applications on Holoscan, let\u2019s go to HoloHub - a central repository for users and developers to share reusable operators and sample applications.</p> <p>On the EC2 instance, clone the HoloHub repo: <pre><code>cd ~\ngit clone https://github.com/nvidia-holoscan/holohub.git\ncd holohub\n</code></pre> To set up and build HoloHub, we will go with the option <code>Building dev container</code>: Run the following command from the holohub directory to build the development container: <pre><code>./holohub build-container\n</code></pre></p> <p>Check the tag for the container we had just build:</p> <p><pre><code>docker images\n</code></pre> There should be an image with repository:tag similar to <code>holohub:ngc-vx.y.z-dgpu</code> where <code>x.y.z</code> is the latest SDK version. We will set this as <code>HOLOHUB_IMAGE</code>: <pre><code># make sure to replace 0.6.0 with the actual SDK version\nexport HOLOHUB_IMAGE=holohub:ngc-v0.6.0-dgpu\n</code></pre> Next, launch the dev container for HoloHub. On a regular machine we can do so by <code>./holohub run-container</code>, however we need to make a few adjustments to the command again since we\u2019re running on an EC2 instance: <pre><code>docker run -it --rm --net host  -v /etc/group:/etc/group:ro -v /etc/passwd:/etc/passwd:ro -v $PWD:/workspace/holohub -w /workspace/holohub --gpus all -e NVIDIA_DRIVER_CAPABILITIES=graphics,video,compute,utility,display -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY --group-add video -v /etc/vulkan/icd.d/nvidia_icd.json:/etc/vulkan/icd.d/nvidia_icd.json:ro  -v $XAUTH:$XAUTH -e XAUTHORITY=$XAUTH $HOLOHUB_IMAGE\n</code></pre> Please refer to HoloHub for instructions on building each application.</p>","tags":["Deployment","Cloud","AWS"]},{"location":"tutorials/holoscan-playground-on-aws/#endoscopy-tool-tracking-application","title":"Endoscopy Tool Tracking Application","text":"<p>Build the sample app and run: <pre><code>./holohub build endoscopy_tool_tracking\n./holohub run endoscopy_tool_tracking --language cpp\n</code></pre> You should see a window like:</p> <p>https://github.com/jin-nvidia/holohub/assets/60405124/8eb93c50-d893-4b2c-897b-57de94b91371</p> <p>Note: Be prepared to wait a few minutes as we\u2019re running the app for the first time, and it will convert the ONNX model to a TensorRT engine. The conversion happens only for the first time, after that, each time we run the app the TensorRT engine is already present.</p> <p>Please visit HoloHub to see the application graph, different input types (although on the EC2 instance we can not use a live source such as the AJA capture card), and the construction of the same application in C++ vs in Python.</p>","tags":["Deployment","Cloud","AWS"]},{"location":"tutorials/holoscan-playground-on-aws/#multi-ai-ultrasound-application","title":"Multi AI Ultrasound Application","text":"<p>In the last application we saw how to run AI inference on the video source. Next, let\u2019s see how we can run inference with multiple AI models at the same time within a Holoscan application, enabled by Holoscan Inference Module. Build the application in applications/multiai_ultrasound <pre><code>./holohub build multiai_ultrasound\n</code></pre></p> <p>Launch the Python application: <pre><code>./holohub run multiai_ultrasound --language python\n</code></pre> You should see a window like below:</p> <p>https://github.com/jin-nvidia/holohub/assets/60405124/9d347b44-d635-4cc6-b013-7d26e3e4e2be</p> <p>You can find more information on Holoscan Inference Module here, including the parameters you can specify to define inference configuration, how to specify the multiple (or single) model(s) you want to run, and how the Holoscan Inference Module functions as an operator within the Holoscan SDK framework.</p> <p>Please see the application graph and more on HoloHub for how the multi AI inference connects to the rest of the operators, the definition of the same application in Python vs in C++, and how the multiai_ultrasound.yaml config file defines parameters for each operator especially the Holoscan Inference Module.</p>","tags":["Deployment","Cloud","AWS"]},{"location":"tutorials/holoscan-playground-on-aws/#stop-ec2-instance","title":"Stop EC2 Instance","text":"<p>Now that you have run several Holoscan Examples and HoloHub Applications, please continue exploring the rest of Examples and Applications, and when you\u2019re ready, stop the instance by going back to EC2 page with the list of <code>Instances</code>, select the launched instance, and select <code>Stop Instance</code> in the dropdown <code>Instance state</code>.</p> <p></p>","tags":["Deployment","Cloud","AWS"]},{"location":"tutorials/holoscan-playground-on-aws/#troubleshooting","title":"Troubleshooting","text":"<p>If you receive a display forwarding error such as <pre><code>unable to open display \"localhost:10.0\"\n</code></pre> <pre><code>Glfw Error 65544: X11: Failed to open display localhost:10.0\nterminate called after throwing an instance of 'std::runtime_error'\n  what():  Failed to initialize glfw\n</code></pre> Please see below to find the suggestion for your OS.</p>","tags":["Deployment","Cloud","AWS"]},{"location":"tutorials/holoscan-playground-on-aws/#from-a-linux-local-machine","title":"From a Linux Local Machine","text":"<ul> <li>Ensure that <code>-X</code> is added to the ssh command when connecting to the EC2 instance.</li> </ul>","tags":["Deployment","Cloud","AWS"]},{"location":"tutorials/holoscan-playground-on-aws/#from-a-windows-local-machine","title":"From a Windows Local Machine","text":"<ul> <li>Ensure that <code>-X</code> is added to the ssh command when connecting to the EC2 instance.</li> <li>Try using MobaXTerm to establish a SSH connection with X11 forwarding enabled.</li> </ul>","tags":["Deployment","Cloud","AWS"]},{"location":"tutorials/holoscan-playground-on-aws/#from-a-mac-local-machine","title":"From a Mac Local Machine","text":"<ul> <li>Download Quartz, reboot, and enable the following.</li> </ul> <p>Once Quartz is downloaded it will automatically launch when running display forwarding apps like <code>xeyes</code>.</p> <ul> <li>Ensure that <code>-X</code> is added to the ssh command when connecting to the EC2 instance.</li> </ul>","tags":["Deployment","Cloud","AWS"]},{"location":"tutorials/holoscan_response_time_analysis/","title":"Holoscan SDK Response-Time Analysis","text":"<p> Authors: Philip Schowitz (The University of British Columbia), Arpan Gujarati (The University of British Columbia), Soham Sinha (NVIDIA) Supported platforms: x86_64, aarch64 Last modified: October 9, 2025 Latest version: 1.0.0 Minimum Holoscan SDK version: 2.2.0 Tested Holoscan SDK versions: 2.2.0 Contribution metric: Level 2 - Trusted</p> <p>We have performed a theoretical response-time analysis of applications created using Holoscan SDK in an RTSS paper [1]. This work accounts for different queuing delays due to different types of connections and dependencies between the operators of a Holoscan application. This directory contains helpful scripts for the timing analysis of Holoscan applications, based on the paper.</p> <p>Detailed instructions for how to reproduce the results of the paper, along with the code, can be found in the <code>artifact</code> directory.</p>","tags":["Optimization","Performance"]},{"location":"tutorials/holoscan_response_time_analysis/#scripts","title":"Scripts","text":"<p>The scripts in current directory (<code>computeWCRT.py</code> and <code>runsimulation.py</code>) are written in Python and require the <code>networkx</code> and <code>pydot</code> packages, which can be easily installed with pip.</p> <pre><code>pip install networkx\npip install pydot\n</code></pre> <p>Each script takes a representation of an application graph in the form of a DOT file. An example DOT file is provided in <code>exampledot.dot</code>. The DOT file defines a Holoscan application graph in two parts. The first section consists of the names of the operators, with their worst-case execution times as attributes. The second section captures the connections between operators.</p> <p>We assume that there is only one root operator and one leaf operator in the application graph, without losing any generality. More information on why this works for more than one root and leaf can be found in the paper [1].</p>","tags":["Optimization","Performance"]},{"location":"tutorials/holoscan_response_time_analysis/#computewcrtpy","title":"<code>computeWCRT.py</code>","text":"<p>This script takes a path to a DOT file, representing a Holoscan application graph and computes an upper bound of worst-case response time (WCRT) for the application. The script does not run a Holoscan application on actual hardware. Instead, it computes the WCRT following the timing analysis done in our paper [1]. The algorithm runs reasonably fast (within quadratic time) for graphs with up to 20-30 nodes.</p> <pre><code>python computeWCRT.py examplegraph.dot\nWorst-case response time: 1600\n</code></pre> <p>This script can also, optionally, account for extra scheduling overheads, empirically observed on Jetson AGX Orin. The <code>-overhead</code> argument can include this overhead in the WCRT analysis. Kindly note that this is a heuristic based on measurements using a Jetson AGX Orin and will not be accurate for all systems.</p> <pre><code>python computeWCRT.py examplegraph.dot --overhead\nWorst-case response time: 1612\n</code></pre>","tags":["Optimization","Performance"]},{"location":"tutorials/holoscan_response_time_analysis/#runsimulationpy","title":"<code>runsimulation.py</code>","text":"<p>This script takes a path to a DOT file representing a Holoscan application graph, an expected runtime, and a root operator period (in this order), and runs a discrete-event simulation of the execution of the Holoscan application under the given conditions, printing the results. The runtime argument determines how long the simulation will run. For example, if an application takes 1000 time units to process an input, and the runtime is 1100 time units, then only one iteration will be simulated. The period argument determines how often the source operator can execute. If the source operator could execute every 50 time units, but period is 100 time units, then the source will be constrained in this script. </p> <p>The period will affect response times, though not necessarily the worst-case response time. For example, lowering the period may increase the queuing time of early iterations with response times still converging to the same value. In the  output below, changing the period to <code>5</code> would increase the response time of iteration 1 by 5, but leave the WCRT unchanged.</p> <pre><code>python runsimulation.py examplegraph.dot 3000 10\nIteration 0\nSource start:  0\nSink finish:   900\nResponse time: 900\n\nIteration 1\nSource start:  10\nSink finish:   1300\nResponse time: 1290\n\nIteration 2\nSource start:  200\nSink finish:   1700\nResponse time: 1500\n\nIteration 3\nSource start:  500\nSink finish:   2100\nResponse time: 1600\n\nIteration 4\nSource start:  900\nSink finish:   2500\nResponse time: 1600\n\nIteration 5\nSource start:  1300\nSink finish:   2900\nResponse time: 1600\n\nWorst-case response time: 1600\n</code></pre> <p>For this application, response times converge to the worst-case response time of 1600. Note that not all applications will converge to single value in this manner, and response times may increase or decrease periodically even after reaching the worst-case response time. The previously mentioned <code>computeWCRT.py</code> script provides a theoretical upper bound, even though it can be more pessimistic than simulated or real-world observed times. More details are available in our paper [1].</p>","tags":["Optimization","Performance"]},{"location":"tutorials/holoscan_response_time_analysis/#citation","title":"Citation","text":"<p>[1] P. Schowitz, S. Sinha, and A. Gujarati, \u201cResponse-Time Analysis  of a Soft Real-time NVIDIA Holoscan Application,\u201d in IEEE Real-Time  Systems Symposium, 2024.</p> <p>BibTeX:</p> <pre><code>@inproceedings{Schowitz2024,\nauthor    = {P. Schowitz and S. Sinha and A. Gujarati},\ntitle     = {Response-Time Analysis of a Soft Real-time NVIDIA Holoscan Application},\nbooktitle = {Proceedings of the IEEE Real-Time Systems Symposium},\nyear      = {2024},\n}\n</code></pre>","tags":["Optimization","Performance"]},{"location":"tutorials/integrate_external_libs_into_pipeline/","title":"Best Practices to integrate external libraries into Holoscan pipelines","text":"<p> Authors: Meiran Peng (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: October 9, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p>The Holoscan SDK is part of NVIDIA Holoscan, the AI sensor processing platform that combines hardware systems for low-latency sensor and network connectivity, optimized libraries for data processing and AI, and core microservices to run streaming, imaging, and other applications, from embedded to edge to cloud. It can be used to build streaming AI pipelines for a variety of domains, including medical devices, high-performance computing at the edge, industrial inspection, and more.</p> <p>With the Holoscan SDK, one can develop an end-to-end GPU-accelerated pipeline with RDMA support. However, with increasing requirements for pre-processing and post-processing beyond inference-only pipelines, integration with other powerful, GPU-accelerated libraries is needed.</p> <p>One of the key features of the Holoscan SDK is its seamless interoperability with other libraries.</p> <p>This tutorial explains how to leverage this capability in your applications. For detailed examples of integrating various libraries with Holoscan applications, refer to the following sections: - Tensor Interoperability   - Integrate MatX library - DLPack support in C++   - Integrate RAPIDS cuCIM library   - Integrate CV-CUDA library   - Integrate OpenCV with CUDA Module   - Integrate PyTorch library - CUDA Interoperability   - Integrate CUDA Python library   - Integrate CuPy library</p>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#interoperability-features","title":"Interoperability Features","text":"","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#dlpack-support","title":"DLPack Support","text":"<p>The Holoscan SDK supports DLPack, enabling efficient data exchange between deep learning frameworks.</p>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#array-interface-support","title":"Array Interface Support","text":"<p>The SDK also supports the array interface, including: - <code>__array_interface__</code> - <code>__cuda_array_interface__</code></p> <p>This allows for seamless integration with various Python libraries such as: - CuPy - PyTorch - JAX - TensorFlow - Numba</p>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#technical-details","title":"Technical Details","text":"<p>The <code>Tensor</code> class is a wrapper around the <code>DLManagedTensorContext</code> struct, which holds the <code>DLManagedTensor</code> object (a DLPack structure).</p> <p>For more information on interoperability, refer to the following sections in the Holoscan SDK documentation: - Interoperability between GXF and native C++ operators - Interoperability between wrapped and native Python operators</p>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#cuda-array-interfacedlpack-support","title":"CUDA Array Interface/DLPack Support","text":"<p>The following Python libraries have adopted the CUDA Array Interface and/or DLPack standards, enabling seamless interoperability with Holoscan Tensors:</p> <ul> <li>CuPy</li> <li>CV-CUDA</li> <li>PyTorch</li> <li>Numba</li> <li>PyArrow</li> <li>mpi4py</li> <li>ArrayViews</li> <li>JAX</li> <li>PyCUDA</li> <li>DALI: the NVIDIA Data Loading Library\u00a0:</li> <li>TensorGPU objects\u00a0expose the CUDA Array Interface.</li> <li>The External Source operator\u00a0consumes objects exporting the CUDA Array Interface.</li> <li>The RAPIDS stack:</li> <li>cuCIM</li> <li>cuDF</li> <li>cuML</li> <li>cuSignal</li> <li>RMM</li> </ul> <p>For more details on using the CUDA Array Interface and DLPack with various libraries, see CuPy's Interoperability guide.</p>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#using-holoscan-tensors-in-python","title":"Using Holoscan Tensors in Python","text":"<p>The Holoscan SDK's Python API provides the <code>holoscan.as_tensor()</code> method to convert objects supporting the (CUDA) Array Interface or DLPack to a Holoscan Tensor. The <code>holoscan.Tensor</code> object itself also supports these interfaces, allowing for easy integration with compatible libraries.</p> <p>Example usage:</p> <pre><code>import cupy as cp\nimport numpy as np\nimport torch\nimport holoscan as hs\n\n# Create tensors using different libraries\ntorch_cpu_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\ntorch_gpu_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]], device=\"cuda\")\nnumpy_tensor = np.array([[1, 2, 3], [4, 5, 6]])\ncupy_tensor = cp.array([[1, 2, 3], [4, 5, 6]])\n\n# Convert to Holoscan Tensors\ntorch_cpu_to_holoscan = hs.as_tensor(torch_cpu_tensor)\ntorch_gpu_to_holoscan = hs.as_tensor(torch_gpu_tensor)\nnumpy_to_holoscan = hs.as_tensor(numpy_tensor)\ncupy_to_holoscan = hs.as_tensor(cupy_tensor)\n</code></pre>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#tensor-interoperability","title":"Tensor Interoperability","text":"","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#integrate-matx-library","title":"Integrate MatX library","text":"<p>MatX library (An efficient C++17 GPU numerical computing library with Python-like syntax) is an open-source, efficient C++17 GPU numerical computing library created by NVIDIA. It provides a NumPy-like interface for GPU-accelerated numerical computing, enabling developers to write high-performance, GPU-accelerated code with ease.</p>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#installation","title":"Installation","text":"<p>MatX is a header-only library. Using it in your own projects is as simple as including only the core <code>matx.h</code> file.</p> <p>Please refer to the MatX documentation for detailed instructions on building and using the MatX library.</p> <p>The following is a sample CMakeLists.txt file for a project that uses MatX:</p> <pre><code>cmake_minimum_required(VERSION 3.20)\nproject(my_app CXX)\n\n# Holoscan\nfind_package(holoscan 2.2 REQUIRED CONFIG\n             PATHS \"/opt/nvidia/holoscan\" \"/workspace/holoscan-sdk/install\")\n\n# Enable cuda language\nset(CMAKE_CUDA_ARCHITECTURES \"70;80\")\nenable_language(CUDA)\n\n# Download MatX (from 'main' branch)\ninclude(FetchContent)\nFetchContent_Declare(\n  MatX\n  GIT_REPOSITORY https://github.com/NVIDIA/MatX.git\n  GIT_TAG main\n)\nFetchContent_MakeAvailable(MatX)\n\nadd_executable(my_app\n  my_app.cpp\n)\n\ntarget_link_libraries(my_app\n  PRIVATE\n  holoscan::core\n  # ...\n  matx::matx\n)\n</code></pre>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#sample-code","title":"Sample code","text":"<p>The following are the sample applications that use the MatX library to integrate with Holoscan SDK.</p> <ul> <li>Multi AI Application with SSD Detection and MONAI Endoscopic Tool Segmentation</li> <li><code>applications/multiai_endoscopy</code></li> <li>Network Radar Pipeline</li> <li><code>applications/network_radar_pipeline/cpp</code></li> <li>Simple Radar Pipeline Application</li> <li><code>applications/simple_radar_pipeline/cpp</code></li> </ul> <p>On the GPU</p> <ul> <li>https://github.com/nvidia-holoscan/holohub/blob/main/applications/multiai_endoscopy/cpp/post-proc-matx-gpu/multi_ai.cu</li> </ul> <pre><code>#include &lt;holoscan/holoscan.hpp&gt;\n#include &lt;matx.h&gt;\n\n// ...\n\nvoid compute(InputContext&amp; op_input, OutputContext&amp; op_output,\n             ExecutionContext&amp; context) override {\n  // Get input message and make output message\n  auto in_message = op_input.receive&lt;gxf::Entity&gt;(\"in\").value();\n  // ...\n  auto boxes = in_message.get&lt;Tensor&gt;(\"inference_output_detection_boxes\");\n  auto scores = in_message.get&lt;Tensor&gt;(\"inference_output_detection_scores\");\n  int32_t Nb = scores-&gt;shape()[1];  // Number of boxes\n  auto Nl = matx::make_tensor&lt;int&gt;({});  // Number of label boxes\n  // ...\n  auto boxesl_mx = matx::make_tensor&lt;float&gt;({1, Nl(), 4});\n  (boxesl_mx = matx::remap&lt;1&gt;(boxes_ix_mx, ixl_mx)).run();\n  // ...\n  // Holoscan tensors to MatX tensors\n  auto boxes_mx = matx::make_tensor&lt;float&gt;((float*)boxes-&gt;data(), {1, Nb, 4});\n  // ...\n  // MatX to Holoscan tensor\n  auto boxes_hs = std::make_shared&lt;holoscan::Tensor&gt;(boxesls_mx.GetDLPackTensor());\n  // ...\n}\n</code></pre> <p>On the CPU</p> <ul> <li>https://github.com/nvidia-holoscan/holohub/blob/main/applications/multiai_endoscopy/cpp/post-proc-matx-cpu/multi_ai.cpp</li> </ul> <p>MatX library usage on the CPU is similar to the GPU version, but the <code>run()</code> function is called with <code>matx::SingleThreadHostExecutor()</code> to run the operation on the CPU.</p> <pre><code>#include &lt;holoscan/holoscan.hpp&gt;\n#include &lt;matx.h&gt;\n\n// ...\n\nvoid compute(InputContext&amp; op_input, OutputContext&amp; op_output,\n             ExecutionContext&amp; context) override {\n  // Get input message and make output message\n  auto in_message = op_input.receive&lt;gxf::Entity&gt;(\"in\").value();\n  // ...\n  auto boxesh = in_message.get&lt;Tensor&gt;(\"inference_output_detection_boxes\");  // (1, num_boxes, 4)\n  auto scoresh = in_message.get&lt;Tensor&gt;(\"inference_output_detection_scores\");  // (1, num_boxes)\n  int32_t Nb = scoresh-&gt;shape()[1];  // Number of boxes\n  auto Nl = matx::make_tensor&lt;int&gt;({});  // Number of label boxes\n  // ...\n  auto boxes = copy_device2vec&lt;float&gt;(boxesh);\n  // Holoscan tensors to MatX tensors\n  auto boxes_mx = matx::make_tensor&lt;float&gt;(boxes.data(), {1, Nb, 4});\n  // ...\n  auto boxesl_mx = matx::make_tensor&lt;float&gt;({1, Nl(), 4});\n  (boxesl_mx = matx::remap&lt;1&gt;(boxes_ix_mx, ixl_mx)).run(matx::SingleThreadHostExecutor());\n  // ...\n  // MatX to Holoscan tensor\n  auto boxes_hs = std::make_shared&lt;holoscan::Tensor&gt;(boxesls_mx.GetDLPackTensor());\n  // ...\n}\n</code></pre>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#integrate-rapids-cucim-library","title":"Integrate RAPIDS cuCIM library","text":"<p>RAPIDS cuCIM (Compute Unified Device Architecture Clara IMage) is an open-source, accelerated computer vision and image processing software library for multidimensional images used in biomedical, geospatial, material and life science, and remote sensing use cases.</p> <p>See the supported Operators in cuCIM documentation.</p> <p>cuCIM offers interoperability with CuPy. We can initialize CuPy arrays directly from Holoscan Tensors and use the arrays in cuCIM operators for processing without memory transfer between host and device.</p>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#installation_1","title":"Installation","text":"<p>Follow the cuCIM documentation to install the RAPIDS cuCIM library.</p>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#sample-code_1","title":"Sample code","text":"<pre><code>import cupy as cp\nimport cucim.skimage.exposure as cu_exposure\nfrom cucim.skimage.util import img_as_ubyte\nfrom cucim.skimage.util import img_as_float\n\ndef CustomizedcuCIMOperator(Operator):\n    ### Other implementation of __init__, setup()... etc.\n\n    def compute(self, op_input, op_output, context):\n        message = op_input.receive(\"input_tensor\")\n        input_tensor = message.get()\n        # Directly use Holoscan tensor to initialize CuPy array\n        cp_array = cp.asarray(input_tensor)\n\n        cp_array = img_as_float(cp_array)\n        cp_res=cu_exposure.equalize_adapthist(cp_array)\n        cp_array = img_as_ubyte(cp_res)\n\n        # Emit CuPy array memory as an item in a `holoscan.TensorMap`\n        op_output.emit(dict(out_tensor=cp_array), \"out\")\n</code></pre>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#integrate-cv-cuda-library","title":"Integrate CV-CUDA library","text":"<p>CV-CUDA is an open-source, graphics processing unit (GPU)-accelerated library for cloud-scale image processing and computer vision developed jointly by NVIDIA and the ByteDance Applied Machine Learning teams. CV-CUDA helps developers build highly efficient pre- and post-processing pipelines that can improve throughput by more than 10x while lowering cloud computing costs.</p> <p>See the supported CV-CUDA Operators in the CV-CUDA developer guide</p>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#installation_2","title":"Installation","text":"<p>Follow the CV-CUDA documentation to install the CV-CUDA library.</p> <p>Requirement: CV-CUDA &gt;= 0.2.1 (From which version DLPack interop is supported)</p>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#sample-code_2","title":"Sample code","text":"<p>CV-CUDA is implemented with DLPack standards. A CV-CUDA tensor can directly access a Holoscan Tensor.</p> <p>Refer to the Holoscan CV-CUDA sample application for an example of how to use CV-CUDA with Holoscan SDK.</p> <pre><code>import cvcuda\n\nclass CustomizedCVCUDAOp(Operator):\n    def __init__(self, *args, **kwargs):\n\n        # Need to call the base class constructor last\n        super().__init__(*args, **kwargs)\n\n    def setup(self, spec: OperatorSpec):\n        spec.input(\"input_tensor\")\n        spec.output(\"output_tensor\")\n\n    def compute(self, op_input, op_output, context):\n        message = op_input.receive(\"input_tensor\")\n        input_tensor = message.get()\n\n        cvcuda_input_tensor = cvcuda.as_tensor(input_tensor,\"HWC\")\n\n        cvcuda_resize_tensor = cvcuda.resize(\n                    cvcuda_input_tensor,\n                    (\n                        640,\n                        640,\n                        3,\n                    ),\n                    cvcuda.Interp.LINEAR,\n                )\n\n        buffer = cvcuda_resize_tensor.cuda()\n\n        # Emits an `holoscan.TensorMap` with a single entry `out_tensor`\n        op_output.emit(dict(out_tensor=buffer), \"output_tensor\")\n</code></pre>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#integrate-opencv-with-cuda-module","title":"Integrate OpenCV with CUDA Module","text":"<p>OpenCV (Open Source Computer Vision Library) is a comprehensive open-source library that contains over 2500 algorithms covering Image &amp; Video Manipulation, Object and Face Detection, OpenCV Deep Learning Module and much more.</p> <p>OpenCV also supports GPU acceleration and includes a CUDA module which is a set of classes and functions to utilize CUDA computational capabilities. It is implemented using NVIDIA CUDA Runtime API and provides utility functions, low-level vision primitives, and high-level algorithms.</p>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#installation_3","title":"Installation","text":"<p>Prerequisites: - OpenCV &gt;= 4.8.0 (From which version, OpenCV GpuMat supports initialization with GPU Memory pointer)</p> <p>Install OpenCV with its CUDA module following the guide in opencv/opencv_contrib</p> <p>We also recommend referring to the Holoscan Endoscopy Depth Estimation application container as an example of how to build an image with Holoscan SDK and OpenCV CUDA.</p>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#sample-code_3","title":"Sample code","text":"<p>The data type of OpenCV is GpuMat which implements neither the cuda_array_interface nor the standard DLPack. To achieve the end-to-end GPU accelerated pipeline/application, we need to implement 2 functions to convert the GpuMat to CuPy array which can be accessed directly with Holoscan Tensor and vice versa.</p> <p>Refer to the Holoscan Endoscopy Depth Estimation sample application for an example of how to use the OpenCV operator with Holoscan SDK.</p> <ol> <li>Conversion from GpuMat to CuPy Array</li> </ol> <p>The GpuMat object of OpenCV Python bindings provides a cudaPtr method that can be used to access the GPU memory address of a GpuMat object. This memory pointer can be utilized to initialize a CuPy array directly, allowing for efficient data handling by avoiding unnecessary data transfers between the host and device.</p> <p>Refer to the function below, which is used to create a CuPy array from a GpuMat. For more details, see the source code in holohub/applications/endoscopy_depth_estimation-gpumat_to_cupy.</p> <pre><code>import cv2\nimport cupy as cp\n\ndef gpumat_to_cupy(gpu_mat: cv2.cuda.GpuMat) -&gt; cp.ndarray:\n    w, h = gpu_mat.size()\n    size_in_bytes = gpu_mat.step * w\n    shapes = (h, w, gpu_mat.channels())\n    assert gpu_mat.channels() &lt;=3, \"Unsupported GpuMat channels\"\n\n    dtype = None\n    if gpu_mat.type() in [cv2.CV_8U,cv2.CV_8UC1,cv2.CV_8UC2,cv2.CV_8UC3]:\n        dtype = cp.uint8\n    elif gpu_mat.type() == cv2.CV_8S:\n        dtype = cp.int8\n    elif gpu_mat.type() == cv2.CV_16U:\n        dtype = cp.uint16\n    elif gpu_mat.type() == cv2.CV_16S:\n        dtype = cp.int16\n    elif gpu_mat.type() == cv2.CV_32S:\n        dtype = cp.int32\n    elif gpu_mat.type() == cv2.CV_32F:\n        dtype = cp.float32\n    elif gpu_mat.type() == cv2.CV_64F:\n        dtype = cp.float64\n\n    assert dtype is not None, \"Unsupported GpuMat type\"\n\n    mem = cp.cuda.UnownedMemory(gpu_mat.cudaPtr(), size_in_bytes, owner=gpu_mat)\n    memptr = cp.cuda.MemoryPointer(mem, offset=0)\n    cp_out = cp.ndarray(\n        shapes,\n        dtype=dtype,\n        memptr=memptr,\n        strides=(gpu_mat.step, gpu_mat.elemSize(), gpu_mat.elemSize1()),\n    )\n    return cp_out\n</code></pre> <p>Note: In this function, we used the UnownedMemory API to create the CuPy array. In some cases, the OpenCV operators will allocate new device memory which needs to be handled by CuPy and the lifetime is not limited to one operator but the whole pipeline. In this case, the CuPy array initiated from the GpuMat shall know the owner and keep the reference to the object. Check the CuPy documentation for more details on CuPy interoperability.</p> <ol> <li>Conversion from Holoscan Tensor to GpuMat via CuPy array</li> </ol> <p>With the release of OpenCV 4.8, the Python bindings for OpenCV now support the initialization of GpuMat objects directly from GPU memory pointers. This capability facilitates more efficient data handling and processing by allowing direct interaction with GPU-resident data, bypassing the need for data transfer between host and device memory.</p> <p>Within pipeline applications based on Holoscan SDK, the GPU Memory pointer can be obtained through the <code>__cuda_array_interface__</code> interface provided by CuPy arrays.</p> <p>Refer to the functions outlined below for creating GpuMat objects utilizing CuPy arrays. For a detailed implementation, see the source code provided in holohub/applications/endoscopy_depth_estimation-gpumat_from_cp_array.</p> <pre><code>import cv2\nimport cupy as cp\nimport holoscan as hs\nfrom holoscan.gxf import Entity\n\ndef gpumat_from_cp_array(arr: cp.ndarray) -&gt; cv2.cuda.GpuMat:\n    assert len(arr.shape) in (2, 3), \"CuPy array must have 2 or 3 dimensions to be a valid GpuMat\"\n    type_map = {\n        cp.dtype('uint8'): cv2.CV_8U,\n        cp.dtype('int8'): cv2.CV_8S,\n        cp.dtype('uint16'): cv2.CV_16U,\n        cp.dtype('int16'): cv2.CV_16S,\n        cp.dtype('int32'): cv2.CV_32S,\n        cp.dtype('float32'): cv2.CV_32F,\n        cp.dtype('float64'): cv2.CV_64F\n    }\n    depth = type_map.get(arr.dtype)\n    assert depth is not None, \"Unsupported CuPy array dtype\"\n    channels = 1 if len(arr.shape) == 2 else arr.shape[2]\n    mat_type = depth + ((channels - 1) &lt;&lt; 3)\n\n     mat = cv2.cuda.createGpuMatFromCudaMemory(\n      arr.__cuda_array_interface__['shape'][1::-1],\n      mat_type,\n      arr.__cuda_array_interface__['data'][0]\n  )\n    return mat\n</code></pre> <ol> <li>Integrate OpenCV Operators inside customized Operator</li> </ol> <p>The demonstration code is provided below. For the complete source code, please refer to the holohub/applications/endoscopy_depth_estimation-customized Operator.</p> <pre><code>   def compute(self, op_input, op_output, context):\n        stream = cv2.cuda_Stream()\n        message = op_input.receive(\"in\")\n\n        cp_frame = cp.asarray(message.get(\"\"))  # CuPy array\n        cv_frame = gpumat_from_cp_array(cp_frame)  # GPU OpenCV mat\n\n        ## Call OpenCV Operator\n        cv_frame = cv2.cuda.XXX(hsv_merge, cv2.COLOR_HSV2RGB)\n\n        cp_frame = gpumat_to_cupy(cv_frame)\n        cp_frame = cp.ascontiguousarray(cp_frame)\n\n        op_output.emit(dict(out_tensor=cp_frame), \"out\")\n</code></pre>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#integrate-pytorch-library","title":"Integrate PyTorch library","text":"<p>PyTorch is a popular open-source machine learning library developed by Facebook's AI Research lab. It provides a flexible and dynamic computational graph that allows for easy model building and training. PyTorch also supports GPU acceleration, making it ideal for deep learning applications that require high-performance computing.</p> <p>Since PyTorch tensors support the array interface and DLPack (link), they can be interoperable with other array/tensor objects including Holoscan Tensors.</p>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#installation_4","title":"Installation","text":"<p>Follow the PyTorch documentation to install the PyTorch library.</p> <p>e.g., for CUDA 12.x with pip:</p> <pre><code>python3 -m pip install torch torchvision torchaudio\n</code></pre>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#sample-code_4","title":"Sample code","text":"<p>The following is a sample application that demonstrates how to use PyTorch with Holoscan SDK:</p> <pre><code>import torch\n\ndef CustomizedTorchOperator(Operator):\n    ### Other implementation of __init__, setup()... etc.\n\n    def compute(self, op_input, op_output, context):\n        message = op_input.receive(\"input_tensor\")\n        input_tensor = message.get()\n        # Directly use Holoscan tensor to initialize PyTorch tensor\n        torch_tensor = torch.as_tensor(input_tensor, device=\"cuda\")\n\n        torch_tensor *= 2\n\n        # Emit PyTorch tensor memory as a `holoscan.Tensor` item in a `holoscan.TensorMap`\n        op_output.emit(dict(out_tensor=torch_tensor), \"out\")\n</code></pre>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#cuda-interoperability","title":"CUDA Interoperability","text":"","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#integrate-cuda-python-library","title":"Integrate CUDA Python library","text":"<p>CUDA Python is a Python library that provides Cython/Python wrappers for CUDA driver and runtime APIs. It offers a convenient way to leverage GPU acceleration for complex computations, making it ideal for high-performance applications that require intensive numerical processing.</p> <p>When using CUDA Python with the Holoscan SDK, you need to use the Primary context (CUDA doc link) by calling <code>cuda.cuDevicePrimaryCtxRetain()</code> (link.</p> <p>Since the Holoscan Operator is executed in an arbitrary non-main thread, you may need to set the CUDA context using the cuda.cuCtxSetCurrent() method in the <code>Operator.compute()</code> method.</p>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#installation_5","title":"Installation","text":"<p>Follow the instructions in the CUDA Python documentation to install the CUDA Python library.</p> <p>CUDA Python can be installed using <code>pip</code>:</p> <pre><code>python3 -m pip install cuda-python\n</code></pre>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#sample-code_5","title":"Sample code","text":"<p>Please see the example application (cuda_example.py) that demonstrates how to use CUDA Python with the Holoscan SDK.</p> <p>In this example, we define a <code>CudaOperator</code> class that encapsulates the CUDA context, stream, and memory management. The <code>CudaOperator</code> class provides methods for allocating device memory, building CUDA kernels, launching kernels, and cleaning up. We also define three operators: <code>CudaTxOp</code>, <code>ApplyGainOp</code>, and <code>CudaRxOp</code>, which perform data initialization, apply a gain operation, and process the output data, respectively. The output of the <code>CudaRxOp</code> operator is passed to both a <code>ProbeOp</code> operator, which inspects the data and prints the metadata information, and a <code>HolovizOp</code> operator, which visualizes the data using the Holoviz module.</p> <p>There are four examples in the <code>CudaRxOp.compute()</code> method that demonstrate different ways to handle data conversion and transfer between tensor libraries. These examples include creating 1) a NumPy array from CUDA memory, 2) converting a NumPy array to a CuPy array, 3) creating a CUDA array interface object, and 4) creating a CuPy array from CUDA memory.</p> <p><code>__cuda_array_interface__</code> is a dictionary that provides a standard interface for exchanging array data between different libraries in Python. It contains metadata such as the shape, data type, and memory location of the array. By using this interface, you can efficiently transfer tensor data between two libraries without copying the data.</p> <p>In the following example, we create a <code>CudaArray</code> class to represent a CUDA array interface object and populate it with the necessary metadata. This object can then be passed to the <code>op_output.emit()</code> method to transfer the data to downstream operators.</p> <p>In the <code>__cuda_array_interface__</code> dictionary, the <code>stream</code> field is the CUDA stream associated with the data. When passing a <code>cuda.cuda.CUstream</code> object (the variable named <code>stream</code>) to the <code>stream</code> field, you need to convert it to an integer using <code>int(stream)</code>:</p> <pre><code>class CudaRxOp(CudaOperator):\n    # ...\n    def compute(self, op_input, op_output, context):\n        # ...\n        class CudaArray:\n            \"\"\"Class to represent a CUDA array interface object.\"\"\"\n\n            pass\n\n        cuda_array = CudaArray()\n\n        # Reference: https://numba.readthedocs.io/en/stable/cuda/cuda_array_interface.html\n        cuda_array.__cuda_array_interface__ = {\n            \"shape\": (self._frame_height, self._frame_width, self._frame_channels),\n            \"typestr\": np.dtype(np.uint8).str,  # \"|u1\"\n            \"descr\": [(\"\", np.dtype(np.uint8).str)],\n            \"stream\": int(stream),\n            \"version\": 3,\n            \"strides\": None,\n            \"data\": (int(value), False),\n        }\n        # ...\n</code></pre> <p>Please don't confuse this with the <code>cuda.cuda.CUstream.getPtr()</code> method. If you use the <code>stream.getPtr()</code> method, it will return a pointer to the CUDA stream object, not the stream ID. To get the stream ID, you need to convert the <code>stream</code> object to an integer using <code>int(stream)</code>. Otherwise, you will get an error that is difficult to debug, like this:</p> <pre><code>[error] [tensor.cpp:479] Runtime call \"Failure during call to cudaEventRecord\" in line 479 of file ../python/holoscan/core/tensor.cpp failed with 'context is destroyed' (709)\n[error] [gxf_wrapper.cpp:84] Exception occurred for operator: 'rx' - RuntimeError: Error occurred in CUDA runtime API call\n</code></pre>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#integrate-cupy-library","title":"Integrate CuPy library","text":"<p>CuPy is an open-source array library for GPU-accelerated computing with a NumPy-compatible API. It provides a convenient way to perform high-performance numerical computations on NVIDIA GPUs, making it ideal for applications that require efficient data processing and manipulation.</p>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#installation_6","title":"Installation","text":"<p>CuPy can be installed using <code>pip</code>:</p> <pre><code>python3 -m pip install cupy-cuda12x  # for CUDA v12.x\n</code></pre> <p>For more detailed installation instructions, refer to the CuPy documentation.</p>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/integrate_external_libs_into_pipeline/#sample-code_6","title":"Sample code","text":"<p>Please see the example application (cupy_example.py) that demonstrates how to use CuPy with the Holoscan SDK.</p> <p>This example performs the same operations as the previous example but uses CuPy instead of CUDA Python. The <code>CudaOperator</code> class is modified to use CuPy arrays, and the <code>ApplyGainOp</code> operator is updated to use CuPy functions for array manipulation. The <code>CudaTxOp</code> and <code>CudaRxOp</code> operators are also updated to work with CuPy arrays.</p> <p>With CuPy, you can conveniently perform GPU-accelerated computations on multidimensional arrays, making it an excellent choice for high-performance data processing tasks in Holoscan applications.</p> <p>Please note that CuPy does not fully support certain CUDA APIs, such as <code>cupy.cuda.driver.occupancyMaxPotentialBlockSize()</code>. While the driver API may be available (link), it is currently undocumented (link) and lacks support for calling the API with RawKernel's pointer (link), or using CUDA Python's <code>cuda.cuOccupancyMaxPotentialBlockSize()</code> Driver API with CuPy-generated RawKernel functions.</p> <p>Currently, direct assignment of a non-default CUDA stream to HolovizOp in Holoscan applications is not supported without utilizing the <code>holoscan.resources.CudaStreamPool</code> resource. CuPy also has limited support for custom stream management, necessitating reliance on the default stream in this context.</p> <p>For more detailed information, please refer to the following resources: - New RawKernel Calling Convention / Kernel Occupancy Functions \u00b7 Issue #3684 \u00b7 cupy/cupy \u00b7 GitHub - CUDA Stream Support:   - Enhancing stream support in CuPy's default memory pool \u00b7 Issue #8068 \u00b7 cupy/cupy   - cupy.cuda.ExternalStream \u2014 CuPy 13.1.0 documentation</p>","tags":["Interoperability","CV CUDA","OpenCV"]},{"location":"tutorials/local-llama/","title":"Deploying Llama-2 70b model on the edge with IGX Orin","text":"<p> Authors: Nigel Nelson (NVIDIA) Supported platforms: aarch64 Language: Python Last modified: October 9, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable</p>","tags":["Deployment","NLP","CUDA","Huggingface","LLM"]},{"location":"tutorials/local-llama/#introduction","title":"\ud83e\udd99 Introduction","text":"<p>With the recent release of the Llama-2 family of models, there has been an excess of excitement in the LLM community due to these models being released freely for research and commercial use. Upon their release, the 70b version of the Llama-2 model quickly rose to the top place on HuggingFace's Open LLM Leaderboard. Additionally, thanks to the publishing of the model weights, fine-tuned versions of these models are consistently being released and raising the bar for the top performing open-LLM. This most recent release of Llama-2 provides some of the first legitimate open-source alternatives to the previously unparalleled performance of closed-source LLMs. This enables developers to deploy these Llama-2 models locally, and benefit from being able to use some of the most advanced LLMs ever created, while also keeping all of their data on their own host machines.</p> <p>The only edge device that is capable of running the Llama-2 70b locally is the NVIDIA IGX Orin. In order to get the Llama-2 70b model running inference, all you need is an IGX Orin, a mouse, a keyboard, and to follow the tutorial below.</p>","tags":["Deployment","NLP","CUDA","Huggingface","LLM"]},{"location":"tutorials/local-llama/#overview","title":"\ud83d\udcd6 Overview","text":"<p>This tutorial will walk you through how to run a quantized version of Meta's Llama-2 70b model as the backend LLM for a Gradio chatbot app, all running on an NVIDIA IGX Orin. Specifically, we will use Llama.cpp, a project that ports Llama models into C and C++ with CUDA acceleration, to load and run the quantized Llama-2 models. We will setup Llama.cpp's <code>api_like_OAI.py</code> Flask app that emulates the OpenAI API. This will then enable us to create a Gradio chatbot app that utilizes the popular OpenAI API Python library to interact with our local Llama-2 model. Thus, at the conclusion of this tutorial you will have a chatbot app that rivals the performance of closed-source models, while keeping all of your data local and running everything self-contained on an NVIDIA IGX Orin.</p>","tags":["Deployment","NLP","CUDA","Huggingface","LLM"]},{"location":"tutorials/local-llama/#hardware-requirements","title":"\ud83d\udcbb Hardware Requirements","text":"<ul> <li>NVIDIA IGX Orin with:</li> <li>RTX A6000 dGPU</li> <li>500 GB SSD</li> </ul>","tags":["Deployment","NLP","CUDA","Huggingface","LLM"]},{"location":"tutorials/local-llama/#dependencies","title":"\ud83d\udce6 Dependencies","text":"<ul> <li>NVIDIA Drivers</li> <li>CUDA Toolkit &gt;= 11.8</li> <li>Python &gt;= 3.8</li> <li><code>build-essential</code> apt package (gcc, g++, etc.)</li> <li>Cmake &gt;= 3.17</li> </ul>","tags":["Deployment","NLP","CUDA","Huggingface","LLM"]},{"location":"tutorials/local-llama/#cloning-and-building-llamacpp","title":"\ud83d\udd28 Cloning and building Llama.cpp","text":"<ol> <li> <p>Clone Llama.cpp:</p> <pre><code>git clone https://github.com/ggerganov/llama.cpp.git\n</code></pre> </li> <li> <p>Checkout a stable commit of llama.cpp:</p> <pre><code>cd llama.cpp\ngit checkout e519621010cac02c6fec0f8f3b16cda0591042c0 # Commit date: 9/27/23\n</code></pre> </li> <li> <p>Follow cuBLAS build instructions for Llama.cpp to provide BLAS acceleration using the CUDA cores of your NVIDIA GPU.</p> <p>Navigate to the <code>/Llama.cpp</code> directory: <pre><code>cd llama.cpp\n</code></pre> Using <code>make</code>: <pre><code>make LLAMA_CUBLAS=1\n</code></pre></p> </li> </ol> <p>By successfully executing these commands you will now be able to run Llama models on your local machine with BLAS acceleration!</p>","tags":["Deployment","NLP","CUDA","Huggingface","LLM"]},{"location":"tutorials/local-llama/#downloading-llama-2-70b","title":"\ud83d\udcbe Downloading Llama-2 70B","text":"<p>In order to use Llama-2 70b as it is provided by Meta, you\u2019d need 140 GB of VRAM (70b params x 2 bytes = 140 GB in FP16). However, by utilizing model quantization, we can reduce the computational and memory costs of running inference by representing the weights and activations as low-precision data types, like int8 and int4, instead of higher-precision data types like FP16 and FP32. To learn more about quantization, check out: The Ultimate Guide to Deep Learning Model Quantization.</p> <p>Llama.cpp uses quantized models that are stored in the GGUF format. Browse to TheBloke on Huggingface.co, who provides hundred of the latest quantized models. Feel free to choose a GGUF model that suits your needs. However, for this tutorial, we will use TheBloke's 4-bit medium GGUF quantization of Meta\u2019s LLama-2-70B-Chat model. 1. Download the GGUF model from Huggingface.co.</p> <p> This model requires ~43 GB of VRAM.</p> <pre><code>cd /media/m2 # Download the model to your SSD drive\nmkdir models # Create a directory for GGUF models\ncd models\nwget https://huggingface.co/TheBloke/Llama-2-70B-chat-GGUF/resolve/main/llama-2-70b-chat.Q4_K_M.gguf\n</code></pre>","tags":["Deployment","NLP","CUDA","Huggingface","LLM"]},{"location":"tutorials/local-llama/#running-llama-2-70b","title":"\ud83e\udd16 Running Llama-2 70B","text":"<ol> <li> <p>Return to the home directory of Llama.cpp:</p> <pre><code>cd &lt;your_parent_dir&gt;/llama.cpp\n</code></pre> </li> <li> <p>Run Llama.cpp\u2019s example server application to set up a HTTP API server and a simple web front end to interact with our Llama model:</p> <pre><code>./server -m /media/m2/models/llama-2-70b-chat.Q4_K_M.gguf -ngl 1000 -c 4096 --alias llama_2\n</code></pre> </li> <li> <p><code>-m</code>: indicates the location of our model.</p> </li> <li><code>-ngl</code>: the number of layers to offload to the GPU (1000 ensures all layers are).</li> <li><code>-c</code>: the size of the prompt context.</li> <li><code>--alias</code>: name given to our model for access through the API.</li> </ol> <p>After executing, you should see the below output indicating the model being loaded to VRAM and the specs of the model: <pre><code>ggml_init_cublas: found 1 CUDA devices:\n  Device 0: NVIDIA RTX A6000, compute capability 8.6\n{\"timestamp\":1695853185,\"level\":\"INFO\",\"function\":\"main\",\"line\":1294,\"message\":\"build info\",\"build\":1279,\"commit\":\"e519621\"}\n{\"timestamp\":1695853185,\"level\":\"INFO\",\"function\":\"main\",\"line\":1296,\"message\":\"system info\",\"n_threads\":6,\"total_threads\":12,\"system_info\":\"AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \"}\nllama_model_loader: loaded meta data with 19 key-value pairs and 723 tensors from /media/m2/models/llama-2-70b-chat.Q4_K_M.gguf (version GGUF V2 (latest))\n**Verbose llama_model_loader output removed for conciseness**\nllm_load_print_meta: format         = GGUF V2 (latest)\nllm_load_print_meta: arch           = llama\nllm_load_print_meta: vocab type     = SPM\nllm_load_print_meta: n_vocab        = 32000\nllm_load_print_meta: n_merges       = 0\nllm_load_print_meta: n_ctx_train    = 4096\nllm_load_print_meta: n_ctx          = 4096\nllm_load_print_meta: n_embd         = 8192\nllm_load_print_meta: n_head         = 64\nllm_load_print_meta: n_head_kv      = 8\nllm_load_print_meta: n_layer        = 80\nllm_load_print_meta: n_rot          = 128\nllm_load_print_meta: n_gqa          = 8\nllm_load_print_meta: f_norm_eps     = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps = 1.0e-05\nllm_load_print_meta: n_ff           = 28672\nllm_load_print_meta: freq_base      = 10000.0\nllm_load_print_meta: freq_scale     = 1\nllm_load_print_meta: model type     = 70B\nllm_load_print_meta: model ftype    = mostly Q4_K - Medium\nllm_load_print_meta: model params   = 68.98 B\nllm_load_print_meta: model size     = 38.58 GiB (4.80 BPW)\nllm_load_print_meta: general.name   = LLaMA v2\nllm_load_print_meta: BOS token = 1 '&lt;s&gt;'\nllm_load_print_meta: EOS token = 2 '&lt;/s&gt;'\nllm_load_print_meta: UNK token = 0 '&lt;unk&gt;'\nllm_load_print_meta: LF token  = 13 '&lt;0x0A&gt;'\nllm_load_tensors: ggml ctx size =    0.23 MB\nllm_load_tensors: using CUDA for GPU acceleration\nllm_load_tensors: mem required  =  140.86 MB (+ 1280.00 MB per state)\nllm_load_tensors: offloading 80 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloading v cache to GPU\nllm_load_tensors: offloading k cache to GPU\nllm_load_tensors: offloaded 83/83 layers to GPU\nllm_load_tensors: VRAM used: 40643 MB\n....................................................................................................\nllama_new_context_with_model: kv self size  = 1280.00 MB\nllama_new_context_with_model: compute buffer total size =  561.47 MB\nllama_new_context_with_model: VRAM scratch buffer: 560.00 MB\n\nllama server listening at http://127.0.0.1:8080\n\n{\"timestamp\":1695853195,\"level\":\"INFO\",\"function\":\"main\",\"line\":1602,\"message\":\"HTTP server listening\",\"hostname\":\"127.0.0.1\",\"port\":8080}\n</code></pre></p> <p>Now, you can interact with the simple web front end by browsing to http://127.0.0.1:8080. Use the provided chat interface to query the Llama-2 model and experiment with manipulating the provided hyperparameters to tune the responses to your liking.</p>","tags":["Deployment","NLP","CUDA","Huggingface","LLM"]},{"location":"tutorials/local-llama/#setting-up-a-local-openai-server","title":"\ud83d\udda5\ufe0f Setting up a local OpenAI server","text":"<p>Llama.cpp includes a nifty Flask app <code>api_like_OAI.py</code>. This Flask app sets up a server that emulates the OpenAI API. Its trick is that it converts the OpenAI API requests into the format expected by the Llama model, and forwards the captured requests to our local Llama-2 model. This allows you to use the popular OpenAI Python backend, and thus, countless powerful LLM libraries like LangChain, Scikit-LLM, Haystack, and more. However, instead of your data being sent to OpenAI\u2019s servers, it is all processed locally on your machine!</p> <ol> <li> <p>In order to run the OpenAI API server and our eventual Gradio chat app, we need to open a new terminal and install a few Python dependencies:</p> <pre><code>cd tutorials/local-llama\npip install -r requirements.txt\n</code></pre> </li> <li> <p>This then allows us to run the Flask server:</p> <pre><code>cd &lt;your_parent_dir&gt;/llama.cpp/examples/server/\npython api_like_OAI.py\n</code></pre> </li> <li> <p>The server should begin running almost immediately and give you the following output:</p> </li> </ol>","tags":["Deployment","NLP","CUDA","Huggingface","LLM"]},{"location":"tutorials/local-llama/#creating-the-gradio-chat-app","title":"\ud83d\udcac Creating the Gradio Chat App","text":"<ol> <li> <p>Create a new project directory and a <code>chatbot.py</code> file that contains the following code:</p> <pre><code>import gradio as gr\nimport openai\n\n# Indicate we'd like to send the request\n# to our local model, not OpenAI's servers\nopenai.api_base = \"http://127.0.0.1:8081\"\nopenai.api_key = \"\"\n\n\ndef to_oai_chat(history):\n    \"\"\"Converts the gradio chat history format to\n    the OpenAI chat history format:\n\n    Gradio format: ['&lt;user message&gt;', '&lt;bot message&gt;']\n    OpenAI format: [{'role': 'user', 'content': '&lt;user message&gt;'},\n                    {'role': 'assistant', 'content': '&lt;bot_message&gt;'}]\n\n    Additionally, this adds the 'system' message to the chat to tell the\n    assistant how to act.\n    \"\"\"\n    chat = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful AI Assistant who ends all of your responses with &lt;/BOT&gt;\",\n        }\n    ]\n\n    for msg_pair in history:\n        if msg_pair[0]:\n            chat.append({\"role\": \"user\", \"content\": msg_pair[0]})\n        if msg_pair[1]:\n            chat.append({\"role\": \"assistant\", \"content\": msg_pair[1]})\n    return chat\n\n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot(height=650)\n    msg = gr.Textbox()\n    clear = gr.Button(\"Clear\")\n\n    def user(user_message, history):\n        \"\"\"Appends a submitted question to the history\"\"\"\n        return \"\", history + [[user_message, None]]\n\n    def bot(history):\n        \"\"\"Sends the chat history to our Llama-2 model server\n        so that the model can respond appropriately\n        \"\"\"\n        # Gradio chat -&gt; OpenAI chat\n        oai_chat = to_oai_chat(history)\n\n        # Send chat history to our Llama-2 server\n        response = openai.ChatCompletion.create(\n            messages=oai_chat,\n            stream=True,\n            model=\"llama_2\",\n            temperature=0,\n            # Used to stop runaway responses\n            stop=[\"&lt;/BOT&gt;\"],\n        )\n\n        history[-1][1] = \"\"\n        for response_chunk in response:\n            # Filter through meta-data in the HTTP response to get response text\n            next_token = response_chunk[\"choices\"][0][\"delta\"].get(\"content\")\n            if next_token:\n                history[-1][1] += next_token\n                # Update the Gradio app with the streamed response\n                yield history\n\n    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(bot, chatbot, chatbot)\n    clear.click(lambda: None, None, chatbot, queue=False)\n\ndemo.queue()\ndemo.launch()\n</code></pre> </li> <li> <p>Begin running the Gradio chat app:</p> <pre><code>python chatbot.py\n</code></pre> </li> <li> <p>The chat app should now be accessible at http://127.0.0.1:7860:</p> </li> </ol> <p>You're now set up to interact with the Llama-2 70b model, with everything running locally! If you want to take this project further, you can experiment with different system messages to suit your needs or add the ability to interact with your local documents using frameworks like LangChain. Enjoy experimenting!</p>","tags":["Deployment","NLP","CUDA","Huggingface","LLM"]},{"location":"tutorials/local-llama/#sources","title":"Sources:","text":"<ul> <li>https://huggingface.co/meta-llama/Llama-2-70b-chat-hf/tree/main</li> <li>https://huggingface.co/docs/optimum/concept_guides/quantization</li> <li>https://deci.ai/quantization-and-quantization-aware-training/</li> <li>https://www.gradio.app/guides/creating-a-custom-chatbot-with-blocks#add-streaming-to-your-chatbot</li> </ul>","tags":["Deployment","NLP","CUDA","Huggingface","LLM"]},{"location":"tutorials/pretrained_foundational_models/self_supervised_training/","title":"Self-Supervised Contrastive Learning for Surgical videos","text":"<p> Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: October 9, 2025 Latest version: 0.1.0 Minimum Holoscan SDK version: 0.6.0 Tested Holoscan SDK versions: 0.6.0 Contribution metric: Level 1 - Highly Reliable The focus of this tutorial is to walkthrough the process of doing Self-Supervised Learning using Contrastive Pre-training on Surgical Video data.  As part of the walk-through we will guide through the steps needed to pre-process and extract the frames from the public Cholec80 Dataset. This will be required to run the tutorial.</p> <p>The repository is organized as follows -  * <code>Contrastive_learning_Notebook.ipynb</code> walks through the process of SSL in a tutorial style * <code>train_simclr_multiGPU.py</code> enables running of \"pre-training\" on surgical data across multiple GPUs through the CLI * <code>downstream_task_tool_segmentation.py</code> shows the process of \"fine-tuning\" for a downstream task starting from a pretrained checkpoint using MONAI</p>","tags":["Interoperability","MONAI","Learning","Healthcare AI","Surgical AI","Video"]},{"location":"tutorials/pretrained_foundational_models/self_supervised_training/#dataset","title":"Dataset","text":"<p>To run through the full tutorial, it is required that the user downloads Cholec80 dataset. Additional preprocessing of the videos to extract individual frames can be performed using the python helper file as follows:</p> <p><code>python extract_frames.py --datadir &lt;path_to_cholec80_dataset&gt;</code> </p>","tags":["Interoperability","MONAI","Learning","Healthcare AI","Surgical AI","Video"]},{"location":"tutorials/pretrained_foundational_models/self_supervised_training/#adapt-to-your-own-dataset","title":"Adapt to your own dataset","text":"<p>To run this with your own dataset, you will need to extract the frames and modify the <code>Pytorch Dataset/Dataloader</code> accordingly. For SSL pre-training, a really simple CSV file formatted as follows can be used.  <pre><code>&lt;path_to_frame&gt;,&lt;label&gt;\n</code></pre> where <code>&lt;label&gt;</code> can be a class/score for a downstream task, and is NOT used during pre-training.</p> <pre><code># Snippet of csv file\n/workspace/data/cholec80/frames/train/video01/1.jpg,0\n/workspace/data/cholec80/frames/train/video01/2.jpg,0\n/workspace/data/cholec80/frames/train/video01/3.jpg,0\n/workspace/data/cholec80/frames/train/video01/4.jpg,0\n/workspace/data/cholec80/frames/train/video01/5.jpg,0\n....\n</code></pre>","tags":["Interoperability","MONAI","Learning","Healthcare AI","Surgical AI","Video"]},{"location":"tutorials/pretrained_foundational_models/self_supervised_training/#environment","title":"Environment","text":"<p>All environment/dependencies are captured in the Dockerfile. The exact software within the base container are described here.</p>","tags":["Interoperability","MONAI","Learning","Healthcare AI","Surgical AI","Video"]},{"location":"tutorials/pretrained_foundational_models/self_supervised_training/#create-docker-imagecontainer","title":"Create Docker Image/Container","text":"<pre><code>DATA_DIR=\"/mnt/sdb/data\"  # location of Cholec80 dataset\ndocker build -t surg_video_ssl_2202:latest Docker/\n\n# sample Docker command (may need to update based on local setup)\ndocker run -it --gpus=\"device=1\" \\\n    --name=SURGSSL_EXPS \\\n    -v $DATA_DIR:/workspace/data \\\n    -v `pwd`:/workspace/codes -w=/workspace/codes/ \\\n    -p 8888:8888 \\\n    --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 \\\n    surg_video_ssl_2202 jupyter lab\n</code></pre> <p>For environment dependencies refer to the Dockerfile</p>","tags":["Interoperability","MONAI","Learning","Healthcare AI","Surgical AI","Video"]},{"location":"tutorials/pretrained_foundational_models/self_supervised_training/#launch-training","title":"Launch Training","text":"<p>PRE-TRAINING <pre><code># Training on single GPU with `efficientnet_b0` backbone\npython3 train_simclr_multigpu.py --gpus 1 --backbone efficientnet_b0 --batch_size 64\n\n# Training on single GPU with `resnet50` backbone\npython3 train_simclr_multigpu.py --gpus 4 --backbone resnet50 --batch_size 128\n</code></pre></p> <p>DOWNSTREAM TASK - Segmentation This script shows an example of taking the checkpoint above and integrating it into MONAI.</p> <pre><code># Fine-Tuning on \"GPU 1\" with 10% of the dataset, while freezing the encoder\npython3 downstream_task_tool_segmentation.py --gpu 1 --perc 10 --exp simclr --freeze\n</code></pre>","tags":["Interoperability","MONAI","Learning","Healthcare AI","Surgical AI","Video"]},{"location":"tutorials/pretrained_foundational_models/self_supervised_training/#modelcheckpoints-information","title":"Model/Checkpoints information","text":"<p>As part of this tutorial, we are also releasing a few different checkpoints for users. These are detailed below. </p> <p>NOTE : These checkpoints were trained using an internal dataset of Chelecystectomy videos provided by Activ Surgical and NOT the Cholec80 dataset. </p>","tags":["Interoperability","MONAI","Learning","Healthcare AI","Surgical AI","Video"]},{"location":"tutorials/pretrained_foundational_models/self_supervised_training/#pre-trained-backbones","title":"Pre-Trained Backbones","text":"<ul> <li>ResNet18        - link</li> <li>ResNet50        - link</li> <li>efficientnet_b0 - link</li> </ul>","tags":["Interoperability","MONAI","Learning","Healthcare AI","Surgical AI","Video"]},{"location":"tutorials/pretrained_foundational_models/self_supervised_training/#tool-segmentation-model","title":"Tool Segmentation Model","text":"<ul> <li>MONAI - FlexibleUNet (efficientnet_b0) - link</li> </ul>","tags":["Interoperability","MONAI","Learning","Healthcare AI","Surgical AI","Video"]},{"location":"tutorials/pretrained_foundational_models/self_supervised_training/#holoscan-sdk","title":"Holoscan SDK","text":"<p>This tool Segmentation Model can be used to build a Holoscan App, using the process under section \"Bring your own Model\" within the Holoscan SDK User guide.</p>","tags":["Interoperability","MONAI","Learning","Healthcare AI","Surgical AI","Video"]},{"location":"tutorials/pretrained_foundational_models/self_supervised_training/#resources","title":"Resources","text":"<p>[1] Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. (2020). A simple framework for contrastive learning of visual representations. In International conference on machine learning (pp. 1597-1607). PMLR. (link)</p> <p>[2] Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G. (2020). Big self-supervised models are strong semi-supervised learners. NeurIPS 2021 (link).</p> <p>[3][Pytorch Lightning SSL Tutorial](https://pytorch-lightning.readthedocs.io/en/latest/notebooks/course_UvA-DL/13-contrastive-learning.html) | Github</p> <p>[4] Ramesh, S., Srivastav, V., Alapatt, D., Yu, T., Muarli, A., et. al. (2023).  Dissecting Self-Supervised Learning Methods for Surgical Computer Vision. arXiv preprint arXiv:2207.00449. (link)</p>","tags":["Interoperability","MONAI","Learning","Healthcare AI","Surgical AI","Video"]},{"location":"tutorials/windows_vm/","title":"Interoperability between Holoscan and a Windows Application on a Single Machine","text":"<p> Authors: NVIDIA (NVIDIA) Supported platforms: x86_64 Language: bash Last modified: October 9, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 2.6.0 Tested Holoscan SDK versions: 2.6.0 Contribution metric: Level 1 - Highly Reliable</p> <p></p>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#overview","title":"Overview","text":"<p>This tutorial enables Holoscan and Windows applications to run concurrently on the same machine or node.</p>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Description</li> <li>Platform Requirements</li> <li>Windows VM Setup Instructions</li> <li>Software Pre-requisites</li> <li>GPU Passthrough<ul> <li>Two Different GPUs (e.g., RTX A4000 and RTX A6000)</li> <li>Two Identical GPUs (e.g., 2x RTX A4000)</li> </ul> </li> <li>Windows VM Configuration for Passed-through GPU<ul> <li>Install NVIDIA Driver in Windows VM</li> </ul> </li> <li>Communication Performance between Linux Host and Windows VM</li> <li>Running Holoscan DDS App and Windows VM App</li> </ul>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#description","title":"Description","text":"<p>Many legacy graphics applications, particularly in medical devices, are developed and operated on the Windows platform. A significant number of these medical devices rely on Windows OS for their functionality. To integrate AI/ML and sensor processing capabilities from NVIDIA Holoscan into such Windows-based systems, we previously introduced a \"sidecar\" architecture. This architecture involved an AI compute node running Holoscan interoperating with a Windows node via a DDS link, showcased in the Holoscan DDS reference application. This tutorial extends the options for such use-cases by providing developers with a straightforward and clear system design to enable interoperability between Holoscan applications and Windows applications running on the same physical machine. It demonstrates how to achieve efficient communication and processing without the need for separate hardware nodes.</p> <p></p> <p>In this tutorial, Holoscan runs on an x86_64 workstation hosting Ubuntu 22.04. Two RTX A4000 GPUs are plugged into the x86_64 workstation. Using Linux KVM, a Windows VM is created on the host Ubuntu Linux. One RTX A4000 GPU is passed through to the Windows VM using VFIO. The other RTX A4000 GPU is reserved for the host Ubuntu OS and used by Holoscan. We provide a step-by-step guide on how to achieve this setup. Furthermore, we also demonstrate how a Holoscan application reads USB-based camera frames and sends the frames via DDS to an application running on the Windows VM. Finally, the Windows VM application renders the camera frames on the screen using its dedicated RTX A4000 GPU.</p>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#platform-requirements","title":"Platform Requirements","text":"<p>The setup described in this tutorial requires an x86_64 workstation and ProViz class of NVIDIA GPUs. The exact platform details are provided below:</p> <ul> <li>CPU: AMD Ryzen 9 7950X 16-Core Processor</li> <li>OS: Ubuntu 22.04</li> <li>Kernel Version: 6.8.0-48-generic</li> <li>GPU: 2x NVIDIA RTX A4000</li> </ul> <p>Although we used two same GPUs in this tutorial, combinations of two different GPUs, for example, RTX A4000 and RTX A6000, can also be used.</p> <p>Since x86_64 workstations are very much diverse, the steps to enable the setup described in this tutorial may not work as-is on all x86_64 workstations. Performances may also vary across different x86_64 workstations.</p>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#windows-vm-setup-instructions","title":"Windows VM Setup Instructions","text":"","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#software-pre-requisites","title":"Software Pre-requisites","text":"<p>Install the NVIDIA Holoscan SDK and the NVIDIA GPU driver on the host Ubuntu 22.04.</p> <p>Install KVM and QEMU</p> <pre><code>sudo apt update\nsudo apt install qemu-kvm libvirt-clients libvirt-daemon-system bridge-utils virt-manager ovmf\n</code></pre> <p>Check if KVM works</p> <pre><code>$ kvm-ok\nINFO: /dev/kvm exists\nKVM acceleration can be used\n</code></pre> <p>If KVM does not work, kindly figure out how KVM can be enabled for your specific Linux kernel version.</p>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#gpu-passthrough","title":"GPU Passthrough","text":"<p>A GPU can be passed through to a virtual machine using Linux's VFIO driver. The VM is, then, capable of using the passed through GPU directly without any host operating system intervention. If a machine has two different GPUs, then it is much more straightforward to passthrough one of the GPUs to a VM. A bit more configuration is required if two GPUs are same. We show how one of the two identical GPUs can be passed through to a VM with the VFIO driver.</p>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#two-different-gpus-eg-rtx-a4000-and-rtx-a6000","title":"Two Different GPUs (e.g., RTX A4000 and RTX A6000)","text":"","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#identify-the-gpu-to-be-passed-through","title":"Identify the GPU to be Passed Through","text":"<pre><code>$ lspci | grep -i NVIDIA\n17:00.0 VGA compatible controller [0300]: NVIDIA Corporation Device [RTX A4000] [10de:24b0] (rev a1)\n17:00.1 Audio device [0403]: NVIDIA Corporation Device [10de:228b] (rev a1)\n65:00.0 VGA compatible controller [0300]: NVIDIA Corporation Device [RTX A6000] [10de:2230] (rev a1)\n65:00.1 Audio device [0403]: NVIDIA Corporation Device [10de:1aef] (rev a1)\n</code></pre> <p>Let's assume that we want to passthrough the RTX A4000 GPU to a VM (in this tutorial it's a Windows VM). We note the PCI ID of RTX A4000 as <code>10de:24b0</code>.</p> <p>Note: Make sure that the RTX A4000 GPU to be passed through is not currently used for displaying to the monitor.</p>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#update-grub-configuration","title":"Update GRUB Configuration","text":"<p>Open the <code>/etc/default/grub</code> file and find the following line:</p> <pre><code>GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash\"\n</code></pre> <p>Update the above line to the following to add the previously noted PCI ID of RTX A4000 for the VFIO driver:</p> <pre><code>GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash intel_iommu=on amd_iommu=on iommu=pt vfio-pci.ids=10de:24b0\"\n</code></pre> <p>The above line enables IOMMU and passes the PCI ID of RTX A4000 to the VFIO driver for Linux kernel.</p>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#update-linux-kernel-module-configuration","title":"Update Linux Kernel Module Configuration","text":"<p>Create a new file <code>/etc/modprobe.d/vfio.conf</code> and add the following lines:</p> <pre><code>options vfio-pci ids=10de:24b0\nsoftdep nvidia pre: vfio-pci\n</code></pre> <p>The above lines ensure that the NVIDIA driver is loaded after the VFIO driver. Therefore, VFIO takes over the RTX A4000 GPU, and the RTX A6000 GPU is used by the default NVIDIA driver.</p>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#update-initial-ramfs-and-grub","title":"Update Initial RAMFS and GRUB","text":"<pre><code>sudo update-initramfs -u # optionally add -k all to update all kernels\nsudo update-grub\n</code></pre>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#check-if-vfio-is-loaded-for-passed-through-gpu","title":"Check if VFIO is loaded for Passed-through GPU","text":"<p>Reboot the system: <code>sudo reboot</code>. Now, check the loaded driver for RTX A4000:</p> <pre><code>$ lspci -nnk -d 10de:24b0\n17:00.0 VGA compatible controller [0300]: NVIDIA Corporation [RTX A4000] [10de:24b0] (rev a1)\n    Subsystem: NVIDIA Corporation [RTX A4000] [10de:14ad]\n    Kernel driver in use: vfio-pci\n    Kernel modules: nvidiafb, nouveau, nvidia_drm, nvidia\n</code></pre>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#two-identical-gpus-eg-2x-rtx-a4000","title":"Two Identical GPUs (e.g., 2x RTX A4000)","text":"<p>In case of two identical GPUs, the PCI IDs of the GPUs are the same.</p> <pre><code>$ lspci -nn | grep \"NVIDIA\"\n01:00.0 VGA compatible controller [0300]: NVIDIA Corporation GA104GL [RTX A4000] [10de:24b0] (rev a1)\n01:00.1 Audio device [0403]: NVIDIA Corporation GA104 High Definition Audio Controller [10de:228b] (rev a1)\n09:00.0 VGA compatible controller [0300]: NVIDIA Corporation GA104GL [RTX A4000] [10de:24b0] (rev a1)\n09:00.1 Audio device [0403]: NVIDIA Corporation GA104 High Definition Audio Controller [10de:228b] (rev a1)\n</code></pre> <p>Therefore, we cannot just pass PCI ID <code>10de:24b0</code> to the VFIO driver to pass through one RTX A4000 to a VM and keep the other one for the host Linux.</p> <p>To mitigate the issue, we force the Linux kernel to load the VFIO driver for one of the RTX A4000 GPUs by identifying the GPU by its PCI bus address at the time of booting Linux. Let's assume that we want to pass through the RTX A4000 GPU at PCI bus address <code>01:00.0</code> to a VM.</p> <p>Note: Make sure that the RTX A4000 GPU to be passed through is not currently used for displaying to the monitor. <code>nvidia-smi</code> can be used to figure out which GPU is being used for display.</p> <p>The following instructions are only verified for Ubuntu 22.04 and Linux kernel 6.8.0-48-generic. For other Ubuntu and Linux kernel versions, the instructions may not work.</p>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#update-grub-configuration_1","title":"Update GRUB Configuration","text":"<p>Open the <code>/etc/default/grub</code> file and find the following line:</p> <pre><code>GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash\"\n</code></pre> <p>Update the above line to the following to enable IOMMU:</p> <pre><code>GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash intel_iommu=on amd_iommu=on iommu=pt vfio-pci.ids=\"\n</code></pre>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#create-a-script-to-bind-the-rtx-a4000-gpu-to-vfio-driver","title":"Create a script to bind the RTX A4000 GPU to VFIO driver","text":"<p>Create a script <code>/usr/local/bin/vfio-pci-override.sh</code> with the following content:</p> <pre><code>#!/bin/sh\n\necho \"vfio-pci\" &gt; /sys/bus/pci/devices/0000\\:01\\:00.0/driver_override\necho \"vfio-pci\" &gt; /sys/bus/pci/devices/0000\\:01\\:00.0/iommu_group/devices/0000\\:01\\:00.0/driver_override\necho \"vfio-pci\" &gt; /sys/bus/pci/devices/0000\\:01\\:00.0/iommu_group/devices/0000\\:01\\:00.1/driver_override\nmodprobe -i vfio-pci\n</code></pre> <p>Kindly, note that we are using the PCI bus address <code>0000:01:00.0</code> of the RTX A4000 in the above script. In specific cases, the PCI bus address may be different and needs to be updated.</p>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#install-vfio-driver-with-the-above-script","title":"Install VFIO driver with the above script","text":"<p>Create a new file <code>/etc/modprobe.d/vfio.conf</code> and add the following lines:</p> <pre><code>install vfio-pci /usr/local/bin/vfio-pci-override.sh\nsoftdep nvidia pre: vfio-pci\n</code></pre>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#update-initial-ramfs-and-grub_1","title":"Update Initial RAMFS and GRUB","text":"<pre><code>sudo update-initramfs -u # optionally add -k all to update all kernels\nsudo update-grub\n</code></pre>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#check-if-vfio-is-loaded-for-passed-through-gpu_1","title":"Check if VFIO is loaded for Passed-through GPU","text":"<p>Reboot the system: <code>sudo reboot</code>. Now, check the loaded driver for RTX A4000:</p> <pre><code>$ lspci -nnk -d 10de:24b0\n01:00.0 VGA compatible controller [0300]: NVIDIA Corporation GA104GL [RTX A4000] [10de:24b0] (rev a1)\n    Subsystem: NVIDIA Corporation GA104GL [RTX A4000] [10de:14ad]\n    Kernel driver in use: vfio-pci\n    Kernel modules: nvidiafb, nouveau, nvidia_drm, nvidia\n09:00.0 VGA compatible controller [0300]: NVIDIA Corporation GA104GL [RTX A4000] [10de:24b0] (rev a1)\n    Subsystem: NVIDIA Corporation GA104GL [RTX A4000] [10de:14ad]\n    Kernel driver in use: nvidia\n    Kernel modules: nvidiafb, nouveau, nvidia_drm, nvidia\n</code></pre> <p>In the above output, we can see that different kernel drivers are loaded for the two RTX A4000 GPUs.</p>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#windows-vm-configuration-for-passed-through-gpu","title":"Windows VM Configuration for Passed-through GPU","text":"<p>A Windows VM can be created using <code>virt-manager</code> or <code>virsh</code> (see references if needed). We assume that the Windows VM is already created and running. We show how to assign the passed-through RTX A4000 GPU to the Windows VM.</p> <p>In the \"Hardware Details\" tab of <code>virt-manager</code> GUI, add a new PCI device by selecting <code>Add Hardware -&gt; PCI Host Device</code> option. Select the RTX A4000 GPU at PCI bus address <code>01:00.0</code> (and also other functions of the PCI device such as <code>01:00.1</code>).</p> <p></p> <p>If the GPU is assigned to the VM correctly, then the left panel of the VM hardware details and the XML of the assigned GPU in <code>virt-manager</code> window should look like the following:</p> <p></p>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#add-vnc-display-for-windows-vm","title":"Add VNC Display for Windows VM","text":"<p>Add VNC display server via \"Add Hardware\" option in <code>virt-manager</code> so that the Windows VM display can be controlled via VNC server running on Linux host.</p> <p></p>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#install-nvidia-driver-in-windows-vm","title":"Install NVIDIA Driver in Windows VM","text":"<p>Download the NVIDIA driver for RTX A4000 GPU from the NVIDIA website.</p> <p>After installing the NVIDIA driver, reboot the Windows VM. Then, open command-prompt by typing <code>cmd</code> in the Windows search bar and type <code>nvidia-smi</code>. The following output should be displayed:</p> <p></p>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#replicate-monitor-display-to-vnc-display","title":"Replicate Monitor Display to VNC Display","text":"<p>Right now, we have two screen for the Windows VM: physical monitor (VGA) and VNC display. It could be difficult to control mouse and keyboard with both the displays. Therefore, the monitor display can be replicated to the monitor display so that the Windows VM's physical monitor display can be controlled via the VNC display.</p> <p>The following configuration shows that the screens are replicated in Windows VM:</p> <p></p>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#communication-performance-between-host-and-vm","title":"Communication Performance between Host and VM","text":"<p>The performance of a Windows VM running on a Linux under KVM-based virtualization is well-studied in the research literature. The performance is dependent on specific hardware and applications. In this tutorial, we provide the data-transfer performance in our hardware configuration.</p> <p>Note: Firewalls in both Windows VM and Linux host need to be properly configured to allow network traffic between the two OS. Configuring exact firewall rules is out-of-scope of this tutorial. However, we provide a few tips below as guiding help on this topic.</p> <p>Allowing Network Traffic between VM and Host</p> <p>Use a \"bridge device\" for the VM's network configuration.</p> <p></p> <p>In Windows VM search bar, type \"Windows Defender Firewall with Advanced Security\" and open the application. For both \"Inbound Rules\" and \"Outbound Rules\", add new rules to allow TCP and UDP traffic on common (or all) port numbers.</p> <p></p> <p>In the host Linux, it may not be needed to configure the firewall to allow network traffic between VM and host. However, in case it does not work, <code>iptables</code> or <code>ufw</code> can be used to configure traffic on specific ports.</p> <p>Configuring firewall</p> <p>Turning off the firewall, either in Windows VM, or in host Linux, is not recommended. However, to test the setup in this tutorial, firewalls can be turned off. In Windows, type \"firewall\" in the search bar and turn off the firewall. The setting will look like below:</p> <p></p>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#iperf3-performance-test","title":"iperf3 Performance Test","text":"<p>Install iperf3 in both Windows VM and host Linux.</p> <p>We need the IP address of the Windows VM and the host Linux. In our setup, the Linux server IP is <code>192.168.122.1</code> and the Windows VM client IP is <code>192.168.122.147</code>. To check the host Linux IP address, run <code>ifconfig</code> and the IP address of the <code>virbr0</code> interface is the IP address of the host. The Windows VM IP address can be checked in <code>virt-manager</code> GUI in the NIC setting, or by running <code>ipconfig</code> in the Command Prompt.</p>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#host-linux-server-and-windows-vm-client","title":"Host Linux Server and Windows VM Client","text":"<p>In host Linux, run the following command:</p> <pre><code>iperf3 -s -B 192.168.122.1\n</code></pre> <p>In the Windows VM, run the following command for a 5 second test: <pre><code>iperf3.exe -c 192.168.122.1 -t 5\n</code></pre></p> <p>Output in Linux should look like below: <pre><code>$ iperf3 -s -B 192.168.122.1\n-----------------------------------------------------------\nServer listening on 5201\n-----------------------------------------------------------\nAccepted connection from 192.168.122.147, port 50807\n[  5] local 192.168.122.1 port 5201 connected to 192.168.122.147 port 50808\n[ ID] Interval           Transfer     Bitrate\n[  5]   0.00-1.00   sec  1.98 GBytes  17.0 Gbits/sec\n[  5]   1.00-2.00   sec  1.88 GBytes  16.1 Gbits/sec\n[  5]   2.00-3.00   sec  2.01 GBytes  17.3 Gbits/sec\n[  5]   3.00-4.00   sec  2.01 GBytes  17.3 Gbits/sec\n[  5]   4.00-5.00   sec  2.00 GBytes  17.2 Gbits/sec\n[  5]   5.00-5.02   sec  46.6 MBytes  16.0 Gbits/sec\n- - - - - - - - - - - - - - - - - - - - - - - - -\n[ ID] Interval           Transfer     Bitrate\n[  5]   0.00-5.02   sec  9.93 GBytes  17.0 Gbits/sec                  receiver\n</code></pre></p> <p>Output in Windows VM should look like below:</p> <p></p> <p>For Linux host to Windows, we have observed 21.9 Gbits/sec transfer rate for a 1 minute test.</p>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#host-linux-client-and-windows-vm-server","title":"Host Linux Client and Windows VM Server","text":"<p>The above commands are reversed for host Linux client and Windows VM server. In the Windows VM, run the following command:</p> <pre><code>iperf3.exe -s -B 192.168.122.147\n</code></pre> <p>In the host Linux, run the following command for a 5 second test:</p> <pre><code>iperf3 -c 192.168.122.147 -t 60\n</code></pre> <p>For a 1 minute test, we have observed 4.33 Gbits/sec transfer rate from Windows VM to host.</p>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#running-holoscan-dds-app-and-windows-vm-app","title":"Running Holoscan DDS App and Windows VM App","text":"<p>If the above configurations are done correctly, then interoperating between a Holoscan application and Windows VM application is straightforward. For this tutorial, we will use the Holoscan DDS app and a simple Windows application. The Holoscan DDS app is used in <code>publisher</code> mode where it reads frames from a USB camera and sends the frames via DDS. The Windows application (available upon request) receives the frames via DDS and renders the frame on the screen using the RTX A4000 GPU with the help of OpenGL.</p> <p>In Linux host, run the Holoscan DDS app in <code>publisher</code> mode:</p> <pre><code>./holohub run --local dds_video --run-args=\"-p\"\n</code></pre> <p>In Windows VM, running the renderer application shows the camera input from Linux host:</p> <p></p>","tags":["Interoperability","VM","Deployment"]},{"location":"tutorials/windows_vm/#references","title":"References","text":"<ul> <li>https://wiki.archlinux.org/title/PCI_passthrough_via_OVMF</li> <li>https://www.makeuseof.com/create-windows-virtual-machine-in-linux-with-kvm/</li> </ul>","tags":["Interoperability","VM","Deployment"]},{"location":"workflows/ai_surgical_video/","title":"Real-Time End-to-end AI Surgical Video Workflow","text":"<p>     \u25b6 Run Locally  Authors: Holoscan Team (NVIDIA) Supported platforms: x86_64, aarch64 Language: Python Last modified: December 19, 2025 Latest version: 1.0 Minimum Holoscan SDK version: 3.0.0 Tested Holoscan SDK versions: 3.1.0 Contribution metric: Level 1 - Highly Reliable</p> <p> Fig.1: The overall diagram illustrating the end-to-end pipeline for real-time AI surgical video processing. The pipeline achieves an average end-to-end latency of 37ms (maximum 54ms). Key latency components are shown: Holoscan Sensor Bridge (HSB) latency averages 21ms (max 28ms), and the AI application averages 16ms (median 17ms, 95th percentile 18ms, 99th percentile 22ms, max 26ms). These results demonstrate the solution's high-performance, low-latency capabilities for demanding surgical video applications.</p>","tags":["Healthcare AI","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#overview","title":"Overview","text":"<p>This reference application offers developers a modular, end-to-end pipeline that spans the entire sensor processing workflow\u2014from sensor data ingestion and accelerated computing to AI inference, real-time visualization, and data stream output.</p> <p>Specifically, we demonstrate a comprehensive real-time end-to-end AI surgical video pipeline that includes:</p> <ol> <li>Sensor I/O: Integration with Holoscan Sensor Bridge, enabling GPU Direct data ingestion for ultra low-latency input of surgical video feeds.</li> <li>Out-of-body detection to determine if the endoscope is inside or outside the patient's body, ensuring patient privacy by removing identifiable information.</li> <li>Dynamic flow condition based on out-of-body detection results.</li> <li>De-identification: pixelate the image to anonymize outside of body elements like people's faces.</li> <li>Multi-AI: Enabling simultaneous execution of multiple models at inference. Surgical Tool processing with:</li> <li>SSD detection for surgical tool detection</li> <li>MONAI segmentation for endoscopic tool segmentation</li> </ol>","tags":["Healthcare AI","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#architecture","title":"Architecture","text":"<p> Fig.2: The workflow diagram representing all the holoscan operators (in green) and holoscan sensor bridge operators (in yellow). The source can be a Holoscan Sensor Bridge, an AJA Card or a video replayer.</p> <p> Fig.3: Endoscopy image from a partial nephrectomy procedure (surgical removal of the diseased portion of the kidney) showing AI tool segmentation results when the camera is inside the body and a deidentified (pixelated) output image when the camera is outside of the body.</p>","tags":["Healthcare AI","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#1-out-of-body-detection","title":"1. Out-of-Body Detection","text":"<p>The workflow first determines if the endoscope is inside or outside the patient's body using an AI model.</p>","tags":["Healthcare AI","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#2-dynamic-flow-control","title":"2. Dynamic Flow Control","text":"<ul> <li>If outside the body: The video is deidentified through pixelation to protect privacy</li> <li>If inside the body: The video is processed by the multi-AI pipeline</li> </ul>","tags":["Healthcare AI","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#3-multi-ai-processing","title":"3. Multi-AI Processing","text":"<p>When inside the body, two AI models run concurrently:</p> <ul> <li>SSD detection model identifies surgical tools with bounding boxes</li> <li>MONAI segmentation model provides pixel-level segmentation of tools</li> </ul>","tags":["Healthcare AI","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#4-visualization","title":"4. Visualization","text":"<p>The HolovizOp displays the processed video with overlaid AI results, including:</p> <ul> <li>Bounding boxes around detected tools</li> <li>Segmentation masks for tools</li> <li>Text labels for detected tools</li> </ul>","tags":["Healthcare AI","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#requirements","title":"Requirements","text":"","tags":["Healthcare AI","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#software","title":"Software","text":"<ul> <li>Holoscan SDK <code>&gt;= v3.0</code>:   Holohub command takes care of this dependency when using Holohub container. However, you can install the Holoscan SDK via one of the methods specified in the SDK user guide.</li> <li>Holoscan Sensor Bridge <code>&gt;= v2.0</code>: Please see the Quick start guide for building the Holoscan Sensor Bridge docker container.</li> </ul>","tags":["Healthcare AI","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#models","title":"Models","text":"<p>This workflow utilizes the following three AI models:</p> Model Description File \ud83d\udce6\ufe0f Out-of-body Detection Model Detects if endoscope is inside or outside the body <code>anonymization_model.onnx</code> \ud83d\udce6\ufe0f SSD Detection for Endoscopy Surgical Tools Detects surgical tools with bounding boxes <code>epoch24_nms.onnx</code> \ud83d\udce6\ufe0f MONAI Endoscopic Tool Segmentation Provides pixel-level segmentation of tools <code>model_endoscopic_tool_seg_sanitized_nhwc_in_nchw_out.onnx</code>","tags":["Healthcare AI","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#sample-data","title":"Sample Data","text":"<ul> <li>\ud83d\udce6\ufe0f Orsi partial nephrectomy procedures - Sample endoscopy video data for use with the <code>replayer</code> source</li> </ul> <p>Note: The directory specified by <code>--data</code> at runtime is assumed to contain three subdirectories, corresponding to the NGC resources specified in Models and Sample Data: <code>orsi</code>, <code>monai_tool_seg_model</code> and <code>ssd_model</code>. These resources will be automatically downloaded to the Holohub data directory when building the application.</p>","tags":["Healthcare AI","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#quick-start-guide","title":"Quick Start Guide","text":"<pre><code>./holohub run ai_surgical_video\n</code></pre> <p>This single command will create and launch Holohub container, build the workflow, and run the workflow with the default arguments set in the config.yaml file and a replayer source.</p>","tags":["Healthcare AI","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#using-aja-card-as-io","title":"Using AJA Card as I/O","text":"<pre><code>./holohub run ai_surgical_video aja\n</code></pre> <p>Note: the AJA video buffer dtype is set to <code>rgba8888</code> by default. If your camera is not providing alpha channel, you can change it to <code>rgb888</code> by modifying <code>in_dtype</code> in the <code>aja_format_converter</code> section of the config.yaml file.</p>","tags":["Healthcare AI","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#using-holoscan-sensor-bridge-as-io","title":"Using Holoscan Sensor Bridge as I/O","text":"<p>When using the workflow with <code>--source hsb</code>, it requires the Holoscan Sensor Bridge software to be installed. You can build a Holoscan Sensor Bridge container using the following commands:</p> <pre><code>git clone https://github.com/nvidia-holoscan/holoscan-sensor-bridge.git\ncd holoscan-sensor-bridge\ngit checkout hsdk-3.0\n./docker/build.sh --dgpu # for discrete GPU\n./docker/build.sh --igpu # for integrated GPU\n</code></pre> <p>This will build a docker image called <code>hololink-demo:2.0.0</code>.</p> <p>Once you have built the Holoscan Sensor Bridge container, you can build the Holohub container and run the workflow using the following command:</p> <pre><code>./holohub run --base-img hololink-demo:2.0.0 --img holohub:link ai_surgical_video --run-args=\"--source hsb\"\n</code></pre>","tags":["Healthcare AI","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#advanced-usage","title":"Advanced Usage","text":"","tags":["Healthcare AI","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#using-holohub-container","title":"Using Holohub Container","text":"<p>First, you need to run the Holohub container:</p> <pre><code>./holohub run-container ai_surgical_video\n</code></pre> <p>Note: If using Holoscan Sensor Bridge, please see the Using Holoscan Sensor Bridge as I/O for building the Holoscan Sensor Bridge docker container first, which is tagged as <code>hololink-demo:2.0.0</code>, and then use the following command to run the Holohub container:</p> <pre><code>./holohub run-container --base-img hololink-demo:2.0.0 --img holohub:link\n</code></pre>","tags":["Healthcare AI","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#building-the-application","title":"Building the Application","text":"<p>Once your environment is set up, you can build the workflow using the following command:</p> <pre><code>./holohub build ai_surgical_video\n</code></pre>","tags":["Healthcare AI","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#running-the-application","title":"Running the Application","text":"","tags":["Healthcare AI","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#use-holohub-container-from-outside-of-the-container","title":"Use Holohub Container from Outside of the Container","text":"<p>Using the Holohub container, you can run the workflow without building it again:</p> <pre><code>./holohub run ai_surgical_video --no-build\n</code></pre> <p>However, if you want to build the workflow, you can just remove the <code>--no-build</code> flag:</p> <pre><code>./holohub run ai_surgical_video\n</code></pre> <p>Alternatively, you can run the application directly from the source directory:</p> <pre><code>cd &lt;HOLOHUB_SOURCE_DIR&gt;/workflows/ai_surgical_video/python\npython3 ai_surgical_video.py --source hsb --data &lt;DATA_DIR&gt; --config &lt;CONFIG_FILE&gt;\n</code></pre> <p>TIP: You can get the exact \"Run command\" along with \"Run environment\" and \"Run workdir\" by executing:</p> <pre><code>./holohub run ai_surgical_video --dryrun --local\n</code></pre>","tags":["Healthcare AI","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#command-line-arguments","title":"Command Line Arguments","text":"<p>The application accepts the following command line arguments:</p> Argument Description Default <code>-s, --source</code> Source of video input: <code>replayer</code>, <code>aja</code>, or <code>hsb</code> <code>replayer</code> <code>-c, --config</code> Path to a custom configuration file <code>config.yaml</code> in the application directory <code>-d, --data</code> Path to the data directory containing model and video files Uses the <code>HOLOHUB_DATA_PATH</code> environment variable <code>--headless</code> Run in headless mode (no visualization) False <code>--fullscreen</code> Run in fullscreen mode False <code>--camera-mode</code> Camera mode to use [0,1,2,3] <code>0</code> <code>--frame-limit</code> Exit after receiving this many frames No limit <code>--hololink</code> IP address of Hololink board <code>192.168.0.2</code> <code>--log-level</code> Logging level to display <code>20</code> <code>--ibv-name</code> IBV device to use First available InfiniBand device <code>--ibv-port</code> Port number of IBV device <code>1</code> <code>--expander-configuration</code> I2C Expander configuration (0 or 1) <code>0</code> <code>--pattern</code> Configure to display a test pattern (0-11) None <code>--ptp-sync</code> After reset, wait for PTP time to synchronize False <code>--skip-reset</code> Don't call reset on the hololink device False","tags":["Healthcare AI","SSD","Detection","MONAI","Segmentation"]},{"location":"workflows/ai_surgical_video/#benchmarking","title":"Benchmarking","text":"<p>Please refer to Holoscan Benchmarking for how to perform benchmarking for this workflow.</p>","tags":["Healthcare AI","SSD","Detection","MONAI","Segmentation"]}]}